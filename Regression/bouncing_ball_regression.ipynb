{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import Tracer\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Bouncing_ball:\n",
    "    \n",
    "    acceleration=9.8\n",
    "    timestep=0.6\n",
    "    \n",
    "    def __init__(self,v0,theta,height):\n",
    "        a=Bouncing_ball.acceleration\n",
    "        self.angle=theta \n",
    "        self.height=height\n",
    "        self.y=height\n",
    "        self.x=0\n",
    "        \n",
    "        self.height_crossed=height\n",
    "        \n",
    "        self.t_reach_top=(v0*np.sin(theta * np.pi/180))/a\n",
    "        self.height_max=height+1/2*a*(self.t_reach_top)**2\n",
    "\n",
    "        self.t_max=math.sqrt(2*self.height_max/9.8)\n",
    "        self.v_max=self.t_max*9.8\n",
    "\n",
    "        \n",
    "        self.current_velocity_x=v0*np.cos(theta * np.pi/180)\n",
    "        self.previous_velocity_x=v0*np.cos(theta * np.pi/180)\n",
    "        self.current_velocity_y=-v0*np.sin(theta * np.pi/180)\n",
    "        self.previous_velocity_y=-v0*np.sin(theta * np.pi/180)\n",
    "        self.time=0\n",
    "    \n",
    "    def advance_step(self,timestep):\n",
    "        new_velocity_y=0\n",
    "        a=Bouncing_ball.acceleration\n",
    "        h=timestep\n",
    "        v_max=self.v_max\n",
    "        \n",
    "        if  self.current_velocity_y < 0  :\n",
    "            new_velocity_y=self.current_velocity_y+a*h\n",
    "            self.height_crossed-=self.current_velocity_y*h+0.5*a*(h)**2\n",
    "        elif self.current_velocity_y > 0:\n",
    "                new_velocity_y=self.current_velocity_y+a*h\n",
    "                self.height_crossed-=abs(self.current_velocity_y)*h-0.5*a*(h)**2\n",
    "        if self.height_crossed<0:\n",
    "            self.height_crossed=0\n",
    "            new_velocity_y=-v_max\n",
    "            self.current_velocity_y=0\n",
    "      \n",
    "        self.previous_velocity_y=self.current_velocity_y\n",
    "        self.current_velocity_y=new_velocity_y\n",
    "        self.time+=h\n",
    "        self.y=self.height_crossed\n",
    "        self.x+=self.current_velocity_x*h+0.5*(a)*h**2\n",
    "        \n",
    "        \n",
    "        return self.current_velocity_y,self.current_velocity_x,self.previous_velocity_x,self.previous_velocity_y,self.y,self.x,self.time\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449.446445864\n"
     ]
    }
   ],
   "source": [
    "b=Bouncing_ball(v0=112,theta=55,height=20)\n",
    "b.height_max,b.height_crossed,b.current_velocity_y,b.v_max\n",
    "b2=Bouncing_ball(v0=12,theta=60,height=20)\n",
    "#b3=Bouncing_ball(v0=112,theta=50,height=22)\n",
    "#b2=Bouncing_ball(v0=100,theta=6,height=10)\n",
    "#b3=Bouncing_ball(v0=80,theta=33,height=52)\n",
    "print(b.height_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-91.25502896,  64.24056087],\n",
       "       [-90.76502896,  64.24056087],\n",
       "       [-90.27502896,  64.24056087],\n",
       "       [-89.78502896,  64.24056087],\n",
       "       [-89.29502896,  64.24056087],\n",
       "       [-88.80502896,  64.24056087],\n",
       "       [-88.31502896,  64.24056087],\n",
       "       [-87.82502896,  64.24056087],\n",
       "       [-87.33502896,  64.24056087],\n",
       "       [-86.84502896,  64.24056087],\n",
       "       [-86.35502896,  64.24056087],\n",
       "       [-85.86502896,  64.24056087],\n",
       "       [-85.37502896,  64.24056087],\n",
       "       [-84.88502896,  64.24056087],\n",
       "       [-84.39502896,  64.24056087],\n",
       "       [-83.90502896,  64.24056087],\n",
       "       [-83.41502896,  64.24056087],\n",
       "       [-82.92502896,  64.24056087],\n",
       "       [-82.43502896,  64.24056087],\n",
       "       [-81.94502896,  64.24056087],\n",
       "       [-81.45502896,  64.24056087],\n",
       "       [-80.96502896,  64.24056087],\n",
       "       [-80.47502896,  64.24056087],\n",
       "       [-79.98502896,  64.24056087],\n",
       "       [-79.49502896,  64.24056087],\n",
       "       [-79.00502896,  64.24056087],\n",
       "       [-78.51502896,  64.24056087],\n",
       "       [-78.02502896,  64.24056087],\n",
       "       [-77.53502896,  64.24056087],\n",
       "       [-77.04502896,  64.24056087],\n",
       "       [-76.55502896,  64.24056087],\n",
       "       [-76.06502896,  64.24056087],\n",
       "       [-75.57502896,  64.24056087],\n",
       "       [-75.08502896,  64.24056087],\n",
       "       [-74.59502896,  64.24056087],\n",
       "       [-74.10502896,  64.24056087],\n",
       "       [-73.61502896,  64.24056087],\n",
       "       [-73.12502896,  64.24056087],\n",
       "       [-72.63502896,  64.24056087],\n",
       "       [-72.14502896,  64.24056087],\n",
       "       [-71.65502896,  64.24056087],\n",
       "       [-71.16502896,  64.24056087],\n",
       "       [-70.67502896,  64.24056087],\n",
       "       [-70.18502896,  64.24056087],\n",
       "       [-69.69502896,  64.24056087],\n",
       "       [-69.20502896,  64.24056087],\n",
       "       [-68.71502896,  64.24056087],\n",
       "       [-68.22502896,  64.24056087],\n",
       "       [-67.73502896,  64.24056087],\n",
       "       [-67.24502896,  64.24056087],\n",
       "       [-66.75502896,  64.24056087],\n",
       "       [-66.26502896,  64.24056087],\n",
       "       [-65.77502896,  64.24056087],\n",
       "       [-65.28502896,  64.24056087],\n",
       "       [-64.79502896,  64.24056087],\n",
       "       [-64.30502896,  64.24056087],\n",
       "       [-63.81502896,  64.24056087],\n",
       "       [-63.32502896,  64.24056087],\n",
       "       [-62.83502896,  64.24056087],\n",
       "       [-62.34502896,  64.24056087],\n",
       "       [-61.85502896,  64.24056087],\n",
       "       [-61.36502896,  64.24056087],\n",
       "       [-60.87502896,  64.24056087],\n",
       "       [-60.38502896,  64.24056087],\n",
       "       [-59.89502896,  64.24056087],\n",
       "       [-59.40502896,  64.24056087],\n",
       "       [-58.91502896,  64.24056087],\n",
       "       [-58.42502896,  64.24056087],\n",
       "       [-57.93502896,  64.24056087],\n",
       "       [-57.44502896,  64.24056087],\n",
       "       [-56.95502896,  64.24056087],\n",
       "       [-56.46502896,  64.24056087],\n",
       "       [-55.97502896,  64.24056087],\n",
       "       [-55.48502896,  64.24056087],\n",
       "       [-54.99502896,  64.24056087],\n",
       "       [-54.50502896,  64.24056087],\n",
       "       [-54.01502896,  64.24056087],\n",
       "       [-53.52502896,  64.24056087],\n",
       "       [-53.03502896,  64.24056087],\n",
       "       [-52.54502896,  64.24056087],\n",
       "       [-52.05502896,  64.24056087],\n",
       "       [-51.56502896,  64.24056087],\n",
       "       [-51.07502896,  64.24056087],\n",
       "       [-50.58502896,  64.24056087],\n",
       "       [-50.09502896,  64.24056087],\n",
       "       [-49.60502896,  64.24056087],\n",
       "       [-49.11502896,  64.24056087],\n",
       "       [-48.62502896,  64.24056087],\n",
       "       [-48.13502896,  64.24056087],\n",
       "       [-47.64502896,  64.24056087],\n",
       "       [-47.15502896,  64.24056087],\n",
       "       [-46.66502896,  64.24056087],\n",
       "       [-46.17502896,  64.24056087],\n",
       "       [-45.68502896,  64.24056087],\n",
       "       [-45.19502896,  64.24056087],\n",
       "       [-44.70502896,  64.24056087],\n",
       "       [-44.21502896,  64.24056087],\n",
       "       [-43.72502896,  64.24056087],\n",
       "       [-43.23502896,  64.24056087],\n",
       "       [-42.74502896,  64.24056087]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_of_steps=100\n",
    "\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "\n",
    "\n",
    "X_train_data=np.empty([1*nb_of_steps,3])\n",
    "#Y_train_data=np.empty([3*nb_of_steps,3])\n",
    "Y_train_data=np.empty([1*nb_of_steps,2])\n",
    "\n",
    "height=[]\n",
    "\n",
    "previous_velocity_y_list=[]\n",
    "previous_velocity_x_list=[]\n",
    "\n",
    "current_velocity_y_list=[]\n",
    "current_velocity_x_list=[]\n",
    "\n",
    "projection_angles=[]\n",
    "\n",
    "velocity_y=[]\n",
    "velocity_x=[]\n",
    "for i in range(nb_of_steps):\n",
    "    current_velocity_y,current_velocity_x,previous_velocity_x, previous_velocity_y,y,x,time=b.advance_step(0.05)\n",
    "    \n",
    "    X.append(x) \n",
    "    Y.append(y)\n",
    "    height.append(y)\n",
    "    \n",
    "    previous_velocity_y_list.append(previous_velocity_y)\n",
    "    angle=180/math.pi* math.atan(previous_velocity_y/previous_velocity_x)\n",
    "\n",
    "    \n",
    "    #print(y)\n",
    "    velocity_y.append(current_velocity_y)\n",
    "    X_train_data[i]=[previous_velocity_y,previous_velocity_x,y]\n",
    "    #X_train_data[i]=[previous_velocity_y,y]\n",
    "    #print(current_velocity_y)\n",
    "    Y_train_data[i]=current_velocity_y,current_velocity_x\n",
    "    #Y_train_data[i]=current_velocity_y\n",
    "#    \n",
    "#X2=[]\n",
    "#Y2=[]\n",
    "#for i in range(nb_of_steps):\n",
    "#    current_velocity_y2,current_velocity_x2,previous_velocity_x2,previous_velocity_y2,y2,x2,time=b2.advance_step(0.005)\n",
    "#    X2.append(x2)\n",
    "#    Y2.append(y2)\n",
    "#    previous_velocity_y_list.append(previous_velocity_y2)\n",
    "#    angle=180/math.pi* math.atan(previous_velocity_y2/previous_velocity_x2)\n",
    "#\n",
    "#    velocity_y.append(current_velocity_y2)\n",
    "#    X_train_data[i+nb_of_steps]=[previous_velocity_y2,previous_velocity_x2,y2]\n",
    "#    #X_train_data[i+nb_of_steps]=[previous_velocity_y2,y2]\n",
    "#\n",
    "#    Y_train_data[i+nb_of_steps]=current_velocity_y2,current_velocity_x2\n",
    "#    #Y_train_data[i+nb_of_steps]=current_velocity_y2\n",
    "#\n",
    "    \n",
    "#X3=[]\n",
    "#Y3=[]\n",
    "#for i in range(nb_of_steps):\n",
    "#    current_velocity_y3,current_velocity_x3,previous_velocity_x3,previous_velocity_y3,y3,x3,time=b3.advance_step(0.07)\n",
    "#    X3.append(x3)\n",
    "#    Y3.append(y3)\n",
    "#    previous_velocity_y_list.append(previous_velocity_y3)\n",
    "#    angle=180/math.pi* math.atan(previous_velocity_y3/previous_velocity_x3)\n",
    "#\n",
    "#    velocity_y.append(current_velocity_y3)\n",
    "#    #X_train_data[i+2*nb_of_steps]=[previous_velocity_y2,previous_velocity_x2,y2]\n",
    "#    X_train_data[i+2*nb_of_steps]=[previous_velocity_y2,y2]\n",
    "#\n",
    "#    #Y_train_data[i+2*nb_of_steps]=current_velocity_y3,current_velocity_x3\n",
    "#    Y_train_data[i+2*nb_of_steps]=current_velocity_y3\n",
    "Y_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e86f225d30>]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOuMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e86f326c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e86f442e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,Y)\n",
    "#plt.scatter(X2,Y2)\n",
    "#plt.scatter(X3,Y3)\n",
    "\n",
    "plt.xlabel('x position')\n",
    "plt.ylabel('y position')\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "ax.plot(velocity_y)\n",
    "#for i in [X,Y,X2,Y2,X3,Y3]: del i\n",
    "#del b,b2,b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proceed to make a regression model based on NN.\n",
    "# inputs of NN: (v_n),(height(position_y)),(theta)-> output v_n+1\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Convert to pytorch tensors\n",
    "\n",
    "X_tensor=torch.from_numpy(X_train_data).type(torch.FloatTensor)\n",
    "Y_tensor=torch.from_numpy(Y_train_data).type(torch.FloatTensor)\n",
    "\n",
    "# Convert to pytorch variables\n",
    "X_variable=Variable(X_tensor,requires_grad=False)\n",
    "Y_variable=Variable(Y_tensor,requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "#mu,std=X_variable.mean(0),X_variable.std(0)\n",
    "#X_variable.sub_(mu).div_(std)\n",
    "#X_variable.size()\n",
    "Y_variable.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myNet(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(myNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))      # activation function for hidden layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(n_feature, n_hidden,bias=True)   # hidden layer\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc2_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "       \n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc3_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "#\n",
    "        self.fc4 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc4_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "#\n",
    "        self.fc5 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc5_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "#\n",
    "        self.fc6 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc6_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "        \n",
    "        self.fc7 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc7_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "#\n",
    "        self.fc8 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc8_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        self.fc9 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc9_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.fc10 = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        #dropout=nn.Dropout(p=0.5)\n",
    "\n",
    "        x = F.relu(self.fc1(x))# activation function for hidden layer\n",
    "        #x=dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x=self.fc2_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x=self.fc3_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x=self.fc4_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x=self.fc5_bn(x)   \n",
    "        \n",
    "        x = F.relu(self.fc6(x))\n",
    "        x=self.fc6_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x=self.fc7_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x=self.fc8_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = F.relu(self.fc9(x))\n",
    "        x=self.fc9_bn(x)      \n",
    "        \n",
    "        #x=dropout(x)\n",
    "        x = self.fc10(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myNet(\n",
      "  (fc1): Linear(in_features=3, out_features=10)\n",
      "  (fc2): Linear(in_features=10, out_features=10)\n",
      "  (fc3): Linear(in_features=10, out_features=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters #\n",
    "net =myNet(n_feature=3, n_hidden=10, n_output=2)     # define the network\n",
    "print(net)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=4e-5)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=1e-2,momentum=0.8)\n",
    "loss_func=torch.nn.MSELoss(size_average=True)  \n",
    "#loss_func = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss: 70399.25341796875 Training error :14056.169921875\n",
      "Epoch : 1 Loss: 70147.00390625 Training error :14006.701171875\n",
      "Epoch : 2 Loss: 69900.2353515625 Training error :13957.5771484375\n",
      "Epoch : 3 Loss: 69655.72802734375 Training error :13908.861328125\n",
      "Epoch : 4 Loss: 69413.3037109375 Training error :13860.55078125\n",
      "Epoch : 5 Loss: 69172.7236328125 Training error :13812.5595703125\n",
      "Epoch : 6 Loss: 68933.66357421875 Training error :13764.8701171875\n",
      "Epoch : 7 Loss: 68696.1103515625 Training error :13717.48828125\n",
      "Epoch : 8 Loss: 68459.92822265625 Training error :13670.3447265625\n",
      "Epoch : 9 Loss: 68224.86083984375 Training error :13623.4091796875\n",
      "Epoch : 10 Loss: 67990.80712890625 Training error :13576.6796875\n",
      "Epoch : 11 Loss: 67757.80712890625 Training error :13530.1591796875\n",
      "Epoch : 12 Loss: 67525.84130859375 Training error :13483.84375\n",
      "Epoch : 13 Loss: 67294.73828125 Training error :13437.6328125\n",
      "Epoch : 14 Loss: 67064.22705078125 Training error :13391.583984375\n",
      "Epoch : 15 Loss: 66834.54541015625 Training error :13345.7236328125\n",
      "Epoch : 16 Loss: 66605.89697265625 Training error :13300.0634765625\n",
      "Epoch : 17 Loss: 66378.09912109375 Training error :13254.5537109375\n",
      "Epoch : 18 Loss: 66151.1328125 Training error :13209.1748046875\n",
      "Epoch : 19 Loss: 65924.78125 Training error :13163.96875\n",
      "Epoch : 20 Loss: 65699.2236328125 Training error :13118.896484375\n",
      "Epoch : 21 Loss: 65474.35205078125 Training error :13073.9697265625\n",
      "Epoch : 22 Loss: 65250.2802734375 Training error :13029.1865234375\n",
      "Epoch : 23 Loss: 65026.8349609375 Training error :12984.5048828125\n",
      "Epoch : 24 Loss: 64803.9521484375 Training error :12939.982421875\n",
      "Epoch : 25 Loss: 64581.8818359375 Training error :12895.634765625\n",
      "Epoch : 26 Loss: 64360.6806640625 Training error :12851.4345703125\n",
      "Epoch : 27 Loss: 64140.11328125 Training error :12807.3212890625\n",
      "Epoch : 28 Loss: 63920.02685546875 Training error :12763.33984375\n",
      "Epoch : 29 Loss: 63700.759765625 Training error :12719.53515625\n",
      "Epoch : 30 Loss: 63482.28564453125 Training error :12675.853515625\n",
      "Epoch : 31 Loss: 63264.29150390625 Training error :12632.30078125\n",
      "Epoch : 32 Loss: 63046.97802734375 Training error :12588.884765625\n",
      "Epoch : 33 Loss: 62830.3798828125 Training error :12545.5634765625\n",
      "Epoch : 34 Loss: 62614.23974609375 Training error :12502.34765625\n",
      "Epoch : 35 Loss: 62398.63037109375 Training error :12459.27734375\n",
      "Epoch : 36 Loss: 62183.73486328125 Training error :12416.291015625\n",
      "Epoch : 37 Loss: 61969.26416015625 Training error :12373.4072265625\n",
      "Epoch : 38 Loss: 61755.298828125 Training error :12330.6552734375\n",
      "Epoch : 39 Loss: 61542.0859375 Training error :12288.037109375\n",
      "Epoch : 40 Loss: 61329.35107421875 Training error :12245.5087890625\n",
      "Epoch : 41 Loss: 61117.2255859375 Training error :12203.111328125\n",
      "Epoch : 42 Loss: 60905.7001953125 Training error :12160.80078125\n",
      "Epoch : 43 Loss: 60694.5712890625 Training error :12118.638671875\n",
      "Epoch : 44 Loss: 60484.29296875 Training error :12076.6650390625\n",
      "Epoch : 45 Loss: 60274.97607421875 Training error :12034.8876953125\n",
      "Epoch : 46 Loss: 60066.65087890625 Training error :11993.30859375\n",
      "Epoch : 47 Loss: 59859.314453125 Training error :11951.92578125\n",
      "Epoch : 48 Loss: 59652.93310546875 Training error :11910.75390625\n",
      "Epoch : 49 Loss: 59447.626953125 Training error :11869.78125\n",
      "Epoch : 50 Loss: 59243.29248046875 Training error :11828.99609375\n",
      "Epoch : 51 Loss: 59039.9033203125 Training error :11788.400390625\n",
      "Epoch : 52 Loss: 58837.44775390625 Training error :11747.9873046875\n",
      "Epoch : 53 Loss: 58635.90185546875 Training error :11707.7548828125\n",
      "Epoch : 54 Loss: 58435.25341796875 Training error :11667.7021484375\n",
      "Epoch : 55 Loss: 58235.50341796875 Training error :11627.8271484375\n",
      "Epoch : 56 Loss: 58036.6298828125 Training error :11588.12890625\n",
      "Epoch : 57 Loss: 57838.63671875 Training error :11548.603515625\n",
      "Epoch : 58 Loss: 57641.49853515625 Training error :11509.2470703125\n",
      "Epoch : 59 Loss: 57445.18798828125 Training error :11470.0712890625\n",
      "Epoch : 60 Loss: 57249.7978515625 Training error :11431.0673828125\n",
      "Epoch : 61 Loss: 57055.2529296875 Training error :11392.2314453125\n",
      "Epoch : 62 Loss: 56861.544921875 Training error :11353.55859375\n",
      "Epoch : 63 Loss: 56668.6494140625 Training error :11315.0458984375\n",
      "Epoch : 64 Loss: 56476.5546875 Training error :11276.693359375\n",
      "Epoch : 65 Loss: 56285.24853515625 Training error :11238.4990234375\n",
      "Epoch : 66 Loss: 56094.72998046875 Training error :11200.4599609375\n",
      "Epoch : 67 Loss: 55904.9873046875 Training error :11162.5771484375\n",
      "Epoch : 68 Loss: 55716.02294921875 Training error :11124.8466796875\n",
      "Epoch : 69 Loss: 55527.8134765625 Training error :11087.267578125\n",
      "Epoch : 70 Loss: 55340.35595703125 Training error :11049.837890625\n",
      "Epoch : 71 Loss: 55153.64404296875 Training error :11012.5576171875\n",
      "Epoch : 72 Loss: 54967.6669921875 Training error :10975.4365234375\n",
      "Epoch : 73 Loss: 54782.484375 Training error :10938.4609375\n",
      "Epoch : 74 Loss: 54598.03564453125 Training error :10901.6337890625\n",
      "Epoch : 75 Loss: 54414.31640625 Training error :10864.94921875\n",
      "Epoch : 76 Loss: 54231.31201171875 Training error :10828.4033203125\n",
      "Epoch : 77 Loss: 54049.001953125 Training error :10792.0\n",
      "Epoch : 78 Loss: 53867.38916015625 Training error :10755.7353515625\n",
      "Epoch : 79 Loss: 53686.4775390625 Training error :10719.607421875\n",
      "Epoch : 80 Loss: 53506.2392578125 Training error :10683.61328125\n",
      "Epoch : 81 Loss: 53326.6806640625 Training error :10647.7578125\n",
      "Epoch : 82 Loss: 53147.79150390625 Training error :10612.0341796875\n",
      "Epoch : 83 Loss: 52969.5703125 Training error :10576.4423828125\n",
      "Epoch : 84 Loss: 52791.9990234375 Training error :10540.9814453125\n",
      "Epoch : 85 Loss: 52615.08544921875 Training error :10505.650390625\n",
      "Epoch : 86 Loss: 52438.8134765625 Training error :10470.44921875\n",
      "Epoch : 87 Loss: 52263.18701171875 Training error :10435.3857421875\n",
      "Epoch : 88 Loss: 52088.2392578125 Training error :10400.4501953125\n",
      "Epoch : 89 Loss: 51913.93798828125 Training error :10365.6416015625\n",
      "Epoch : 90 Loss: 51740.2666015625 Training error :10330.9560546875\n",
      "Epoch : 91 Loss: 51567.21484375 Training error :10296.3935546875\n",
      "Epoch : 92 Loss: 51394.76904296875 Training error :10261.953125\n",
      "Epoch : 93 Loss: 51222.9365234375 Training error :10227.634765625\n",
      "Epoch : 94 Loss: 51051.69921875 Training error :10193.4345703125\n",
      "Epoch : 95 Loss: 50881.05908203125 Training error :10159.3515625\n",
      "Epoch : 96 Loss: 50710.99853515625 Training error :10125.3837890625\n",
      "Epoch : 97 Loss: 50541.5107421875 Training error :10091.5322265625\n",
      "Epoch : 98 Loss: 50372.60205078125 Training error :10057.7939453125\n",
      "Epoch : 99 Loss: 50204.263671875 Training error :10024.1708984375\n",
      "Epoch : 100 Loss: 50036.490234375 Training error :9990.6591796875\n",
      "Epoch : 101 Loss: 49869.279296875 Training error :9957.287109375\n",
      "Epoch : 102 Loss: 49702.841796875 Training error :9924.068359375\n",
      "Epoch : 103 Loss: 49537.10986328125 Training error :9890.966796875\n",
      "Epoch : 104 Loss: 49371.94873046875 Training error :9858.009765625\n",
      "Epoch : 105 Loss: 49207.57373046875 Training error :9825.20703125\n",
      "Epoch : 106 Loss: 49043.9169921875 Training error :9792.5205078125\n",
      "Epoch : 107 Loss: 48880.818359375 Training error :9759.9677734375\n",
      "Epoch : 108 Loss: 48718.46923828125 Training error :9727.5634765625\n",
      "Epoch : 109 Loss: 48556.79736328125 Training error :9695.2685546875\n",
      "Epoch : 110 Loss: 48395.6484375 Training error :9663.1220703125\n",
      "Epoch : 111 Loss: 48235.33447265625 Training error :9631.1220703125\n",
      "Epoch : 112 Loss: 48075.697265625 Training error :9599.2578125\n",
      "Epoch : 113 Loss: 47916.80224609375 Training error :9567.5712890625\n",
      "Epoch : 114 Loss: 47758.712890625 Training error :9536.0234375\n",
      "Epoch : 115 Loss: 47601.3154296875 Training error :9504.6337890625\n",
      "Epoch : 116 Loss: 47444.77783203125 Training error :9473.38671875\n",
      "Epoch : 117 Loss: 47288.8779296875 Training error :9442.2861328125\n",
      "Epoch : 118 Loss: 47133.79296875 Training error :9411.337890625\n",
      "Epoch : 119 Loss: 46979.390625 Training error :9380.5302734375\n",
      "Epoch : 120 Loss: 46825.7646484375 Training error :9349.8779296875\n",
      "Epoch : 121 Loss: 46672.838134765625 Training error :9319.365234375\n",
      "Epoch : 122 Loss: 46520.687255859375 Training error :9289.0107421875\n",
      "Epoch : 123 Loss: 46369.259033203125 Training error :9258.8115234375\n",
      "Epoch : 124 Loss: 46218.66455078125 Training error :9228.76171875\n",
      "Epoch : 125 Loss: 46068.74365234375 Training error :9198.8203125\n",
      "Epoch : 126 Loss: 45919.3310546875 Training error :9168.974609375\n",
      "Epoch : 127 Loss: 45770.387451171875 Training error :9139.2216796875\n",
      "Epoch : 128 Loss: 45621.906005859375 Training error :9109.5849609375\n",
      "Epoch : 129 Loss: 45474.1240234375 Training error :9080.095703125\n",
      "Epoch : 130 Loss: 45327.00439453125 Training error :9050.7099609375\n",
      "Epoch : 131 Loss: 45180.360107421875 Training error :9021.416015625\n",
      "Epoch : 132 Loss: 45034.16455078125 Training error :8992.2080078125\n",
      "Epoch : 133 Loss: 44888.41015625 Training error :8963.0927734375\n",
      "Epoch : 134 Loss: 44743.092529296875 Training error :8934.1142578125\n",
      "Epoch : 135 Loss: 44598.583740234375 Training error :8905.265625\n",
      "Epoch : 136 Loss: 44454.651611328125 Training error :8876.5146484375\n",
      "Epoch : 137 Loss: 44311.178466796875 Training error :8847.853515625\n",
      "Epoch : 138 Loss: 44168.13037109375 Training error :8819.2734375\n",
      "Epoch : 139 Loss: 44025.496826171875 Training error :8790.802734375\n",
      "Epoch : 140 Loss: 43883.529052734375 Training error :8762.474609375\n",
      "Epoch : 141 Loss: 43742.19873046875 Training error :8734.2421875\n",
      "Epoch : 142 Loss: 43601.29833984375 Training error :8706.0908203125\n",
      "Epoch : 143 Loss: 43460.814697265625 Training error :8678.0234375\n",
      "Epoch : 144 Loss: 43320.72998046875 Training error :8650.0576171875\n",
      "Epoch : 145 Loss: 43181.2802734375 Training error :8622.234375\n",
      "Epoch : 146 Loss: 43042.463134765625 Training error :8594.50390625\n",
      "Epoch : 147 Loss: 42904.0634765625 Training error :8566.865234375\n",
      "Epoch : 148 Loss: 42766.123291015625 Training error :8539.30859375\n",
      "Epoch : 149 Loss: 42628.576416015625 Training error :8511.8603515625\n",
      "Epoch : 150 Loss: 42491.71533203125 Training error :8484.544921875\n",
      "Epoch : 151 Loss: 42355.432861328125 Training error :8457.3193359375\n",
      "Epoch : 152 Loss: 42219.562744140625 Training error :8430.1728515625\n",
      "Epoch : 153 Loss: 42084.078369140625 Training error :8403.103515625\n",
      "Epoch : 154 Loss: 41949.10498046875 Training error :8376.197265625\n",
      "Epoch : 155 Loss: 41814.859130859375 Training error :8349.3779296875\n",
      "Epoch : 156 Loss: 41681.015869140625 Training error :8322.6357421875\n",
      "Epoch : 157 Loss: 41547.54443359375 Training error :8295.966796875\n",
      "Epoch : 158 Loss: 41414.44189453125 Training error :8269.41015625\n",
      "Epoch : 159 Loss: 41282.0283203125 Training error :8242.9775390625\n",
      "Epoch : 160 Loss: 41150.14404296875 Training error :8216.62890625\n",
      "Epoch : 161 Loss: 41018.65283203125 Training error :8190.35791015625\n",
      "Epoch : 162 Loss: 40887.525146484375 Training error :8164.19873046875\n",
      "Epoch : 163 Loss: 40757.09326171875 Training error :8138.17236328125\n",
      "Epoch : 164 Loss: 40627.2373046875 Training error :8112.22998046875\n",
      "Epoch : 165 Loss: 40497.767822265625 Training error :8086.361328125\n",
      "Epoch : 166 Loss: 40368.657470703125 Training error :8060.59814453125\n",
      "Epoch : 167 Loss: 40240.2099609375 Training error :8034.96240234375\n",
      "Epoch : 168 Loss: 40112.305419921875 Training error :8009.4091796875\n",
      "Epoch : 169 Loss: 39984.776123046875 Training error :7983.927734375\n",
      "Epoch : 170 Loss: 39857.60009765625 Training error :7958.56396484375\n",
      "Epoch : 171 Loss: 39731.14599609375 Training error :7933.31298828125\n",
      "Epoch : 172 Loss: 39605.159423828125 Training error :7908.142578125\n",
      "Epoch : 173 Loss: 39479.544921875 Training error :7883.05615234375\n",
      "Epoch : 174 Loss: 39354.493408203125 Training error :7858.1181640625\n",
      "Epoch : 175 Loss: 39230.072509765625 Training error :7833.2607421875\n",
      "Epoch : 176 Loss: 39106.02001953125 Training error :7808.47705078125\n",
      "Epoch : 177 Loss: 38982.317626953125 Training error :7783.81201171875\n",
      "Epoch : 178 Loss: 38859.355712890625 Training error :7759.2607421875\n",
      "Epoch : 179 Loss: 38736.87109375 Training error :7734.78125\n",
      "Epoch : 180 Loss: 38614.708740234375 Training error :7710.3974609375\n",
      "Epoch : 181 Loss: 38493.159912109375 Training error :7686.14501953125\n",
      "Epoch : 182 Loss: 38372.167236328125 Training error :7661.9716796875\n",
      "Epoch : 183 Loss: 38251.527587890625 Training error :7637.88134765625\n",
      "Epoch : 184 Loss: 38131.450439453125 Training error :7613.9345703125\n",
      "Epoch : 185 Loss: 38011.9814453125 Training error :7590.0654296875\n",
      "Epoch : 186 Loss: 37892.859130859375 Training error :7566.27490234375\n",
      "Epoch : 187 Loss: 37774.278564453125 Training error :7542.63134765625\n",
      "Epoch : 188 Loss: 37656.322998046875 Training error :7519.0654296875\n",
      "Epoch : 189 Loss: 37538.717529296875 Training error :7495.580078125\n",
      "Epoch : 190 Loss: 37421.66064453125 Training error :7472.23828125\n",
      "Epoch : 191 Loss: 37305.20703125 Training error :7448.98388671875\n",
      "Epoch : 192 Loss: 37189.14990234375 Training error :7425.82080078125\n",
      "Epoch : 193 Loss: 37073.706787109375 Training error :7402.791015625\n",
      "Epoch : 194 Loss: 36958.819091796875 Training error :7379.83935546875\n",
      "Epoch : 195 Loss: 36844.275146484375 Training error :7356.99365234375\n",
      "Epoch : 196 Loss: 36730.416748046875 Training error :7334.26416015625\n",
      "Epoch : 197 Loss: 36617.027587890625 Training error :7311.61328125\n",
      "Epoch : 198 Loss: 36504.154296875 Training error :7289.12060546875\n",
      "Epoch : 199 Loss: 36391.956298828125 Training error :7266.70556640625\n",
      "Epoch : 200 Loss: 36280.09765625 Training error :7244.38623046875\n",
      "Epoch : 201 Loss: 36168.8740234375 Training error :7222.193359375\n",
      "Epoch : 202 Loss: 36058.1630859375 Training error :7200.07763671875\n",
      "Epoch : 203 Loss: 35947.976806640625 Training error :7178.119140625\n",
      "Epoch : 204 Loss: 35838.451171875 Training error :7156.23828125\n",
      "Epoch : 205 Loss: 35729.258544921875 Training error :7134.46728515625\n",
      "Epoch : 206 Loss: 35620.77783203125 Training error :7112.81494140625\n",
      "Epoch : 207 Loss: 35512.767822265625 Training error :7091.26318359375\n",
      "Epoch : 208 Loss: 35405.39990234375 Training error :7069.8486328125\n",
      "Epoch : 209 Loss: 35298.58935546875 Training error :7048.5185546875\n",
      "Epoch : 210 Loss: 35192.33740234375 Training error :7027.34130859375\n",
      "Epoch : 211 Loss: 35086.7119140625 Training error :7006.24072265625\n",
      "Epoch : 212 Loss: 34981.58935546875 Training error :6985.29638671875\n",
      "Epoch : 213 Loss: 34877.141845703125 Training error :6964.4306640625\n",
      "Epoch : 214 Loss: 34773.1650390625 Training error :6943.7158203125\n",
      "Epoch : 215 Loss: 34669.89306640625 Training error :6923.0849609375\n",
      "Epoch : 216 Loss: 34567.076171875 Training error :6902.60205078125\n",
      "Epoch : 217 Loss: 34464.978515625 Training error :6882.20556640625\n",
      "Epoch : 218 Loss: 34363.340576171875 Training error :6861.9580078125\n",
      "Epoch : 219 Loss: 34262.40869140625 Training error :6841.80615234375\n",
      "Epoch : 220 Loss: 34162.01171875 Training error :6821.80859375\n",
      "Epoch : 221 Loss: 34062.3076171875 Training error :6801.89306640625\n",
      "Epoch : 222 Loss: 33963.131103515625 Training error :6782.13671875\n",
      "Epoch : 223 Loss: 33864.602783203125 Training error :6762.466796875\n",
      "Epoch : 224 Loss: 33766.65234375 Training error :6742.93994140625\n",
      "Epoch : 225 Loss: 33669.2783203125 Training error :6723.515625\n",
      "Epoch : 226 Loss: 33572.55810546875 Training error :6704.22021484375\n",
      "Epoch : 227 Loss: 33476.429931640625 Training error :6685.07373046875\n",
      "Epoch : 228 Loss: 33381.096435546875 Training error :6666.03125\n",
      "Epoch : 229 Loss: 33286.271484375 Training error :6647.14111328125\n",
      "Epoch : 230 Loss: 33192.10107421875 Training error :6628.3486328125\n",
      "Epoch : 231 Loss: 33098.539306640625 Training error :6609.6923828125\n",
      "Epoch : 232 Loss: 33005.604248046875 Training error :6591.18603515625\n",
      "Epoch : 233 Loss: 32913.468994140625 Training error :6572.79296875\n",
      "Epoch : 234 Loss: 32821.90625 Training error :6554.5537109375\n",
      "Epoch : 235 Loss: 32730.962890625 Training error :6536.42041015625\n",
      "Epoch : 236 Loss: 32640.695556640625 Training error :6518.40478515625\n",
      "Epoch : 237 Loss: 32551.0634765625 Training error :6500.5537109375\n",
      "Epoch : 238 Loss: 32462.100341796875 Training error :6482.810546875\n",
      "Epoch : 239 Loss: 32373.78759765625 Training error :6465.1875\n",
      "Epoch : 240 Loss: 32286.12939453125 Training error :6447.724609375\n",
      "Epoch : 241 Loss: 32199.1025390625 Training error :6430.37060546875\n",
      "Epoch : 242 Loss: 32112.7421875 Training error :6413.13671875\n",
      "Epoch : 243 Loss: 32027.043701171875 Training error :6396.0556640625\n",
      "Epoch : 244 Loss: 31942.025634765625 Training error :6379.12060546875\n",
      "Epoch : 245 Loss: 31857.743896484375 Training error :6362.30126953125\n",
      "Epoch : 246 Loss: 31774.048583984375 Training error :6345.61083984375\n",
      "Epoch : 247 Loss: 31691.03466796875 Training error :6329.056640625\n",
      "Epoch : 248 Loss: 31608.640380859375 Training error :6312.642578125\n",
      "Epoch : 249 Loss: 31526.9794921875 Training error :6296.33935546875\n",
      "Epoch : 250 Loss: 31445.86865234375 Training error :6280.162109375\n",
      "Epoch : 251 Loss: 31365.412841796875 Training error :6264.1279296875\n",
      "Epoch : 252 Loss: 31285.63720703125 Training error :6248.236328125\n",
      "Epoch : 253 Loss: 31206.570068359375 Training error :6232.45556640625\n",
      "Epoch : 254 Loss: 31128.06005859375 Training error :6216.7939453125\n",
      "Epoch : 255 Loss: 31050.21630859375 Training error :6201.283203125\n",
      "Epoch : 256 Loss: 30973.077880859375 Training error :6185.9150390625\n",
      "Epoch : 257 Loss: 30896.65087890625 Training error :6170.68896484375\n",
      "Epoch : 258 Loss: 30820.94677734375 Training error :6155.5849609375\n",
      "Epoch : 259 Loss: 30745.817138671875 Training error :6140.6005859375\n",
      "Epoch : 260 Loss: 30671.339599609375 Training error :6125.76171875\n",
      "Epoch : 261 Loss: 30597.5576171875 Training error :6111.06103515625\n",
      "Epoch : 262 Loss: 30524.467041015625 Training error :6096.49951171875\n",
      "Epoch : 263 Loss: 30452.04150390625 Training error :6082.0556640625\n",
      "Epoch : 264 Loss: 30380.19921875 Training error :6067.71484375\n",
      "Epoch : 265 Loss: 30308.924072265625 Training error :6053.50390625\n",
      "Epoch : 266 Loss: 30238.29736328125 Training error :6039.427734375\n",
      "Epoch : 267 Loss: 30168.320556640625 Training error :6025.478515625\n",
      "Epoch : 268 Loss: 30098.979248046875 Training error :6011.65869140625\n",
      "Epoch : 269 Loss: 30030.277099609375 Training error :5997.94921875\n",
      "Epoch : 270 Loss: 29962.0771484375 Training error :5984.3330078125\n",
      "Epoch : 271 Loss: 29894.3955078125 Training error :5970.84765625\n",
      "Epoch : 272 Loss: 29827.350830078125 Training error :5957.482421875\n",
      "Epoch : 273 Loss: 29760.89501953125 Training error :5944.232421875\n",
      "Epoch : 274 Loss: 29695.01123046875 Training error :5931.0908203125\n",
      "Epoch : 275 Loss: 29629.681884765625 Training error :5918.03955078125\n",
      "Epoch : 276 Loss: 29564.75048828125 Training error :5905.0830078125\n",
      "Epoch : 277 Loss: 29500.298095703125 Training error :5892.228515625\n",
      "Epoch : 278 Loss: 29436.35107421875 Training error :5879.47119140625\n",
      "Epoch : 279 Loss: 29372.8876953125 Training error :5866.7919921875\n",
      "Epoch : 280 Loss: 29309.794921875 Training error :5854.19384765625\n",
      "Epoch : 281 Loss: 29247.12890625 Training error :5841.69140625\n",
      "Epoch : 282 Loss: 29184.9150390625 Training error :5829.27880859375\n",
      "Epoch : 283 Loss: 29123.145263671875 Training error :5816.93701171875\n",
      "Epoch : 284 Loss: 29061.719482421875 Training error :5804.6806640625\n",
      "Epoch : 285 Loss: 29000.71044921875 Training error :5792.5087890625\n",
      "Epoch : 286 Loss: 28940.10693359375 Training error :5780.41259765625\n",
      "Epoch : 287 Loss: 28879.934448242188 Training error :5768.38427734375\n",
      "Epoch : 288 Loss: 28820.003784179688 Training error :5756.42041015625\n",
      "Epoch : 289 Loss: 28760.397827148438 Training error :5744.5185546875\n",
      "Epoch : 290 Loss: 28701.12744140625 Training error :5732.66748046875\n",
      "Epoch : 291 Loss: 28642.078125 Training error :5720.880859375\n",
      "Epoch : 292 Loss: 28583.373779296875 Training error :5709.15625\n",
      "Epoch : 293 Loss: 28524.954223632812 Training error :5697.470703125\n",
      "Epoch : 294 Loss: 28466.689819335938 Training error :5685.81494140625\n",
      "Epoch : 295 Loss: 28408.621704101562 Training error :5674.21923828125\n",
      "Epoch : 296 Loss: 28350.78759765625 Training error :5662.6611328125\n",
      "Epoch : 297 Loss: 28293.199340820312 Training error :5651.1591796875\n",
      "Epoch : 298 Loss: 28235.830444335938 Training error :5639.697265625\n",
      "Epoch : 299 Loss: 28178.706420898438 Training error :5628.2763671875\n",
      "Epoch : 300 Loss: 28121.73291015625 Training error :5616.89453125\n",
      "Epoch : 301 Loss: 28064.987182617188 Training error :5605.54443359375\n",
      "Epoch : 302 Loss: 28008.362548828125 Training error :5594.23193359375\n",
      "Epoch : 303 Loss: 27951.9384765625 Training error :5582.95263671875\n",
      "Epoch : 304 Loss: 27895.67724609375 Training error :5571.70703125\n",
      "Epoch : 305 Loss: 27839.540283203125 Training error :5560.47998046875\n",
      "Epoch : 306 Loss: 27783.532104492188 Training error :5549.28125\n",
      "Epoch : 307 Loss: 27727.64990234375 Training error :5538.10888671875\n",
      "Epoch : 308 Loss: 27671.866821289062 Training error :5526.95947265625\n",
      "Epoch : 309 Loss: 27616.24267578125 Training error :5515.82421875\n",
      "Epoch : 310 Loss: 27560.65673828125 Training error :5504.71435546875\n",
      "Epoch : 311 Loss: 27505.171752929688 Training error :5493.60693359375\n",
      "Epoch : 312 Loss: 27449.7275390625 Training error :5482.51611328125\n",
      "Epoch : 313 Loss: 27394.376586914062 Training error :5471.4423828125\n",
      "Epoch : 314 Loss: 27339.067749023438 Training error :5460.36572265625\n",
      "Epoch : 315 Loss: 27283.757934570312 Training error :5449.294921875\n",
      "Epoch : 316 Loss: 27228.460815429688 Training error :5438.2236328125\n",
      "Epoch : 317 Loss: 27173.191162109375 Training error :5427.15478515625\n",
      "Epoch : 318 Loss: 27117.891235351562 Training error :5416.08203125\n",
      "Epoch : 319 Loss: 27062.559448242188 Training error :5404.99755859375\n",
      "Epoch : 320 Loss: 27007.173828125 Training error :5393.90771484375\n",
      "Epoch : 321 Loss: 26951.752075195312 Training error :5382.798828125\n",
      "Epoch : 322 Loss: 26896.236206054688 Training error :5371.6689453125\n",
      "Epoch : 323 Loss: 26840.609375 Training error :5360.513671875\n",
      "Epoch : 324 Loss: 26784.855102539062 Training error :5349.3310546875\n",
      "Epoch : 325 Loss: 26728.936767578125 Training error :5338.10302734375\n",
      "Epoch : 326 Loss: 26672.800170898438 Training error :5326.8349609375\n",
      "Epoch : 327 Loss: 26616.421752929688 Training error :5315.513671875\n",
      "Epoch : 328 Loss: 26559.777221679688 Training error :5304.1298828125\n",
      "Epoch : 329 Loss: 26502.796020507812 Training error :5292.67236328125\n",
      "Epoch : 330 Loss: 26445.438110351562 Training error :5281.1376953125\n",
      "Epoch : 331 Loss: 26387.675537109375 Training error :5269.52001953125\n",
      "Epoch : 332 Loss: 26329.46240234375 Training error :5257.8076171875\n",
      "Epoch : 333 Loss: 26270.750244140625 Training error :5246.0048828125\n",
      "Epoch : 334 Loss: 26211.579956054688 Training error :5234.0888671875\n",
      "Epoch : 335 Loss: 26151.791870117188 Training error :5222.05615234375\n",
      "Epoch : 336 Loss: 26091.411376953125 Training error :5209.90771484375\n",
      "Epoch : 337 Loss: 26030.45751953125 Training error :5197.646484375\n",
      "Epoch : 338 Loss: 25968.968872070312 Training error :5185.29833984375\n",
      "Epoch : 339 Loss: 25907.18359375 Training error :5172.916015625\n",
      "Epoch : 340 Loss: 25845.488403320312 Training error :5160.57373046875\n",
      "Epoch : 341 Loss: 25784.423095703125 Training error :5148.38427734375\n",
      "Epoch : 342 Loss: 25724.032592773438 Training error :5136.3544921875\n",
      "Epoch : 343 Loss: 25663.8173828125 Training error :5124.3154296875\n",
      "Epoch : 344 Loss: 25603.503173828125 Training error :5112.23193359375\n",
      "Epoch : 345 Loss: 25542.944580078125 Training error :5100.08984375\n",
      "Epoch : 346 Loss: 25482.085571289062 Training error :5087.884765625\n",
      "Epoch : 347 Loss: 25420.901000976562 Training error :5075.619140625\n",
      "Epoch : 348 Loss: 25359.417846679688 Training error :5063.29296875\n",
      "Epoch : 349 Loss: 25297.622802734375 Training error :5050.9013671875\n",
      "Epoch : 350 Loss: 25235.50244140625 Training error :5038.44482421875\n",
      "Epoch : 351 Loss: 25173.056518554688 Training error :5025.923828125\n",
      "Epoch : 352 Loss: 25110.286499023438 Training error :5013.3388671875\n",
      "Epoch : 353 Loss: 25047.201171875 Training error :5000.69189453125\n",
      "Epoch : 354 Loss: 24983.805908203125 Training error :4987.98486328125\n",
      "Epoch : 355 Loss: 24920.113647460938 Training error :4975.21923828125\n",
      "Epoch : 356 Loss: 24856.12841796875 Training error :4962.39599609375\n",
      "Epoch : 357 Loss: 24791.863037109375 Training error :4949.517578125\n",
      "Epoch : 358 Loss: 24727.323486328125 Training error :4936.58642578125\n",
      "Epoch : 359 Loss: 24662.522338867188 Training error :4923.6044921875\n",
      "Epoch : 360 Loss: 24597.468872070312 Training error :4910.5732421875\n",
      "Epoch : 361 Loss: 24532.175537109375 Training error :4897.49609375\n",
      "Epoch : 362 Loss: 24466.65380859375 Training error :4884.37548828125\n",
      "Epoch : 363 Loss: 24400.918823242188 Training error :4871.21923828125\n",
      "Epoch : 364 Loss: 24335.01416015625 Training error :4858.02490234375\n",
      "Epoch : 365 Loss: 24268.917602539062 Training error :4844.7919921875\n",
      "Epoch : 366 Loss: 24202.634887695312 Training error :4831.52294921875\n",
      "Epoch : 367 Loss: 24136.17431640625 Training error :4818.22021484375\n",
      "Epoch : 368 Loss: 24069.547973632812 Training error :4804.884765625\n",
      "Epoch : 369 Loss: 24002.762329101562 Training error :4791.51904296875\n",
      "Epoch : 370 Loss: 23935.825805664062 Training error :4778.125\n",
      "Epoch : 371 Loss: 23868.753662109375 Training error :4764.70361328125\n",
      "Epoch : 372 Loss: 23801.54638671875 Training error :4751.2568359375\n",
      "Epoch : 373 Loss: 23734.217651367188 Training error :4737.787109375\n",
      "Epoch : 374 Loss: 23666.772094726562 Training error :4724.29443359375\n",
      "Epoch : 375 Loss: 23599.219848632812 Training error :4710.78173828125\n",
      "Epoch : 376 Loss: 23531.568603515625 Training error :4697.24951171875\n",
      "Epoch : 377 Loss: 23463.8212890625 Training error :4683.69970703125\n",
      "Epoch : 378 Loss: 23395.988891601562 Training error :4670.13330078125\n",
      "Epoch : 379 Loss: 23328.074829101562 Training error :4656.55126953125\n",
      "Epoch : 380 Loss: 23260.0888671875 Training error :4642.962890625\n",
      "Epoch : 381 Loss: 23192.070922851562 Training error :4629.36328125\n",
      "Epoch : 382 Loss: 23123.99560546875 Training error :4615.75146484375\n",
      "Epoch : 383 Loss: 23055.863525390625 Training error :4602.12841796875\n",
      "Epoch : 384 Loss: 22987.677368164062 Training error :4588.4951171875\n",
      "Epoch : 385 Loss: 22919.441650390625 Training error :4574.8525390625\n",
      "Epoch : 386 Loss: 22851.15771484375 Training error :4561.201171875\n",
      "Epoch : 387 Loss: 22782.833740234375 Training error :4547.5419921875\n",
      "Epoch : 388 Loss: 22714.470825195312 Training error :4533.87548828125\n",
      "Epoch : 389 Loss: 22646.072998046875 Training error :4520.20263671875\n",
      "Epoch : 390 Loss: 22577.643310546875 Training error :4506.52294921875\n",
      "Epoch : 391 Loss: 22509.18603515625 Training error :4492.83837890625\n",
      "Epoch : 392 Loss: 22440.700805664062 Training error :4479.14892578125\n",
      "Epoch : 393 Loss: 22372.191528320312 Training error :4465.45458984375\n",
      "Epoch : 394 Loss: 22303.659790039062 Training error :4451.75634765625\n",
      "Epoch : 395 Loss: 22235.109375 Training error :4438.05419921875\n",
      "Epoch : 396 Loss: 22166.539184570312 Training error :4424.34912109375\n",
      "Epoch : 397 Loss: 22097.952758789062 Training error :4410.640625\n",
      "Epoch : 398 Loss: 22029.354125976562 Training error :4396.9296875\n",
      "Epoch : 399 Loss: 21960.74267578125 Training error :4383.21875\n",
      "Epoch : 400 Loss: 21892.130981445312 Training error :4369.509765625\n",
      "Epoch : 401 Loss: 21823.531494140625 Training error :4355.7998046875\n",
      "Epoch : 402 Loss: 21754.924682617188 Training error :4342.0888671875\n",
      "Epoch : 403 Loss: 21686.3115234375 Training error :4328.37548828125\n",
      "Epoch : 404 Loss: 21617.689331054688 Training error :4314.65478515625\n",
      "Epoch : 405 Loss: 21549.035522460938 Training error :4300.92578125\n",
      "Epoch : 406 Loss: 21480.334838867188 Training error :4287.193359375\n",
      "Epoch : 407 Loss: 21411.61669921875 Training error :4273.458984375\n",
      "Epoch : 408 Loss: 21342.889404296875 Training error :4259.724609375\n",
      "Epoch : 409 Loss: 21274.160522460938 Training error :4245.98876953125\n",
      "Epoch : 410 Loss: 21205.429077148438 Training error :4232.251953125\n",
      "Epoch : 411 Loss: 21136.693603515625 Training error :4218.51513671875\n",
      "Epoch : 412 Loss: 21067.954223632812 Training error :4204.7763671875\n",
      "Epoch : 413 Loss: 20999.2060546875 Training error :4191.037109375\n",
      "Epoch : 414 Loss: 20930.455078125 Training error :4177.296875\n",
      "Epoch : 415 Loss: 20861.700317382812 Training error :4163.55517578125\n",
      "Epoch : 416 Loss: 20792.937622070312 Training error :4149.81298828125\n",
      "Epoch : 417 Loss: 20724.171630859375 Training error :4136.06884765625\n",
      "Epoch : 418 Loss: 20655.402099609375 Training error :4122.32421875\n",
      "Epoch : 419 Loss: 20586.622924804688 Training error :4108.57861328125\n",
      "Epoch : 420 Loss: 20517.838012695312 Training error :4094.8310546875\n",
      "Epoch : 421 Loss: 20449.04931640625 Training error :4081.083984375\n",
      "Epoch : 422 Loss: 20380.26025390625 Training error :4067.34033203125\n",
      "Epoch : 423 Loss: 20311.48681640625 Training error :4053.59521484375\n",
      "Epoch : 424 Loss: 20242.710815429688 Training error :4039.849365234375\n",
      "Epoch : 425 Loss: 20173.925903320312 Training error :4026.101806640625\n",
      "Epoch : 426 Loss: 20105.136108398438 Training error :4012.352783203125\n",
      "Epoch : 427 Loss: 20036.33740234375 Training error :3998.601806640625\n",
      "Epoch : 428 Loss: 19967.529663085938 Training error :3984.84912109375\n",
      "Epoch : 429 Loss: 19898.713256835938 Training error :3971.094482421875\n",
      "Epoch : 430 Loss: 19829.888916015625 Training error :3957.337890625\n",
      "Epoch : 431 Loss: 19761.053833007812 Training error :3943.580078125\n",
      "Epoch : 432 Loss: 19692.21044921875 Training error :3929.819580078125\n",
      "Epoch : 433 Loss: 19623.359130859375 Training error :3916.057861328125\n",
      "Epoch : 434 Loss: 19554.498168945312 Training error :3902.29345703125\n",
      "Epoch : 435 Loss: 19485.626953125 Training error :3888.52783203125\n",
      "Epoch : 436 Loss: 19416.747924804688 Training error :3874.759765625\n",
      "Epoch : 437 Loss: 19347.858032226562 Training error :3860.989990234375\n",
      "Epoch : 438 Loss: 19278.96044921875 Training error :3847.218505859375\n",
      "Epoch : 439 Loss: 19210.052612304688 Training error :3833.4453125\n",
      "Epoch : 440 Loss: 19141.139282226562 Training error :3819.669921875\n",
      "Epoch : 441 Loss: 19072.214477539062 Training error :3805.89306640625\n",
      "Epoch : 442 Loss: 19003.282958984375 Training error :3792.11474609375\n",
      "Epoch : 443 Loss: 18934.342407226562 Training error :3778.33447265625\n",
      "Epoch : 444 Loss: 18865.39697265625 Training error :3764.552734375\n",
      "Epoch : 445 Loss: 18796.44287109375 Training error :3750.77001953125\n",
      "Epoch : 446 Loss: 18727.483764648438 Training error :3736.98583984375\n",
      "Epoch : 447 Loss: 18658.5166015625 Training error :3723.203125\n",
      "Epoch : 448 Loss: 18589.56103515625 Training error :3709.422119140625\n",
      "Epoch : 449 Loss: 18520.612670898438 Training error :3695.640869140625\n",
      "Epoch : 450 Loss: 18451.663330078125 Training error :3681.858642578125\n",
      "Epoch : 451 Loss: 18382.713500976562 Training error :3668.07666015625\n",
      "Epoch : 452 Loss: 18313.760986328125 Training error :3654.2939453125\n",
      "Epoch : 453 Loss: 18244.810668945312 Training error :3640.511474609375\n",
      "Epoch : 454 Loss: 18175.858032226562 Training error :3626.728759765625\n",
      "Epoch : 455 Loss: 18106.909423828125 Training error :3612.946533203125\n",
      "Epoch : 456 Loss: 18037.962036132812 Training error :3599.1650390625\n",
      "Epoch : 457 Loss: 17969.019409179688 Training error :3585.384033203125\n",
      "Epoch : 458 Loss: 17900.08349609375 Training error :3571.605224609375\n",
      "Epoch : 459 Loss: 17831.15673828125 Training error :3557.828857421875\n",
      "Epoch : 460 Loss: 17762.24169921875 Training error :3544.053955078125\n",
      "Epoch : 461 Loss: 17693.339233398438 Training error :3530.281494140625\n",
      "Epoch : 462 Loss: 17624.44921875 Training error :3516.511962890625\n",
      "Epoch : 463 Loss: 17555.570922851562 Training error :3502.74462890625\n",
      "Epoch : 464 Loss: 17486.712524414062 Training error :3488.98095703125\n",
      "Epoch : 465 Loss: 17417.866943359375 Training error :3475.220703125\n",
      "Epoch : 466 Loss: 17349.043334960938 Training error :3461.464599609375\n",
      "Epoch : 467 Loss: 17280.24267578125 Training error :3447.713134765625\n",
      "Epoch : 468 Loss: 17211.465087890625 Training error :3433.96630859375\n",
      "Epoch : 469 Loss: 17142.713256835938 Training error :3420.22509765625\n",
      "Epoch : 470 Loss: 17073.988830566406 Training error :3406.489013671875\n",
      "Epoch : 471 Loss: 17005.29541015625 Training error :3392.760009765625\n",
      "Epoch : 472 Loss: 16936.63494873047 Training error :3379.037109375\n",
      "Epoch : 473 Loss: 16868.010314941406 Training error :3365.32177734375\n",
      "Epoch : 474 Loss: 16799.423461914062 Training error :3351.6142578125\n",
      "Epoch : 475 Loss: 16730.876831054688 Training error :3337.914794921875\n",
      "Epoch : 476 Loss: 16662.37432861328 Training error :3324.224365234375\n",
      "Epoch : 477 Loss: 16593.917114257812 Training error :3310.544677734375\n",
      "Epoch : 478 Loss: 16525.515197753906 Training error :3296.8779296875\n",
      "Epoch : 479 Loss: 16457.179321289062 Training error :3283.221923828125\n",
      "Epoch : 480 Loss: 16388.90106201172 Training error :3269.5771484375\n",
      "Epoch : 481 Loss: 16320.679016113281 Training error :3255.944091796875\n",
      "Epoch : 482 Loss: 16252.520446777344 Training error :3242.323486328125\n",
      "Epoch : 483 Loss: 16184.423217773438 Training error :3228.715576171875\n",
      "Epoch : 484 Loss: 16116.391052246094 Training error :3215.120849609375\n",
      "Epoch : 485 Loss: 16048.431335449219 Training error :3201.54052734375\n",
      "Epoch : 486 Loss: 15980.541931152344 Training error :3187.97509765625\n",
      "Epoch : 487 Loss: 15912.728454589844 Training error :3174.42529296875\n",
      "Epoch : 488 Loss: 15844.996459960938 Training error :3160.891357421875\n",
      "Epoch : 489 Loss: 15777.344421386719 Training error :3147.373779296875\n",
      "Epoch : 490 Loss: 15709.779418945312 Training error :3133.873779296875\n",
      "Epoch : 491 Loss: 15642.304077148438 Training error :3120.391845703125\n",
      "Epoch : 492 Loss: 15574.919372558594 Training error :3106.928466796875\n",
      "Epoch : 493 Loss: 15507.63232421875 Training error :3093.484619140625\n",
      "Epoch : 494 Loss: 15440.443420410156 Training error :3080.061279296875\n",
      "Epoch : 495 Loss: 15373.356506347656 Training error :3066.65869140625\n",
      "Epoch : 496 Loss: 15306.379638671875 Training error :3053.277587890625\n",
      "Epoch : 497 Loss: 15239.510070800781 Training error :3039.9189453125\n",
      "Epoch : 498 Loss: 15172.753051757812 Training error :3026.5830078125\n",
      "Epoch : 499 Loss: 15106.117248535156 Training error :3013.27099609375\n",
      "Epoch : 500 Loss: 15039.599243164062 Training error :2999.9833984375\n",
      "Epoch : 501 Loss: 14973.20751953125 Training error :2986.72119140625\n",
      "Epoch : 502 Loss: 14906.943542480469 Training error :2973.484619140625\n",
      "Epoch : 503 Loss: 14840.811645507812 Training error :2960.274658203125\n",
      "Epoch : 504 Loss: 14774.813842773438 Training error :2947.09228515625\n",
      "Epoch : 505 Loss: 14708.955505371094 Training error :2933.937255859375\n",
      "Epoch : 506 Loss: 14643.244079589844 Training error :2920.80224609375\n",
      "Epoch : 507 Loss: 14577.632934570312 Training error :2907.6962890625\n",
      "Epoch : 508 Loss: 14512.164733886719 Training error :2894.620849609375\n",
      "Epoch : 509 Loss: 14446.85009765625 Training error :2881.576904296875\n",
      "Epoch : 510 Loss: 14381.696350097656 Training error :2868.5654296875\n",
      "Epoch : 511 Loss: 14316.705932617188 Training error :2855.586181640625\n",
      "Epoch : 512 Loss: 14251.878845214844 Training error :2842.640380859375\n",
      "Epoch : 513 Loss: 14187.221008300781 Training error :2829.72802734375\n",
      "Epoch : 514 Loss: 14122.737976074219 Training error :2816.853759765625\n",
      "Epoch : 515 Loss: 14058.438781738281 Training error :2804.015625\n",
      "Epoch : 516 Loss: 13994.327453613281 Training error :2791.21435546875\n",
      "Epoch : 517 Loss: 13930.401672363281 Training error :2778.4501953125\n",
      "Epoch : 518 Loss: 13866.663269042969 Training error :2765.72412109375\n",
      "Epoch : 519 Loss: 13803.116821289062 Training error :2753.035888671875\n",
      "Epoch : 520 Loss: 13739.76513671875 Training error :2740.387451171875\n",
      "Epoch : 521 Loss: 13676.609130859375 Training error :2727.7783203125\n",
      "Epoch : 522 Loss: 13613.658508300781 Training error :2715.2099609375\n",
      "Epoch : 523 Loss: 13550.909912109375 Training error :2702.68310546875\n",
      "Epoch : 524 Loss: 13488.369995117188 Training error :2690.198486328125\n",
      "Epoch : 525 Loss: 13426.043029785156 Training error :2677.755859375\n",
      "Epoch : 526 Loss: 13363.931457519531 Training error :2665.35693359375\n",
      "Epoch : 527 Loss: 13302.037780761719 Training error :2653.001953125\n",
      "Epoch : 528 Loss: 13240.364562988281 Training error :2640.691650390625\n",
      "Epoch : 529 Loss: 13178.921081542969 Training error :2628.4267578125\n",
      "Epoch : 530 Loss: 13117.703063964844 Training error :2616.2080078125\n",
      "Epoch : 531 Loss: 13056.72021484375 Training error :2604.03662109375\n",
      "Epoch : 532 Loss: 12995.971862792969 Training error :2591.91162109375\n",
      "Epoch : 533 Loss: 12935.462951660156 Training error :2579.835205078125\n",
      "Epoch : 534 Loss: 12875.195678710938 Training error :2567.8076171875\n",
      "Epoch : 535 Loss: 12815.174499511719 Training error :2555.8291015625\n",
      "Epoch : 536 Loss: 12755.401062011719 Training error :2543.90087890625\n",
      "Epoch : 537 Loss: 12695.88037109375 Training error :2532.022705078125\n",
      "Epoch : 538 Loss: 12636.611633300781 Training error :2520.196044921875\n",
      "Epoch : 539 Loss: 12577.601684570312 Training error :2508.42138671875\n",
      "Epoch : 540 Loss: 12518.853454589844 Training error :2496.698486328125\n",
      "Epoch : 541 Loss: 12460.368103027344 Training error :2485.029052734375\n",
      "Epoch : 542 Loss: 12402.1494140625 Training error :2473.4130859375\n",
      "Epoch : 543 Loss: 12344.19873046875 Training error :2461.851318359375\n",
      "Epoch : 544 Loss: 12286.525024414062 Training error :2450.345703125\n",
      "Epoch : 545 Loss: 12229.131469726562 Training error :2438.8984375\n",
      "Epoch : 546 Loss: 12172.030883789062 Training error :2427.5078125\n",
      "Epoch : 547 Loss: 12115.212158203125 Training error :2416.17333984375\n",
      "Epoch : 548 Loss: 12058.677215576172 Training error :2404.8955078125\n",
      "Epoch : 549 Loss: 12002.42886352539 Training error :2393.675537109375\n",
      "Epoch : 550 Loss: 11946.470153808594 Training error :2382.512939453125\n",
      "Epoch : 551 Loss: 11890.800659179688 Training error :2371.40869140625\n",
      "Epoch : 552 Loss: 11835.423065185547 Training error :2360.36328125\n",
      "Epoch : 553 Loss: 11780.342559814453 Training error :2349.37744140625\n",
      "Epoch : 554 Loss: 11725.561798095703 Training error :2338.45166015625\n",
      "Epoch : 555 Loss: 11671.078887939453 Training error :2327.585693359375\n",
      "Epoch : 556 Loss: 11616.898895263672 Training error :2316.78076171875\n",
      "Epoch : 557 Loss: 11563.02505493164 Training error :2306.037109375\n",
      "Epoch : 558 Loss: 11509.456848144531 Training error :2295.3544921875\n",
      "Epoch : 559 Loss: 11456.198120117188 Training error :2284.738037109375\n",
      "Epoch : 560 Loss: 11403.26919555664 Training error :2274.18603515625\n",
      "Epoch : 561 Loss: 11350.665405273438 Training error :2263.69775390625\n",
      "Epoch : 562 Loss: 11298.378448486328 Training error :2253.273193359375\n",
      "Epoch : 563 Loss: 11246.413696289062 Training error :2242.9130859375\n",
      "Epoch : 564 Loss: 11194.769714355469 Training error :2232.616943359375\n",
      "Epoch : 565 Loss: 11143.447784423828 Training error :2222.38525390625\n",
      "Epoch : 566 Loss: 11092.451354980469 Training error :2212.218017578125\n",
      "Epoch : 567 Loss: 11041.776306152344 Training error :2202.11572265625\n",
      "Epoch : 568 Loss: 10991.425903320312 Training error :2192.0791015625\n",
      "Epoch : 569 Loss: 10941.40478515625 Training error :2182.107421875\n",
      "Epoch : 570 Loss: 10891.710693359375 Training error :2172.201904296875\n",
      "Epoch : 571 Loss: 10842.349060058594 Training error :2162.362548828125\n",
      "Epoch : 572 Loss: 10793.31802368164 Training error :2152.590087890625\n",
      "Epoch : 573 Loss: 10744.61752319336 Training error :2142.8837890625\n",
      "Epoch : 574 Loss: 10696.254943847656 Training error :2133.24462890625\n",
      "Epoch : 575 Loss: 10648.22445678711 Training error :2123.672607421875\n",
      "Epoch : 576 Loss: 10600.53012084961 Training error :2114.16796875\n",
      "Epoch : 577 Loss: 10553.175659179688 Training error :2104.732421875\n",
      "Epoch : 578 Loss: 10506.16567993164 Training error :2095.3662109375\n",
      "Epoch : 579 Loss: 10459.505126953125 Training error :2086.068359375\n",
      "Epoch : 580 Loss: 10413.184814453125 Training error :2076.83837890625\n",
      "Epoch : 581 Loss: 10367.20590209961 Training error :2067.67724609375\n",
      "Epoch : 582 Loss: 10321.57113647461 Training error :2058.5849609375\n",
      "Epoch : 583 Loss: 10276.278228759766 Training error :2049.5615234375\n",
      "Epoch : 584 Loss: 10231.332427978516 Training error :2040.606201171875\n",
      "Epoch : 585 Loss: 10186.726989746094 Training error :2031.7203369140625\n",
      "Epoch : 586 Loss: 10142.469207763672 Training error :2022.9034423828125\n",
      "Epoch : 587 Loss: 10098.556091308594 Training error :2014.1556396484375\n",
      "Epoch : 588 Loss: 10054.988891601562 Training error :2005.4771728515625\n",
      "Epoch : 589 Loss: 10011.769287109375 Training error :1996.867919921875\n",
      "Epoch : 590 Loss: 9968.895690917969 Training error :1988.3282470703125\n",
      "Epoch : 591 Loss: 9926.369812011719 Training error :1979.8582763671875\n",
      "Epoch : 592 Loss: 9884.190673828125 Training error :1971.4576416015625\n",
      "Epoch : 593 Loss: 9842.359771728516 Training error :1963.1268310546875\n",
      "Epoch : 594 Loss: 9800.877166748047 Training error :1954.8653564453125\n",
      "Epoch : 595 Loss: 9759.74234008789 Training error :1946.6734619140625\n",
      "Epoch : 596 Loss: 9718.956329345703 Training error :1938.55126953125\n",
      "Epoch : 597 Loss: 9678.51611328125 Training error :1930.498779296875\n",
      "Epoch : 598 Loss: 9638.426208496094 Training error :1922.5159912109375\n",
      "Epoch : 599 Loss: 9598.68179321289 Training error :1914.6026611328125\n",
      "Epoch : 600 Loss: 9559.286575317383 Training error :1906.7607421875\n",
      "Epoch : 601 Loss: 9520.24819946289 Training error :1898.9893798828125\n",
      "Epoch : 602 Loss: 9481.561065673828 Training error :1891.2889404296875\n",
      "Epoch : 603 Loss: 9443.229614257812 Training error :1883.657958984375\n",
      "Epoch : 604 Loss: 9405.244430541992 Training error :1876.09619140625\n",
      "Epoch : 605 Loss: 9367.606140136719 Training error :1868.6038818359375\n",
      "Epoch : 606 Loss: 9330.31265258789 Training error :1861.1802978515625\n",
      "Epoch : 607 Loss: 9293.364135742188 Training error :1853.8258056640625\n",
      "Epoch : 608 Loss: 9256.760330200195 Training error :1846.53955078125\n",
      "Epoch : 609 Loss: 9220.49771118164 Training error :1839.3223876953125\n",
      "Epoch : 610 Loss: 9184.578842163086 Training error :1832.17333984375\n",
      "Epoch : 611 Loss: 9148.999771118164 Training error :1825.0921630859375\n",
      "Epoch : 612 Loss: 9113.76155090332 Training error :1818.07958984375\n",
      "Epoch : 613 Loss: 9078.863510131836 Training error :1811.1346435546875\n",
      "Epoch : 614 Loss: 9044.305862426758 Training error :1804.2578125\n",
      "Epoch : 615 Loss: 9010.085861206055 Training error :1797.448486328125\n",
      "Epoch : 616 Loss: 8976.203826904297 Training error :1790.7064208984375\n",
      "Epoch : 617 Loss: 8942.655212402344 Training error :1784.03125\n",
      "Epoch : 618 Loss: 8909.442886352539 Training error :1777.4229736328125\n",
      "Epoch : 619 Loss: 8876.562759399414 Training error :1770.881591796875\n",
      "Epoch : 620 Loss: 8844.017669677734 Training error :1764.4061279296875\n",
      "Epoch : 621 Loss: 8811.799560546875 Training error :1757.997314453125\n",
      "Epoch : 622 Loss: 8779.916641235352 Training error :1751.6541748046875\n",
      "Epoch : 623 Loss: 8748.358703613281 Training error :1745.3768310546875\n",
      "Epoch : 624 Loss: 8717.129989624023 Training error :1739.16455078125\n",
      "Epoch : 625 Loss: 8686.22525024414 Training error :1733.0174560546875\n",
      "Epoch : 626 Loss: 8655.646423339844 Training error :1726.9351806640625\n",
      "Epoch : 627 Loss: 8625.389831542969 Training error :1720.9169921875\n",
      "Epoch : 628 Loss: 8595.45393371582 Training error :1714.96337890625\n",
      "Epoch : 629 Loss: 8565.839126586914 Training error :1709.073486328125\n",
      "Epoch : 630 Loss: 8536.538513183594 Training error :1703.24755859375\n",
      "Epoch : 631 Loss: 8507.55989074707 Training error :1697.4864501953125\n",
      "Epoch : 632 Loss: 8478.904495239258 Training error :1691.786376953125\n",
      "Epoch : 633 Loss: 8450.561538696289 Training error :1686.143798828125\n",
      "Epoch : 634 Loss: 8422.497344970703 Training error :1680.5618896484375\n",
      "Epoch : 635 Loss: 8394.737075805664 Training error :1675.04150390625\n",
      "Epoch : 636 Loss: 8367.281677246094 Training error :1669.58251953125\n",
      "Epoch : 637 Loss: 8340.130065917969 Training error :1664.1842041015625\n",
      "Epoch : 638 Loss: 8313.28158569336 Training error :1658.846435546875\n",
      "Epoch : 639 Loss: 8286.735900878906 Training error :1653.569091796875\n",
      "Epoch : 640 Loss: 8260.488876342773 Training error :1648.3511962890625\n",
      "Epoch : 641 Loss: 8234.539276123047 Training error :1643.192626953125\n",
      "Epoch : 642 Loss: 8208.884925842285 Training error :1638.0931396484375\n",
      "Epoch : 643 Loss: 8183.524429321289 Training error :1633.0518798828125\n",
      "Epoch : 644 Loss: 8158.455520629883 Training error :1628.069580078125\n",
      "Epoch : 645 Loss: 8133.676849365234 Training error :1623.14404296875\n",
      "Epoch : 646 Loss: 8109.184494018555 Training error :1618.2762451171875\n",
      "Epoch : 647 Loss: 8084.9780197143555 Training error :1613.4649658203125\n",
      "Epoch : 648 Loss: 8061.052551269531 Training error :1608.7103271484375\n",
      "Epoch : 649 Loss: 8037.409210205078 Training error :1604.0113525390625\n",
      "Epoch : 650 Loss: 8014.041915893555 Training error :1599.367919921875\n",
      "Epoch : 651 Loss: 7990.953407287598 Training error :1594.779541015625\n",
      "Epoch : 652 Loss: 7968.1362228393555 Training error :1590.2454833984375\n",
      "Epoch : 653 Loss: 7945.591323852539 Training error :1585.765625\n",
      "Epoch : 654 Loss: 7923.314796447754 Training error :1581.3392333984375\n",
      "Epoch : 655 Loss: 7901.306243896484 Training error :1576.966064453125\n",
      "Epoch : 656 Loss: 7879.559700012207 Training error :1572.6451416015625\n",
      "Epoch : 657 Loss: 7858.075225830078 Training error :1568.3765869140625\n",
      "Epoch : 658 Loss: 7836.850860595703 Training error :1564.1595458984375\n",
      "Epoch : 659 Loss: 7815.883827209473 Training error :1559.9942626953125\n",
      "Epoch : 660 Loss: 7795.170516967773 Training error :1555.8790283203125\n",
      "Epoch : 661 Loss: 7774.708473205566 Training error :1551.814208984375\n",
      "Epoch : 662 Loss: 7754.495071411133 Training error :1547.7987060546875\n",
      "Epoch : 663 Loss: 7734.531051635742 Training error :1543.83251953125\n",
      "Epoch : 664 Loss: 7714.809722900391 Training error :1539.9151611328125\n",
      "Epoch : 665 Loss: 7695.329574584961 Training error :1536.0458984375\n",
      "Epoch : 666 Loss: 7676.09090423584 Training error :1532.2244873046875\n",
      "Epoch : 667 Loss: 7657.089538574219 Training error :1528.449951171875\n",
      "Epoch : 668 Loss: 7638.3222732543945 Training error :1524.72216796875\n",
      "Epoch : 669 Loss: 7619.784797668457 Training error :1521.04052734375\n",
      "Epoch : 670 Loss: 7601.478981018066 Training error :1517.404541015625\n",
      "Epoch : 671 Loss: 7583.398696899414 Training error :1513.8128662109375\n",
      "Epoch : 672 Loss: 7565.529113769531 Training error :1510.264404296875\n",
      "Epoch : 673 Loss: 7547.880859375 Training error :1506.759521484375\n",
      "Epoch : 674 Loss: 7530.452331542969 Training error :1503.297607421875\n",
      "Epoch : 675 Loss: 7513.237686157227 Training error :1499.87890625\n",
      "Epoch : 676 Loss: 7496.23645401001 Training error :1496.502197265625\n",
      "Epoch : 677 Loss: 7479.427528381348 Training error :1493.1641845703125\n",
      "Epoch : 678 Loss: 7462.824729919434 Training error :1489.8668212890625\n",
      "Epoch : 679 Loss: 7446.425712585449 Training error :1486.610595703125\n",
      "Epoch : 680 Loss: 7430.225311279297 Training error :1483.392333984375\n",
      "Epoch : 681 Loss: 7414.2044677734375 Training error :1480.2110595703125\n",
      "Epoch : 682 Loss: 7398.378479003906 Training error :1477.068115234375\n",
      "Epoch : 683 Loss: 7382.738582611084 Training error :1473.9615478515625\n",
      "Epoch : 684 Loss: 7367.2659034729 Training error :1470.8890380859375\n",
      "Epoch : 685 Loss: 7351.979057312012 Training error :1467.8533935546875\n",
      "Epoch : 686 Loss: 7336.861068725586 Training error :1464.849853515625\n",
      "Epoch : 687 Loss: 7321.906623840332 Training error :1461.88037109375\n",
      "Epoch : 688 Loss: 7307.114906311035 Training error :1458.9415283203125\n",
      "Epoch : 689 Loss: 7292.472625732422 Training error :1456.03369140625\n",
      "Epoch : 690 Loss: 7277.988792419434 Training error :1453.1552734375\n",
      "Epoch : 691 Loss: 7263.642333984375 Training error :1450.3062744140625\n",
      "Epoch : 692 Loss: 7249.438022613525 Training error :1447.4837646484375\n",
      "Epoch : 693 Loss: 7235.373046875 Training error :1444.688232421875\n",
      "Epoch : 694 Loss: 7221.411838531494 Training error :1441.9156494140625\n",
      "Epoch : 695 Loss: 7207.591827392578 Training error :1439.16845703125\n",
      "Epoch : 696 Loss: 7193.885711669922 Training error :1436.4451904296875\n",
      "Epoch : 697 Loss: 7180.297721862793 Training error :1433.7437744140625\n",
      "Epoch : 698 Loss: 7166.814758300781 Training error :1431.0626220703125\n",
      "Epoch : 699 Loss: 7153.426097869873 Training error :1428.399658203125\n",
      "Epoch : 700 Loss: 7140.117652893066 Training error :1425.754638671875\n",
      "Epoch : 701 Loss: 7126.912200927734 Training error :1423.128173828125\n",
      "Epoch : 702 Loss: 7113.791244506836 Training error :1420.5172119140625\n",
      "Epoch : 703 Loss: 7100.744956970215 Training error :1417.921142578125\n",
      "Epoch : 704 Loss: 7087.766067504883 Training error :1415.33935546875\n",
      "Epoch : 705 Loss: 7074.868083953857 Training error :1412.7734375\n",
      "Epoch : 706 Loss: 7062.038993835449 Training error :1410.2193603515625\n",
      "Epoch : 707 Loss: 7049.2673416137695 Training error :1407.67724609375\n",
      "Epoch : 708 Loss: 7036.552474975586 Training error :1405.1451416015625\n",
      "Epoch : 709 Loss: 7023.889301300049 Training error :1402.6231689453125\n",
      "Epoch : 710 Loss: 7011.277076721191 Training error :1400.1116943359375\n",
      "Epoch : 711 Loss: 6998.729215621948 Training error :1397.61279296875\n",
      "Epoch : 712 Loss: 6986.232549667358 Training error :1395.1239013671875\n",
      "Epoch : 713 Loss: 6973.782936096191 Training error :1392.643310546875\n",
      "Epoch : 714 Loss: 6961.3777713775635 Training error :1390.17138671875\n",
      "Epoch : 715 Loss: 6949.017580032349 Training error :1387.7080078125\n",
      "Epoch : 716 Loss: 6936.699825286865 Training error :1385.253173828125\n",
      "Epoch : 717 Loss: 6924.4302616119385 Training error :1382.8067626953125\n",
      "Epoch : 718 Loss: 6912.202934265137 Training error :1380.370361328125\n",
      "Epoch : 719 Loss: 6900.031049728394 Training error :1377.9443359375\n",
      "Epoch : 720 Loss: 6887.905910491943 Training error :1375.5274658203125\n",
      "Epoch : 721 Loss: 6875.827167510986 Training error :1373.1190185546875\n",
      "Epoch : 722 Loss: 6863.794130325317 Training error :1370.7198486328125\n",
      "Epoch : 723 Loss: 6851.8097133636475 Training error :1368.3299560546875\n",
      "Epoch : 724 Loss: 6839.873365402222 Training error :1365.9501953125\n",
      "Epoch : 725 Loss: 6827.989025115967 Training error :1363.58056640625\n",
      "Epoch : 726 Loss: 6816.155830383301 Training error :1361.2205810546875\n",
      "Epoch : 727 Loss: 6804.371492385864 Training error :1358.87060546875\n",
      "Epoch : 728 Loss: 6792.639450073242 Training error :1356.5308837890625\n",
      "Epoch : 729 Loss: 6780.957761764526 Training error :1354.2012939453125\n",
      "Epoch : 730 Loss: 6769.32859992981 Training error :1351.8818359375\n",
      "Epoch : 731 Loss: 6757.753005981445 Training error :1349.572998046875\n",
      "Epoch : 732 Loss: 6746.2283363342285 Training error :1347.27490234375\n",
      "Epoch : 733 Loss: 6734.759078979492 Training error :1344.986572265625\n",
      "Epoch : 734 Loss: 6723.342460632324 Training error :1342.709716796875\n",
      "Epoch : 735 Loss: 6711.977783203125 Training error :1340.443115234375\n",
      "Epoch : 736 Loss: 6700.668836593628 Training error :1338.1864013671875\n",
      "Epoch : 737 Loss: 6689.409250259399 Training error :1335.9412841796875\n",
      "Epoch : 738 Loss: 6678.204248428345 Training error :1333.7064208984375\n",
      "Epoch : 739 Loss: 6667.057426452637 Training error :1331.4814453125\n",
      "Epoch : 740 Loss: 6655.958297729492 Training error :1329.2679443359375\n",
      "Epoch : 741 Loss: 6644.908645629883 Training error :1327.0631103515625\n",
      "Epoch : 742 Loss: 6633.911911010742 Training error :1324.868896484375\n",
      "Epoch : 743 Loss: 6622.966100692749 Training error :1322.6864013671875\n",
      "Epoch : 744 Loss: 6612.081502914429 Training error :1320.51318359375\n",
      "Epoch : 745 Loss: 6601.240719795227 Training error :1318.3511962890625\n",
      "Epoch : 746 Loss: 6590.453052520752 Training error :1316.1971435546875\n",
      "Epoch : 747 Loss: 6579.714385986328 Training error :1314.055419921875\n",
      "Epoch : 748 Loss: 6569.021615028381 Training error :1311.920654296875\n",
      "Epoch : 749 Loss: 6558.380990982056 Training error :1309.7984619140625\n",
      "Epoch : 750 Loss: 6547.786037445068 Training error :1307.683349609375\n",
      "Epoch : 751 Loss: 6537.242250442505 Training error :1305.58251953125\n",
      "Epoch : 752 Loss: 6526.751846313477 Training error :1303.48779296875\n",
      "Epoch : 753 Loss: 6516.31280708313 Training error :1301.4066162109375\n",
      "Epoch : 754 Loss: 6505.923269271851 Training error :1299.3310546875\n",
      "Epoch : 755 Loss: 6495.581076622009 Training error :1297.269287109375\n",
      "Epoch : 756 Loss: 6485.288087844849 Training error :1295.21337890625\n",
      "Epoch : 757 Loss: 6475.040098190308 Training error :1293.17041015625\n",
      "Epoch : 758 Loss: 6464.851154327393 Training error :1291.1346435546875\n",
      "Epoch : 759 Loss: 6454.676399230957 Training error :1289.1058349609375\n",
      "Epoch : 760 Loss: 6444.572641372681 Training error :1287.087890625\n",
      "Epoch : 761 Loss: 6434.497337341309 Training error :1285.075439453125\n",
      "Epoch : 762 Loss: 6424.47709274292 Training error :1283.074462890625\n",
      "Epoch : 763 Loss: 6414.494601249695 Training error :1281.0745849609375\n",
      "Epoch : 764 Loss: 6404.503868103027 Training error :1279.081298828125\n",
      "Epoch : 765 Loss: 6394.579918861389 Training error :1277.0982666015625\n",
      "Epoch : 766 Loss: 6384.680143356323 Training error :1275.1201171875\n",
      "Epoch : 767 Loss: 6374.831339836121 Training error :1273.1568603515625\n",
      "Epoch : 768 Loss: 6365.037709236145 Training error :1271.2001953125\n",
      "Epoch : 769 Loss: 6355.2608280181885 Training error :1269.2479248046875\n",
      "Epoch : 770 Loss: 6345.54519367218 Training error :1267.3099365234375\n",
      "Epoch : 771 Loss: 6335.873175621033 Training error :1265.37744140625\n",
      "Epoch : 772 Loss: 6326.2168827056885 Training error :1263.4501953125\n",
      "Epoch : 773 Loss: 6316.626682281494 Training error :1261.5367431640625\n",
      "Epoch : 774 Loss: 6307.078216552734 Training error :1259.62890625\n",
      "Epoch : 775 Loss: 6297.5466413497925 Training error :1257.725341796875\n",
      "Epoch : 776 Loss: 6288.07439994812 Training error :1255.837158203125\n",
      "Epoch : 777 Loss: 6278.651375770569 Training error :1253.954345703125\n",
      "Epoch : 778 Loss: 6269.243829727173 Training error :1252.0745849609375\n",
      "Epoch : 779 Loss: 6259.887475967407 Training error :1250.2103271484375\n",
      "Epoch : 780 Loss: 6250.588574409485 Training error :1248.352294921875\n",
      "Epoch : 781 Loss: 6241.306085586548 Training error :1246.4974365234375\n",
      "Epoch : 782 Loss: 6232.034708023071 Training error :1244.6495361328125\n",
      "Epoch : 783 Loss: 6222.845540046692 Training error :1242.81396484375\n",
      "Epoch : 784 Loss: 6213.682611465454 Training error :1240.9832763671875\n",
      "Epoch : 785 Loss: 6204.537690162659 Training error :1239.15576171875\n",
      "Epoch : 786 Loss: 6195.444470405579 Training error :1237.3438720703125\n",
      "Epoch : 787 Loss: 6186.411470413208 Training error :1235.53857421875\n",
      "Epoch : 788 Loss: 6177.392244338989 Training error :1233.7364501953125\n",
      "Epoch : 789 Loss: 6168.384384155273 Training error :1231.936767578125\n",
      "Epoch : 790 Loss: 6159.437364578247 Training error :1230.1539306640625\n",
      "Epoch : 791 Loss: 6150.542449951172 Training error :1228.3759765625\n",
      "Epoch : 792 Loss: 6141.661954879761 Training error :1226.6015625\n",
      "Epoch : 793 Loss: 6132.793068885803 Training error :1224.829345703125\n",
      "Epoch : 794 Loss: 6123.9804763793945 Training error :1223.0733642578125\n",
      "Epoch : 795 Loss: 6115.222846984863 Training error :1221.3232421875\n",
      "Epoch : 796 Loss: 6106.481206893921 Training error :1219.57666015625\n",
      "Epoch : 797 Loss: 6097.75200843811 Training error :1217.8321533203125\n",
      "Epoch : 798 Loss: 6089.032329559326 Training error :1216.094970703125\n",
      "Epoch : 799 Loss: 6080.402491569519 Training error :1214.36962890625\n",
      "Epoch : 800 Loss: 6071.793918609619 Training error :1212.6492919921875\n",
      "Epoch : 801 Loss: 6063.200241088867 Training error :1210.9317626953125\n",
      "Epoch : 802 Loss: 6054.6177110672 Training error :1209.2169189453125\n",
      "Epoch : 803 Loss: 6046.092604637146 Training error :1207.5177001953125\n",
      "Epoch : 804 Loss: 6037.623608589172 Training error :1205.8248291015625\n",
      "Epoch : 805 Loss: 6029.166765213013 Training error :1204.1346435546875\n",
      "Epoch : 806 Loss: 6020.720697402954 Training error :1202.4473876953125\n",
      "Epoch : 807 Loss: 6012.287078857422 Training error :1200.7620849609375\n",
      "Epoch : 808 Loss: 6003.9111223220825 Training error :1199.0921630859375\n",
      "Epoch : 809 Loss: 5995.586634635925 Training error :1197.4287109375\n",
      "Epoch : 810 Loss: 5987.2776165008545 Training error :1195.76806640625\n",
      "Epoch : 811 Loss: 5978.978378295898 Training error :1194.109619140625\n",
      "Epoch : 812 Loss: 5970.689302444458 Training error :1192.453369140625\n",
      "Epoch : 813 Loss: 5962.4095821380615 Training error :1190.80322265625\n",
      "Epoch : 814 Loss: 5954.218113899231 Training error :1189.1654052734375\n",
      "Epoch : 815 Loss: 5946.050296783447 Training error :1187.5325927734375\n",
      "Epoch : 816 Loss: 5937.893376350403 Training error :1185.90234375\n",
      "Epoch : 817 Loss: 5929.747214317322 Training error :1184.274658203125\n",
      "Epoch : 818 Loss: 5921.611875534058 Training error :1182.648681640625\n",
      "Epoch : 819 Loss: 5913.4841022491455 Training error :1181.0313720703125\n",
      "Epoch : 820 Loss: 5905.451120376587 Training error :1179.424072265625\n",
      "Epoch : 821 Loss: 5897.4331340789795 Training error :1177.8212890625\n",
      "Epoch : 822 Loss: 5889.426216125488 Training error :1176.22119140625\n",
      "Epoch : 823 Loss: 5881.428966522217 Training error :1174.622802734375\n",
      "Epoch : 824 Loss: 5873.4395208358765 Training error :1173.0267333984375\n",
      "Epoch : 825 Loss: 5865.460509300232 Training error :1171.4356689453125\n",
      "Epoch : 826 Loss: 5857.559175491333 Training error :1169.8575439453125\n",
      "Epoch : 827 Loss: 5849.686382293701 Training error :1168.28369140625\n",
      "Epoch : 828 Loss: 5841.824843406677 Training error :1166.712158203125\n",
      "Epoch : 829 Loss: 5833.971887588501 Training error :1165.142822265625\n",
      "Epoch : 830 Loss: 5826.1273021698 Training error :1163.5750732421875\n",
      "Epoch : 831 Loss: 5818.289778709412 Training error :1162.009033203125\n",
      "Epoch : 832 Loss: 5810.460600852966 Training error :1160.4495849609375\n",
      "Epoch : 833 Loss: 5802.718812942505 Training error :1158.90087890625\n",
      "Epoch : 834 Loss: 5794.993633270264 Training error :1157.3565673828125\n",
      "Epoch : 835 Loss: 5787.278520584106 Training error :1155.8143310546875\n",
      "Epoch : 836 Loss: 5779.570831298828 Training error :1154.274169921875\n",
      "Epoch : 837 Loss: 5771.870832443237 Training error :1152.735107421875\n",
      "Epoch : 838 Loss: 5764.177354812622 Training error :1151.197998046875\n",
      "Epoch : 839 Loss: 5756.49197101593 Training error :1149.66259765625\n",
      "Epoch : 840 Loss: 5748.872620582581 Training error :1148.1422119140625\n",
      "Epoch : 841 Loss: 5741.288369178772 Training error :1146.6259765625\n",
      "Epoch : 842 Loss: 5733.713514328003 Training error :1145.1116943359375\n",
      "Epoch : 843 Loss: 5726.145037651062 Training error :1143.59912109375\n",
      "Epoch : 844 Loss: 5718.584359169006 Training error :1142.0880126953125\n",
      "Epoch : 845 Loss: 5711.028489112854 Training error :1140.5782470703125\n",
      "Epoch : 846 Loss: 5703.480724334717 Training error :1139.0697021484375\n",
      "Epoch : 847 Loss: 5695.939178466797 Training error :1137.5626220703125\n",
      "Epoch : 848 Loss: 5688.454397201538 Training error :1136.0692138671875\n",
      "Epoch : 849 Loss: 5681.011967658997 Training error :1134.580810546875\n",
      "Epoch : 850 Loss: 5673.576350212097 Training error :1133.094482421875\n",
      "Epoch : 851 Loss: 5666.146438598633 Training error :1131.6097412109375\n",
      "Epoch : 852 Loss: 5658.722867965698 Training error :1130.1260986328125\n",
      "Epoch : 853 Loss: 5651.304849624634 Training error :1128.6436767578125\n",
      "Epoch : 854 Loss: 5643.894735336304 Training error :1127.1627197265625\n",
      "Epoch : 855 Loss: 5636.489168167114 Training error :1125.6826171875\n",
      "Epoch : 856 Loss: 5629.090713500977 Training error :1124.2034912109375\n",
      "Epoch : 857 Loss: 5621.745699882507 Training error :1122.7379150390625\n",
      "Epoch : 858 Loss: 5614.441388130188 Training error :1121.2769775390625\n",
      "Epoch : 859 Loss: 5607.14288520813 Training error :1119.8182373046875\n",
      "Epoch : 860 Loss: 5599.850935935974 Training error :1118.360595703125\n",
      "Epoch : 861 Loss: 5592.563650131226 Training error :1116.9041748046875\n",
      "Epoch : 862 Loss: 5585.282862663269 Training error :1115.4490966796875\n",
      "Epoch : 863 Loss: 5578.006509780884 Training error :1113.9947509765625\n",
      "Epoch : 864 Loss: 5570.735715866089 Training error :1112.5413818359375\n",
      "Epoch : 865 Loss: 5563.469017028809 Training error :1111.089111328125\n",
      "Epoch : 866 Loss: 5556.208026885986 Training error :1109.6378173828125\n",
      "Epoch : 867 Loss: 5549.001930236816 Training error :1108.1990966796875\n",
      "Epoch : 868 Loss: 5541.83261680603 Training error :1106.764892578125\n",
      "Epoch : 869 Loss: 5534.6687297821045 Training error :1105.33251953125\n",
      "Epoch : 870 Loss: 5527.507759094238 Training error :1103.901123046875\n",
      "Epoch : 871 Loss: 5520.352262496948 Training error :1102.4710693359375\n",
      "Epoch : 872 Loss: 5513.201053619385 Training error :1101.0418701171875\n",
      "Epoch : 873 Loss: 5506.054430007935 Training error :1099.6136474609375\n",
      "Epoch : 874 Loss: 5498.9132833480835 Training error :1098.186279296875\n",
      "Epoch : 875 Loss: 5491.776554107666 Training error :1096.759521484375\n",
      "Epoch : 876 Loss: 5484.644186973572 Training error :1095.333251953125\n",
      "Epoch : 877 Loss: 5477.51734828949 Training error :1093.90087890625\n",
      "Epoch : 878 Loss: 5470.398163795471 Training error :1092.47802734375\n",
      "Epoch : 879 Loss: 5463.316679954529 Training error :1091.0609130859375\n",
      "Epoch : 880 Loss: 5456.237917900085 Training error :1089.64501953125\n",
      "Epoch : 881 Loss: 5449.158735275269 Training error :1088.2305908203125\n",
      "Epoch : 882 Loss: 5442.085842132568 Training error :1086.81689453125\n",
      "Epoch : 883 Loss: 5435.018102645874 Training error :1085.4039306640625\n",
      "Epoch : 884 Loss: 5427.952818870544 Training error :1083.9920654296875\n",
      "Epoch : 885 Loss: 5420.892484664917 Training error :1082.5810546875\n",
      "Epoch : 886 Loss: 5413.837096214294 Training error :1081.1707763671875\n",
      "Epoch : 887 Loss: 5406.784803390503 Training error :1079.76123046875\n",
      "Epoch : 888 Loss: 5399.737007141113 Training error :1078.352294921875\n",
      "Epoch : 889 Loss: 5392.693176269531 Training error :1076.944580078125\n",
      "Epoch : 890 Loss: 5385.652912139893 Training error :1075.5380859375\n",
      "Epoch : 891 Loss: 5378.677936553955 Training error :1074.1439208984375\n",
      "Epoch : 892 Loss: 5371.723431587219 Training error :1072.7523193359375\n",
      "Epoch : 893 Loss: 5364.771279335022 Training error :1071.362060546875\n",
      "Epoch : 894 Loss: 5357.821989059448 Training error :1069.9730224609375\n",
      "Epoch : 895 Loss: 5350.875577926636 Training error :1068.5848388671875\n",
      "Epoch : 896 Loss: 5343.9345779418945 Training error :1067.1973876953125\n",
      "Epoch : 897 Loss: 5336.996117591858 Training error :1065.8104248046875\n",
      "Epoch : 898 Loss: 5330.061708450317 Training error :1064.424560546875\n",
      "Epoch : 899 Loss: 5323.1312084198 Training error :1063.039306640625\n",
      "Epoch : 900 Loss: 5316.204474449158 Training error :1061.65478515625\n",
      "Epoch : 901 Loss: 5309.280843734741 Training error :1060.2708740234375\n",
      "Epoch : 902 Loss: 5302.360001564026 Training error :1058.8875732421875\n",
      "Epoch : 903 Loss: 5295.444295883179 Training error :1057.505126953125\n",
      "Epoch : 904 Loss: 5288.531255722046 Training error :1056.1231689453125\n",
      "Epoch : 905 Loss: 5281.6752071380615 Training error :1054.752685546875\n",
      "Epoch : 906 Loss: 5274.842216491699 Training error :1053.38525390625\n",
      "Epoch : 907 Loss: 5268.009816169739 Training error :1052.019287109375\n",
      "Epoch : 908 Loss: 5261.181921958923 Training error :1050.6541748046875\n",
      "Epoch : 909 Loss: 5254.355659484863 Training error :1049.289794921875\n",
      "Epoch : 910 Loss: 5247.533740997314 Training error :1047.92626953125\n",
      "Epoch : 911 Loss: 5240.714218139648 Training error :1046.5643310546875\n",
      "Epoch : 912 Loss: 5233.900297164917 Training error :1045.203857421875\n",
      "Epoch : 913 Loss: 5227.094440460205 Training error :1043.843994140625\n",
      "Epoch : 914 Loss: 5220.29522895813 Training error :1042.4849853515625\n",
      "Epoch : 915 Loss: 5213.4976053237915 Training error :1041.12646484375\n",
      "Epoch : 916 Loss: 5206.704837799072 Training error :1039.7686767578125\n",
      "Epoch : 917 Loss: 5199.914826393127 Training error :1038.4112548828125\n",
      "Epoch : 918 Loss: 5193.128999710083 Training error :1037.0545654296875\n",
      "Epoch : 919 Loss: 5186.344202041626 Training error :1035.698486328125\n",
      "Epoch : 920 Loss: 5179.563508033752 Training error :1034.343017578125\n",
      "Epoch : 921 Loss: 5172.785797119141 Training error :1032.99267578125\n",
      "Epoch : 922 Loss: 5166.083681106567 Training error :1031.6490478515625\n",
      "Epoch : 923 Loss: 5159.378980636597 Training error :1030.3077392578125\n",
      "Epoch : 924 Loss: 5152.676394462585 Training error :1028.96728515625\n",
      "Epoch : 925 Loss: 5145.975470542908 Training error :1027.6280517578125\n",
      "Epoch : 926 Loss: 5139.278546333313 Training error :1026.2891845703125\n",
      "Epoch : 927 Loss: 5132.583219528198 Training error :1024.9510498046875\n",
      "Epoch : 928 Loss: 5125.890554428101 Training error :1023.61328125\n",
      "Epoch : 929 Loss: 5119.201071739197 Training error :1022.276123046875\n",
      "Epoch : 930 Loss: 5112.513795852661 Training error :1020.9395141601562\n",
      "Epoch : 931 Loss: 5105.829469680786 Training error :1019.603515625\n",
      "Epoch : 932 Loss: 5099.148122787476 Training error :1018.2678833007812\n",
      "Epoch : 933 Loss: 5092.468688011169 Training error :1016.9329833984375\n",
      "Epoch : 934 Loss: 5085.792900085449 Training error :1015.5982666015625\n",
      "Epoch : 935 Loss: 5079.119403839111 Training error :1014.2640380859375\n",
      "Epoch : 936 Loss: 5072.447790145874 Training error :1012.930419921875\n",
      "Epoch : 937 Loss: 5065.779188156128 Training error :1011.5973510742188\n",
      "Epoch : 938 Loss: 5059.1121616363525 Training error :1010.2645874023438\n",
      "Epoch : 939 Loss: 5052.4472398757935 Training error :1008.9322509765625\n",
      "Epoch : 940 Loss: 5045.829215049744 Training error :1007.6097412109375\n",
      "Epoch : 941 Loss: 5039.236289978027 Training error :1006.2901000976562\n",
      "Epoch : 942 Loss: 5032.640923500061 Training error :1004.9716186523438\n",
      "Epoch : 943 Loss: 5026.04763507843 Training error :1003.653564453125\n",
      "Epoch : 944 Loss: 5019.458212852478 Training error :1002.3362426757812\n",
      "Epoch : 945 Loss: 5012.869010925293 Training error :1001.0193481445312\n",
      "Epoch : 946 Loss: 5006.284012794495 Training error :999.7030639648438\n",
      "Epoch : 947 Loss: 4999.700401306152 Training error :998.3870849609375\n",
      "Epoch : 948 Loss: 4993.118528366089 Training error :997.0717163085938\n",
      "Epoch : 949 Loss: 4986.540214538574 Training error :995.7564697265625\n",
      "Epoch : 950 Loss: 4979.961961746216 Training error :994.44189453125\n",
      "Epoch : 951 Loss: 4973.388078689575 Training error :993.1278686523438\n",
      "Epoch : 952 Loss: 4966.815996170044 Training error :991.8141479492188\n",
      "Epoch : 953 Loss: 4960.246634483337 Training error :990.5010986328125\n",
      "Epoch : 954 Loss: 4953.678337097168 Training error :989.188232421875\n",
      "Epoch : 955 Loss: 4947.113958358765 Training error :987.8757934570312\n",
      "Epoch : 956 Loss: 4940.550883293152 Training error :986.5639038085938\n",
      "Epoch : 957 Loss: 4933.989638328552 Training error :985.2523193359375\n",
      "Epoch : 958 Loss: 4927.431074142456 Training error :983.9414672851562\n",
      "Epoch : 959 Loss: 4920.876574516296 Training error :982.6309204101562\n",
      "Epoch : 960 Loss: 4914.3222007751465 Training error :981.3206787109375\n",
      "Epoch : 961 Loss: 4907.770153999329 Training error :980.0109252929688\n",
      "Epoch : 962 Loss: 4901.221148490906 Training error :978.705078125\n",
      "Epoch : 963 Loss: 4894.738653182983 Training error :977.4052124023438\n",
      "Epoch : 964 Loss: 4888.253709793091 Training error :976.1073608398438\n",
      "Epoch : 965 Loss: 4881.76877784729 Training error :974.81005859375\n",
      "Epoch : 966 Loss: 4875.281901359558 Training error :973.5137329101562\n",
      "Epoch : 967 Loss: 4868.796476364136 Training error :972.2176513671875\n",
      "Epoch : 968 Loss: 4862.31577205658 Training error :970.922119140625\n",
      "Epoch : 969 Loss: 4855.8350076675415 Training error :969.6268920898438\n",
      "Epoch : 970 Loss: 4849.356892585754 Training error :968.3320922851562\n",
      "Epoch : 971 Loss: 4842.882437705994 Training error :967.037841796875\n",
      "Epoch : 972 Loss: 4836.407914161682 Training error :965.7438354492188\n",
      "Epoch : 973 Loss: 4829.935503005981 Training error :964.4503784179688\n",
      "Epoch : 974 Loss: 4823.466884613037 Training error :963.1571044921875\n",
      "Epoch : 975 Loss: 4816.999842643738 Training error :961.8646850585938\n",
      "Epoch : 976 Loss: 4810.535117149353 Training error :960.5726318359375\n",
      "Epoch : 977 Loss: 4804.072685241699 Training error :959.28076171875\n",
      "Epoch : 978 Loss: 4797.612277984619 Training error :957.9896240234375\n",
      "Epoch : 979 Loss: 4791.15415763855 Training error :956.698486328125\n",
      "Epoch : 980 Loss: 4784.69716835022 Training error :955.4078979492188\n",
      "Epoch : 981 Loss: 4778.2426290512085 Training error :954.11767578125\n",
      "Epoch : 982 Loss: 4771.791604042053 Training error :952.827880859375\n",
      "Epoch : 983 Loss: 4765.340232849121 Training error :951.5382690429688\n",
      "Epoch : 984 Loss: 4758.891050338745 Training error :950.2491455078125\n",
      "Epoch : 985 Loss: 4752.444446563721 Training error :948.9603271484375\n",
      "Epoch : 986 Loss: 4745.99854183197 Training error :947.671875\n",
      "Epoch : 987 Loss: 4739.554253578186 Training error :946.3812255859375\n",
      "Epoch : 988 Loss: 4733.102924346924 Training error :945.0865478515625\n",
      "Epoch : 989 Loss: 4726.67878818512 Training error :943.7997436523438\n",
      "Epoch : 990 Loss: 4720.257457733154 Training error :942.51416015625\n",
      "Epoch : 991 Loss: 4713.832788467407 Training error :941.2293701171875\n",
      "Epoch : 992 Loss: 4707.408121109009 Training error :939.9449462890625\n",
      "Epoch : 993 Loss: 4700.9845724105835 Training error :938.6610717773438\n",
      "Epoch : 994 Loss: 4694.562171936035 Training error :937.3778076171875\n",
      "Epoch : 995 Loss: 4688.143179893494 Training error :936.094970703125\n",
      "Epoch : 996 Loss: 4681.726371765137 Training error :934.8125\n",
      "Epoch : 997 Loss: 4675.311871051788 Training error :933.530517578125\n",
      "Epoch : 998 Loss: 4668.89889383316 Training error :932.2491455078125\n",
      "Epoch : 999 Loss: 4662.489263057709 Training error :930.9679565429688\n",
      "Epoch : 1000 Loss: 4656.082064151764 Training error :929.6874389648438\n",
      "Epoch : 1001 Loss: 4649.677087306976 Training error :928.4072875976562\n",
      "Epoch : 1002 Loss: 4643.273433208466 Training error :927.1275634765625\n",
      "Epoch : 1003 Loss: 4636.872815132141 Training error :925.848388671875\n",
      "Epoch : 1004 Loss: 4630.474495887756 Training error :924.5695190429688\n",
      "Epoch : 1005 Loss: 4624.0769028663635 Training error :923.2911987304688\n",
      "Epoch : 1006 Loss: 4617.683411598206 Training error :922.0128784179688\n",
      "Epoch : 1007 Loss: 4611.292339324951 Training error :920.7354125976562\n",
      "Epoch : 1008 Loss: 4604.901926994324 Training error :919.4581298828125\n",
      "Epoch : 1009 Loss: 4598.512818336487 Training error :918.1812744140625\n",
      "Epoch : 1010 Loss: 4592.127863883972 Training error :916.905029296875\n",
      "Epoch : 1011 Loss: 4585.745190143585 Training error :915.6290893554688\n",
      "Epoch : 1012 Loss: 4579.363796710968 Training error :914.3536987304688\n",
      "Epoch : 1013 Loss: 4572.984579086304 Training error :913.0786743164062\n",
      "Epoch : 1014 Loss: 4566.608116149902 Training error :911.8043212890625\n",
      "Epoch : 1015 Loss: 4560.233419418335 Training error :910.5300903320312\n",
      "Epoch : 1016 Loss: 4553.861914634705 Training error :909.2562255859375\n",
      "Epoch : 1017 Loss: 4547.49079990387 Training error :907.9829711914062\n",
      "Epoch : 1018 Loss: 4541.122933387756 Training error :906.7100219726562\n",
      "Epoch : 1019 Loss: 4534.756736278534 Training error :905.4375610351562\n",
      "Epoch : 1020 Loss: 4528.392664909363 Training error :904.1654663085938\n",
      "Epoch : 1021 Loss: 4522.078642368317 Training error :902.9012451171875\n",
      "Epoch : 1022 Loss: 4515.769911766052 Training error :901.6384887695312\n",
      "Epoch : 1023 Loss: 4509.458051681519 Training error :900.3764038085938\n",
      "Epoch : 1024 Loss: 4503.147387981415 Training error :899.114990234375\n",
      "Epoch : 1025 Loss: 4496.838203430176 Training error :897.8543701171875\n",
      "Epoch : 1026 Loss: 4490.531753063202 Training error :896.5935668945312\n",
      "Epoch : 1027 Loss: 4484.225965976715 Training error :895.333740234375\n",
      "Epoch : 1028 Loss: 4477.923160076141 Training error :894.0744018554688\n",
      "Epoch : 1029 Loss: 4471.622400760651 Training error :892.8153076171875\n",
      "Epoch : 1030 Loss: 4465.324150085449 Training error :891.5567016601562\n",
      "Epoch : 1031 Loss: 4459.029298782349 Training error :890.2984619140625\n",
      "Epoch : 1032 Loss: 4452.735030174255 Training error :889.0407104492188\n",
      "Epoch : 1033 Loss: 4446.443444728851 Training error :887.783203125\n",
      "Epoch : 1034 Loss: 4440.1535387039185 Training error :886.5259399414062\n",
      "Epoch : 1035 Loss: 4433.8650460243225 Training error :885.2693481445312\n",
      "Epoch : 1036 Loss: 4427.5805077552795 Training error :884.01318359375\n",
      "Epoch : 1037 Loss: 4421.297069549561 Training error :882.7576293945312\n",
      "Epoch : 1038 Loss: 4415.016304016113 Training error :881.5023193359375\n",
      "Epoch : 1039 Loss: 4408.73851108551 Training error :880.24755859375\n",
      "Epoch : 1040 Loss: 4402.462552547455 Training error :878.9932861328125\n",
      "Epoch : 1041 Loss: 4396.188823223114 Training error :877.7391357421875\n",
      "Epoch : 1042 Loss: 4389.916983127594 Training error :876.4855346679688\n",
      "Epoch : 1043 Loss: 4383.646106719971 Training error :875.2327270507812\n",
      "Epoch : 1044 Loss: 4377.378290653229 Training error :873.979736328125\n",
      "Epoch : 1045 Loss: 4371.1128396987915 Training error :872.7276000976562\n",
      "Epoch : 1046 Loss: 4364.849773406982 Training error :871.4757690429688\n",
      "Epoch : 1047 Loss: 4358.587534427643 Training error :870.2240600585938\n",
      "Epoch : 1048 Loss: 4352.329399585724 Training error :868.973388671875\n",
      "Epoch : 1049 Loss: 4346.072986125946 Training error :867.7228393554688\n",
      "Epoch : 1050 Loss: 4339.819243431091 Training error :866.47265625\n",
      "Epoch : 1051 Loss: 4333.565985202789 Training error :865.22265625\n",
      "Epoch : 1052 Loss: 4327.314158439636 Training error :863.973388671875\n",
      "Epoch : 1053 Loss: 4321.065724372864 Training error :862.724609375\n",
      "Epoch : 1054 Loss: 4314.820133686066 Training error :861.4761962890625\n",
      "Epoch : 1055 Loss: 4308.575496673584 Training error :860.2279663085938\n",
      "Epoch : 1056 Loss: 4302.334790229797 Training error :858.9802856445312\n",
      "Epoch : 1057 Loss: 4296.0943422317505 Training error :857.7332153320312\n",
      "Epoch : 1058 Loss: 4289.855607032776 Training error :856.486328125\n",
      "Epoch : 1059 Loss: 4283.620816707611 Training error :855.2399291992188\n",
      "Epoch : 1060 Loss: 4277.387701511383 Training error :853.9938354492188\n",
      "Epoch : 1061 Loss: 4271.155326843262 Training error :852.7510986328125\n",
      "Epoch : 1062 Loss: 4264.986079216003 Training error :851.5125732421875\n",
      "Epoch : 1063 Loss: 4258.804668903351 Training error :850.2748413085938\n",
      "Epoch : 1064 Loss: 4252.618594169617 Training error :849.0379638671875\n",
      "Epoch : 1065 Loss: 4246.433260440826 Training error :847.8018798828125\n",
      "Epoch : 1066 Loss: 4240.2494559288025 Training error :846.5661010742188\n",
      "Epoch : 1067 Loss: 4234.068118095398 Training error :845.3309326171875\n",
      "Epoch : 1068 Loss: 4227.888836860657 Training error :844.0960083007812\n",
      "Epoch : 1069 Loss: 4221.711443901062 Training error :842.8615112304688\n",
      "Epoch : 1070 Loss: 4215.536056995392 Training error :841.627685546875\n",
      "Epoch : 1071 Loss: 4209.363409042358 Training error :840.394287109375\n",
      "Epoch : 1072 Loss: 4203.193144321442 Training error :839.1612548828125\n",
      "Epoch : 1073 Loss: 4197.025188446045 Training error :837.9286499023438\n",
      "Epoch : 1074 Loss: 4190.860476016998 Training error :836.6964111328125\n",
      "Epoch : 1075 Loss: 4184.6964774131775 Training error :835.46484375\n",
      "Epoch : 1076 Loss: 4178.53534412384 Training error :834.233642578125\n",
      "Epoch : 1077 Loss: 4172.376452445984 Training error :833.0027465820312\n",
      "Epoch : 1078 Loss: 4166.219356536865 Training error :831.7723999023438\n",
      "Epoch : 1079 Loss: 4160.0650181770325 Training error :830.5424194335938\n",
      "Epoch : 1080 Loss: 4153.912505149841 Training error :829.31298828125\n",
      "Epoch : 1081 Loss: 4147.762700080872 Training error :828.08447265625\n",
      "Epoch : 1082 Loss: 4141.614583015442 Training error :826.8573608398438\n",
      "Epoch : 1083 Loss: 4135.474879741669 Training error :825.630859375\n",
      "Epoch : 1084 Loss: 4129.340119361877 Training error :824.4049072265625\n",
      "Epoch : 1085 Loss: 4123.207651138306 Training error :823.1793823242188\n",
      "Epoch : 1086 Loss: 4117.0787563323975 Training error :821.9542236328125\n",
      "Epoch : 1087 Loss: 4110.952317714691 Training error :820.7299194335938\n",
      "Epoch : 1088 Loss: 4104.827517986298 Training error :819.505615234375\n",
      "Epoch : 1089 Loss: 4098.7038230896 Training error :818.2817993164062\n",
      "Epoch : 1090 Loss: 4092.582962989807 Training error :817.0586547851562\n",
      "Epoch : 1091 Loss: 4086.4643754959106 Training error :815.8358764648438\n",
      "Epoch : 1092 Loss: 4080.3485164642334 Training error :814.613525390625\n",
      "Epoch : 1093 Loss: 4074.2357845306396 Training error :813.3915405273438\n",
      "Epoch : 1094 Loss: 4068.1241731643677 Training error :812.1701049804688\n",
      "Epoch : 1095 Loss: 4062.0143637657166 Training error :810.948974609375\n",
      "Epoch : 1096 Loss: 4055.9066467285156 Training error :809.7284545898438\n",
      "Epoch : 1097 Loss: 4049.802065372467 Training error :808.5081176757812\n",
      "Epoch : 1098 Loss: 4043.699487686157 Training error :807.2884521484375\n",
      "Epoch : 1099 Loss: 4037.5990691184998 Training error :806.069091796875\n",
      "Epoch : 1100 Loss: 4031.5007314682007 Training error :804.850341796875\n",
      "Epoch : 1101 Loss: 4025.404061794281 Training error :803.6319580078125\n",
      "Epoch : 1102 Loss: 4019.3112440109253 Training error :802.4140014648438\n",
      "Epoch : 1103 Loss: 4013.219720840454 Training error :801.1964721679688\n",
      "Epoch : 1104 Loss: 4007.130778312683 Training error :799.9795532226562\n",
      "Epoch : 1105 Loss: 4001.044404029846 Training error :798.7630615234375\n",
      "Epoch : 1106 Loss: 3994.9595127105713 Training error :797.547119140625\n",
      "Epoch : 1107 Loss: 3988.8780159950256 Training error :796.3311767578125\n",
      "Epoch : 1108 Loss: 3982.7982902526855 Training error :795.1161499023438\n",
      "Epoch : 1109 Loss: 3976.721707344055 Training error :793.9015502929688\n",
      "Epoch : 1110 Loss: 3970.645887851715 Training error :792.6871337890625\n",
      "Epoch : 1111 Loss: 3964.5723400115967 Training error :791.4734497070312\n",
      "Epoch : 1112 Loss: 3958.5023822784424 Training error :790.2599487304688\n",
      "Epoch : 1113 Loss: 3952.433009147644 Training error :789.0471801757812\n",
      "Epoch : 1114 Loss: 3946.4134244918823 Training error :787.8399047851562\n",
      "Epoch : 1115 Loss: 3940.388466835022 Training error :786.6339721679688\n",
      "Epoch : 1116 Loss: 3934.3596391677856 Training error :785.4285888671875\n",
      "Epoch : 1117 Loss: 3928.3314957618713 Training error :784.2237548828125\n",
      "Epoch : 1118 Loss: 3922.3034048080444 Training error :783.0193481445312\n",
      "Epoch : 1119 Loss: 3916.2793774604797 Training error :781.8155517578125\n",
      "Epoch : 1120 Loss: 3910.2569947242737 Training error :780.6122436523438\n",
      "Epoch : 1121 Loss: 3904.2371492385864 Training error :779.4095458984375\n",
      "Epoch : 1122 Loss: 3898.219307899475 Training error :778.2067260742188\n",
      "Epoch : 1123 Loss: 3892.203257083893 Training error :777.0049438476562\n",
      "Epoch : 1124 Loss: 3886.190528869629 Training error :775.8033447265625\n",
      "Epoch : 1125 Loss: 3880.179690361023 Training error :774.6024169921875\n",
      "Epoch : 1126 Loss: 3874.172842979431 Training error :773.4019775390625\n",
      "Epoch : 1127 Loss: 3868.168334007263 Training error :772.2020874023438\n",
      "Epoch : 1128 Loss: 3862.1653723716736 Training error :771.0025024414062\n",
      "Epoch : 1129 Loss: 3856.164632797241 Training error :769.8035278320312\n",
      "Epoch : 1130 Loss: 3850.1662998199463 Training error :768.604736328125\n",
      "Epoch : 1131 Loss: 3844.170726299286 Training error :767.4068603515625\n",
      "Epoch : 1132 Loss: 3838.1780638694763 Training error :766.2092895507812\n",
      "Epoch : 1133 Loss: 3832.187156200409 Training error :765.0120849609375\n",
      "Epoch : 1134 Loss: 3826.198655128479 Training error :763.8153076171875\n",
      "Epoch : 1135 Loss: 3820.2132501602173 Training error :762.6190795898438\n",
      "Epoch : 1136 Loss: 3814.229790687561 Training error :761.42333984375\n",
      "Epoch : 1137 Loss: 3808.248115539551 Training error :760.2283325195312\n",
      "Epoch : 1138 Loss: 3802.269742488861 Training error :759.0335083007812\n",
      "Epoch : 1139 Loss: 3796.2931699752808 Training error :757.83935546875\n",
      "Epoch : 1140 Loss: 3790.320764541626 Training error :756.6454467773438\n",
      "Epoch : 1141 Loss: 3784.3497819900513 Training error :755.4525146484375\n",
      "Epoch : 1142 Loss: 3778.381770133972 Training error :754.2597045898438\n",
      "Epoch : 1143 Loss: 3772.4151282310486 Training error :753.0673217773438\n",
      "Epoch : 1144 Loss: 3766.451542854309 Training error :751.8755493164062\n",
      "Epoch : 1145 Loss: 3760.4902181625366 Training error :750.684326171875\n",
      "Epoch : 1146 Loss: 3754.531708717346 Training error :749.4932861328125\n",
      "Epoch : 1147 Loss: 3748.5759630203247 Training error :748.30322265625\n",
      "Epoch : 1148 Loss: 3742.621814250946 Training error :747.1135864257812\n",
      "Epoch : 1149 Loss: 3736.670564174652 Training error :745.9243774414062\n",
      "Epoch : 1150 Loss: 3730.7223958969116 Training error :744.7354736328125\n",
      "Epoch : 1151 Loss: 3724.7772641181946 Training error :743.5471801757812\n",
      "Epoch : 1152 Loss: 3718.8336987495422 Training error :742.359619140625\n",
      "Epoch : 1153 Loss: 3712.893210411072 Training error :741.1720581054688\n",
      "Epoch : 1154 Loss: 3706.954363822937 Training error :739.9854736328125\n",
      "Epoch : 1155 Loss: 3701.018623352051 Training error :738.7991333007812\n",
      "Epoch : 1156 Loss: 3695.0847783088684 Training error :737.613525390625\n",
      "Epoch : 1157 Loss: 3689.154148578644 Training error :736.4283447265625\n",
      "Epoch : 1158 Loss: 3683.226378917694 Training error :735.24365234375\n",
      "Epoch : 1159 Loss: 3677.301875114441 Training error :734.0595092773438\n",
      "Epoch : 1160 Loss: 3671.3790640830994 Training error :732.875732421875\n",
      "Epoch : 1161 Loss: 3665.4581537246704 Training error :731.6925048828125\n",
      "Epoch : 1162 Loss: 3659.5396881103516 Training error :730.5099487304688\n",
      "Epoch : 1163 Loss: 3653.6242179870605 Training error :729.3277587890625\n",
      "Epoch : 1164 Loss: 3647.7116322517395 Training error :728.1461181640625\n",
      "Epoch : 1165 Loss: 3641.801636695862 Training error :726.9650268554688\n",
      "Epoch : 1166 Loss: 3635.8946766853333 Training error :725.7845458984375\n",
      "Epoch : 1167 Loss: 3629.9895644187927 Training error :724.6043090820312\n",
      "Epoch : 1168 Loss: 3624.087908267975 Training error :723.4246826171875\n",
      "Epoch : 1169 Loss: 3618.188201904297 Training error :722.2457885742188\n",
      "Epoch : 1170 Loss: 3612.290395259857 Training error :721.067138671875\n",
      "Epoch : 1171 Loss: 3606.395299434662 Training error :719.8889770507812\n",
      "Epoch : 1172 Loss: 3600.503089904785 Training error :718.7115478515625\n",
      "Epoch : 1173 Loss: 3594.613772392273 Training error :717.5343627929688\n",
      "Epoch : 1174 Loss: 3588.726963996887 Training error :716.35791015625\n",
      "Epoch : 1175 Loss: 3582.8418798446655 Training error :715.18212890625\n",
      "Epoch : 1176 Loss: 3576.9613552093506 Training error :714.0067138671875\n",
      "Epoch : 1177 Loss: 3571.082818031311 Training error :712.831787109375\n",
      "Epoch : 1178 Loss: 3565.207236289978 Training error :711.657470703125\n",
      "Epoch : 1179 Loss: 3559.3328557014465 Training error :710.4835815429688\n",
      "Epoch : 1180 Loss: 3553.462543964386 Training error :709.310546875\n",
      "Epoch : 1181 Loss: 3547.5951104164124 Training error :708.1378173828125\n",
      "Epoch : 1182 Loss: 3541.730040073395 Training error :706.9657592773438\n",
      "Epoch : 1183 Loss: 3535.867199897766 Training error :705.7941284179688\n",
      "Epoch : 1184 Loss: 3530.007734298706 Training error :704.6232299804688\n",
      "Epoch : 1185 Loss: 3524.151201725006 Training error :703.4525756835938\n",
      "Epoch : 1186 Loss: 3518.297019481659 Training error :702.2835693359375\n",
      "Epoch : 1187 Loss: 3512.4917516708374 Training error :701.1179809570312\n",
      "Epoch : 1188 Loss: 3506.674229621887 Training error :699.9533081054688\n",
      "Epoch : 1189 Loss: 3500.852650165558 Training error :698.7894287109375\n",
      "Epoch : 1190 Loss: 3495.030957221985 Training error :697.626220703125\n",
      "Epoch : 1191 Loss: 3489.21102809906 Training error :696.4635009765625\n",
      "Epoch : 1192 Loss: 3483.3938093185425 Training error :695.301513671875\n",
      "Epoch : 1193 Loss: 3477.581482887268 Training error :694.1403198242188\n",
      "Epoch : 1194 Loss: 3471.770848274231 Training error :692.9795532226562\n",
      "Epoch : 1195 Loss: 3465.9633169174194 Training error :691.8191528320312\n",
      "Epoch : 1196 Loss: 3460.1588859558105 Training error :690.65966796875\n",
      "Epoch : 1197 Loss: 3454.35683965683 Training error :689.5003662109375\n",
      "Epoch : 1198 Loss: 3448.5581846237183 Training error :688.341796875\n",
      "Epoch : 1199 Loss: 3442.7624621391296 Training error :687.183837890625\n",
      "Epoch : 1200 Loss: 3436.9693994522095 Training error :686.0264282226562\n",
      "Epoch : 1201 Loss: 3431.179762840271 Training error :684.8695068359375\n",
      "Epoch : 1202 Loss: 3425.3914976119995 Training error :683.7131958007812\n",
      "Epoch : 1203 Loss: 3419.6066732406616 Training error :682.5574340820312\n",
      "Epoch : 1204 Loss: 3413.825578689575 Training error :681.4025268554688\n",
      "Epoch : 1205 Loss: 3408.0483598709106 Training error :680.2479858398438\n",
      "Epoch : 1206 Loss: 3402.273293495178 Training error :679.093994140625\n",
      "Epoch : 1207 Loss: 3396.4998030662537 Training error :677.9404907226562\n",
      "Epoch : 1208 Loss: 3390.730630874634 Training error :676.7879028320312\n",
      "Epoch : 1209 Loss: 3384.964771747589 Training error :675.6358032226562\n",
      "Epoch : 1210 Loss: 3379.2009930610657 Training error :674.4838256835938\n",
      "Epoch : 1211 Loss: 3373.43984746933 Training error :673.3329467773438\n",
      "Epoch : 1212 Loss: 3367.681035041809 Training error :672.1824340820312\n",
      "Epoch : 1213 Loss: 3361.9270853996277 Training error :671.0325927734375\n",
      "Epoch : 1214 Loss: 3356.1756315231323 Training error :669.88330078125\n",
      "Epoch : 1215 Loss: 3350.426483154297 Training error :668.734375\n",
      "Epoch : 1216 Loss: 3344.679929256439 Training error :667.5863037109375\n",
      "Epoch : 1217 Loss: 3338.93701171875 Training error :666.4390869140625\n",
      "Epoch : 1218 Loss: 3333.1972308158875 Training error :665.2918701171875\n",
      "Epoch : 1219 Loss: 3327.4609365463257 Training error :664.1456298828125\n",
      "Epoch : 1220 Loss: 3321.7263746261597 Training error :662.9996948242188\n",
      "Epoch : 1221 Loss: 3315.994707584381 Training error :661.8546142578125\n",
      "Epoch : 1222 Loss: 3310.266821861267 Training error :660.7101440429688\n",
      "Epoch : 1223 Loss: 3304.542338371277 Training error :659.5661010742188\n",
      "Epoch : 1224 Loss: 3298.8209013938904 Training error :658.4229125976562\n",
      "Epoch : 1225 Loss: 3293.101451396942 Training error :657.2800903320312\n",
      "Epoch : 1226 Loss: 3287.3849024772644 Training error :656.1378784179688\n",
      "Epoch : 1227 Loss: 3281.67205953598 Training error :654.996337890625\n",
      "Epoch : 1228 Loss: 3275.962586402893 Training error :653.8555908203125\n",
      "Epoch : 1229 Loss: 3270.2565603256226 Training error :652.7152099609375\n",
      "Epoch : 1230 Loss: 3264.552809715271 Training error :651.5756225585938\n",
      "Epoch : 1231 Loss: 3258.8531188964844 Training error :650.4365844726562\n",
      "Epoch : 1232 Loss: 3253.1551628112793 Training error :649.298095703125\n",
      "Epoch : 1233 Loss: 3247.461049556732 Training error :648.1600952148438\n",
      "Epoch : 1234 Loss: 3241.768578529358 Training error :647.0228271484375\n",
      "Epoch : 1235 Loss: 3236.080018043518 Training error :645.8861694335938\n",
      "Epoch : 1236 Loss: 3230.3945817947388 Training error :644.7500610351562\n",
      "Epoch : 1237 Loss: 3224.7120399475098 Training error :643.6145629882812\n",
      "Epoch : 1238 Loss: 3219.0324087142944 Training error :642.4798583984375\n",
      "Epoch : 1239 Loss: 3213.3569927215576 Training error :641.3458862304688\n",
      "Epoch : 1240 Loss: 3207.685128211975 Training error :640.2123413085938\n",
      "Epoch : 1241 Loss: 3202.0145511627197 Training error :639.0792846679688\n",
      "Epoch : 1242 Loss: 3196.3484630584717 Training error :637.947021484375\n",
      "Epoch : 1243 Loss: 3190.68457365036 Training error :636.8151245117188\n",
      "Epoch : 1244 Loss: 3185.0235838890076 Training error :635.6840209960938\n",
      "Epoch : 1245 Loss: 3179.365743637085 Training error :634.5535888671875\n",
      "Epoch : 1246 Loss: 3173.71151638031 Training error :633.4237670898438\n",
      "Epoch : 1247 Loss: 3168.060217857361 Training error :632.2945556640625\n",
      "Epoch : 1248 Loss: 3162.41232585907 Training error :631.1660766601562\n",
      "Epoch : 1249 Loss: 3156.768183708191 Training error :630.0381469726562\n",
      "Epoch : 1250 Loss: 3151.125949382782 Training error :628.9109497070312\n",
      "Epoch : 1251 Loss: 3145.488199710846 Training error :627.784423828125\n",
      "Epoch : 1252 Loss: 3139.853196144104 Training error :626.6582641601562\n",
      "Epoch : 1253 Loss: 3134.2216424942017 Training error :625.5330200195312\n",
      "Epoch : 1254 Loss: 3128.5933656692505 Training error :624.4082641601562\n",
      "Epoch : 1255 Loss: 3122.9679555892944 Training error :623.2841796875\n",
      "Epoch : 1256 Loss: 3117.3452591896057 Training error :622.1607055664062\n",
      "Epoch : 1257 Loss: 3111.725983142853 Training error :621.0379028320312\n",
      "Epoch : 1258 Loss: 3106.1098470687866 Training error :619.9158325195312\n",
      "Epoch : 1259 Loss: 3100.4983673095703 Training error :618.7946166992188\n",
      "Epoch : 1260 Loss: 3094.8899068832397 Training error :617.6736450195312\n",
      "Epoch : 1261 Loss: 3089.284236907959 Training error :616.5535888671875\n",
      "Epoch : 1262 Loss: 3083.6813049316406 Training error :615.4341430664062\n",
      "Epoch : 1263 Loss: 3078.0823607444763 Training error :614.3161010742188\n",
      "Epoch : 1264 Loss: 3072.485384941101 Training error :613.1983642578125\n",
      "Epoch : 1265 Loss: 3066.8987760543823 Training error :612.08154296875\n",
      "Epoch : 1266 Loss: 3061.312312602997 Training error :610.9647827148438\n",
      "Epoch : 1267 Loss: 3055.7276244163513 Training error :609.8488159179688\n",
      "Epoch : 1268 Loss: 3050.1462602615356 Training error :608.7332153320312\n",
      "Epoch : 1269 Loss: 3044.566385746002 Training error :607.6185302734375\n",
      "Epoch : 1270 Loss: 3038.989839553833 Training error :606.5042724609375\n",
      "Epoch : 1271 Loss: 3033.4178323745728 Training error :605.3908081054688\n",
      "Epoch : 1272 Loss: 3027.8486618995667 Training error :604.278076171875\n",
      "Epoch : 1273 Loss: 3022.2834434509277 Training error :603.166015625\n",
      "Epoch : 1274 Loss: 3016.7209057807922 Training error :602.0546264648438\n",
      "Epoch : 1275 Loss: 3011.162148475647 Training error :600.9440307617188\n",
      "Epoch : 1276 Loss: 3005.607280254364 Training error :599.8338012695312\n",
      "Epoch : 1277 Loss: 3000.0550775527954 Training error :598.7244262695312\n",
      "Epoch : 1278 Loss: 2994.5060782432556 Training error :597.6156005859375\n",
      "Epoch : 1279 Loss: 2988.9605021476746 Training error :596.5077514648438\n",
      "Epoch : 1280 Loss: 2983.418978214264 Training error :595.4005126953125\n",
      "Epoch : 1281 Loss: 2977.88099193573 Training error :594.2937622070312\n",
      "Epoch : 1282 Loss: 2972.345099925995 Training error :593.18798828125\n",
      "Epoch : 1283 Loss: 2966.813705444336 Training error :592.08251953125\n",
      "Epoch : 1284 Loss: 2961.285324573517 Training error :590.9779052734375\n",
      "Epoch : 1285 Loss: 2955.7611498832703 Training error :589.8740234375\n",
      "Epoch : 1286 Loss: 2950.2395277023315 Training error :588.770751953125\n",
      "Epoch : 1287 Loss: 2944.7213373184204 Training error :587.668212890625\n",
      "Epoch : 1288 Loss: 2939.206822872162 Training error :586.5662231445312\n",
      "Epoch : 1289 Loss: 2933.69530582428 Training error :585.4652099609375\n",
      "Epoch : 1290 Loss: 2928.188153743744 Training error :584.3649291992188\n",
      "Epoch : 1291 Loss: 2922.6842246055603 Training error :583.2650756835938\n",
      "Epoch : 1292 Loss: 2917.1843767166138 Training error :582.1659545898438\n",
      "Epoch : 1293 Loss: 2911.6874084472656 Training error :581.0675659179688\n",
      "Epoch : 1294 Loss: 2906.193500995636 Training error :579.9700927734375\n",
      "Epoch : 1295 Loss: 2900.7043781280518 Training error :578.8729858398438\n",
      "Epoch : 1296 Loss: 2895.2171659469604 Training error :577.7771606445312\n",
      "Epoch : 1297 Loss: 2889.7740001678467 Training error :576.6842651367188\n",
      "Epoch : 1298 Loss: 2884.3192472457886 Training error :575.5925903320312\n",
      "Epoch : 1299 Loss: 2878.861081600189 Training error :574.50146484375\n",
      "Epoch : 1300 Loss: 2873.403480529785 Training error :573.4110717773438\n",
      "Epoch : 1301 Loss: 2867.948467731476 Training error :572.321533203125\n",
      "Epoch : 1302 Loss: 2862.4969396591187 Training error :571.2326049804688\n",
      "Epoch : 1303 Loss: 2857.0495839118958 Training error :570.1445922851562\n",
      "Epoch : 1304 Loss: 2851.605887413025 Training error :569.057373046875\n",
      "Epoch : 1305 Loss: 2846.165735244751 Training error :567.9714965820312\n",
      "Epoch : 1306 Loss: 2840.729037284851 Training error :566.8866577148438\n",
      "Epoch : 1307 Loss: 2835.300919532776 Training error :565.802978515625\n",
      "Epoch : 1308 Loss: 2829.8787031173706 Training error :564.7198486328125\n",
      "Epoch : 1309 Loss: 2824.461040496826 Training error :563.6376342773438\n",
      "Epoch : 1310 Loss: 2819.0470309257507 Training error :562.55615234375\n",
      "Epoch : 1311 Loss: 2813.6354031562805 Training error :561.4752197265625\n",
      "Epoch : 1312 Loss: 2808.2300004959106 Training error :560.395263671875\n",
      "Epoch : 1313 Loss: 2802.826705932617 Training error :559.3161010742188\n",
      "Epoch : 1314 Loss: 2797.4291582107544 Training error :558.2374877929688\n",
      "Epoch : 1315 Loss: 2792.0339765548706 Training error :557.1598510742188\n",
      "Epoch : 1316 Loss: 2786.641927242279 Training error :556.083251953125\n",
      "Epoch : 1317 Loss: 2781.2527570724487 Training error :555.0078735351562\n",
      "Epoch : 1318 Loss: 2775.8758249282837 Training error :553.9326782226562\n",
      "Epoch : 1319 Loss: 2770.499726295471 Training error :552.858154296875\n",
      "Epoch : 1320 Loss: 2765.1249079704285 Training error :551.7839965820312\n",
      "Epoch : 1321 Loss: 2759.752622127533 Training error :550.7106323242188\n",
      "Epoch : 1322 Loss: 2754.3833475112915 Training error :549.63818359375\n",
      "Epoch : 1323 Loss: 2749.018557548523 Training error :548.5663452148438\n",
      "Epoch : 1324 Loss: 2743.6576352119446 Training error :547.495361328125\n",
      "Epoch : 1325 Loss: 2738.299901485443 Training error :546.4248657226562\n",
      "Epoch : 1326 Loss: 2732.9457206726074 Training error :545.3554077148438\n",
      "Epoch : 1327 Loss: 2727.5960264205933 Training error :544.2864990234375\n",
      "Epoch : 1328 Loss: 2722.2501883506775 Training error :543.2186889648438\n",
      "Epoch : 1329 Loss: 2716.9084634780884 Training error :542.1513671875\n",
      "Epoch : 1330 Loss: 2711.5696091651917 Training error :541.0847778320312\n",
      "Epoch : 1331 Loss: 2706.2353167533875 Training error :540.01904296875\n",
      "Epoch : 1332 Loss: 2700.90438747406 Training error :538.9541015625\n",
      "Epoch : 1333 Loss: 2695.5771355628967 Training error :537.8897094726562\n",
      "Epoch : 1334 Loss: 2690.2531909942627 Training error :536.8263549804688\n",
      "Epoch : 1335 Loss: 2684.93368768692 Training error :535.7634887695312\n",
      "Epoch : 1336 Loss: 2679.6179990768433 Training error :534.7017822265625\n",
      "Epoch : 1337 Loss: 2674.307476043701 Training error :533.6405639648438\n",
      "Epoch : 1338 Loss: 2668.999409675598 Training error :532.5801391601562\n",
      "Epoch : 1339 Loss: 2663.695574760437 Training error :531.5205688476562\n",
      "Epoch : 1340 Loss: 2658.3960886001587 Training error :530.4618530273438\n",
      "Epoch : 1341 Loss: 2653.100067615509 Training error :529.4036865234375\n",
      "Epoch : 1342 Loss: 2647.8082089424133 Training error :528.346435546875\n",
      "Epoch : 1343 Loss: 2642.519317626953 Training error :527.2900390625\n",
      "Epoch : 1344 Loss: 2637.2356185913086 Training error :526.2344360351562\n",
      "Epoch : 1345 Loss: 2631.9561862945557 Training error :525.1795043945312\n",
      "Epoch : 1346 Loss: 2626.679377555847 Training error :524.1254272460938\n",
      "Epoch : 1347 Loss: 2621.407573699951 Training error :523.0722045898438\n",
      "Epoch : 1348 Loss: 2616.1386246681213 Training error :522.0195922851562\n",
      "Epoch : 1349 Loss: 2610.873875141144 Training error :520.9680786132812\n",
      "Epoch : 1350 Loss: 2605.6145510673523 Training error :519.9169921875\n",
      "Epoch : 1351 Loss: 2600.3578724861145 Training error :518.8670043945312\n",
      "Epoch : 1352 Loss: 2595.105936527252 Training error :517.817626953125\n",
      "Epoch : 1353 Loss: 2589.8564529418945 Training error :516.768798828125\n",
      "Epoch : 1354 Loss: 2584.6121096611023 Training error :515.72119140625\n",
      "Epoch : 1355 Loss: 2579.3713579177856 Training error :514.67431640625\n",
      "Epoch : 1356 Loss: 2574.134669780731 Training error :513.6280517578125\n",
      "Epoch : 1357 Loss: 2568.901810646057 Training error :512.58447265625\n",
      "Epoch : 1358 Loss: 2563.6778535842896 Training error :511.53948974609375\n",
      "Epoch : 1359 Loss: 2558.4538531303406 Training error :510.4960632324219\n",
      "Epoch : 1360 Loss: 2553.2367277145386 Training error :509.45330810546875\n",
      "Epoch : 1361 Loss: 2548.0207352638245 Training error :508.4111328125\n",
      "Epoch : 1362 Loss: 2542.808334827423 Training error :507.3697204589844\n",
      "Epoch : 1363 Loss: 2537.6006631851196 Training error :506.32928466796875\n",
      "Epoch : 1364 Loss: 2532.3953053951263 Training error :505.2893371582031\n",
      "Epoch : 1365 Loss: 2527.1946544647217 Training error :504.2506103515625\n",
      "Epoch : 1366 Loss: 2521.999344587326 Training error :503.21246337890625\n",
      "Epoch : 1367 Loss: 2516.8063094615936 Training error :502.1751708984375\n",
      "Epoch : 1368 Loss: 2511.618896961212 Training error :501.1387023925781\n",
      "Epoch : 1369 Loss: 2506.434328556061 Training error :500.10321044921875\n",
      "Epoch : 1370 Loss: 2501.255028486252 Training error :499.0682067871094\n",
      "Epoch : 1371 Loss: 2496.0786752700806 Training error :498.0342712402344\n",
      "Epoch : 1372 Loss: 2490.907147169113 Training error :497.0009460449219\n",
      "Epoch : 1373 Loss: 2485.739274740219 Training error :495.9687805175781\n",
      "Epoch : 1374 Loss: 2480.575833082199 Training error :494.93731689453125\n",
      "Epoch : 1375 Loss: 2475.4165654182434 Training error :493.90655517578125\n",
      "Epoch : 1376 Loss: 2470.2616374492645 Training error :492.87677001953125\n",
      "Epoch : 1377 Loss: 2465.1112937927246 Training error :491.8476257324219\n",
      "Epoch : 1378 Loss: 2459.9642939567566 Training error :490.819580078125\n",
      "Epoch : 1379 Loss: 2454.822689294815 Training error :489.7925109863281\n",
      "Epoch : 1380 Loss: 2449.684464454651 Training error :488.76593017578125\n",
      "Epoch : 1381 Loss: 2444.5505039691925 Training error :487.740478515625\n",
      "Epoch : 1382 Loss: 2439.4212584495544 Training error :486.715576171875\n",
      "Epoch : 1383 Loss: 2434.2954335212708 Training error :485.6917724609375\n",
      "Epoch : 1384 Loss: 2429.1744134426117 Training error :484.6685485839844\n",
      "Epoch : 1385 Loss: 2424.0572679042816 Training error :483.6463623046875\n",
      "Epoch : 1386 Loss: 2418.945369243622 Training error :482.6250915527344\n",
      "Epoch : 1387 Loss: 2413.83629488945 Training error :481.6044921875\n",
      "Epoch : 1388 Loss: 2408.7313919067383 Training error :480.58477783203125\n",
      "Epoch : 1389 Loss: 2403.6313922405243 Training error :479.56573486328125\n",
      "Epoch : 1390 Loss: 2398.5350806713104 Training error :478.5478210449219\n",
      "Epoch : 1391 Loss: 2393.4434962272644 Training error :477.53045654296875\n",
      "Epoch : 1392 Loss: 2388.3560180664062 Training error :476.5142822265625\n",
      "Epoch : 1393 Loss: 2383.2693309783936 Training error :475.4999694824219\n",
      "Epoch : 1394 Loss: 2378.198348760605 Training error :474.4857482910156\n",
      "Epoch : 1395 Loss: 2373.1276557445526 Training error :473.47222900390625\n",
      "Epoch : 1396 Loss: 2368.058227777481 Training error :472.4593505859375\n",
      "Epoch : 1397 Loss: 2362.9927563667297 Training error :471.44744873046875\n",
      "Epoch : 1398 Loss: 2357.930997133255 Training error :470.4362487792969\n",
      "Epoch : 1399 Loss: 2352.87321805954 Training error :469.426025390625\n",
      "Epoch : 1400 Loss: 2347.8201298713684 Training error :468.4163818359375\n",
      "Epoch : 1401 Loss: 2342.770850419998 Training error :467.4079284667969\n",
      "Epoch : 1402 Loss: 2337.726588487625 Training error :466.4001159667969\n",
      "Epoch : 1403 Loss: 2332.686691761017 Training error :465.39337158203125\n",
      "Epoch : 1404 Loss: 2327.6511278152466 Training error :464.3872985839844\n",
      "Epoch : 1405 Loss: 2322.6202800273895 Training error :463.3824157714844\n",
      "Epoch : 1406 Loss: 2317.5930466651917 Training error :462.37823486328125\n",
      "Epoch : 1407 Loss: 2312.571468114853 Training error :461.375\n",
      "Epoch : 1408 Loss: 2307.5529532432556 Training error :460.3724670410156\n",
      "Epoch : 1409 Loss: 2302.5395374298096 Training error :459.3710632324219\n",
      "Epoch : 1410 Loss: 2297.5299701690674 Training error :458.37042236328125\n",
      "Epoch : 1411 Loss: 2292.525240421295 Training error :457.3708190917969\n",
      "Epoch : 1412 Loss: 2287.525270462036 Training error :456.3719177246094\n",
      "Epoch : 1413 Loss: 2282.5300710201263 Training error :455.3741455078125\n",
      "Epoch : 1414 Loss: 2277.539278268814 Training error :454.3771057128906\n",
      "Epoch : 1415 Loss: 2272.5532336235046 Training error :453.3808898925781\n",
      "Epoch : 1416 Loss: 2267.5706222057343 Training error :452.3857116699219\n",
      "Epoch : 1417 Loss: 2262.5930218696594 Training error :451.391357421875\n",
      "Epoch : 1418 Loss: 2257.6199486255646 Training error :450.39776611328125\n",
      "Epoch : 1419 Loss: 2252.650294780731 Training error :449.4053039550781\n",
      "Epoch : 1420 Loss: 2247.6867241859436 Training error :448.413818359375\n",
      "Epoch : 1421 Loss: 2242.72678232193 Training error :447.4229736328125\n",
      "Epoch : 1422 Loss: 2237.7715237140656 Training error :446.4334716796875\n",
      "Epoch : 1423 Loss: 2232.8221521377563 Training error :445.4444580078125\n",
      "Epoch : 1424 Loss: 2227.8761026859283 Training error :444.4564514160156\n",
      "Epoch : 1425 Loss: 2222.934830188751 Training error :443.4694519042969\n",
      "Epoch : 1426 Loss: 2217.9942989349365 Training error :442.48455810546875\n",
      "Epoch : 1427 Loss: 2213.0707244873047 Training error :441.4996337890625\n",
      "Epoch : 1428 Loss: 2208.1458353996277 Training error :440.5154724121094\n",
      "Epoch : 1429 Loss: 2203.2236580848694 Training error :439.5322265625\n",
      "Epoch : 1430 Loss: 2198.305879354477 Training error :438.5497131347656\n",
      "Epoch : 1431 Loss: 2193.3919644355774 Training error :437.5682678222656\n",
      "Epoch : 1432 Loss: 2188.483861207962 Training error :436.5876159667969\n",
      "Epoch : 1433 Loss: 2183.578417301178 Training error :435.6079406738281\n",
      "Epoch : 1434 Loss: 2178.6795041561127 Training error :434.6293640136719\n",
      "Epoch : 1435 Loss: 2173.784096956253 Training error :433.6513977050781\n",
      "Epoch : 1436 Loss: 2168.8933658599854 Training error :432.6746520996094\n",
      "Epoch : 1437 Loss: 2164.0073907375336 Training error :431.6986389160156\n",
      "Epoch : 1438 Loss: 2159.1252262592316 Training error :430.7236022949219\n",
      "Epoch : 1439 Loss: 2154.249782562256 Training error :429.74969482421875\n",
      "Epoch : 1440 Loss: 2149.3783164024353 Training error :428.7763977050781\n",
      "Epoch : 1441 Loss: 2144.5106995105743 Training error :427.80426025390625\n",
      "Epoch : 1442 Loss: 2139.6482899188995 Training error :426.8331298828125\n",
      "Epoch : 1443 Loss: 2134.7907910346985 Training error :425.86297607421875\n",
      "Epoch : 1444 Loss: 2129.9386377334595 Training error :424.89373779296875\n",
      "Epoch : 1445 Loss: 2125.091020822525 Training error :423.9252624511719\n",
      "Epoch : 1446 Loss: 2120.247231245041 Training error :422.9578857421875\n",
      "Epoch : 1447 Loss: 2115.4088521003723 Training error :421.9915771484375\n",
      "Epoch : 1448 Loss: 2110.5749917030334 Training error :421.0258483886719\n",
      "Epoch : 1449 Loss: 2105.7453956604004 Training error :420.0614318847656\n",
      "Epoch : 1450 Loss: 2100.922147989273 Training error :419.0980224609375\n",
      "Epoch : 1451 Loss: 2096.103625059128 Training error :418.13543701171875\n",
      "Epoch : 1452 Loss: 2091.2889885902405 Training error :417.1737976074219\n",
      "Epoch : 1453 Loss: 2086.479885816574 Training error :416.21331787109375\n",
      "Epoch : 1454 Loss: 2081.6747946739197 Training error :415.2535095214844\n",
      "Epoch : 1455 Loss: 2076.874662399292 Training error :414.2947998046875\n",
      "Epoch : 1456 Loss: 2072.0801331996918 Training error :413.33721923828125\n",
      "Epoch : 1457 Loss: 2067.290378808975 Training error :412.381591796875\n",
      "Epoch : 1458 Loss: 2062.5070884227753 Training error :411.4256591796875\n",
      "Epoch : 1459 Loss: 2057.728929042816 Training error :410.4713134765625\n",
      "Epoch : 1460 Loss: 2052.9560680389404 Training error :409.517578125\n",
      "Epoch : 1461 Loss: 2048.1855340003967 Training error :408.564697265625\n",
      "Epoch : 1462 Loss: 2043.4205870628357 Training error :407.6128845214844\n",
      "Epoch : 1463 Loss: 2038.6591517925262 Training error :406.66192626953125\n",
      "Epoch : 1464 Loss: 2033.902868270874 Training error :405.7119445800781\n",
      "Epoch : 1465 Loss: 2029.1518368721008 Training error :404.76287841796875\n",
      "Epoch : 1466 Loss: 2024.4059734344482 Training error :403.81512451171875\n",
      "Epoch : 1467 Loss: 2019.6645419597626 Training error :402.8681640625\n",
      "Epoch : 1468 Loss: 2014.9284358024597 Training error :401.922119140625\n",
      "Epoch : 1469 Loss: 2010.1966581344604 Training error :400.9772644042969\n",
      "Epoch : 1470 Loss: 2005.4706633090973 Training error :400.0332946777344\n",
      "Epoch : 1471 Loss: 2000.74968957901 Training error :399.0903625488281\n",
      "Epoch : 1472 Loss: 1996.033498287201 Training error :398.1484069824219\n",
      "Epoch : 1473 Loss: 1991.3227171897888 Training error :397.20770263671875\n",
      "Epoch : 1474 Loss: 1986.6170363426208 Training error :396.267822265625\n",
      "Epoch : 1475 Loss: 1981.9158778190613 Training error :395.3289489746094\n",
      "Epoch : 1476 Loss: 1977.2203350067139 Training error :394.3910217285156\n",
      "Epoch : 1477 Loss: 1972.529619693756 Training error :393.4542541503906\n",
      "Epoch : 1478 Loss: 1967.8440115451813 Training error :392.5185241699219\n",
      "Epoch : 1479 Loss: 1963.1638190746307 Training error :391.5838317871094\n",
      "Epoch : 1480 Loss: 1958.4892172813416 Training error :390.6498718261719\n",
      "Epoch : 1481 Loss: 1953.8183600902557 Training error :389.7172546386719\n",
      "Epoch : 1482 Loss: 1949.1536107063293 Training error :388.7855529785156\n",
      "Epoch : 1483 Loss: 1944.4929583072662 Training error :387.8548889160156\n",
      "Epoch : 1484 Loss: 1939.8387641906738 Training error :386.9251708984375\n",
      "Epoch : 1485 Loss: 1935.187987089157 Training error :385.9964599609375\n",
      "Epoch : 1486 Loss: 1930.5433387756348 Training error :385.0687561035156\n",
      "Epoch : 1487 Loss: 1925.903297662735 Training error :384.14300537109375\n",
      "Epoch : 1488 Loss: 1921.2691321372986 Training error :383.2177429199219\n",
      "Epoch : 1489 Loss: 1916.644206047058 Training error :382.29351806640625\n",
      "Epoch : 1490 Loss: 1912.0228247642517 Training error :381.3701477050781\n",
      "Epoch : 1491 Loss: 1907.4329028129578 Training error :380.4487609863281\n",
      "Epoch : 1492 Loss: 1902.832840204239 Training error :379.5285949707031\n",
      "Epoch : 1493 Loss: 1898.2316632270813 Training error :378.6094665527344\n",
      "Epoch : 1494 Loss: 1893.6344797611237 Training error :377.6915588378906\n",
      "Epoch : 1495 Loss: 1889.0418815612793 Training error :376.7745361328125\n",
      "Epoch : 1496 Loss: 1884.4538655281067 Training error :375.8588562011719\n",
      "Epoch : 1497 Loss: 1879.8721852302551 Training error :374.9441833496094\n",
      "Epoch : 1498 Loss: 1875.295266866684 Training error :374.030517578125\n",
      "Epoch : 1499 Loss: 1870.7236948013306 Training error :373.1180114746094\n",
      "Epoch : 1500 Loss: 1866.1578660011292 Training error :372.20648193359375\n",
      "Epoch : 1501 Loss: 1861.5978910923004 Training error :371.2959289550781\n",
      "Epoch : 1502 Loss: 1857.0425791740417 Training error :370.38671875\n",
      "Epoch : 1503 Loss: 1852.4947638511658 Training error :369.4784851074219\n",
      "Epoch : 1504 Loss: 1847.9505004882812 Training error :368.57147216796875\n",
      "Epoch : 1505 Loss: 1843.4126431941986 Training error :367.6651916503906\n",
      "Epoch : 1506 Loss: 1838.8790230751038 Training error :366.76019287109375\n",
      "Epoch : 1507 Loss: 1834.3512279987335 Training error :365.856201171875\n",
      "Epoch : 1508 Loss: 1829.8295335769653 Training error :364.953369140625\n",
      "Epoch : 1509 Loss: 1825.312450170517 Training error :364.051513671875\n",
      "Epoch : 1510 Loss: 1820.8011860847473 Training error :363.1507568359375\n",
      "Epoch : 1511 Loss: 1816.2949719429016 Training error :362.251220703125\n",
      "Epoch : 1512 Loss: 1811.7957904338837 Training error :361.3528137207031\n",
      "Epoch : 1513 Loss: 1807.3010334968567 Training error :360.455322265625\n",
      "Epoch : 1514 Loss: 1802.8118097782135 Training error :359.55902099609375\n",
      "Epoch : 1515 Loss: 1798.3281388282776 Training error :358.6649475097656\n",
      "Epoch : 1516 Loss: 1793.8527643680573 Training error :357.7706604003906\n",
      "Epoch : 1517 Loss: 1789.3811814785004 Training error :356.8779296875\n",
      "Epoch : 1518 Loss: 1784.916515827179 Training error :355.98602294921875\n",
      "Epoch : 1519 Loss: 1780.455649137497 Training error :355.0951843261719\n",
      "Epoch : 1520 Loss: 1775.9997906684875 Training error :354.20550537109375\n",
      "Epoch : 1521 Loss: 1771.549156665802 Training error :353.31683349609375\n",
      "Epoch : 1522 Loss: 1767.1035187244415 Training error :352.4292907714844\n",
      "Epoch : 1523 Loss: 1762.6638944149017 Training error :351.5428161621094\n",
      "Epoch : 1524 Loss: 1758.2297620773315 Training error :350.657470703125\n",
      "Epoch : 1525 Loss: 1753.8014352321625 Training error :349.7733154296875\n",
      "Epoch : 1526 Loss: 1749.3791027069092 Training error :348.89019775390625\n",
      "Epoch : 1527 Loss: 1744.9616374969482 Training error :348.00836181640625\n",
      "Epoch : 1528 Loss: 1740.5501520633698 Training error :347.12750244140625\n",
      "Epoch : 1529 Loss: 1736.1443810462952 Training error :346.24786376953125\n",
      "Epoch : 1530 Loss: 1731.7449219226837 Training error :345.3692932128906\n",
      "Epoch : 1531 Loss: 1727.3504402637482 Training error :344.49200439453125\n",
      "Epoch : 1532 Loss: 1722.9620440006256 Training error :343.6156005859375\n",
      "Epoch : 1533 Loss: 1718.5783269405365 Training error :342.74041748046875\n",
      "Epoch : 1534 Loss: 1714.2005143165588 Training error :341.86639404296875\n",
      "Epoch : 1535 Loss: 1709.8279855251312 Training error :340.9935302734375\n",
      "Epoch : 1536 Loss: 1705.4618647098541 Training error :340.1217956542969\n",
      "Epoch : 1537 Loss: 1701.1014938354492 Training error :339.25115966796875\n",
      "Epoch : 1538 Loss: 1696.7464661598206 Training error :338.38153076171875\n",
      "Epoch : 1539 Loss: 1692.3968319892883 Training error :337.5132141113281\n",
      "Epoch : 1540 Loss: 1688.053079366684 Training error :336.64593505859375\n",
      "Epoch : 1541 Loss: 1683.71568775177 Training error :335.7799072265625\n",
      "Epoch : 1542 Loss: 1679.3839461803436 Training error :334.91619873046875\n",
      "Epoch : 1543 Loss: 1675.060212612152 Training error :334.0523376464844\n",
      "Epoch : 1544 Loss: 1670.7425274848938 Training error :333.1899719238281\n",
      "Epoch : 1545 Loss: 1666.4299993515015 Training error :332.32855224609375\n",
      "Epoch : 1546 Loss: 1662.1214663982391 Training error :331.4682312011719\n",
      "Epoch : 1547 Loss: 1657.8182101249695 Training error :330.6092529296875\n",
      "Epoch : 1548 Loss: 1653.5212607383728 Training error :329.75115966796875\n",
      "Epoch : 1549 Loss: 1649.22935628891 Training error :328.89434814453125\n",
      "Epoch : 1550 Loss: 1644.9430527687073 Training error :328.0385437011719\n",
      "Epoch : 1551 Loss: 1640.6628997325897 Training error :327.1841125488281\n",
      "Epoch : 1552 Loss: 1636.3886420726776 Training error :326.3306884765625\n",
      "Epoch : 1553 Loss: 1632.1199362277985 Training error :325.4784240722656\n",
      "Epoch : 1554 Loss: 1627.8576102256775 Training error :324.62744140625\n",
      "Epoch : 1555 Loss: 1623.6007714271545 Training error :323.777587890625\n",
      "Epoch : 1556 Loss: 1619.349824666977 Training error :322.92901611328125\n",
      "Epoch : 1557 Loss: 1615.1049499511719 Training error :322.08135986328125\n",
      "Epoch : 1558 Loss: 1610.8653111457825 Training error :321.2349548339844\n",
      "Epoch : 1559 Loss: 1606.6320984363556 Training error :320.3897705078125\n",
      "Epoch : 1560 Loss: 1602.4046740531921 Training error :319.5456237792969\n",
      "Epoch : 1561 Loss: 1598.182722568512 Training error :318.702880859375\n",
      "Epoch : 1562 Loss: 1593.967167377472 Training error :317.8613586425781\n",
      "Epoch : 1563 Loss: 1589.7585780620575 Training error :317.0208740234375\n",
      "Epoch : 1564 Loss: 1585.5544936656952 Training error :316.1816101074219\n",
      "Epoch : 1565 Loss: 1581.3564190864563 Training error :315.3436279296875\n",
      "Epoch : 1566 Loss: 1577.1653351783752 Training error :314.5067443847656\n",
      "Epoch : 1567 Loss: 1572.9790315628052 Training error :313.6711120605469\n",
      "Epoch : 1568 Loss: 1568.7993748188019 Training error :312.8368225097656\n",
      "Epoch : 1569 Loss: 1564.6235797405243 Training error :312.0043029785156\n",
      "Epoch : 1570 Loss: 1560.4618830680847 Training error :311.1725769042969\n",
      "Epoch : 1571 Loss: 1556.3013672828674 Training error :310.341796875\n",
      "Epoch : 1572 Loss: 1552.1470804214478 Training error :309.5123291015625\n",
      "Epoch : 1573 Loss: 1547.9981489181519 Training error :308.68377685546875\n",
      "Epoch : 1574 Loss: 1543.8537805080414 Training error :307.8564758300781\n",
      "Epoch : 1575 Loss: 1539.716735124588 Training error :307.0304260253906\n",
      "Epoch : 1576 Loss: 1535.5849883556366 Training error :306.2055358886719\n",
      "Epoch : 1577 Loss: 1531.4584422111511 Training error :305.38201904296875\n",
      "Epoch : 1578 Loss: 1527.339077949524 Training error :304.5594787597656\n",
      "Epoch : 1579 Loss: 1523.2255489826202 Training error :303.7383728027344\n",
      "Epoch : 1580 Loss: 1519.1182639598846 Training error :302.9183349609375\n",
      "Epoch : 1581 Loss: 1515.0165305137634 Training error :302.0995178222656\n",
      "Epoch : 1582 Loss: 1510.9212424755096 Training error :301.28204345703125\n",
      "Epoch : 1583 Loss: 1506.8323764801025 Training error :300.4656066894531\n",
      "Epoch : 1584 Loss: 1502.7488481998444 Training error :299.65057373046875\n",
      "Epoch : 1585 Loss: 1498.6716418266296 Training error :298.83660888671875\n",
      "Epoch : 1586 Loss: 1494.6001229286194 Training error :298.0238952636719\n",
      "Epoch : 1587 Loss: 1490.5351116657257 Training error :297.2123107910156\n",
      "Epoch : 1588 Loss: 1486.4764363765717 Training error :296.40203857421875\n",
      "Epoch : 1589 Loss: 1482.424060344696 Training error :295.5929260253906\n",
      "Epoch : 1590 Loss: 1478.3770866394043 Training error :294.7851257324219\n",
      "Epoch : 1591 Loss: 1474.3367111682892 Training error :293.9786071777344\n",
      "Epoch : 1592 Loss: 1470.3019461631775 Training error :293.17315673828125\n",
      "Epoch : 1593 Loss: 1466.2734434604645 Training error :292.3690490722656\n",
      "Epoch : 1594 Loss: 1462.2519059181213 Training error :291.56634521484375\n",
      "Epoch : 1595 Loss: 1458.2336769104004 Training error :290.76556396484375\n",
      "Epoch : 1596 Loss: 1454.2307229042053 Training error :289.965576171875\n",
      "Epoch : 1597 Loss: 1450.2303037643433 Training error :289.16656494140625\n",
      "Epoch : 1598 Loss: 1446.234007358551 Training error :288.3686828613281\n",
      "Epoch : 1599 Loss: 1442.2431654930115 Training error :287.5719909667969\n",
      "Epoch : 1600 Loss: 1438.2582936286926 Training error :286.7764892578125\n",
      "Epoch : 1601 Loss: 1434.2793200016022 Training error :285.9822998046875\n",
      "Epoch : 1602 Loss: 1430.3066651821136 Training error :285.1893615722656\n",
      "Epoch : 1603 Loss: 1426.3407225608826 Training error :284.3976745605469\n",
      "Epoch : 1604 Loss: 1422.381049156189 Training error :283.6071472167969\n",
      "Epoch : 1605 Loss: 1418.4272661209106 Training error :282.8178405761719\n",
      "Epoch : 1606 Loss: 1414.4792606830597 Training error :282.02978515625\n",
      "Epoch : 1607 Loss: 1410.5378503799438 Training error :281.24285888671875\n",
      "Epoch : 1608 Loss: 1406.6025047302246 Training error :280.4574279785156\n",
      "Epoch : 1609 Loss: 1402.673172235489 Training error :279.67303466796875\n",
      "Epoch : 1610 Loss: 1398.7502784729004 Training error :278.8899841308594\n",
      "Epoch : 1611 Loss: 1394.8335545063019 Training error :278.1082763671875\n",
      "Epoch : 1612 Loss: 1390.9229860305786 Training error :277.3277893066406\n",
      "Epoch : 1613 Loss: 1387.0198106765747 Training error :276.5487060546875\n",
      "Epoch : 1614 Loss: 1383.1229500770569 Training error :275.7706298828125\n",
      "Epoch : 1615 Loss: 1379.231243610382 Training error :274.99395751953125\n",
      "Epoch : 1616 Loss: 1375.3469014167786 Training error :274.2184753417969\n",
      "Epoch : 1617 Loss: 1371.4680979251862 Training error :273.4442138671875\n",
      "Epoch : 1618 Loss: 1367.5955729484558 Training error :272.6713562011719\n",
      "Epoch : 1619 Loss: 1363.7296302318573 Training error :271.89959716796875\n",
      "Epoch : 1620 Loss: 1359.8697361946106 Training error :271.12945556640625\n",
      "Epoch : 1621 Loss: 1356.0149765014648 Training error :270.3610534667969\n",
      "Epoch : 1622 Loss: 1352.1736707687378 Training error :269.5936279296875\n",
      "Epoch : 1623 Loss: 1348.3356370925903 Training error :268.82708740234375\n",
      "Epoch : 1624 Loss: 1344.5025553703308 Training error :268.06182861328125\n",
      "Epoch : 1625 Loss: 1340.6746542453766 Training error :267.29766845703125\n",
      "Epoch : 1626 Loss: 1336.8528320789337 Training error :266.5347900390625\n",
      "Epoch : 1627 Loss: 1333.0370826721191 Training error :265.77325439453125\n",
      "Epoch : 1628 Loss: 1329.2277455329895 Training error :265.012939453125\n",
      "Epoch : 1629 Loss: 1325.424876689911 Training error :264.2540283203125\n",
      "Epoch : 1630 Loss: 1321.6290016174316 Training error :263.4963684082031\n",
      "Epoch : 1631 Loss: 1317.8393423557281 Training error :262.73992919921875\n",
      "Epoch : 1632 Loss: 1314.0560643672943 Training error :261.98468017578125\n",
      "Epoch : 1633 Loss: 1310.2786755561829 Training error :261.23077392578125\n",
      "Epoch : 1634 Loss: 1306.5079176425934 Training error :260.4780578613281\n",
      "Epoch : 1635 Loss: 1302.743156194687 Training error :259.7265930175781\n",
      "Epoch : 1636 Loss: 1298.9843075275421 Training error :258.97637939453125\n",
      "Epoch : 1637 Loss: 1295.2322669029236 Training error :258.2275390625\n",
      "Epoch : 1638 Loss: 1291.4871888160706 Training error :257.48004150390625\n",
      "Epoch : 1639 Loss: 1287.748170375824 Training error :256.73345947265625\n",
      "Epoch : 1640 Loss: 1284.0142567157745 Training error :255.98854064941406\n",
      "Epoch : 1641 Loss: 1280.2880413532257 Training error :255.24464416503906\n",
      "Epoch : 1642 Loss: 1276.567465543747 Training error :254.50210571289062\n",
      "Epoch : 1643 Loss: 1272.8534898757935 Training error :253.76072692871094\n",
      "Epoch : 1644 Loss: 1269.145263671875 Training error :253.0207977294922\n",
      "Epoch : 1645 Loss: 1265.4448895454407 Training error :252.28213500976562\n",
      "Epoch : 1646 Loss: 1261.7497742176056 Training error :251.5451202392578\n",
      "Epoch : 1647 Loss: 1258.060767173767 Training error :250.8094482421875\n",
      "Epoch : 1648 Loss: 1254.3825659751892 Training error :250.07492065429688\n",
      "Epoch : 1649 Loss: 1250.7099177837372 Training error :249.34156799316406\n",
      "Epoch : 1650 Loss: 1247.0420718193054 Training error :248.60928344726562\n",
      "Epoch : 1651 Loss: 1243.3800640106201 Training error :247.87840270996094\n",
      "Epoch : 1652 Loss: 1239.724237203598 Training error :247.14918518066406\n",
      "Epoch : 1653 Loss: 1236.0716423988342 Training error :246.4213104248047\n",
      "Epoch : 1654 Loss: 1232.430416584015 Training error :245.69488525390625\n",
      "Epoch : 1655 Loss: 1228.7968516349792 Training error :244.96966552734375\n",
      "Epoch : 1656 Loss: 1225.1697716712952 Training error :244.24586486816406\n",
      "Epoch : 1657 Loss: 1221.5495793819427 Training error :243.52317810058594\n",
      "Epoch : 1658 Loss: 1217.9358444213867 Training error :242.80181884765625\n",
      "Epoch : 1659 Loss: 1214.327998161316 Training error :242.08181762695312\n",
      "Epoch : 1660 Loss: 1210.7268686294556 Training error :241.36294555664062\n",
      "Epoch : 1661 Loss: 1207.1320296525955 Training error :240.6455078125\n",
      "Epoch : 1662 Loss: 1203.5436117649078 Training error :239.9292755126953\n",
      "Epoch : 1663 Loss: 1199.9617456197739 Training error :239.21412658691406\n",
      "Epoch : 1664 Loss: 1196.3858668804169 Training error :238.5004119873047\n",
      "Epoch : 1665 Loss: 1192.8159084320068 Training error :237.78794860839844\n",
      "Epoch : 1666 Loss: 1189.2523809671402 Training error :237.0765838623047\n",
      "Epoch : 1667 Loss: 1185.694618821144 Training error :236.36660766601562\n",
      "Epoch : 1668 Loss: 1182.1441631317139 Training error :235.65792846679688\n",
      "Epoch : 1669 Loss: 1178.5991636514664 Training error :234.95037841796875\n",
      "Epoch : 1670 Loss: 1175.0612026453018 Training error :234.24423217773438\n",
      "Epoch : 1671 Loss: 1171.5299837589264 Training error :233.53921508789062\n",
      "Epoch : 1672 Loss: 1168.0034391880035 Training error :232.83628845214844\n",
      "Epoch : 1673 Loss: 1164.4858667850494 Training error :232.13394165039062\n",
      "Epoch : 1674 Loss: 1160.9743599891663 Training error :231.43310546875\n",
      "Epoch : 1675 Loss: 1157.46974670887 Training error :230.73324584960938\n",
      "Epoch : 1676 Loss: 1153.9697034358978 Training error :230.03451538085938\n",
      "Epoch : 1677 Loss: 1150.4752082824707 Training error :229.33709716796875\n",
      "Epoch : 1678 Loss: 1146.9875593185425 Training error :228.64097595214844\n",
      "Epoch : 1679 Loss: 1143.505875825882 Training error :227.94601440429688\n",
      "Epoch : 1680 Loss: 1140.0303664207458 Training error :227.2523193359375\n",
      "Epoch : 1681 Loss: 1136.561017870903 Training error :226.56004333496094\n",
      "Epoch : 1682 Loss: 1133.0980758666992 Training error :225.86891174316406\n",
      "Epoch : 1683 Loss: 1129.641399383545 Training error :225.17880249023438\n",
      "Epoch : 1684 Loss: 1126.1903097629547 Training error :224.49017333984375\n",
      "Epoch : 1685 Loss: 1122.746086359024 Training error :223.80274963378906\n",
      "Epoch : 1686 Loss: 1119.3081656694412 Training error :223.1166229248047\n",
      "Epoch : 1687 Loss: 1115.876281261444 Training error :222.43165588378906\n",
      "Epoch : 1688 Loss: 1112.4505174160004 Training error :221.74790954589844\n",
      "Epoch : 1689 Loss: 1109.0311924219131 Training error :221.06552124023438\n",
      "Epoch : 1690 Loss: 1105.617882490158 Training error :220.3843231201172\n",
      "Epoch : 1691 Loss: 1102.2110517024994 Training error :219.70437622070312\n",
      "Epoch : 1692 Loss: 1098.8103988170624 Training error :219.02565002441406\n",
      "Epoch : 1693 Loss: 1095.4161590337753 Training error :218.34812927246094\n",
      "Epoch : 1694 Loss: 1092.0276803970337 Training error :217.67193603515625\n",
      "Epoch : 1695 Loss: 1088.645848274231 Training error :216.9969482421875\n",
      "Epoch : 1696 Loss: 1085.2698247432709 Training error :216.32310485839844\n",
      "Epoch : 1697 Loss: 1081.899750471115 Training error :215.65037536621094\n",
      "Epoch : 1698 Loss: 1078.5352499485016 Training error :214.9801788330078\n",
      "Epoch : 1699 Loss: 1075.1805291175842 Training error :214.30979919433594\n",
      "Epoch : 1700 Loss: 1071.829826116562 Training error :213.64120483398438\n",
      "Epoch : 1701 Loss: 1068.4863361120224 Training error :212.97361755371094\n",
      "Epoch : 1702 Loss: 1065.1475476026535 Training error :212.3072052001953\n",
      "Epoch : 1703 Loss: 1061.8146169185638 Training error :211.64190673828125\n",
      "Epoch : 1704 Loss: 1058.487595319748 Training error :210.9779052734375\n",
      "Epoch : 1705 Loss: 1055.1669439077377 Training error :210.31500244140625\n",
      "Epoch : 1706 Loss: 1051.8512674570084 Training error :209.65338134765625\n",
      "Epoch : 1707 Loss: 1048.5420035123825 Training error :208.99298095703125\n",
      "Epoch : 1708 Loss: 1045.238750219345 Training error :208.33377075195312\n",
      "Epoch : 1709 Loss: 1041.9419699907303 Training error :207.6758270263672\n",
      "Epoch : 1710 Loss: 1038.6513056755066 Training error :207.0189971923828\n",
      "Epoch : 1711 Loss: 1035.3666520118713 Training error :206.36354064941406\n",
      "Epoch : 1712 Loss: 1032.087952375412 Training error :205.7091064453125\n",
      "Epoch : 1713 Loss: 1028.815792798996 Training error :205.05599975585938\n",
      "Epoch : 1714 Loss: 1025.549143075943 Training error :204.40399169921875\n",
      "Epoch : 1715 Loss: 1022.2881639003754 Training error :203.7531280517578\n",
      "Epoch : 1716 Loss: 1019.032919049263 Training error :203.1035614013672\n",
      "Epoch : 1717 Loss: 1015.7836902141571 Training error :202.45498657226562\n",
      "Epoch : 1718 Loss: 1012.5407180786133 Training error :201.80767822265625\n",
      "Epoch : 1719 Loss: 1009.3030695915222 Training error :201.16162109375\n",
      "Epoch : 1720 Loss: 1006.072344660759 Training error :200.51681518554688\n",
      "Epoch : 1721 Loss: 1002.8468732833862 Training error :199.87290954589844\n",
      "Epoch : 1722 Loss: 999.6274710893631 Training error :199.2305145263672\n",
      "Epoch : 1723 Loss: 996.4137541055679 Training error :198.5894775390625\n",
      "Epoch : 1724 Loss: 993.2056605815887 Training error :197.9497528076172\n",
      "Epoch : 1725 Loss: 990.0079643726349 Training error :197.31109619140625\n",
      "Epoch : 1726 Loss: 986.8148396015167 Training error :196.67340087890625\n",
      "Epoch : 1727 Loss: 983.6252269744873 Training error :196.03668212890625\n",
      "Epoch : 1728 Loss: 980.4412224292755 Training error :195.4011688232422\n",
      "Epoch : 1729 Loss: 977.2624757289886 Training error :194.76687622070312\n",
      "Epoch : 1730 Loss: 974.0904278755188 Training error :194.13369750976562\n",
      "Epoch : 1731 Loss: 970.9238398075104 Training error :193.50167846679688\n",
      "Epoch : 1732 Loss: 967.7628092765808 Training error :192.87078857421875\n",
      "Epoch : 1733 Loss: 964.6076242923737 Training error :192.24098205566406\n",
      "Epoch : 1734 Loss: 961.4585053920746 Training error :191.6123809814453\n",
      "Epoch : 1735 Loss: 958.3144820928574 Training error :190.9849853515625\n",
      "Epoch : 1736 Loss: 955.1767445802689 Training error :190.35873413085938\n",
      "Epoch : 1737 Loss: 952.0445410013199 Training error :189.73355102539062\n",
      "Epoch : 1738 Loss: 948.9180829524994 Training error :189.10963439941406\n",
      "Epoch : 1739 Loss: 945.7973037958145 Training error :188.48667907714844\n",
      "Epoch : 1740 Loss: 942.6820193529129 Training error :187.8650360107422\n",
      "Epoch : 1741 Loss: 939.5729973316193 Training error :187.24436950683594\n",
      "Epoch : 1742 Loss: 936.4691965579987 Training error :186.6249542236328\n",
      "Epoch : 1743 Loss: 933.3714483976364 Training error :186.00665283203125\n",
      "Epoch : 1744 Loss: 930.2794544696808 Training error :185.38951110839844\n",
      "Epoch : 1745 Loss: 927.192431807518 Training error :184.7734375\n",
      "Epoch : 1746 Loss: 924.111489534378 Training error :184.1586151123047\n",
      "Epoch : 1747 Loss: 921.0364279747009 Training error :183.54490661621094\n",
      "Epoch : 1748 Loss: 917.9677996635437 Training error :182.93238830566406\n",
      "Epoch : 1749 Loss: 914.903906583786 Training error :182.32179260253906\n",
      "Epoch : 1750 Loss: 911.8480907678604 Training error :181.71127319335938\n",
      "Epoch : 1751 Loss: 908.7960766553879 Training error :181.10227966308594\n",
      "Epoch : 1752 Loss: 905.7512104511261 Training error :180.4942169189453\n",
      "Epoch : 1753 Loss: 902.7104852199554 Training error :179.88714599609375\n",
      "Epoch : 1754 Loss: 899.6748015880585 Training error :179.28118896484375\n",
      "Epoch : 1755 Loss: 896.6445910930634 Training error :178.6763153076172\n",
      "Epoch : 1756 Loss: 893.6198118925095 Training error :178.0725555419922\n",
      "Epoch : 1757 Loss: 890.600648522377 Training error :177.46998596191406\n",
      "Epoch : 1758 Loss: 887.586652636528 Training error :176.86843872070312\n",
      "Epoch : 1759 Loss: 884.5787794589996 Training error :176.26806640625\n",
      "Epoch : 1760 Loss: 881.5760241746902 Training error :175.6686553955078\n",
      "Epoch : 1761 Loss: 878.5783120393753 Training error :175.0703887939453\n",
      "Epoch : 1762 Loss: 875.5867404937744 Training error :174.4732666015625\n",
      "Epoch : 1763 Loss: 872.600469827652 Training error :173.87725830078125\n",
      "Epoch : 1764 Loss: 869.6198751926422 Training error :173.28265380859375\n",
      "Epoch : 1765 Loss: 866.6556342840195 Training error :172.68939208984375\n",
      "Epoch : 1766 Loss: 863.6916923522949 Training error :172.09716796875\n",
      "Epoch : 1767 Loss: 860.7301557064056 Training error :171.50599670410156\n",
      "Epoch : 1768 Loss: 857.7737329006195 Training error :170.91600036621094\n",
      "Epoch : 1769 Loss: 854.8220760822296 Training error :170.32720947265625\n",
      "Epoch : 1770 Loss: 851.8761796951294 Training error :169.7393035888672\n",
      "Epoch : 1771 Loss: 848.9355580806732 Training error :169.15264892578125\n",
      "Epoch : 1772 Loss: 846.000651717186 Training error :168.56703186035156\n",
      "Epoch : 1773 Loss: 843.0713013410568 Training error :167.982421875\n",
      "Epoch : 1774 Loss: 840.1473832130432 Training error :167.3997802734375\n",
      "Epoch : 1775 Loss: 837.2307478189468 Training error :166.8173370361328\n",
      "Epoch : 1776 Loss: 834.3193476200104 Training error :166.2364044189453\n",
      "Epoch : 1777 Loss: 831.4136964082718 Training error :165.65623474121094\n",
      "Epoch : 1778 Loss: 828.5125482082367 Training error :165.0771484375\n",
      "Epoch : 1779 Loss: 825.6161788702011 Training error :164.49916076660156\n",
      "Epoch : 1780 Loss: 822.7252089977264 Training error :163.9221649169922\n",
      "Epoch : 1781 Loss: 819.8392354249954 Training error :163.3463134765625\n",
      "Epoch : 1782 Loss: 816.9591304063797 Training error :162.77157592773438\n",
      "Epoch : 1783 Loss: 814.0846267938614 Training error :162.19781494140625\n",
      "Epoch : 1784 Loss: 811.2150526046753 Training error :161.6251983642578\n",
      "Epoch : 1785 Loss: 808.3511607646942 Training error :161.0535430908203\n",
      "Epoch : 1786 Loss: 805.4923384189606 Training error :160.4829864501953\n",
      "Epoch : 1787 Loss: 802.6386725902557 Training error :159.9136199951172\n",
      "Epoch : 1788 Loss: 799.7908388376236 Training error :159.3452606201172\n",
      "Epoch : 1789 Loss: 796.9484221935272 Training error :158.77809143066406\n",
      "Epoch : 1790 Loss: 794.1117069721222 Training error :158.21189880371094\n",
      "Epoch : 1791 Loss: 791.2798483371735 Training error :157.6467742919922\n",
      "Epoch : 1792 Loss: 788.453598856926 Training error :157.08277893066406\n",
      "Epoch : 1793 Loss: 785.6328572034836 Training error :156.51986694335938\n",
      "Epoch : 1794 Loss: 782.8175785541534 Training error :155.9580535888672\n",
      "Epoch : 1795 Loss: 780.0081661939621 Training error :155.3972625732422\n",
      "Epoch : 1796 Loss: 777.2036466598511 Training error :154.8375244140625\n",
      "Epoch : 1797 Loss: 774.4039751291275 Training error :154.2788848876953\n",
      "Epoch : 1798 Loss: 771.6100710630417 Training error :153.7214813232422\n",
      "Epoch : 1799 Loss: 768.8206766843796 Training error :153.16555786132812\n",
      "Epoch : 1800 Loss: 766.0412604808807 Training error :152.61050415039062\n",
      "Epoch : 1801 Loss: 763.2660162448883 Training error :152.05621337890625\n",
      "Epoch : 1802 Loss: 760.4947246313095 Training error :151.5030059814453\n",
      "Epoch : 1803 Loss: 757.7280006408691 Training error :150.95079040527344\n",
      "Epoch : 1804 Loss: 754.966694355011 Training error :150.39976501464844\n",
      "Epoch : 1805 Loss: 752.2105351686478 Training error :149.8496856689453\n",
      "Epoch : 1806 Loss: 749.4593632221222 Training error :149.30062866210938\n",
      "Epoch : 1807 Loss: 746.7138184309006 Training error :148.75270080566406\n",
      "Epoch : 1808 Loss: 743.973625421524 Training error :148.20587158203125\n",
      "Epoch : 1809 Loss: 741.2391045093536 Training error :147.66014099121094\n",
      "Epoch : 1810 Loss: 738.50945520401 Training error :147.1153564453125\n",
      "Epoch : 1811 Loss: 735.7852473258972 Training error :146.5718536376953\n",
      "Epoch : 1812 Loss: 733.0668694972992 Training error :146.02935791015625\n",
      "Epoch : 1813 Loss: 730.353745341301 Training error :145.48785400390625\n",
      "Epoch : 1814 Loss: 727.6458792686462 Training error :144.9476318359375\n",
      "Epoch : 1815 Loss: 724.9442660808563 Training error :144.40830993652344\n",
      "Epoch : 1816 Loss: 722.2472026348114 Training error :143.87013244628906\n",
      "Epoch : 1817 Loss: 719.5555889606476 Training error :143.33299255371094\n",
      "Epoch : 1818 Loss: 716.8691381216049 Training error :142.79693603515625\n",
      "Epoch : 1819 Loss: 714.1884673833847 Training error :142.26190185546875\n",
      "Epoch : 1820 Loss: 711.5127184391022 Training error :141.7281036376953\n",
      "Epoch : 1821 Loss: 708.8431603908539 Training error :141.19525146484375\n",
      "Epoch : 1822 Loss: 706.1783530712128 Training error :140.66348266601562\n",
      "Epoch : 1823 Loss: 703.5189998149872 Training error :140.133544921875\n",
      "Epoch : 1824 Loss: 700.8667093515396 Training error :139.6040496826172\n",
      "Epoch : 1825 Loss: 698.2201415300369 Training error :139.07569885253906\n",
      "Epoch : 1826 Loss: 695.5784431695938 Training error :138.5483856201172\n",
      "Epoch : 1827 Loss: 692.9415751695633 Training error :138.0220184326172\n",
      "Epoch : 1828 Loss: 690.3092284202576 Training error :137.49685668945312\n",
      "Epoch : 1829 Loss: 687.6825579404831 Training error :136.9725341796875\n",
      "Epoch : 1830 Loss: 685.0608358383179 Training error :136.4494171142578\n",
      "Epoch : 1831 Loss: 682.4447926282883 Training error :135.92730712890625\n",
      "Epoch : 1832 Loss: 679.8339680433273 Training error :135.40634155273438\n",
      "Epoch : 1833 Loss: 677.2282218933105 Training error :134.886474609375\n",
      "Epoch : 1834 Loss: 674.6284321546555 Training error :134.3675994873047\n",
      "Epoch : 1835 Loss: 672.0334805250168 Training error :133.85000610351562\n",
      "Epoch : 1836 Loss: 669.444450378418 Training error :133.3332977294922\n",
      "Epoch : 1837 Loss: 666.8606419563293 Training error :132.81793212890625\n",
      "Epoch : 1838 Loss: 664.2831057310104 Training error :132.303466796875\n",
      "Epoch : 1839 Loss: 661.7103655338287 Training error :131.79022216796875\n",
      "Epoch : 1840 Loss: 659.143748998642 Training error :131.27798461914062\n",
      "Epoch : 1841 Loss: 656.5822261571884 Training error :130.7667999267578\n",
      "Epoch : 1842 Loss: 654.0255234241486 Training error :130.25668334960938\n",
      "Epoch : 1843 Loss: 651.4745193719864 Training error :129.7477569580078\n",
      "Epoch : 1844 Loss: 648.9291183948517 Training error :129.23976135253906\n",
      "Epoch : 1845 Loss: 646.3884640932083 Training error :128.7329864501953\n",
      "Epoch : 1846 Loss: 643.8540723323822 Training error :128.22718811035156\n",
      "Epoch : 1847 Loss: 641.3250515460968 Training error :127.72268676757812\n",
      "Epoch : 1848 Loss: 638.8016830682755 Training error :127.21981811523438\n",
      "Epoch : 1849 Loss: 636.2851102352142 Training error :126.71722412109375\n",
      "Epoch : 1850 Loss: 633.7729799747467 Training error :126.21603393554688\n",
      "Epoch : 1851 Loss: 631.2671141624451 Training error :125.71591186523438\n",
      "Epoch : 1852 Loss: 628.7658612728119 Training error :125.216796875\n",
      "Epoch : 1853 Loss: 626.269676566124 Training error :124.7187271118164\n",
      "Epoch : 1854 Loss: 623.7788906097412 Training error :124.2219009399414\n",
      "Epoch : 1855 Loss: 621.2940797805786 Training error :123.7259292602539\n",
      "Epoch : 1856 Loss: 618.8138449192047 Training error :123.2312240600586\n",
      "Epoch : 1857 Loss: 616.3398382663727 Training error :122.7375259399414\n",
      "Epoch : 1858 Loss: 613.8710964918137 Training error :122.24504852294922\n",
      "Epoch : 1859 Loss: 611.4079051017761 Training error :121.75360107421875\n",
      "Epoch : 1860 Loss: 608.9500550031662 Training error :121.2631607055664\n",
      "Epoch : 1861 Loss: 606.4971721172333 Training error :120.7738265991211\n",
      "Epoch : 1862 Loss: 604.0500180721283 Training error :120.28560638427734\n",
      "Epoch : 1863 Loss: 601.6080935001373 Training error :119.79849243164062\n",
      "Epoch : 1864 Loss: 599.1723855733871 Training error :119.3125228881836\n",
      "Epoch : 1865 Loss: 596.7416436672211 Training error :118.82772827148438\n",
      "Epoch : 1866 Loss: 594.3168921470642 Training error :118.34385681152344\n",
      "Epoch : 1867 Loss: 591.8972570896149 Training error :117.86123657226562\n",
      "Epoch : 1868 Loss: 589.4834588766098 Training error :117.3797378540039\n",
      "Epoch : 1869 Loss: 587.0753848552704 Training error :116.89927673339844\n",
      "Epoch : 1870 Loss: 584.672437787056 Training error :116.4200210571289\n",
      "Epoch : 1871 Loss: 582.2755848169327 Training error :115.94178771972656\n",
      "Epoch : 1872 Loss: 579.8842821121216 Training error :115.46473693847656\n",
      "Epoch : 1873 Loss: 577.4979228973389 Training error :114.98932647705078\n",
      "Epoch : 1874 Loss: 575.1185193061829 Training error :114.51448059082031\n",
      "Epoch : 1875 Loss: 572.7453954219818 Training error :114.04094696044922\n",
      "Epoch : 1876 Loss: 570.3778215646744 Training error :113.56838989257812\n",
      "Epoch : 1877 Loss: 568.0142501592636 Training error :113.09677124023438\n",
      "Epoch : 1878 Loss: 565.6557356119156 Training error :112.62637329101562\n",
      "Epoch : 1879 Loss: 563.3030871152878 Training error :112.15705108642578\n",
      "Epoch : 1880 Loss: 560.9556223154068 Training error :111.68887329101562\n",
      "Epoch : 1881 Loss: 558.614328622818 Training error :111.22174072265625\n",
      "Epoch : 1882 Loss: 556.2780166864395 Training error :110.75581359863281\n",
      "Epoch : 1883 Loss: 553.9475046396255 Training error :110.29093933105469\n",
      "Epoch : 1884 Loss: 551.622966170311 Training error :109.82717895507812\n",
      "Epoch : 1885 Loss: 549.3037499189377 Training error :109.3646011352539\n",
      "Epoch : 1886 Loss: 546.9899053573608 Training error :108.90306854248047\n",
      "Epoch : 1887 Loss: 544.6820547580719 Training error :108.4426498413086\n",
      "Epoch : 1888 Loss: 542.3790279626846 Training error :107.98347473144531\n",
      "Epoch : 1889 Loss: 540.0822528600693 Training error :107.52534484863281\n",
      "Epoch : 1890 Loss: 537.7914539575577 Training error :107.0683822631836\n",
      "Epoch : 1891 Loss: 535.5059456825256 Training error :106.61255645751953\n",
      "Epoch : 1892 Loss: 533.2263075113297 Training error :106.15779113769531\n",
      "Epoch : 1893 Loss: 530.9520952701569 Training error :105.7042007446289\n",
      "Epoch : 1894 Loss: 528.6831834316254 Training error :105.25157928466797\n",
      "Epoch : 1895 Loss: 526.4200419187546 Training error :104.80017852783203\n",
      "Epoch : 1896 Loss: 524.1622250080109 Training error :104.35000610351562\n",
      "Epoch : 1897 Loss: 521.9104853868484 Training error :103.90095520019531\n",
      "Epoch : 1898 Loss: 519.6649521589279 Training error :103.45313262939453\n",
      "Epoch : 1899 Loss: 517.4232933521271 Training error :103.00662994384766\n",
      "Epoch : 1900 Loss: 515.1912829875946 Training error :102.56116485595703\n",
      "Epoch : 1901 Loss: 512.9638172388077 Training error :102.11663055419922\n",
      "Epoch : 1902 Loss: 510.7408698797226 Training error :101.67316436767578\n",
      "Epoch : 1903 Loss: 508.5227242708206 Training error :101.23086547851562\n",
      "Epoch : 1904 Loss: 506.3111206293106 Training error :100.78968811035156\n",
      "Epoch : 1905 Loss: 504.1044577360153 Training error :100.3495864868164\n",
      "Epoch : 1906 Loss: 501.9033578634262 Training error :99.91067504882812\n",
      "Epoch : 1907 Loss: 499.70811849832535 Training error :99.47279357910156\n",
      "Epoch : 1908 Loss: 497.51817816495895 Training error :99.03610229492188\n",
      "Epoch : 1909 Loss: 495.3342343568802 Training error :98.60052490234375\n",
      "Epoch : 1910 Loss: 493.1557091474533 Training error :98.16612243652344\n",
      "Epoch : 1911 Loss: 490.9832428097725 Training error :97.7328872680664\n",
      "Epoch : 1912 Loss: 488.8159744143486 Training error :97.30074310302734\n",
      "Epoch : 1913 Loss: 486.65472650527954 Training error :96.86978149414062\n",
      "Epoch : 1914 Loss: 484.4994166493416 Training error :96.43997192382812\n",
      "Epoch : 1915 Loss: 482.34942203760147 Training error :96.01123809814453\n",
      "Epoch : 1916 Loss: 480.2052279114723 Training error :95.58364868164062\n",
      "Epoch : 1917 Loss: 478.06683534383774 Training error :95.15713500976562\n",
      "Epoch : 1918 Loss: 475.9340683221817 Training error :94.73185729980469\n",
      "Epoch : 1919 Loss: 473.8072246313095 Training error :94.30776977539062\n",
      "Epoch : 1920 Loss: 471.68571305274963 Training error :93.88475799560547\n",
      "Epoch : 1921 Loss: 469.56990826129913 Training error :93.46286010742188\n",
      "Epoch : 1922 Loss: 467.4596669077873 Training error :93.04206848144531\n",
      "Epoch : 1923 Loss: 465.3552225828171 Training error :92.62252807617188\n",
      "Epoch : 1924 Loss: 463.25684863328934 Training error :92.20447540283203\n",
      "Epoch : 1925 Loss: 461.16450196504593 Training error :91.78736114501953\n",
      "Epoch : 1926 Loss: 459.0798782110214 Training error :91.37141418457031\n",
      "Epoch : 1927 Loss: 456.99985486269 Training error :90.9564437866211\n",
      "Epoch : 1928 Loss: 454.92469227313995 Training error :90.54255676269531\n",
      "Epoch : 1929 Loss: 452.8544369339943 Training error :90.12980651855469\n",
      "Epoch : 1930 Loss: 450.7903228998184 Training error :89.71823120117188\n",
      "Epoch : 1931 Loss: 448.7318475842476 Training error :89.30785369873047\n",
      "Epoch : 1932 Loss: 446.67913377285004 Training error :88.89854431152344\n",
      "Epoch : 1933 Loss: 444.63185852766037 Training error :88.49044799804688\n",
      "Epoch : 1934 Loss: 442.5908291339874 Training error :88.0834732055664\n",
      "Epoch : 1935 Loss: 440.5553215742111 Training error :87.67762756347656\n",
      "Epoch : 1936 Loss: 438.52594316005707 Training error :87.27298736572266\n",
      "Epoch : 1937 Loss: 436.5019869208336 Training error :86.86949920654297\n",
      "Epoch : 1938 Loss: 434.48381090164185 Training error :86.46707153320312\n",
      "Epoch : 1939 Loss: 432.4712620973587 Training error :86.0659408569336\n",
      "Epoch : 1940 Loss: 430.4646886587143 Training error :85.6658706665039\n",
      "Epoch : 1941 Loss: 428.4636996984482 Training error :85.26690673828125\n",
      "Epoch : 1942 Loss: 426.4684827923775 Training error :84.8690414428711\n",
      "Epoch : 1943 Loss: 424.4788952469826 Training error :84.47248077392578\n",
      "Epoch : 1944 Loss: 422.4955491423607 Training error :84.07694244384766\n",
      "Epoch : 1945 Loss: 420.5170188546181 Training error :83.68260955810547\n",
      "Epoch : 1946 Loss: 418.54464089870453 Training error :83.28947448730469\n",
      "Epoch : 1947 Loss: 416.57852268218994 Training error :82.8974838256836\n",
      "Epoch : 1948 Loss: 414.6179535984993 Training error :82.50670623779297\n",
      "Epoch : 1949 Loss: 412.66334265470505 Training error :82.1169662475586\n",
      "Epoch : 1950 Loss: 410.71395897865295 Training error :81.72848510742188\n",
      "Epoch : 1951 Loss: 408.7696513533592 Training error :81.3415756225586\n",
      "Epoch : 1952 Loss: 406.83558136224747 Training error :80.95549011230469\n",
      "Epoch : 1953 Loss: 404.90510737895966 Training error :80.5705337524414\n",
      "Epoch : 1954 Loss: 402.97952204942703 Training error :80.18663787841797\n",
      "Epoch : 1955 Loss: 401.059588432312 Training error :79.80386352539062\n",
      "Epoch : 1956 Loss: 399.1451272368431 Training error :79.42230224609375\n",
      "Epoch : 1957 Loss: 397.23699033260345 Training error :79.04180145263672\n",
      "Epoch : 1958 Loss: 395.3341295719147 Training error :78.66251373291016\n",
      "Epoch : 1959 Loss: 393.4371135830879 Training error :78.28430938720703\n",
      "Epoch : 1960 Loss: 391.5453664660454 Training error :77.9073257446289\n",
      "Epoch : 1961 Loss: 389.6596009731293 Training error :77.53146362304688\n",
      "Epoch : 1962 Loss: 387.7795720696449 Training error :77.15676879882812\n",
      "Epoch : 1963 Loss: 385.905459523201 Training error :76.78311920166016\n",
      "Epoch : 1964 Loss: 384.03734171390533 Training error :76.41080474853516\n",
      "Epoch : 1965 Loss: 382.1751074194908 Training error :76.03954315185547\n",
      "Epoch : 1966 Loss: 380.3181217312813 Training error :75.66950225830078\n",
      "Epoch : 1967 Loss: 378.46697384119034 Training error :75.30047607421875\n",
      "Epoch : 1968 Loss: 376.62175661325455 Training error :74.93276977539062\n",
      "Epoch : 1969 Loss: 374.7823206782341 Training error :74.56620788574219\n",
      "Epoch : 1970 Loss: 372.948689699173 Training error :74.20068359375\n",
      "Epoch : 1971 Loss: 371.12086522579193 Training error :73.83647155761719\n",
      "Epoch : 1972 Loss: 369.2990942001343 Training error :73.47332000732422\n",
      "Epoch : 1973 Loss: 367.48292392492294 Training error :73.11133575439453\n",
      "Epoch : 1974 Loss: 365.6724292039871 Training error :72.75057983398438\n",
      "Epoch : 1975 Loss: 363.8680309653282 Training error :72.39085388183594\n",
      "Epoch : 1976 Loss: 362.06902343034744 Training error :72.03239440917969\n",
      "Epoch : 1977 Loss: 360.276195704937 Training error :71.67514038085938\n",
      "Epoch : 1978 Loss: 358.48817652463913 Training error :71.31937408447266\n",
      "Epoch : 1979 Loss: 356.7094298005104 Training error :70.9645004272461\n",
      "Epoch : 1980 Loss: 354.9352321624756 Training error :70.61064147949219\n",
      "Epoch : 1981 Loss: 353.1654618382454 Training error :70.25794219970703\n",
      "Epoch : 1982 Loss: 351.40152621269226 Training error :69.9063720703125\n",
      "Epoch : 1983 Loss: 349.6431645154953 Training error :69.55601501464844\n",
      "Epoch : 1984 Loss: 347.89040899276733 Training error :69.20669555664062\n",
      "Epoch : 1985 Loss: 346.14365833997726 Training error :68.8585433959961\n",
      "Epoch : 1986 Loss: 344.40229094028473 Training error :68.51158142089844\n",
      "Epoch : 1987 Loss: 342.66684222221375 Training error :68.165771484375\n",
      "Epoch : 1988 Loss: 340.9371766448021 Training error :67.82110595703125\n",
      "Epoch : 1989 Loss: 339.2136054635048 Training error :67.47758483886719\n",
      "Epoch : 1990 Loss: 337.49507588148117 Training error :67.1352310180664\n",
      "Epoch : 1991 Loss: 335.78275924921036 Training error :66.79405975341797\n",
      "Epoch : 1992 Loss: 334.07660484313965 Training error :66.45394897460938\n",
      "Epoch : 1993 Loss: 332.3756886124611 Training error :66.11509704589844\n",
      "Epoch : 1994 Loss: 330.6806728243828 Training error :65.77733612060547\n",
      "Epoch : 1995 Loss: 328.9914131760597 Training error :65.4408187866211\n",
      "Epoch : 1996 Loss: 327.3081787824631 Training error :65.10542297363281\n",
      "Epoch : 1997 Loss: 325.63073033094406 Training error :64.77116394042969\n",
      "Epoch : 1998 Loss: 323.9589973092079 Training error :64.43800354003906\n",
      "Epoch : 1999 Loss: 322.29262441396713 Training error :64.1061019897461\n",
      "Epoch : 2000 Loss: 320.6324498653412 Training error :63.775325775146484\n",
      "Epoch : 2001 Loss: 318.978020966053 Training error :63.44571304321289\n",
      "Epoch : 2002 Loss: 317.329745054245 Training error :63.117225646972656\n",
      "Epoch : 2003 Loss: 315.68670785427094 Training error :62.78984832763672\n",
      "Epoch : 2004 Loss: 314.049518764019 Training error :62.463600158691406\n",
      "Epoch : 2005 Loss: 312.4176134467125 Training error :62.13870620727539\n",
      "Epoch : 2006 Loss: 310.79169404506683 Training error :61.814979553222656\n",
      "Epoch : 2007 Loss: 309.1732804775238 Training error :61.492347717285156\n",
      "Epoch : 2008 Loss: 307.55979001522064 Training error :61.17079162597656\n",
      "Epoch : 2009 Loss: 305.951429605484 Training error :60.850318908691406\n",
      "Epoch : 2010 Loss: 304.3489272594452 Training error :60.530982971191406\n",
      "Epoch : 2011 Loss: 302.75173366069794 Training error :60.212799072265625\n",
      "Epoch : 2012 Loss: 301.1602518558502 Training error :59.8957633972168\n",
      "Epoch : 2013 Loss: 299.57469856739044 Training error :59.579898834228516\n",
      "Epoch : 2014 Loss: 297.9949516057968 Training error :59.26510238647461\n",
      "Epoch : 2015 Loss: 296.4202147722244 Training error :58.95146179199219\n",
      "Epoch : 2016 Loss: 294.8515555858612 Training error :58.63900375366211\n",
      "Epoch : 2017 Loss: 293.28895008563995 Training error :58.327640533447266\n",
      "Epoch : 2018 Loss: 291.7318923473358 Training error :58.01744079589844\n",
      "Epoch : 2019 Loss: 290.1803377866745 Training error :57.708274841308594\n",
      "Epoch : 2020 Loss: 288.63399744033813 Training error :57.40024948120117\n",
      "Epoch : 2021 Loss: 287.09350311756134 Training error :57.09339904785156\n",
      "Epoch : 2022 Loss: 285.5581154227257 Training error :56.787803649902344\n",
      "Epoch : 2023 Loss: 284.0299736857414 Training error :56.48341369628906\n",
      "Epoch : 2024 Loss: 282.50718343257904 Training error :56.1800422668457\n",
      "Epoch : 2025 Loss: 280.9899697303772 Training error :55.8778076171875\n",
      "Epoch : 2026 Loss: 279.4787999391556 Training error :55.57666015625\n",
      "Epoch : 2027 Loss: 277.97232687473297 Training error :55.276641845703125\n",
      "Epoch : 2028 Loss: 276.4720606803894 Training error :54.97774887084961\n",
      "Epoch : 2029 Loss: 274.97710448503494 Training error :54.68008804321289\n",
      "Epoch : 2030 Loss: 273.4880811572075 Training error :54.383445739746094\n",
      "Epoch : 2031 Loss: 272.0045856833458 Training error :54.088050842285156\n",
      "Epoch : 2032 Loss: 270.5272526741028 Training error :53.793701171875\n",
      "Epoch : 2033 Loss: 269.0551391839981 Training error :53.500465393066406\n",
      "Epoch : 2034 Loss: 267.588754594326 Training error :53.208709716796875\n",
      "Epoch : 2035 Loss: 266.1280218362808 Training error :52.91783142089844\n",
      "Epoch : 2036 Loss: 264.6738888025284 Training error :52.62812423706055\n",
      "Epoch : 2037 Loss: 263.2254789471626 Training error :52.339508056640625\n",
      "Epoch : 2038 Loss: 261.7820323705673 Training error :52.051918029785156\n",
      "Epoch : 2039 Loss: 260.3437294960022 Training error :51.76542663574219\n",
      "Epoch : 2040 Loss: 258.910813331604 Training error :51.480010986328125\n",
      "Epoch : 2041 Loss: 257.4836332798004 Training error :51.19575119018555\n",
      "Epoch : 2042 Loss: 256.0616352558136 Training error :50.9126091003418\n",
      "Epoch : 2043 Loss: 254.64528846740723 Training error :50.630699157714844\n",
      "Epoch : 2044 Loss: 253.2354925274849 Training error :50.34980010986328\n",
      "Epoch : 2045 Loss: 251.83057641983032 Training error :50.06999969482422\n",
      "Epoch : 2046 Loss: 250.4309927225113 Training error :49.791446685791016\n",
      "Epoch : 2047 Loss: 249.0378640294075 Training error :49.513851165771484\n",
      "Epoch : 2048 Loss: 247.64961230754852 Training error :49.23741149902344\n",
      "Epoch : 2049 Loss: 246.26686763763428 Training error :48.962120056152344\n",
      "Epoch : 2050 Loss: 244.89013397693634 Training error :48.68793487548828\n",
      "Epoch : 2051 Loss: 243.51889729499817 Training error :48.414833068847656\n",
      "Epoch : 2052 Loss: 242.1530635356903 Training error :48.142730712890625\n",
      "Epoch : 2053 Loss: 240.79184651374817 Training error :47.871856689453125\n",
      "Epoch : 2054 Loss: 239.43714779615402 Training error :47.602027893066406\n",
      "Epoch : 2055 Loss: 238.08759367465973 Training error :47.33330154418945\n",
      "Epoch : 2056 Loss: 236.74376565217972 Training error :47.0656852722168\n",
      "Epoch : 2057 Loss: 235.40541434288025 Training error :46.799251556396484\n",
      "Epoch : 2058 Loss: 234.0728404521942 Training error :46.53376007080078\n",
      "Epoch : 2059 Loss: 232.74517041444778 Training error :46.26945495605469\n",
      "Epoch : 2060 Loss: 231.42298364639282 Training error :46.006187438964844\n",
      "Epoch : 2061 Loss: 230.10632693767548 Training error :45.744083404541016\n",
      "Epoch : 2062 Loss: 228.7950400710106 Training error :45.483097076416016\n",
      "Epoch : 2063 Loss: 227.4897980093956 Training error :45.22312545776367\n",
      "Epoch : 2064 Loss: 226.1898249387741 Training error :44.96449279785156\n",
      "Epoch : 2065 Loss: 224.89524972438812 Training error :44.70683288574219\n",
      "Epoch : 2066 Loss: 223.607113301754 Training error :44.45026779174805\n",
      "Epoch : 2067 Loss: 222.32411742210388 Training error :44.194725036621094\n",
      "Epoch : 2068 Loss: 221.04662346839905 Training error :43.940216064453125\n",
      "Epoch : 2069 Loss: 219.77370238304138 Training error :43.686767578125\n",
      "Epoch : 2070 Loss: 218.50611466169357 Training error :43.434356689453125\n",
      "Epoch : 2071 Loss: 217.24365240335464 Training error :43.183040618896484\n",
      "Epoch : 2072 Loss: 215.98667067289352 Training error :42.932899475097656\n",
      "Epoch : 2073 Loss: 214.73538625240326 Training error :42.683807373046875\n",
      "Epoch : 2074 Loss: 213.4895052909851 Training error :42.435691833496094\n",
      "Epoch : 2075 Loss: 212.24889653921127 Training error :42.18870544433594\n",
      "Epoch : 2076 Loss: 211.01377415657043 Training error :41.942779541015625\n",
      "Epoch : 2077 Loss: 209.7837563753128 Training error :41.69794845581055\n",
      "Epoch : 2078 Loss: 208.55911087989807 Training error :41.45408630371094\n",
      "Epoch : 2079 Loss: 207.3394387960434 Training error :41.21141052246094\n",
      "Epoch : 2080 Loss: 206.12554770708084 Training error :40.9697265625\n",
      "Epoch : 2081 Loss: 204.91695749759674 Training error :40.729183197021484\n",
      "Epoch : 2082 Loss: 203.71377182006836 Training error :40.48967742919922\n",
      "Epoch : 2083 Loss: 202.51590424776077 Training error :40.251094818115234\n",
      "Epoch : 2084 Loss: 201.32308846712112 Training error :40.013675689697266\n",
      "Epoch : 2085 Loss: 200.13553935289383 Training error :39.77733612060547\n",
      "Epoch : 2086 Loss: 198.95360696315765 Training error :39.54201889038086\n",
      "Epoch : 2087 Loss: 197.7764163017273 Training error :39.307708740234375\n",
      "Epoch : 2088 Loss: 196.6045196056366 Training error :39.074520111083984\n",
      "Epoch : 2089 Loss: 195.43831753730774 Training error :38.84242248535156\n",
      "Epoch : 2090 Loss: 194.27911120653152 Training error :38.61155700683594\n",
      "Epoch : 2091 Loss: 193.12478053569794 Training error :38.381710052490234\n",
      "Epoch : 2092 Loss: 191.9751294851303 Training error :38.152915954589844\n",
      "Epoch : 2093 Loss: 190.8305368423462 Training error :37.92520523071289\n",
      "Epoch : 2094 Loss: 189.6915118098259 Training error :37.69853210449219\n",
      "Epoch : 2095 Loss: 188.55758559703827 Training error :37.47285842895508\n",
      "Epoch : 2096 Loss: 187.42863178253174 Training error :37.248600006103516\n",
      "Epoch : 2097 Loss: 186.3058254122734 Training error :37.02494812011719\n",
      "Epoch : 2098 Loss: 185.1879968047142 Training error :36.802467346191406\n",
      "Epoch : 2099 Loss: 184.07564681768417 Training error :36.5809440612793\n",
      "Epoch : 2100 Loss: 182.9679532647133 Training error :36.360389709472656\n",
      "Epoch : 2101 Loss: 181.86448150873184 Training error :36.140838623046875\n",
      "Epoch : 2102 Loss: 180.7663628757 Training error :35.922325134277344\n",
      "Epoch : 2103 Loss: 179.67370784282684 Training error :35.704856872558594\n",
      "Epoch : 2104 Loss: 178.58601501584053 Training error :35.488380432128906\n",
      "Epoch : 2105 Loss: 177.50320574641228 Training error :35.27287292480469\n",
      "Epoch : 2106 Loss: 176.42543622851372 Training error :35.05836868286133\n",
      "Epoch : 2107 Loss: 175.35237762331963 Training error :34.845027923583984\n",
      "Epoch : 2108 Loss: 174.2853193283081 Training error :34.632537841796875\n",
      "Epoch : 2109 Loss: 173.22293186187744 Training error :34.42108154296875\n",
      "Epoch : 2110 Loss: 172.16528636217117 Training error :34.210655212402344\n",
      "Epoch : 2111 Loss: 171.11255899071693 Training error :34.00114440917969\n",
      "Epoch : 2112 Loss: 170.064843416214 Training error :33.79270935058594\n",
      "Epoch : 2113 Loss: 169.02259367704391 Training error :33.58527374267578\n",
      "Epoch : 2114 Loss: 167.9853172004223 Training error :33.37875747680664\n",
      "Epoch : 2115 Loss: 166.9526160955429 Training error :33.173240661621094\n",
      "Epoch : 2116 Loss: 165.92445296049118 Training error :32.96882247924805\n",
      "Epoch : 2117 Loss: 164.9019121825695 Training error :32.765281677246094\n",
      "Epoch : 2118 Loss: 163.8841504752636 Training error :32.562747955322266\n",
      "Epoch : 2119 Loss: 162.87143391370773 Training error :32.36115264892578\n",
      "Epoch : 2120 Loss: 161.86327090859413 Training error :32.16056442260742\n",
      "Epoch : 2121 Loss: 160.85997664928436 Training error :31.960918426513672\n",
      "Epoch : 2122 Loss: 159.86147314310074 Training error :31.762195587158203\n",
      "Epoch : 2123 Loss: 158.86791291832924 Training error :31.5645751953125\n",
      "Epoch : 2124 Loss: 157.87936148047447 Training error :31.367820739746094\n",
      "Epoch : 2125 Loss: 156.89528399705887 Training error :31.17205238342285\n",
      "Epoch : 2126 Loss: 155.91615974903107 Training error :30.977197647094727\n",
      "Epoch : 2127 Loss: 154.94184264540672 Training error :30.783334732055664\n",
      "Epoch : 2128 Loss: 153.97246772050858 Training error :30.590436935424805\n",
      "Epoch : 2129 Loss: 153.00749784708023 Training error :30.398714065551758\n",
      "Epoch : 2130 Loss: 152.04788371920586 Training error :30.207679748535156\n",
      "Epoch : 2131 Loss: 151.09318780899048 Training error :30.017648696899414\n",
      "Epoch : 2132 Loss: 150.14305207133293 Training error :29.82855987548828\n",
      "Epoch : 2133 Loss: 149.19747176766396 Training error :29.64031219482422\n",
      "Epoch : 2134 Loss: 148.25605151057243 Training error :29.453123092651367\n",
      "Epoch : 2135 Loss: 147.31985127925873 Training error :29.266803741455078\n",
      "Epoch : 2136 Loss: 146.38817179203033 Training error :29.081438064575195\n",
      "Epoch : 2137 Loss: 145.46116515994072 Training error :28.89693832397461\n",
      "Epoch : 2138 Loss: 144.53840199112892 Training error :28.713401794433594\n",
      "Epoch : 2139 Loss: 143.62036383152008 Training error :28.530778884887695\n",
      "Epoch : 2140 Loss: 142.7073518037796 Training error :28.349098205566406\n",
      "Epoch : 2141 Loss: 141.79868924617767 Training error :28.168312072753906\n",
      "Epoch : 2142 Loss: 140.89464795589447 Training error :27.988435745239258\n",
      "Epoch : 2143 Loss: 139.9950042963028 Training error :27.809490203857422\n",
      "Epoch : 2144 Loss: 139.10008120536804 Training error :27.631452560424805\n",
      "Epoch : 2145 Loss: 138.20992240309715 Training error :27.454309463500977\n",
      "Epoch : 2146 Loss: 137.32379141449928 Training error :27.278053283691406\n",
      "Epoch : 2147 Loss: 136.44241538643837 Training error :27.10271644592285\n",
      "Epoch : 2148 Loss: 135.56573516130447 Training error :26.92827606201172\n",
      "Epoch : 2149 Loss: 134.69312646985054 Training error :26.75473403930664\n",
      "Epoch : 2150 Loss: 133.82550916075706 Training error :26.582134246826172\n",
      "Epoch : 2151 Loss: 132.96212330460548 Training error :26.410341262817383\n",
      "Epoch : 2152 Loss: 132.10320496559143 Training error :26.239513397216797\n",
      "Epoch : 2153 Loss: 131.24878838658333 Training error :26.069538116455078\n",
      "Epoch : 2154 Loss: 130.39881935715675 Training error :25.900432586669922\n",
      "Epoch : 2155 Loss: 129.55320939421654 Training error :25.732152938842773\n",
      "Epoch : 2156 Loss: 128.7116801738739 Training error :25.56479263305664\n",
      "Epoch : 2157 Loss: 127.87457942962646 Training error :25.39828109741211\n",
      "Epoch : 2158 Loss: 127.04206818342209 Training error :25.232637405395508\n",
      "Epoch : 2159 Loss: 126.21368059515953 Training error :25.067859649658203\n",
      "Epoch : 2160 Loss: 125.38956227898598 Training error :24.903963088989258\n",
      "Epoch : 2161 Loss: 124.56995540857315 Training error :24.740890502929688\n",
      "Epoch : 2162 Loss: 123.75458797812462 Training error :24.578699111938477\n",
      "Epoch : 2163 Loss: 122.94352793693542 Training error :24.417346954345703\n",
      "Epoch : 2164 Loss: 122.13652393221855 Training error :24.25689697265625\n",
      "Epoch : 2165 Loss: 121.33418542146683 Training error :24.09758949279785\n",
      "Epoch : 2166 Loss: 120.53684514760971 Training error :23.938758850097656\n",
      "Epoch : 2167 Loss: 119.74314174056053 Training error :23.780895233154297\n",
      "Epoch : 2168 Loss: 118.95385527610779 Training error :23.623828887939453\n",
      "Epoch : 2169 Loss: 118.16854873299599 Training error :23.467561721801758\n",
      "Epoch : 2170 Loss: 117.38720786571503 Training error :23.312061309814453\n",
      "Epoch : 2171 Loss: 116.60963582992554 Training error :23.157424926757812\n",
      "Epoch : 2172 Loss: 115.83619579672813 Training error :23.00363540649414\n",
      "Epoch : 2173 Loss: 115.06722775101662 Training error :22.850635528564453\n",
      "Epoch : 2174 Loss: 114.30207780003548 Training error :22.698495864868164\n",
      "Epoch : 2175 Loss: 113.54124003648758 Training error :22.54718589782715\n",
      "Epoch : 2176 Loss: 112.78453621268272 Training error :22.39669418334961\n",
      "Epoch : 2177 Loss: 112.03195902705193 Training error :22.246990203857422\n",
      "Epoch : 2178 Loss: 111.28344237804413 Training error :22.098081588745117\n",
      "Epoch : 2179 Loss: 110.53881976008415 Training error :21.950002670288086\n",
      "Epoch : 2180 Loss: 109.79820731282234 Training error :21.802719116210938\n",
      "Epoch : 2181 Loss: 109.06170412898064 Training error :21.656238555908203\n",
      "Epoch : 2182 Loss: 108.32941457629204 Training error :21.510543823242188\n",
      "Epoch : 2183 Loss: 107.60092052817345 Training error :21.365684509277344\n",
      "Epoch : 2184 Loss: 106.87626034021378 Training error :21.2215576171875\n",
      "Epoch : 2185 Loss: 106.15565717220306 Training error :21.07825469970703\n",
      "Epoch : 2186 Loss: 105.43919289112091 Training error :20.935670852661133\n",
      "Epoch : 2187 Loss: 104.7261651456356 Training error :20.79387855529785\n",
      "Epoch : 2188 Loss: 104.01699995994568 Training error :20.65290069580078\n",
      "Epoch : 2189 Loss: 103.31207540631294 Training error :20.512710571289062\n",
      "Epoch : 2190 Loss: 102.61109349131584 Training error :20.373279571533203\n",
      "Epoch : 2191 Loss: 101.91369378566742 Training error :20.234603881835938\n",
      "Epoch : 2192 Loss: 101.22042328119278 Training error :20.096677780151367\n",
      "Epoch : 2193 Loss: 100.53083598613739 Training error :19.959564208984375\n",
      "Epoch : 2194 Loss: 99.84503248333931 Training error :19.823162078857422\n",
      "Epoch : 2195 Loss: 99.16310086846352 Training error :19.687519073486328\n",
      "Epoch : 2196 Loss: 98.48480370640755 Training error :19.55265998840332\n",
      "Epoch : 2197 Loss: 97.81051644682884 Training error :19.41854476928711\n",
      "Epoch : 2198 Loss: 97.13958793878555 Training error :19.285146713256836\n",
      "Epoch : 2199 Loss: 96.47279396653175 Training error :19.152542114257812\n",
      "Epoch : 2200 Loss: 95.81013035774231 Training error :19.020992279052734\n",
      "Epoch : 2201 Loss: 95.1524168252945 Training error :18.890283584594727\n",
      "Epoch : 2202 Loss: 94.49860027432442 Training error :18.760278701782227\n",
      "Epoch : 2203 Loss: 93.84847612679005 Training error :18.631019592285156\n",
      "Epoch : 2204 Loss: 93.20218032598495 Training error :18.502452850341797\n",
      "Epoch : 2205 Loss: 92.55923940241337 Training error :18.37489891052246\n",
      "Epoch : 2206 Loss: 91.92065678536892 Training error :18.24772834777832\n",
      "Epoch : 2207 Loss: 91.28514957427979 Training error :18.121429443359375\n",
      "Epoch : 2208 Loss: 90.6539214849472 Training error :17.995790481567383\n",
      "Epoch : 2209 Loss: 90.02570497989655 Training error :17.870851516723633\n",
      "Epoch : 2210 Loss: 89.40108563005924 Training error :17.746536254882812\n",
      "Epoch : 2211 Loss: 88.77944375574589 Training error :17.62296485900879\n",
      "Epoch : 2212 Loss: 88.16143602132797 Training error :17.50013542175293\n",
      "Epoch : 2213 Loss: 87.5472029298544 Training error :17.378013610839844\n",
      "Epoch : 2214 Loss: 86.93656468391418 Training error :17.25654411315918\n",
      "Epoch : 2215 Loss: 86.32903322577477 Training error :17.135740280151367\n",
      "Epoch : 2216 Loss: 85.72511984407902 Training error :17.015705108642578\n",
      "Epoch : 2217 Loss: 85.12490426003933 Training error :16.896299362182617\n",
      "Epoch : 2218 Loss: 84.52775816619396 Training error :16.7775821685791\n",
      "Epoch : 2219 Loss: 83.93420794606209 Training error :16.659547805786133\n",
      "Epoch : 2220 Loss: 83.34388361871243 Training error :16.54222297668457\n",
      "Epoch : 2221 Loss: 82.75735691189766 Training error :16.4255313873291\n",
      "Epoch : 2222 Loss: 82.17391835153103 Training error :16.30950164794922\n",
      "Epoch : 2223 Loss: 81.59383663535118 Training error :16.19416046142578\n",
      "Epoch : 2224 Loss: 81.01699039340019 Training error :16.0794620513916\n",
      "Epoch : 2225 Loss: 80.44341759383678 Training error :15.965435981750488\n",
      "Epoch : 2226 Loss: 79.87332516908646 Training error :15.852084159851074\n",
      "Epoch : 2227 Loss: 79.30651167035103 Training error :15.739411354064941\n",
      "Epoch : 2228 Loss: 78.74308609962463 Training error :15.627357482910156\n",
      "Epoch : 2229 Loss: 78.18284426629543 Training error :15.51596450805664\n",
      "Epoch : 2230 Loss: 77.62588322162628 Training error :15.405255317687988\n",
      "Epoch : 2231 Loss: 77.07216431200504 Training error :15.295138359069824\n",
      "Epoch : 2232 Loss: 76.52177499234676 Training error :15.185688018798828\n",
      "Epoch : 2233 Loss: 75.97450502216816 Training error :15.076872825622559\n",
      "Epoch : 2234 Loss: 75.43031311035156 Training error :14.968698501586914\n",
      "Epoch : 2235 Loss: 74.88955481350422 Training error :14.861167907714844\n",
      "Epoch : 2236 Loss: 74.35181000828743 Training error :14.754276275634766\n",
      "Epoch : 2237 Loss: 73.8172724545002 Training error :14.647989273071289\n",
      "Epoch : 2238 Loss: 73.28582587838173 Training error :14.542348861694336\n",
      "Epoch : 2239 Loss: 72.75772620737553 Training error :14.437299728393555\n",
      "Epoch : 2240 Loss: 72.23235577344894 Training error :14.332897186279297\n",
      "Epoch : 2241 Loss: 71.71039301156998 Training error :14.22911262512207\n",
      "Epoch : 2242 Loss: 71.19138468801975 Training error :14.125919342041016\n",
      "Epoch : 2243 Loss: 70.67547951638699 Training error :14.023362159729004\n",
      "Epoch : 2244 Loss: 70.16259030997753 Training error :13.921425819396973\n",
      "Epoch : 2245 Loss: 69.65293225646019 Training error :13.820038795471191\n",
      "Epoch : 2246 Loss: 69.14609855413437 Training error :13.719313621520996\n",
      "Epoch : 2247 Loss: 68.64235830307007 Training error :13.619179725646973\n",
      "Epoch : 2248 Loss: 68.14172191917896 Training error :13.519652366638184\n",
      "Epoch : 2249 Loss: 67.64416164159775 Training error :13.420733451843262\n",
      "Epoch : 2250 Loss: 67.14963281154633 Training error :13.322488784790039\n",
      "Epoch : 2251 Loss: 66.65802100300789 Training error :13.224780082702637\n",
      "Epoch : 2252 Loss: 66.16962110996246 Training error :13.127671241760254\n",
      "Epoch : 2253 Loss: 65.68417851626873 Training error :13.031139373779297\n",
      "Epoch : 2254 Loss: 65.20168969035149 Training error :12.93515396118164\n",
      "Epoch : 2255 Loss: 64.72173030674458 Training error :12.839757919311523\n",
      "Epoch : 2256 Loss: 64.24464106559753 Training error :12.744922637939453\n",
      "Epoch : 2257 Loss: 63.7705904841423 Training error :12.650683403015137\n",
      "Epoch : 2258 Loss: 63.299487859010696 Training error :12.556984901428223\n",
      "Epoch : 2259 Loss: 62.83091074228287 Training error :12.463884353637695\n",
      "Epoch : 2260 Loss: 62.36544480919838 Training error :12.371330261230469\n",
      "Epoch : 2261 Loss: 61.90260551124811 Training error :12.279374122619629\n",
      "Epoch : 2262 Loss: 61.44280153512955 Training error :12.187957763671875\n",
      "Epoch : 2263 Loss: 60.98569905012846 Training error :12.097087860107422\n",
      "Epoch : 2264 Loss: 60.53138817846775 Training error :12.006814956665039\n",
      "Epoch : 2265 Loss: 60.07997692376375 Training error :11.917236328125\n",
      "Epoch : 2266 Loss: 59.63217244297266 Training error :11.828577041625977\n",
      "Epoch : 2267 Loss: 59.18907265365124 Training error :11.740560531616211\n",
      "Epoch : 2268 Loss: 58.74893523007631 Training error :11.653084754943848\n",
      "Epoch : 2269 Loss: 58.31160581111908 Training error :11.566163063049316\n",
      "Epoch : 2270 Loss: 57.8770257383585 Training error :11.479798316955566\n",
      "Epoch : 2271 Loss: 57.44511014968157 Training error :11.393915176391602\n",
      "Epoch : 2272 Loss: 57.01570211350918 Training error :11.308642387390137\n",
      "Epoch : 2273 Loss: 56.58916709572077 Training error :11.223873138427734\n",
      "Epoch : 2274 Loss: 56.16542910039425 Training error :11.139632225036621\n",
      "Epoch : 2275 Loss: 55.74426106363535 Training error :11.055903434753418\n",
      "Epoch : 2276 Loss: 55.32561897486448 Training error :10.972728729248047\n",
      "Epoch : 2277 Loss: 54.90975972265005 Training error :10.890032768249512\n",
      "Epoch : 2278 Loss: 54.49622017890215 Training error :10.807894706726074\n",
      "Epoch : 2279 Loss: 54.08547156304121 Training error :10.726293563842773\n",
      "Epoch : 2280 Loss: 53.677527375519276 Training error :10.645195007324219\n",
      "Epoch : 2281 Loss: 53.27203396707773 Training error :10.564592361450195\n",
      "Epoch : 2282 Loss: 52.86906881630421 Training error :10.48449420928955\n",
      "Epoch : 2283 Loss: 52.468703396618366 Training error :10.404945373535156\n",
      "Epoch : 2284 Loss: 52.07087582349777 Training error :10.325855255126953\n",
      "Epoch : 2285 Loss: 51.675459772348404 Training error :10.247289657592773\n",
      "Epoch : 2286 Loss: 51.282578840851784 Training error :10.169219017028809\n",
      "Epoch : 2287 Loss: 50.8921602666378 Training error :10.091641426086426\n",
      "Epoch : 2288 Loss: 50.50439518690109 Training error :10.014575958251953\n",
      "Epoch : 2289 Loss: 50.11903101950884 Training error :9.938013076782227\n",
      "Epoch : 2290 Loss: 49.736236825585365 Training error :9.861917495727539\n",
      "Epoch : 2291 Loss: 49.35578493028879 Training error :9.786324501037598\n",
      "Epoch : 2292 Loss: 48.97771052271128 Training error :9.711235046386719\n",
      "Epoch : 2293 Loss: 48.60223061591387 Training error :9.636641502380371\n",
      "Epoch : 2294 Loss: 48.22935000061989 Training error :9.562500953674316\n",
      "Epoch : 2295 Loss: 47.85873248428106 Training error :9.488866806030273\n",
      "Epoch : 2296 Loss: 47.4905042424798 Training error :9.415706634521484\n",
      "Epoch : 2297 Loss: 47.124856390058994 Training error :9.343040466308594\n",
      "Epoch : 2298 Loss: 46.76135001331568 Training error :9.270837783813477\n",
      "Epoch : 2299 Loss: 46.40039183199406 Training error :9.199110984802246\n",
      "Epoch : 2300 Loss: 46.041700065135956 Training error :9.127873420715332\n",
      "Epoch : 2301 Loss: 45.685566283762455 Training error :9.057085990905762\n",
      "Epoch : 2302 Loss: 45.33161027729511 Training error :8.986784934997559\n",
      "Epoch : 2303 Loss: 44.980172485113144 Training error :8.916922569274902\n",
      "Epoch : 2304 Loss: 44.63090070337057 Training error :8.847541809082031\n",
      "Epoch : 2305 Loss: 44.283527694642544 Training error :8.778698921203613\n",
      "Epoch : 2306 Loss: 43.93961973488331 Training error :8.710265159606934\n",
      "Epoch : 2307 Loss: 43.597549602389336 Training error :8.642254829406738\n",
      "Epoch : 2308 Loss: 43.25763671845198 Training error :8.574716567993164\n",
      "Epoch : 2309 Loss: 42.91984224319458 Training error :8.507587432861328\n",
      "Epoch : 2310 Loss: 42.58429679274559 Training error :8.440939903259277\n",
      "Epoch : 2311 Loss: 42.2509895414114 Training error :8.374746322631836\n",
      "Epoch : 2312 Loss: 41.91999013721943 Training error :8.308993339538574\n",
      "Epoch : 2313 Loss: 41.59124682843685 Training error :8.243695259094238\n",
      "Epoch : 2314 Loss: 41.2647296115756 Training error :8.178841590881348\n",
      "Epoch : 2315 Loss: 40.94050043076277 Training error :8.114419937133789\n",
      "Epoch : 2316 Loss: 40.61834458261728 Training error :8.050443649291992\n",
      "Epoch : 2317 Loss: 40.29849845170975 Training error :7.986888885498047\n",
      "Epoch : 2318 Loss: 39.98064696788788 Training error :7.923776626586914\n",
      "Epoch : 2319 Loss: 39.66518922150135 Training error :7.861116409301758\n",
      "Epoch : 2320 Loss: 39.35159669816494 Training error :7.799535751342773\n",
      "Epoch : 2321 Loss: 39.04379294812679 Training error :7.738468647003174\n",
      "Epoch : 2322 Loss: 38.73847405612469 Training error :7.677842617034912\n",
      "Epoch : 2323 Loss: 38.43547452241182 Training error :7.617663383483887\n",
      "Epoch : 2324 Loss: 38.13452784344554 Training error :7.557880401611328\n",
      "Epoch : 2325 Loss: 37.83564981818199 Training error :7.4985246658325195\n",
      "Epoch : 2326 Loss: 37.53891886025667 Training error :7.439566612243652\n",
      "Epoch : 2327 Loss: 37.244170773774385 Training error :7.38104248046875\n",
      "Epoch : 2328 Loss: 36.95155202224851 Training error :7.322918891906738\n",
      "Epoch : 2329 Loss: 36.66105840355158 Training error :7.2652082443237305\n",
      "Epoch : 2330 Loss: 36.37243286520243 Training error :7.207889556884766\n",
      "Epoch : 2331 Loss: 36.085865527391434 Training error :7.150992393493652\n",
      "Epoch : 2332 Loss: 35.80145064741373 Training error :7.094489097595215\n",
      "Epoch : 2333 Loss: 35.51891250163317 Training error :7.038378715515137\n",
      "Epoch : 2334 Loss: 35.23838875442743 Training error :6.982693672180176\n",
      "Epoch : 2335 Loss: 34.96003257483244 Training error :6.927379131317139\n",
      "Epoch : 2336 Loss: 34.68343064934015 Training error :6.872451305389404\n",
      "Epoch : 2337 Loss: 34.40876238420606 Training error :6.8179121017456055\n",
      "Epoch : 2338 Loss: 34.13611588999629 Training error :6.763757228851318\n",
      "Epoch : 2339 Loss: 33.86525798588991 Training error :6.710007190704346\n",
      "Epoch : 2340 Loss: 33.5965685620904 Training error :6.656659126281738\n",
      "Epoch : 2341 Loss: 33.32987507432699 Training error :6.6036882400512695\n",
      "Epoch : 2342 Loss: 33.06496874243021 Training error :6.551066875457764\n",
      "Epoch : 2343 Loss: 32.80191780999303 Training error :6.498873710632324\n",
      "Epoch : 2344 Loss: 32.54089463874698 Training error :6.447025775909424\n",
      "Epoch : 2345 Loss: 32.28168262168765 Training error :6.3955793380737305\n",
      "Epoch : 2346 Loss: 32.02452480047941 Training error :6.344494819641113\n",
      "Epoch : 2347 Loss: 31.769031513482332 Training error :6.293792724609375\n",
      "Epoch : 2348 Loss: 31.515516500920057 Training error :6.243459701538086\n",
      "Epoch : 2349 Loss: 31.263864684849977 Training error :6.193495273590088\n",
      "Epoch : 2350 Loss: 31.014035932719707 Training error :6.1439080238342285\n",
      "Epoch : 2351 Loss: 30.766163934022188 Training error :6.094690322875977\n",
      "Epoch : 2352 Loss: 30.520021807402372 Training error :6.0458245277404785\n",
      "Epoch : 2353 Loss: 30.275674354285 Training error :5.9973225593566895\n",
      "Epoch : 2354 Loss: 30.03316967561841 Training error :5.94919490814209\n",
      "Epoch : 2355 Loss: 29.792543541640043 Training error :5.901400566101074\n",
      "Epoch : 2356 Loss: 29.553644008934498 Training error :5.853972911834717\n",
      "Epoch : 2357 Loss: 29.316398076713085 Training error :5.806904315948486\n",
      "Epoch : 2358 Loss: 29.081182070076466 Training error :5.7602009773254395\n",
      "Epoch : 2359 Loss: 28.847636438906193 Training error :5.713825225830078\n",
      "Epoch : 2360 Loss: 28.615675758570433 Training error :5.667811393737793\n",
      "Epoch : 2361 Loss: 28.385659504681826 Training error :5.6221604347229\n",
      "Epoch : 2362 Loss: 28.157332718372345 Training error :5.576827526092529\n",
      "Epoch : 2363 Loss: 27.930693984031677 Training error :5.531871318817139\n",
      "Epoch : 2364 Loss: 27.705947048962116 Training error :5.487222671508789\n",
      "Epoch : 2365 Loss: 27.48271058872342 Training error :5.442938804626465\n",
      "Epoch : 2366 Loss: 27.261263828724623 Training error :5.399002552032471\n",
      "Epoch : 2367 Loss: 27.041599042713642 Training error :5.355388641357422\n",
      "Epoch : 2368 Loss: 26.82349544018507 Training error :5.312112331390381\n",
      "Epoch : 2369 Loss: 26.60715965554118 Training error :5.269186973571777\n",
      "Epoch : 2370 Loss: 26.392424892634153 Training error :5.2265825271606445\n",
      "Epoch : 2371 Loss: 26.17938172072172 Training error :5.1843061447143555\n",
      "Epoch : 2372 Loss: 25.968086145818233 Training error :5.142487049102783\n",
      "Epoch : 2373 Loss: 25.758700292557478 Training error :5.10177755355835\n",
      "Epoch : 2374 Loss: 25.555224437266588 Training error :5.061501502990723\n",
      "Epoch : 2375 Loss: 25.35395646095276 Training error :5.021609306335449\n",
      "Epoch : 2376 Loss: 25.154680214822292 Training error :4.982049465179443\n",
      "Epoch : 2377 Loss: 24.95695323497057 Training error :4.942824363708496\n",
      "Epoch : 2378 Loss: 24.760651420801878 Training error :4.903927803039551\n",
      "Epoch : 2379 Loss: 24.566384620964527 Training error :4.8653178215026855\n",
      "Epoch : 2380 Loss: 24.373518876731396 Training error :4.8270182609558105\n",
      "Epoch : 2381 Loss: 24.18199907615781 Training error :4.789031982421875\n",
      "Epoch : 2382 Loss: 23.992085497826338 Training error :4.751337051391602\n",
      "Epoch : 2383 Loss: 23.803640358150005 Training error :4.713935375213623\n",
      "Epoch : 2384 Loss: 23.61671657115221 Training error :4.676846504211426\n",
      "Epoch : 2385 Loss: 23.431251406669617 Training error :4.640057563781738\n",
      "Epoch : 2386 Loss: 23.247368566691875 Training error :4.603562831878662\n",
      "Epoch : 2387 Loss: 23.06492578238249 Training error :4.567382335662842\n",
      "Epoch : 2388 Loss: 22.883965589106083 Training error :4.531485080718994\n",
      "Epoch : 2389 Loss: 22.704431392252445 Training error :4.495879173278809\n",
      "Epoch : 2390 Loss: 22.52646493911743 Training error :4.460574626922607\n",
      "Epoch : 2391 Loss: 22.349994719028473 Training error :4.425572395324707\n",
      "Epoch : 2392 Loss: 22.17497841641307 Training error :4.390853404998779\n",
      "Epoch : 2393 Loss: 22.001326501369476 Training error :4.356420993804932\n",
      "Epoch : 2394 Loss: 21.829274129122496 Training error :4.322288990020752\n",
      "Epoch : 2395 Loss: 21.65859141200781 Training error :4.288427829742432\n",
      "Epoch : 2396 Loss: 21.48927902430296 Training error :4.254866123199463\n",
      "Epoch : 2397 Loss: 21.321474581956863 Training error :4.221564769744873\n",
      "Epoch : 2398 Loss: 21.15508099272847 Training error :4.188554763793945\n",
      "Epoch : 2399 Loss: 20.989900212734938 Training error :4.155813694000244\n",
      "Epoch : 2400 Loss: 20.826292879879475 Training error :4.123355865478516\n",
      "Epoch : 2401 Loss: 20.663977868855 Training error :4.091154098510742\n",
      "Epoch : 2402 Loss: 20.503026209771633 Training error :4.059247016906738\n",
      "Epoch : 2403 Loss: 20.34344194084406 Training error :4.027594089508057\n",
      "Epoch : 2404 Loss: 20.1852533146739 Training error :3.9962317943573\n",
      "Epoch : 2405 Loss: 20.028452068567276 Training error :3.9651267528533936\n",
      "Epoch : 2406 Loss: 19.87289283797145 Training error :3.9342899322509766\n",
      "Epoch : 2407 Loss: 19.718785528093576 Training error :3.9037060737609863\n",
      "Epoch : 2408 Loss: 19.565878182649612 Training error :3.8733932971954346\n",
      "Epoch : 2409 Loss: 19.414315462112427 Training error :3.843337297439575\n",
      "Epoch : 2410 Loss: 19.26407504826784 Training error :3.8135321140289307\n",
      "Epoch : 2411 Loss: 19.115047708153725 Training error :3.7839999198913574\n",
      "Epoch : 2412 Loss: 18.96735766530037 Training error :3.7547149658203125\n",
      "Epoch : 2413 Loss: 18.82099210470915 Training error :3.72567081451416\n",
      "Epoch : 2414 Loss: 18.675686925649643 Training error :3.696897268295288\n",
      "Epoch : 2415 Loss: 18.531849712133408 Training error :3.668355703353882\n",
      "Epoch : 2416 Loss: 18.38914691656828 Training error :3.640052080154419\n",
      "Epoch : 2417 Loss: 18.247745767235756 Training error :3.612017869949341\n",
      "Epoch : 2418 Loss: 18.107494190335274 Training error :3.584226608276367\n",
      "Epoch : 2419 Loss: 17.968556329607964 Training error :3.5566983222961426\n",
      "Epoch : 2420 Loss: 17.83092389255762 Training error :3.5293917655944824\n",
      "Epoch : 2421 Loss: 17.694445692002773 Training error :3.502340793609619\n",
      "Epoch : 2422 Loss: 17.55918075144291 Training error :3.475541591644287\n",
      "Epoch : 2423 Loss: 17.425147026777267 Training error :3.448964834213257\n",
      "Epoch : 2424 Loss: 17.29229459911585 Training error :3.4226152896881104\n",
      "Epoch : 2425 Loss: 17.16057749837637 Training error :3.396501064300537\n",
      "Epoch : 2426 Loss: 17.030067332088947 Training error :3.370636224746704\n",
      "Epoch : 2427 Loss: 16.900729343295097 Training error :3.344987154006958\n",
      "Epoch : 2428 Loss: 16.772509768605232 Training error :3.3195865154266357\n",
      "Epoch : 2429 Loss: 16.645470216870308 Training error :3.294567823410034\n",
      "Epoch : 2430 Loss: 16.519972272217274 Training error :3.270620346069336\n",
      "Epoch : 2431 Loss: 16.400341361761093 Training error :3.247009515762329\n",
      "Epoch : 2432 Loss: 16.282594069838524 Training error :3.2236859798431396\n",
      "Epoch : 2433 Loss: 16.166137397289276 Training error :3.2005727291107178\n",
      "Epoch : 2434 Loss: 16.050669640302658 Training error :3.1776845455169678\n",
      "Epoch : 2435 Loss: 15.936380803585052 Training error :3.1550111770629883\n",
      "Epoch : 2436 Loss: 15.823037013411522 Training error :3.132560968399048\n",
      "Epoch : 2437 Loss: 15.710866212844849 Training error :3.110318183898926\n",
      "Epoch : 2438 Loss: 15.599723130464554 Training error :3.088294267654419\n",
      "Epoch : 2439 Loss: 15.48970826715231 Training error :3.0664784908294678\n",
      "Epoch : 2440 Loss: 15.380656063556671 Training error :3.0448665618896484\n",
      "Epoch : 2441 Loss: 15.272631108760834 Training error :3.023467779159546\n",
      "Epoch : 2442 Loss: 15.165685035288334 Training error :3.002256393432617\n",
      "Epoch : 2443 Loss: 15.059735707938671 Training error :2.9812679290771484\n",
      "Epoch : 2444 Loss: 14.954832181334496 Training error :2.9604601860046387\n",
      "Epoch : 2445 Loss: 14.85083245486021 Training error :2.939864158630371\n",
      "Epoch : 2446 Loss: 14.747872352600098 Training error :2.919454336166382\n",
      "Epoch : 2447 Loss: 14.64592320472002 Training error :2.899245262145996\n",
      "Epoch : 2448 Loss: 14.544898495078087 Training error :2.8792412281036377\n",
      "Epoch : 2449 Loss: 14.44491422176361 Training error :2.8594167232513428\n",
      "Epoch : 2450 Loss: 14.345833763480186 Training error :2.8397762775421143\n",
      "Epoch : 2451 Loss: 14.247659124433994 Training error :2.8203377723693848\n",
      "Epoch : 2452 Loss: 14.150506645441055 Training error :2.8010799884796143\n",
      "Epoch : 2453 Loss: 14.054301999509335 Training error :2.7820234298706055\n",
      "Epoch : 2454 Loss: 13.958993747830391 Training error :2.7631490230560303\n",
      "Epoch : 2455 Loss: 13.864676967263222 Training error :2.7444536685943604\n",
      "Epoch : 2456 Loss: 13.771209955215454 Training error :2.7259325981140137\n",
      "Epoch : 2457 Loss: 13.678720086812973 Training error :2.7076096534729004\n",
      "Epoch : 2458 Loss: 13.587153211236 Training error :2.6894571781158447\n",
      "Epoch : 2459 Loss: 13.496411092579365 Training error :2.6714866161346436\n",
      "Epoch : 2460 Loss: 13.406596288084984 Training error :2.653689384460449\n",
      "Epoch : 2461 Loss: 13.317611791193485 Training error :2.6360671520233154\n",
      "Epoch : 2462 Loss: 13.229554772377014 Training error :2.6186270713806152\n",
      "Epoch : 2463 Loss: 13.142394505441189 Training error :2.601353168487549\n",
      "Epoch : 2464 Loss: 13.05608256161213 Training error :2.5842502117156982\n",
      "Epoch : 2465 Loss: 12.970576122403145 Training error :2.5673129558563232\n",
      "Epoch : 2466 Loss: 12.885979652404785 Training error :2.5505454540252686\n",
      "Epoch : 2467 Loss: 12.802203886210918 Training error :2.53395414352417\n",
      "Epoch : 2468 Loss: 12.719227626919746 Training error :2.5175278186798096\n",
      "Epoch : 2469 Loss: 12.6371049284935 Training error :2.5012552738189697\n",
      "Epoch : 2470 Loss: 12.555791653692722 Training error :2.4851367473602295\n",
      "Epoch : 2471 Loss: 12.475291877985 Training error :2.469188928604126\n",
      "Epoch : 2472 Loss: 12.395572327077389 Training error :2.453394889831543\n",
      "Epoch : 2473 Loss: 12.316649675369263 Training error :2.4377622604370117\n",
      "Epoch : 2474 Loss: 12.23848221451044 Training error :2.4222817420959473\n",
      "Epoch : 2475 Loss: 12.161188878118992 Training error :2.4069433212280273\n",
      "Epoch : 2476 Loss: 12.084570929408073 Training error :2.3917813301086426\n",
      "Epoch : 2477 Loss: 12.008711785078049 Training error :2.3767621517181396\n",
      "Epoch : 2478 Loss: 11.933685004711151 Training error :2.361891508102417\n",
      "Epoch : 2479 Loss: 11.859427392482758 Training error :2.3471813201904297\n",
      "Epoch : 2480 Loss: 11.785887323319912 Training error :2.332606077194214\n",
      "Epoch : 2481 Loss: 11.713038131594658 Training error :2.318186044692993\n",
      "Epoch : 2482 Loss: 11.64101868122816 Training error :2.303903579711914\n",
      "Epoch : 2483 Loss: 11.569629490375519 Training error :2.28977108001709\n",
      "Epoch : 2484 Loss: 11.49903466552496 Training error :2.275777578353882\n",
      "Epoch : 2485 Loss: 11.42912333458662 Training error :2.2619309425354004\n",
      "Epoch : 2486 Loss: 11.359914280474186 Training error :2.248222589492798\n",
      "Epoch : 2487 Loss: 11.291357956826687 Training error :2.234644889831543\n",
      "Epoch : 2488 Loss: 11.223568469285965 Training error :2.2212071418762207\n",
      "Epoch : 2489 Loss: 11.156446851789951 Training error :2.207911491394043\n",
      "Epoch : 2490 Loss: 11.089996106922626 Training error :2.194751262664795\n",
      "Epoch : 2491 Loss: 11.024187453091145 Training error :2.181715488433838\n",
      "Epoch : 2492 Loss: 10.959095999598503 Training error :2.1688220500946045\n",
      "Epoch : 2493 Loss: 10.894671343266964 Training error :2.1560497283935547\n",
      "Epoch : 2494 Loss: 10.83088704943657 Training error :2.1434102058410645\n",
      "Epoch : 2495 Loss: 10.767777390778065 Training error :2.1309022903442383\n",
      "Epoch : 2496 Loss: 10.705255463719368 Training error :2.1185903549194336\n",
      "Epoch : 2497 Loss: 10.643179155886173 Training error :2.1073033809661865\n",
      "Epoch : 2498 Loss: 10.586979493498802 Training error :2.096252918243408\n",
      "Epoch : 2499 Loss: 10.532059326767921 Training error :2.085355043411255\n",
      "Epoch : 2500 Loss: 10.477847374975681 Training error :2.0745811462402344\n",
      "Epoch : 2501 Loss: 10.42418048530817 Training error :2.06392765045166\n",
      "Epoch : 2502 Loss: 10.371023707091808 Training error :2.0533969402313232\n",
      "Epoch : 2503 Loss: 10.318500988185406 Training error :2.042987585067749\n",
      "Epoch : 2504 Loss: 10.266573570668697 Training error :2.032698392868042\n",
      "Epoch : 2505 Loss: 10.215247668325901 Training error :2.0225226879119873\n",
      "Epoch : 2506 Loss: 10.164443917572498 Training error :2.0124611854553223\n",
      "Epoch : 2507 Loss: 10.114250171929598 Training error :2.0025055408477783\n",
      "Epoch : 2508 Loss: 10.064573273062706 Training error :1.9926681518554688\n",
      "Epoch : 2509 Loss: 10.015517756342888 Training error :1.982937216758728\n",
      "Epoch : 2510 Loss: 9.966940116137266 Training error :1.9733169078826904\n",
      "Epoch : 2511 Loss: 9.918895274400711 Training error :1.9637948274612427\n",
      "Epoch : 2512 Loss: 9.871394902467728 Training error :1.9543838500976562\n",
      "Epoch : 2513 Loss: 9.824372258037329 Training error :1.9448142051696777\n",
      "Epoch : 2514 Loss: 9.776577401906252 Training error :1.9353249073028564\n",
      "Epoch : 2515 Loss: 9.72919212281704 Training error :1.9259148836135864\n",
      "Epoch : 2516 Loss: 9.682237826287746 Training error :1.9165949821472168\n",
      "Epoch : 2517 Loss: 9.635752085596323 Training error :1.9073762893676758\n",
      "Epoch : 2518 Loss: 9.589754063636065 Training error :1.8982638120651245\n",
      "Epoch : 2519 Loss: 9.544284664094448 Training error :1.8892358541488647\n",
      "Epoch : 2520 Loss: 9.49922677129507 Training error :1.8803118467330933\n",
      "Epoch : 2521 Loss: 9.454715687781572 Training error :1.8714839220046997\n",
      "Epoch : 2522 Loss: 9.410637956112623 Training error :1.8627369403839111\n",
      "Epoch : 2523 Loss: 9.366989765316248 Training error :1.8540972471237183\n",
      "Epoch : 2524 Loss: 9.323850680142641 Training error :1.8455418348312378\n",
      "Epoch : 2525 Loss: 9.281161200255156 Training error :1.8370786905288696\n",
      "Epoch : 2526 Loss: 9.238949101418257 Training error :1.8287017345428467\n",
      "Epoch : 2527 Loss: 9.197130855172873 Training error :1.8204238414764404\n",
      "Epoch : 2528 Loss: 9.155806165188551 Training error :1.812211036682129\n",
      "Epoch : 2529 Loss: 9.11484245210886 Training error :1.8041024208068848\n",
      "Epoch : 2530 Loss: 9.07438488677144 Training error :1.7960773706436157\n",
      "Epoch : 2531 Loss: 9.034328114241362 Training error :1.7881269454956055\n",
      "Epoch : 2532 Loss: 8.994679413735867 Training error :1.7802643775939941\n",
      "Epoch : 2533 Loss: 8.955415070056915 Training error :1.7724831104278564\n",
      "Epoch : 2534 Loss: 8.916581969708204 Training error :1.764783501625061\n",
      "Epoch : 2535 Loss: 8.878146525472403 Training error :1.7571667432785034\n",
      "Epoch : 2536 Loss: 8.840158306062222 Training error :1.7496265172958374\n",
      "Epoch : 2537 Loss: 8.80256224796176 Training error :1.7421674728393555\n",
      "Epoch : 2538 Loss: 8.765325251966715 Training error :1.7347784042358398\n",
      "Epoch : 2539 Loss: 8.728510148823261 Training error :1.727476954460144\n",
      "Epoch : 2540 Loss: 8.691994290798903 Training error :1.7202404737472534\n",
      "Epoch : 2541 Loss: 8.655978936702013 Training error :1.7130874395370483\n",
      "Epoch : 2542 Loss: 8.620291754603386 Training error :1.7060023546218872\n",
      "Epoch : 2543 Loss: 8.584967032074928 Training error :1.6989870071411133\n",
      "Epoch : 2544 Loss: 8.549996249377728 Training error :1.6920429468154907\n",
      "Epoch : 2545 Loss: 8.515325460582972 Training error :1.6851718425750732\n",
      "Epoch : 2546 Loss: 8.481099184602499 Training error :1.6783746480941772\n",
      "Epoch : 2547 Loss: 8.447152312844992 Training error :1.6716406345367432\n",
      "Epoch : 2548 Loss: 8.41357371211052 Training error :1.6649807691574097\n",
      "Epoch : 2549 Loss: 8.380353506654501 Training error :1.6583830118179321\n",
      "Epoch : 2550 Loss: 8.347445294260979 Training error :1.6518456935882568\n",
      "Epoch : 2551 Loss: 8.314844317734241 Training error :1.6453828811645508\n",
      "Epoch : 2552 Loss: 8.282662559300661 Training error :1.6389901638031006\n",
      "Epoch : 2553 Loss: 8.2506939843297 Training error :1.6326504945755005\n",
      "Epoch : 2554 Loss: 8.219089861959219 Training error :1.626379132270813\n",
      "Epoch : 2555 Loss: 8.18782950565219 Training error :1.620166301727295\n",
      "Epoch : 2556 Loss: 8.156896099448204 Training error :1.614021897315979\n",
      "Epoch : 2557 Loss: 8.12624029070139 Training error :1.6079323291778564\n",
      "Epoch : 2558 Loss: 8.095922198146582 Training error :1.6019076108932495\n",
      "Epoch : 2559 Loss: 8.06589262932539 Training error :1.5959395170211792\n",
      "Epoch : 2560 Loss: 8.036119174212217 Training error :1.5900274515151978\n",
      "Epoch : 2561 Loss: 8.00665708631277 Training error :1.584173321723938\n",
      "Epoch : 2562 Loss: 7.9774627313017845 Training error :1.5783833265304565\n",
      "Epoch : 2563 Loss: 7.948599774390459 Training error :1.5726494789123535\n",
      "Epoch : 2564 Loss: 7.919988475739956 Training error :1.566965937614441\n",
      "Epoch : 2565 Loss: 7.891730606555939 Training error :1.5613406896591187\n",
      "Epoch : 2566 Loss: 7.8636890314519405 Training error :1.5557655096054077\n",
      "Epoch : 2567 Loss: 7.835913024842739 Training error :1.5502427816390991\n",
      "Epoch : 2568 Loss: 7.808390863239765 Training error :1.5447758436203003\n",
      "Epoch : 2569 Loss: 7.7811285480856895 Training error :1.5393539667129517\n",
      "Epoch : 2570 Loss: 7.754133868962526 Training error :1.5339893102645874\n",
      "Epoch : 2571 Loss: 7.7273722887039185 Training error :1.528677225112915\n",
      "Epoch : 2572 Loss: 7.70091999322176 Training error :1.5234092473983765\n",
      "Epoch : 2573 Loss: 7.674703199416399 Training error :1.5181937217712402\n",
      "Epoch : 2574 Loss: 7.648713804781437 Training error :1.5130270719528198\n",
      "Epoch : 2575 Loss: 7.622952815145254 Training error :1.5079047679901123\n",
      "Epoch : 2576 Loss: 7.597443152219057 Training error :1.502837061882019\n",
      "Epoch : 2577 Loss: 7.572224862873554 Training error :1.4978121519088745\n",
      "Epoch : 2578 Loss: 7.547216676175594 Training error :1.4928350448608398\n",
      "Epoch : 2579 Loss: 7.522414345294237 Training error :1.4878963232040405\n",
      "Epoch : 2580 Loss: 7.497830800712109 Training error :1.483007788658142\n",
      "Epoch : 2581 Loss: 7.4734694212675095 Training error :1.478155255317688\n",
      "Epoch : 2582 Loss: 7.449316680431366 Training error :1.4733490943908691\n",
      "Epoch : 2583 Loss: 7.425407733768225 Training error :1.4685945510864258\n",
      "Epoch : 2584 Loss: 7.401686906814575 Training error :1.4638744592666626\n",
      "Epoch : 2585 Loss: 7.378230154514313 Training error :1.4591978788375854\n",
      "Epoch : 2586 Loss: 7.354931488633156 Training error :1.4546074867248535\n",
      "Epoch : 2587 Loss: 7.331583637744188 Training error :1.4507782459259033\n",
      "Epoch : 2588 Loss: 7.312655154615641 Training error :1.4470402002334595\n",
      "Epoch : 2589 Loss: 7.294338844716549 Training error :1.4433382749557495\n",
      "Epoch : 2590 Loss: 7.2762009128928185 Training error :1.439682126045227\n",
      "Epoch : 2591 Loss: 7.258136488497257 Training error :1.4360718727111816\n",
      "Epoch : 2592 Loss: 7.2402601055800915 Training error :1.4325064420700073\n",
      "Epoch : 2593 Loss: 7.222567863762379 Training error :1.4289807081222534\n",
      "Epoch : 2594 Loss: 7.2050861194729805 Training error :1.4254851341247559\n",
      "Epoch : 2595 Loss: 7.187837947160006 Training error :1.4220314025878906\n",
      "Epoch : 2596 Loss: 7.17074279114604 Training error :1.4186162948608398\n",
      "Epoch : 2597 Loss: 7.153791099786758 Training error :1.4152352809906006\n",
      "Epoch : 2598 Loss: 7.1370407193899155 Training error :1.411892294883728\n",
      "Epoch : 2599 Loss: 7.1204439252614975 Training error :1.408579707145691\n",
      "Epoch : 2600 Loss: 7.1040274277329445 Training error :1.4053045511245728\n",
      "Epoch : 2601 Loss: 7.087791122496128 Training error :1.4020605087280273\n",
      "Epoch : 2602 Loss: 7.071745727211237 Training error :1.3988510370254517\n",
      "Epoch : 2603 Loss: 7.0557852648198605 Training error :1.3956773281097412\n",
      "Epoch : 2604 Loss: 7.040058102458715 Training error :1.3925329446792603\n",
      "Epoch : 2605 Loss: 7.024458058178425 Training error :1.3894115686416626\n",
      "Epoch : 2606 Loss: 7.009017698466778 Training error :1.3863308429718018\n",
      "Epoch : 2607 Loss: 6.9937427788972855 Training error :1.3832762241363525\n",
      "Epoch : 2608 Loss: 6.978607773780823 Training error :1.3802417516708374\n",
      "Epoch : 2609 Loss: 6.96354316547513 Training error :1.37723970413208\n",
      "Epoch : 2610 Loss: 6.948670439422131 Training error :1.3742753267288208\n",
      "Epoch : 2611 Loss: 6.933961376547813 Training error :1.3713299036026\n",
      "Epoch : 2612 Loss: 6.91938279569149 Training error :1.3684148788452148\n",
      "Epoch : 2613 Loss: 6.904940120875835 Training error :1.3655235767364502\n",
      "Epoch : 2614 Loss: 6.890594471246004 Training error :1.3626652956008911\n",
      "Epoch : 2615 Loss: 6.876415617763996 Training error :1.3598246574401855\n",
      "Epoch : 2616 Loss: 6.86237820237875 Training error :1.3570142984390259\n",
      "Epoch : 2617 Loss: 6.848458711057901 Training error :1.354231357574463\n",
      "Epoch : 2618 Loss: 6.83465101569891 Training error :1.3514689207077026\n",
      "Epoch : 2619 Loss: 6.820972476154566 Training error :1.3487305641174316\n",
      "Epoch : 2620 Loss: 6.807411216199398 Training error :1.346022129058838\n",
      "Epoch : 2621 Loss: 6.793977744877338 Training error :1.3433293104171753\n",
      "Epoch : 2622 Loss: 6.780675087124109 Training error :1.3406600952148438\n",
      "Epoch : 2623 Loss: 6.767428588122129 Training error :1.3380202054977417\n",
      "Epoch : 2624 Loss: 6.754358433187008 Training error :1.3353954553604126\n",
      "Epoch : 2625 Loss: 6.74133687466383 Training error :1.332800030708313\n",
      "Epoch : 2626 Loss: 6.7284818813204765 Training error :1.3302191495895386\n",
      "Epoch : 2627 Loss: 6.715738158673048 Training error :1.327663540840149\n",
      "Epoch : 2628 Loss: 6.703084234148264 Training error :1.3251287937164307\n",
      "Epoch : 2629 Loss: 6.690542805939913 Training error :1.3226172924041748\n",
      "Epoch : 2630 Loss: 6.678086061030626 Training error :1.3201173543930054\n",
      "Epoch : 2631 Loss: 6.6657015308737755 Training error :1.3176448345184326\n",
      "Epoch : 2632 Loss: 6.653459280729294 Training error :1.3151938915252686\n",
      "Epoch : 2633 Loss: 6.641295131295919 Training error :1.3127552270889282\n",
      "Epoch : 2634 Loss: 6.629259817302227 Training error :1.3103392124176025\n",
      "Epoch : 2635 Loss: 6.617317281663418 Training error :1.3079392910003662\n",
      "Epoch : 2636 Loss: 6.605450008064508 Training error :1.3055634498596191\n",
      "Epoch : 2637 Loss: 6.593693107366562 Training error :1.3032037019729614\n",
      "Epoch : 2638 Loss: 6.581997863948345 Training error :1.300857424736023\n",
      "Epoch : 2639 Loss: 6.570395801216364 Training error :1.2985323667526245\n",
      "Epoch : 2640 Loss: 6.558898810297251 Training error :1.2962230443954468\n",
      "Epoch : 2641 Loss: 6.547461215406656 Training error :1.293934941291809\n",
      "Epoch : 2642 Loss: 6.53614916279912 Training error :1.291658639907837\n",
      "Epoch : 2643 Loss: 6.524906363338232 Training error :1.2894046306610107\n",
      "Epoch : 2644 Loss: 6.5137108489871025 Training error :1.287160038948059\n",
      "Epoch : 2645 Loss: 6.502657454460859 Training error :1.2849355936050415\n",
      "Epoch : 2646 Loss: 6.491658873856068 Training error :1.2827244997024536\n",
      "Epoch : 2647 Loss: 6.480687960982323 Training error :1.2805317640304565\n",
      "Epoch : 2648 Loss: 6.469850305467844 Training error :1.2783496379852295\n",
      "Epoch : 2649 Loss: 6.459081776440144 Training error :1.2761874198913574\n",
      "Epoch : 2650 Loss: 6.448370836675167 Training error :1.2740384340286255\n",
      "Epoch : 2651 Loss: 6.437755681574345 Training error :1.271903157234192\n",
      "Epoch : 2652 Loss: 6.4272207245230675 Training error :1.2697821855545044\n",
      "Epoch : 2653 Loss: 6.416727088391781 Training error :1.267675518989563\n",
      "Epoch : 2654 Loss: 6.406324304640293 Training error :1.2655835151672363\n",
      "Epoch : 2655 Loss: 6.396008655428886 Training error :1.2635092735290527\n",
      "Epoch : 2656 Loss: 6.385725911706686 Training error :1.2614463567733765\n",
      "Epoch : 2657 Loss: 6.375559862703085 Training error :1.259395718574524\n",
      "Epoch : 2658 Loss: 6.365413628518581 Training error :1.2573541402816772\n",
      "Epoch : 2659 Loss: 6.355362959206104 Training error :1.2553342580795288\n",
      "Epoch : 2660 Loss: 6.345343250781298 Training error :1.253322720527649\n",
      "Epoch : 2661 Loss: 6.335405588150024 Training error :1.2513229846954346\n",
      "Epoch : 2662 Loss: 6.325525742024183 Training error :1.2493371963500977\n",
      "Epoch : 2663 Loss: 6.315739694982767 Training error :1.2473593950271606\n",
      "Epoch : 2664 Loss: 6.305986173450947 Training error :1.2453936338424683\n",
      "Epoch : 2665 Loss: 6.2963229194283485 Training error :1.2434457540512085\n",
      "Epoch : 2666 Loss: 6.2866604551672935 Training error :1.2415108680725098\n",
      "Epoch : 2667 Loss: 6.277083586901426 Training error :1.2394475936889648\n",
      "Epoch : 2668 Loss: 6.266902111470699 Training error :1.2372639179229736\n",
      "Epoch : 2669 Loss: 6.256165750324726 Training error :1.2350598573684692\n",
      "Epoch : 2670 Loss: 6.24526184424758 Training error :1.232864260673523\n",
      "Epoch : 2671 Loss: 6.234386391937733 Training error :1.2306734323501587\n",
      "Epoch : 2672 Loss: 6.223554991185665 Training error :1.2284926176071167\n",
      "Epoch : 2673 Loss: 6.212808195501566 Training error :1.2263296842575073\n",
      "Epoch : 2674 Loss: 6.202081594616175 Training error :1.2241663932800293\n",
      "Epoch : 2675 Loss: 6.191407717764378 Training error :1.222017765045166\n",
      "Epoch : 2676 Loss: 6.180795669555664 Training error :1.2198832035064697\n",
      "Epoch : 2677 Loss: 6.1702262088656425 Training error :1.217757225036621\n",
      "Epoch : 2678 Loss: 6.159723214805126 Training error :1.2156468629837036\n",
      "Epoch : 2679 Loss: 6.149272613227367 Training error :1.2135405540466309\n",
      "Epoch : 2680 Loss: 6.1388672813773155 Training error :1.211445689201355\n",
      "Epoch : 2681 Loss: 6.128531564027071 Training error :1.2093639373779297\n",
      "Epoch : 2682 Loss: 6.118227880448103 Training error :1.2072956562042236\n",
      "Epoch : 2683 Loss: 6.107981864362955 Training error :1.2052370309829712\n",
      "Epoch : 2684 Loss: 6.097831025719643 Training error :1.2031834125518799\n",
      "Epoch : 2685 Loss: 6.087701633572578 Training error :1.2011460065841675\n",
      "Epoch : 2686 Loss: 6.077620718628168 Training error :1.199112057685852\n",
      "Epoch : 2687 Loss: 6.06757403537631 Training error :1.1970908641815186\n",
      "Epoch : 2688 Loss: 6.057610601186752 Training error :1.1950818300247192\n",
      "Epoch : 2689 Loss: 6.04766209423542 Training error :1.1930804252624512\n",
      "Epoch : 2690 Loss: 6.037794716656208 Training error :1.191090703010559\n",
      "Epoch : 2691 Loss: 6.027946811169386 Training error :1.1891082525253296\n",
      "Epoch : 2692 Loss: 6.018155314028263 Training error :1.1871356964111328\n",
      "Epoch : 2693 Loss: 6.008445292711258 Training error :1.1851685047149658\n",
      "Epoch : 2694 Loss: 5.998734779655933 Training error :1.1832175254821777\n",
      "Epoch : 2695 Loss: 5.989082343876362 Training error :1.1812729835510254\n",
      "Epoch : 2696 Loss: 5.979451980441809 Training error :1.179335355758667\n",
      "Epoch : 2697 Loss: 5.969921197742224 Training error :1.1774086952209473\n",
      "Epoch : 2698 Loss: 5.9604206047952175 Training error :1.175490379333496\n",
      "Epoch : 2699 Loss: 5.950927741825581 Training error :1.1735825538635254\n",
      "Epoch : 2700 Loss: 5.941532678902149 Training error :1.171683669090271\n",
      "Epoch : 2701 Loss: 5.932140450924635 Training error :1.1697871685028076\n",
      "Epoch : 2702 Loss: 5.922821097075939 Training error :1.1679056882858276\n",
      "Epoch : 2703 Loss: 5.913508854806423 Training error :1.166031002998352\n",
      "Epoch : 2704 Loss: 5.904280371963978 Training error :1.1641682386398315\n",
      "Epoch : 2705 Loss: 5.895082142204046 Training error :1.162306785583496\n",
      "Epoch : 2706 Loss: 5.885891288518906 Training error :1.1604551076889038\n",
      "Epoch : 2707 Loss: 5.876752972602844 Training error :1.1586135625839233\n",
      "Epoch : 2708 Loss: 5.867659341543913 Training error :1.1567789316177368\n",
      "Epoch : 2709 Loss: 5.858628045767546 Training error :1.154955506324768\n",
      "Epoch : 2710 Loss: 5.849640879780054 Training error :1.1531332731246948\n",
      "Epoch : 2711 Loss: 5.840645592659712 Training error :1.1513255834579468\n",
      "Epoch : 2712 Loss: 5.831708088517189 Training error :1.1495177745819092\n",
      "Epoch : 2713 Loss: 5.822839949280024 Training error :1.147729516029358\n",
      "Epoch : 2714 Loss: 5.813992757350206 Training error :1.1459406614303589\n",
      "Epoch : 2715 Loss: 5.805152256041765 Training error :1.1441608667373657\n",
      "Epoch : 2716 Loss: 5.796416334807873 Training error :1.1423866748809814\n",
      "Epoch : 2717 Loss: 5.787653706967831 Training error :1.1406270265579224\n",
      "Epoch : 2718 Loss: 5.778960030525923 Training error :1.1388702392578125\n",
      "Epoch : 2719 Loss: 5.770295865833759 Training error :1.1371153593063354\n",
      "Epoch : 2720 Loss: 5.761666387319565 Training error :1.1353754997253418\n",
      "Epoch : 2721 Loss: 5.753098003566265 Training error :1.133642315864563\n",
      "Epoch : 2722 Loss: 5.744521263986826 Training error :1.1319154500961304\n",
      "Epoch : 2723 Loss: 5.736042287200689 Training error :1.1301904916763306\n",
      "Epoch : 2724 Loss: 5.72755029425025 Training error :1.1284784078598022\n",
      "Epoch : 2725 Loss: 5.719079781323671 Training error :1.1267741918563843\n",
      "Epoch : 2726 Loss: 5.710662230849266 Training error :1.1250715255737305\n",
      "Epoch : 2727 Loss: 5.702309645712376 Training error :1.1233820915222168\n",
      "Epoch : 2728 Loss: 5.693967420607805 Training error :1.1216975450515747\n",
      "Epoch : 2729 Loss: 5.68566569685936 Training error :1.120018482208252\n",
      "Epoch : 2730 Loss: 5.677377466112375 Training error :1.118343472480774\n",
      "Epoch : 2731 Loss: 5.669151932001114 Training error :1.1166796684265137\n",
      "Epoch : 2732 Loss: 5.660941891372204 Training error :1.1150225400924683\n",
      "Epoch : 2733 Loss: 5.652796097099781 Training error :1.113369107246399\n",
      "Epoch : 2734 Loss: 5.644634060561657 Training error :1.1117256879806519\n",
      "Epoch : 2735 Loss: 5.636525809764862 Training error :1.110088586807251\n",
      "Epoch : 2736 Loss: 5.628464691340923 Training error :1.1084579229354858\n",
      "Epoch : 2737 Loss: 5.6204342767596245 Training error :1.1068294048309326\n",
      "Epoch : 2738 Loss: 5.612424813210964 Training error :1.105211615562439\n",
      "Epoch : 2739 Loss: 5.604455653578043 Training error :1.1035999059677124\n",
      "Epoch : 2740 Loss: 5.596512895077467 Training error :1.1019939184188843\n",
      "Epoch : 2741 Loss: 5.588573180139065 Training error :1.1003928184509277\n",
      "Epoch : 2742 Loss: 5.580713592469692 Training error :1.098799705505371\n",
      "Epoch : 2743 Loss: 5.572862681001425 Training error :1.0972117185592651\n",
      "Epoch : 2744 Loss: 5.5650287345051765 Training error :1.0956315994262695\n",
      "Epoch : 2745 Loss: 5.557258114218712 Training error :1.0941898822784424\n",
      "Epoch : 2746 Loss: 5.549626115709543 Training error :1.0929523706436157\n",
      "Epoch : 2747 Loss: 5.5434203296899796 Training error :1.0916643142700195\n",
      "Epoch : 2748 Loss: 5.537347741425037 Training error :1.0903724431991577\n",
      "Epoch : 2749 Loss: 5.531792361289263 Training error :1.0890501737594604\n",
      "Epoch : 2750 Loss: 5.525094896554947 Training error :1.0878214836120605\n",
      "Epoch : 2751 Loss: 5.51868736743927 Training error :1.0865894556045532\n",
      "Epoch : 2752 Loss: 5.512636944651604 Training error :1.0853297710418701\n",
      "Epoch : 2753 Loss: 5.507204424589872 Training error :1.0840526819229126\n",
      "Epoch : 2754 Loss: 5.500719480216503 Training error :1.0828487873077393\n",
      "Epoch : 2755 Loss: 5.494405325502157 Training error :1.0816524028778076\n",
      "Epoch : 2756 Loss: 5.488546464592218 Training error :1.0804065465927124\n",
      "Epoch : 2757 Loss: 5.482636068016291 Training error :1.0792196989059448\n",
      "Epoch : 2758 Loss: 5.477511256933212 Training error :1.077939510345459\n",
      "Epoch : 2759 Loss: 5.470802001655102 Training error :1.0767847299575806\n",
      "Epoch : 2760 Loss: 5.464770548045635 Training error :1.0756243467330933\n",
      "Epoch : 2761 Loss: 5.4590323232114315 Training error :1.0744078159332275\n",
      "Epoch : 2762 Loss: 5.453327637165785 Training error :1.0732767581939697\n",
      "Epoch : 2763 Loss: 5.448415644466877 Training error :1.072006106376648\n",
      "Epoch : 2764 Loss: 5.441780909895897 Training error :1.0708836317062378\n",
      "Epoch : 2765 Loss: 5.435901742428541 Training error :1.0697473287582397\n",
      "Epoch : 2766 Loss: 5.430320076644421 Training error :1.0685712099075317\n",
      "Epoch : 2767 Loss: 5.424740433692932 Training error :1.0674548149108887\n",
      "Epoch : 2768 Loss: 5.419980846345425 Training error :1.0662213563919067\n",
      "Epoch : 2769 Loss: 5.4134970381855965 Training error :1.0651311874389648\n",
      "Epoch : 2770 Loss: 5.40772258490324 Training error :1.0640276670455933\n",
      "Epoch : 2771 Loss: 5.402314685285091 Training error :1.0628759860992432\n",
      "Epoch : 2772 Loss: 5.396886989474297 Training error :1.0617578029632568\n",
      "Epoch : 2773 Loss: 5.392065819352865 Training error :1.0605837106704712\n",
      "Epoch : 2774 Loss: 5.385880000889301 Training error :1.059517741203308\n",
      "Epoch : 2775 Loss: 5.380227621644735 Training error :1.0584442615509033\n",
      "Epoch : 2776 Loss: 5.374946244060993 Training error :1.0573198795318604\n",
      "Epoch : 2777 Loss: 5.369633410125971 Training error :1.056187629699707\n",
      "Epoch : 2778 Loss: 5.364216227084398 Training error :1.0551544427871704\n",
      "Epoch : 2779 Loss: 5.35967318341136 Training error :1.053981065750122\n",
      "Epoch : 2780 Loss: 5.353485971689224 Training error :1.0529453754425049\n",
      "Epoch : 2781 Loss: 5.348009992390871 Training error :1.05189847946167\n",
      "Epoch : 2782 Loss: 5.3429153971374035 Training error :1.0508050918579102\n",
      "Epoch : 2783 Loss: 5.337743788957596 Training error :1.0497134923934937\n",
      "Epoch : 2784 Loss: 5.333050422370434 Training error :1.0486254692077637\n",
      "Epoch : 2785 Loss: 5.327287420630455 Training error :1.0476213693618774\n",
      "Epoch : 2786 Loss: 5.321893949061632 Training error :1.0465987920761108\n",
      "Epoch : 2787 Loss: 5.31693397462368 Training error :1.0455323457717896\n",
      "Epoch : 2788 Loss: 5.311880227178335 Training error :1.0444519519805908\n",
      "Epoch : 2789 Loss: 5.306668005883694 Training error :1.0434108972549438\n",
      "Epoch : 2790 Loss: 5.302145730704069 Training error :1.042353630065918\n",
      "Epoch : 2791 Loss: 5.296529099345207 Training error :1.0413771867752075\n",
      "Epoch : 2792 Loss: 5.291299864649773 Training error :1.0403838157653809\n",
      "Epoch : 2793 Loss: 5.286453202366829 Training error :1.0393404960632324\n",
      "Epoch : 2794 Loss: 5.281524959951639 Training error :1.0382921695709229\n",
      "Epoch : 2795 Loss: 5.276484616100788 Training error :1.0372705459594727\n",
      "Epoch : 2796 Loss: 5.272038526833057 Training error :1.036247730255127\n",
      "Epoch : 2797 Loss: 5.266589276492596 Training error :1.0352998971939087\n",
      "Epoch : 2798 Loss: 5.261538490653038 Training error :1.0343343019485474\n",
      "Epoch : 2799 Loss: 5.256802998483181 Training error :1.0333207845687866\n",
      "Epoch : 2800 Loss: 5.252042070031166 Training error :1.0322985649108887\n",
      "Epoch : 2801 Loss: 5.247124847024679 Training error :1.0313067436218262\n",
      "Epoch : 2802 Loss: 5.242218427360058 Training error :1.0303764343261719\n",
      "Epoch : 2803 Loss: 5.238237652927637 Training error :1.0293240547180176\n",
      "Epoch : 2804 Loss: 5.232701301574707 Training error :1.0284061431884766\n",
      "Epoch : 2805 Loss: 5.227771393954754 Training error :1.0274752378463745\n",
      "Epoch : 2806 Loss: 5.223170571029186 Training error :1.0264915227890015\n",
      "Epoch : 2807 Loss: 5.21855328604579 Training error :1.0255085229873657\n",
      "Epoch : 2808 Loss: 5.213807541877031 Training error :1.0245556831359863\n",
      "Epoch : 2809 Loss: 5.209741644561291 Training error :1.0235743522644043\n",
      "Epoch : 2810 Loss: 5.204502735286951 Training error :1.0226869583129883\n",
      "Epoch : 2811 Loss: 5.199726607650518 Training error :1.021781086921692\n",
      "Epoch : 2812 Loss: 5.195291645824909 Training error :1.0208230018615723\n",
      "Epoch : 2813 Loss: 5.190794315189123 Training error :1.0198606252670288\n",
      "Epoch : 2814 Loss: 5.18617270514369 Training error :1.0189205408096313\n",
      "Epoch : 2815 Loss: 5.181533131748438 Training error :1.018034815788269\n",
      "Epoch : 2816 Loss: 5.177745819091797 Training error :1.0170512199401855\n",
      "Epoch : 2817 Loss: 5.17257359996438 Training error :1.0161899328231812\n",
      "Epoch : 2818 Loss: 5.167898535728455 Training error :1.0153112411499023\n",
      "Epoch : 2819 Loss: 5.163574740290642 Training error :1.0143840312957764\n",
      "Epoch : 2820 Loss: 5.159257985651493 Training error :1.0134602785110474\n",
      "Epoch : 2821 Loss: 5.154804050922394 Training error :1.012545108795166\n",
      "Epoch : 2822 Loss: 5.150280945003033 Training error :1.0116965770721436\n",
      "Epoch : 2823 Loss: 5.146662298589945 Training error :1.0107306241989136\n",
      "Epoch : 2824 Loss: 5.141613021492958 Training error :1.009899377822876\n",
      "Epoch : 2825 Loss: 5.137075744569302 Training error :1.0090513229370117\n",
      "Epoch : 2826 Loss: 5.132906664162874 Training error :1.008152723312378\n",
      "Epoch : 2827 Loss: 5.1287025064229965 Training error :1.0072511434555054\n",
      "Epoch : 2828 Loss: 5.124354839324951 Training error :1.0063681602478027\n",
      "Epoch : 2829 Loss: 5.120028957724571 Training error :1.0055434703826904\n",
      "Epoch : 2830 Loss: 5.1165675185620785 Training error :1.0046101808547974\n",
      "Epoch : 2831 Loss: 5.111624710261822 Training error :1.0038076639175415\n",
      "Epoch : 2832 Loss: 5.107245489954948 Training error :1.0029842853546143\n",
      "Epoch : 2833 Loss: 5.1032068356871605 Training error :1.002113699913025\n",
      "Epoch : 2834 Loss: 5.099126052111387 Training error :1.0012377500534058\n",
      "Epoch : 2835 Loss: 5.094937972724438 Training error :1.000383734703064\n",
      "Epoch : 2836 Loss: 5.090710952877998 Training error :0.9995739459991455\n",
      "Epoch : 2837 Loss: 5.087329629808664 Training error :0.9986740946769714\n",
      "Epoch : 2838 Loss: 5.082573521882296 Training error :0.9978407025337219\n",
      "Epoch : 2839 Loss: 5.078029405325651 Training error :0.9968351721763611\n",
      "Epoch : 2840 Loss: 5.073156479746103 Training error :0.9957512021064758\n",
      "Epoch : 2841 Loss: 5.068026065826416 Training error :0.9946601986885071\n",
      "Epoch : 2842 Loss: 5.062734726816416 Training error :0.9935869574546814\n",
      "Epoch : 2843 Loss: 5.057415910065174 Training error :0.9925580024719238\n",
      "Epoch : 2844 Loss: 5.052941493690014 Training error :0.9914382100105286\n",
      "Epoch : 2845 Loss: 5.047094218432903 Training error :0.9904475212097168\n",
      "Epoch : 2846 Loss: 5.041706044226885 Training error :0.9894362688064575\n",
      "Epoch : 2847 Loss: 5.0366786271333694 Training error :0.9883700609207153\n",
      "Epoch : 2848 Loss: 5.031692270189524 Training error :0.9873066544532776\n",
      "Epoch : 2849 Loss: 5.026483446359634 Training error :0.9862591624259949\n",
      "Epoch : 2850 Loss: 5.0213127844035625 Training error :0.9852854013442993\n",
      "Epoch : 2851 Loss: 5.01709159091115 Training error :0.9841774106025696\n",
      "Epoch : 2852 Loss: 5.011326357722282 Training error :0.9832152724266052\n",
      "Epoch : 2853 Loss: 5.0060830526053905 Training error :0.9822373986244202\n",
      "Epoch : 2854 Loss: 5.0012442991137505 Training error :0.9812042713165283\n",
      "Epoch : 2855 Loss: 4.99635273963213 Training error :0.9801660180091858\n",
      "Epoch : 2856 Loss: 4.991352207958698 Training error :0.9791610240936279\n",
      "Epoch : 2857 Loss: 4.9869678132236 Training error :0.9781371355056763\n",
      "Epoch : 2858 Loss: 4.981564491987228 Training error :0.9772033095359802\n",
      "Epoch : 2859 Loss: 4.976455871015787 Training error :0.9762505888938904\n",
      "Epoch : 2860 Loss: 4.97174397110939 Training error :0.975239634513855\n",
      "Epoch : 2861 Loss: 4.966992147266865 Training error :0.9742294549942017\n",
      "Epoch : 2862 Loss: 4.962137829512358 Training error :0.9732437133789062\n",
      "Epoch : 2863 Loss: 4.957240168005228 Training error :0.9722952246665955\n",
      "Epoch : 2864 Loss: 4.953125946223736 Training error :0.9712714552879333\n",
      "Epoch : 2865 Loss: 4.947808925062418 Training error :0.9703688025474548\n",
      "Epoch : 2866 Loss: 4.942866835743189 Training error :0.9694451689720154\n",
      "Epoch : 2867 Loss: 4.938277393579483 Training error :0.9684654474258423\n",
      "Epoch : 2868 Loss: 4.933646194636822 Training error :0.9674887657165527\n",
      "Epoch : 2869 Loss: 4.9289623983204365 Training error :0.9665319323539734\n",
      "Epoch : 2870 Loss: 4.924217898398638 Training error :0.965618371963501\n",
      "Epoch : 2871 Loss: 4.920297726988792 Training error :0.9646154046058655\n",
      "Epoch : 2872 Loss: 4.9150720946490765 Training error :0.9637454748153687\n",
      "Epoch : 2873 Loss: 4.910246409475803 Training error :0.9628516435623169\n",
      "Epoch : 2874 Loss: 4.905835956335068 Training error :0.9618986248970032\n",
      "Epoch : 2875 Loss: 4.901354566216469 Training error :0.9609458446502686\n",
      "Epoch : 2876 Loss: 4.896742857992649 Training error :0.9600170254707336\n",
      "Epoch : 2877 Loss: 4.892152048647404 Training error :0.9591394066810608\n",
      "Epoch : 2878 Loss: 4.888423640280962 Training error :0.9581615924835205\n",
      "Epoch : 2879 Loss: 4.883323512971401 Training error :0.9573218822479248\n",
      "Epoch : 2880 Loss: 4.8786666840314865 Training error :0.956452488899231\n",
      "Epoch : 2881 Loss: 4.874342255294323 Training error :0.9555290937423706\n",
      "Epoch : 2882 Loss: 4.870024506002665 Training error :0.9546067714691162\n",
      "Epoch : 2883 Loss: 4.865567117929459 Training error :0.9537091851234436\n",
      "Epoch : 2884 Loss: 4.861114609986544 Training error :0.9528467655181885\n",
      "Epoch : 2885 Loss: 4.857484970241785 Training error :0.9519028663635254\n",
      "Epoch : 2886 Loss: 4.852533515542746 Training error :0.9510863423347473\n",
      "Epoch : 2887 Loss: 4.847974065691233 Training error :0.9502460360527039\n",
      "Epoch : 2888 Loss: 4.843816217035055 Training error :0.9493528008460999\n",
      "Epoch : 2889 Loss: 4.839631751179695 Training error :0.9484509825706482\n",
      "Epoch : 2890 Loss: 4.835317552089691 Training error :0.9475849270820618\n",
      "Epoch : 2891 Loss: 4.83098578453064 Training error :0.9467380046844482\n",
      "Epoch : 2892 Loss: 4.827433321624994 Training error :0.9458286166191101\n",
      "Epoch : 2893 Loss: 4.822687696665525 Training error :0.9450425505638123\n",
      "Epoch : 2894 Loss: 4.818247735500336 Training error :0.9442293047904968\n",
      "Epoch : 2895 Loss: 4.814207069575787 Training error :0.9433574080467224\n",
      "Epoch : 2896 Loss: 4.810142517089844 Training error :0.9424902200698853\n",
      "Epoch : 2897 Loss: 4.806006375700235 Training error :0.9416450262069702\n",
      "Epoch : 2898 Loss: 4.801805153489113 Training error :0.9408078193664551\n",
      "Epoch : 2899 Loss: 4.797665387392044 Training error :0.9400278329849243\n",
      "Epoch : 2900 Loss: 4.794435031712055 Training error :0.9391021132469177\n",
      "Epoch : 2901 Loss: 4.789644878357649 Training error :0.9383490681648254\n",
      "Epoch : 2902 Loss: 4.785332806408405 Training error :0.937563955783844\n",
      "Epoch : 2903 Loss: 4.781420417129993 Training error :0.9367247819900513\n",
      "Epoch : 2904 Loss: 4.777509778738022 Training error :0.9358806610107422\n",
      "Epoch : 2905 Loss: 4.7734910771250725 Training error :0.9350689649581909\n",
      "Epoch : 2906 Loss: 4.7694373950362206 Training error :0.9342840313911438\n",
      "Epoch : 2907 Loss: 4.766180612146854 Training error :0.9334250688552856\n",
      "Epoch : 2908 Loss: 4.761700239032507 Training error :0.9326879978179932\n",
      "Epoch : 2909 Loss: 4.7574911043047905 Training error :0.9319320917129517\n",
      "Epoch : 2910 Loss: 4.75373500213027 Training error :0.9311142563819885\n",
      "Epoch : 2911 Loss: 4.7499233447015285 Training error :0.9302945733070374\n",
      "Epoch : 2912 Loss: 4.7460155710577965 Training error :0.9295081496238708\n",
      "Epoch : 2913 Loss: 4.742074951529503 Training error :0.9287253022193909\n",
      "Epoch : 2914 Loss: 4.738225668668747 Training error :0.9279811382293701\n",
      "Epoch : 2915 Loss: 4.735199324786663 Training error :0.9271221160888672\n",
      "Epoch : 2916 Loss: 4.730722580105066 Training error :0.9264209866523743\n",
      "Epoch : 2917 Loss: 4.72669604793191 Training error :0.9256889224052429\n",
      "Epoch : 2918 Loss: 4.723025158047676 Training error :0.9249001145362854\n",
      "Epoch : 2919 Loss: 4.719363562762737 Training error :0.9241112470626831\n",
      "Epoch : 2920 Loss: 4.715622302144766 Training error :0.9233493804931641\n",
      "Epoch : 2921 Loss: 4.711829766631126 Training error :0.9225946068763733\n",
      "Epoch : 2922 Loss: 4.708052847534418 Training error :0.9218881130218506\n",
      "Epoch : 2923 Loss: 4.7052192240953445 Training error :0.9210444688796997\n",
      "Epoch : 2924 Loss: 4.7008452750742435 Training error :0.9203695058822632\n",
      "Epoch : 2925 Loss: 4.696914661675692 Training error :0.9196678400039673\n",
      "Epoch : 2926 Loss: 4.69340742751956 Training error :0.9188986420631409\n",
      "Epoch : 2927 Loss: 4.689874898642302 Training error :0.9181392788887024\n",
      "Epoch : 2928 Loss: 4.686229094862938 Training error :0.9174046516418457\n",
      "Epoch : 2929 Loss: 4.682563658803701 Training error :0.9166710376739502\n",
      "Epoch : 2930 Loss: 4.678955219686031 Training error :0.9159892201423645\n",
      "Epoch : 2931 Loss: 4.676218267530203 Training error :0.9151768684387207\n",
      "Epoch : 2932 Loss: 4.6720025688409805 Training error :0.9145234823226929\n",
      "Epoch : 2933 Loss: 4.668186172842979 Training error :0.9138482809066772\n",
      "Epoch : 2934 Loss: 4.664772838354111 Training error :0.9131094217300415\n",
      "Epoch : 2935 Loss: 4.661430358886719 Training error :0.912369966506958\n",
      "Epoch : 2936 Loss: 4.657875690609217 Training error :0.9116616249084473\n",
      "Epoch : 2937 Loss: 4.6543391942977905 Training error :0.9109588861465454\n",
      "Epoch : 2938 Loss: 4.650865517556667 Training error :0.9102879166603088\n",
      "Epoch : 2939 Loss: 4.64819360524416 Training error :0.9095089435577393\n",
      "Epoch : 2940 Loss: 4.644148882478476 Training error :0.9088838696479797\n",
      "Epoch : 2941 Loss: 4.640438664704561 Training error :0.9082286953926086\n",
      "Epoch : 2942 Loss: 4.637177776545286 Training error :0.9075154662132263\n",
      "Epoch : 2943 Loss: 4.633908860385418 Training error :0.9068048000335693\n",
      "Epoch : 2944 Loss: 4.630491025745869 Training error :0.9061180949211121\n",
      "Epoch : 2945 Loss: 4.627083636820316 Training error :0.905439019203186\n",
      "Epoch : 2946 Loss: 4.623715445399284 Training error :0.9047737121582031\n",
      "Epoch : 2947 Loss: 4.62110660597682 Training error :0.9040371775627136\n",
      "Epoch : 2948 Loss: 4.617242645472288 Training error :0.9034366011619568\n",
      "Epoch : 2949 Loss: 4.613667018711567 Training error :0.9028113484382629\n",
      "Epoch : 2950 Loss: 4.610528606921434 Training error :0.9021172523498535\n",
      "Epoch : 2951 Loss: 4.6073882058262825 Training error :0.9014292359352112\n",
      "Epoch : 2952 Loss: 4.604073271155357 Training error :0.900769054889679\n",
      "Epoch : 2953 Loss: 4.600766330957413 Training error :0.9001108407974243\n",
      "Epoch : 2954 Loss: 4.597519803792238 Training error :0.8994519114494324\n",
      "Epoch : 2955 Loss: 4.59431142732501 Training error :0.8988359570503235\n",
      "Epoch : 2956 Loss: 4.591930788010359 Training error :0.8981002569198608\n",
      "Epoch : 2957 Loss: 4.588053155690432 Training error :0.8975297808647156\n",
      "Epoch : 2958 Loss: 4.584611125290394 Training error :0.8969236612319946\n",
      "Epoch : 2959 Loss: 4.581594433635473 Training error :0.8962594866752625\n",
      "Epoch : 2960 Loss: 4.578593771904707 Training error :0.895602285861969\n",
      "Epoch : 2961 Loss: 4.5754079930484295 Training error :0.8949622511863708\n",
      "Epoch : 2962 Loss: 4.572231702506542 Training error :0.8943327069282532\n",
      "Epoch : 2963 Loss: 4.569085702300072 Training error :0.8937039971351624\n",
      "Epoch : 2964 Loss: 4.56668571010232 Training error :0.8930274844169617\n",
      "Epoch : 2965 Loss: 4.5631443820893764 Training error :0.8924781680107117\n",
      "Epoch : 2966 Loss: 4.559775710105896 Training error :0.8918977379798889\n",
      "Epoch : 2967 Loss: 4.556903604418039 Training error :0.8912502527236938\n",
      "Epoch : 2968 Loss: 4.5539449863135815 Training error :0.8906111717224121\n",
      "Epoch : 2969 Loss: 4.550888210535049 Training error :0.8900010585784912\n",
      "Epoch : 2970 Loss: 4.54785780236125 Training error :0.8893917798995972\n",
      "Epoch : 2971 Loss: 4.544808845967054 Training error :0.8887790441513062\n",
      "Epoch : 2972 Loss: 4.541844863444567 Training error :0.8881752490997314\n",
      "Epoch : 2973 Loss: 4.539510581642389 Training error :0.8875198364257812\n",
      "Epoch : 2974 Loss: 4.536081079393625 Training error :0.8869961500167847\n",
      "Epoch : 2975 Loss: 4.5328600481152534 Training error :0.8864383101463318\n",
      "Epoch : 2976 Loss: 4.530038960278034 Training error :0.8858189582824707\n",
      "Epoch : 2977 Loss: 4.5272781029343605 Training error :0.8852037787437439\n",
      "Epoch : 2978 Loss: 4.524311397224665 Training error :0.8846135139465332\n",
      "Epoch : 2979 Loss: 4.5213613063097 Training error :0.8840324282646179\n",
      "Epoch : 2980 Loss: 4.5184728763997555 Training error :0.883442223072052\n",
      "Epoch : 2981 Loss: 4.51562387496233 Training error :0.8828584551811218\n",
      "Epoch : 2982 Loss: 4.5133828818798065 Training error :0.8822290897369385\n",
      "Epoch : 2983 Loss: 4.510096255689859 Training error :0.8817315697669983\n",
      "Epoch : 2984 Loss: 4.506959594786167 Training error :0.8811975717544556\n",
      "Epoch : 2985 Loss: 4.50430066511035 Training error :0.8805978894233704\n",
      "Epoch : 2986 Loss: 4.5016032457351685 Training error :0.8800038695335388\n",
      "Epoch : 2987 Loss: 4.498760908842087 Training error :0.8794416785240173\n",
      "Epoch : 2988 Loss: 4.4959378503263 Training error :0.8788806796073914\n",
      "Epoch : 2989 Loss: 4.493167832493782 Training error :0.8783112168312073\n",
      "Epoch : 2990 Loss: 4.490421772003174 Training error :0.8777524828910828\n",
      "Epoch : 2991 Loss: 4.487675823271275 Training error :0.8772231340408325\n",
      "Epoch : 2992 Loss: 4.485694445669651 Training error :0.8765899538993835\n",
      "Epoch : 2993 Loss: 4.482385143637657 Training error :0.8761127591133118\n",
      "Epoch : 2994 Loss: 4.479385353624821 Training error :0.8756056427955627\n",
      "Epoch : 2995 Loss: 4.476823382079601 Training error :0.8750321269035339\n",
      "Epoch : 2996 Loss: 4.474267363548279 Training error :0.8744649291038513\n",
      "Epoch : 2997 Loss: 4.471573006361723 Training error :0.8739202618598938\n",
      "Epoch : 2998 Loss: 4.468859560787678 Training error :0.8733858466148376\n",
      "Epoch : 2999 Loss: 4.466167725622654 Training error :0.872846782207489\n",
      "Epoch : 3000 Loss: 4.463557835668325 Training error :0.8723130226135254\n",
      "Epoch : 3001 Loss: 4.461603969335556 Training error :0.8717241883277893\n",
      "Epoch : 3002 Loss: 4.458478756248951 Training error :0.87127286195755\n",
      "Epoch : 3003 Loss: 4.4555912129580975 Training error :0.8707852959632874\n",
      "Epoch : 3004 Loss: 4.453151851892471 Training error :0.8702328205108643\n",
      "Epoch : 3005 Loss: 4.450654808431864 Training error :0.8696861267089844\n",
      "Epoch : 3006 Loss: 4.448067773133516 Training error :0.8691679239273071\n",
      "Epoch : 3007 Loss: 4.445457372814417 Training error :0.8686546087265015\n",
      "Epoch : 3008 Loss: 4.442948825657368 Training error :0.8681328296661377\n",
      "Epoch : 3009 Loss: 4.44040434807539 Training error :0.8676126003265381\n",
      "Epoch : 3010 Loss: 4.4378627724945545 Training error :0.8671157956123352\n",
      "Epoch : 3011 Loss: 4.436051990836859 Training error :0.8665400743484497\n",
      "Epoch : 3012 Loss: 4.433048035949469 Training error :0.8661100268363953\n",
      "Epoch : 3013 Loss: 4.43023993819952 Training error :0.8656457662582397\n",
      "Epoch : 3014 Loss: 4.427905734628439 Training error :0.865115761756897\n",
      "Epoch : 3015 Loss: 4.425545115023851 Training error :0.8645948767662048\n",
      "Epoch : 3016 Loss: 4.423068195581436 Training error :0.8640979528427124\n",
      "Epoch : 3017 Loss: 4.4205613024532795 Training error :0.8636062741279602\n",
      "Epoch : 3018 Loss: 4.418127220124006 Training error :0.8631033897399902\n",
      "Epoch : 3019 Loss: 4.415710397064686 Training error :0.8626073598861694\n",
      "Epoch : 3020 Loss: 4.413286171853542 Training error :0.8621346354484558\n",
      "Epoch : 3021 Loss: 4.411582432687283 Training error :0.8615801334381104\n",
      "Epoch : 3022 Loss: 4.408672042191029 Training error :0.8611708283424377\n",
      "Epoch : 3023 Loss: 4.406000081449747 Training error :0.8607313632965088\n",
      "Epoch : 3024 Loss: 4.403769321739674 Training error :0.860220730304718\n",
      "Epoch : 3025 Loss: 4.401543762534857 Training error :0.8597223162651062\n",
      "Epoch : 3026 Loss: 4.399150997400284 Training error :0.8592481017112732\n",
      "Epoch : 3027 Loss: 4.396794222295284 Training error :0.8587772250175476\n",
      "Epoch : 3028 Loss: 4.394442178308964 Training error :0.8583016991615295\n",
      "Epoch : 3029 Loss: 4.392105843871832 Training error :0.8578264117240906\n",
      "Epoch : 3030 Loss: 4.389792751520872 Training error :0.8573611974716187\n",
      "Epoch : 3031 Loss: 4.388135604560375 Training error :0.8568372130393982\n",
      "Epoch : 3032 Loss: 4.385388940572739 Training error :0.8564513921737671\n",
      "Epoch : 3033 Loss: 4.382807161659002 Training error :0.8560327887535095\n",
      "Epoch : 3034 Loss: 4.3806916400790215 Training error :0.8555420637130737\n",
      "Epoch : 3035 Loss: 4.378559581935406 Training error :0.855063259601593\n",
      "Epoch : 3036 Loss: 4.376273676753044 Training error :0.8546091318130493\n",
      "Epoch : 3037 Loss: 4.3739931024611 Training error :0.8541603684425354\n",
      "Epoch : 3038 Loss: 4.37173930183053 Training error :0.8537050485610962\n",
      "Epoch : 3039 Loss: 4.369549449533224 Training error :0.8532493114471436\n",
      "Epoch : 3040 Loss: 4.367318648844957 Training error :0.8528006076812744\n",
      "Epoch : 3041 Loss: 4.365124315023422 Training error :0.8523757457733154\n",
      "Epoch : 3042 Loss: 4.363696090877056 Training error :0.8518553972244263\n",
      "Epoch : 3043 Loss: 4.360944826155901 Training error :0.8514956831932068\n",
      "Epoch : 3044 Loss: 4.358483370393515 Training error :0.8510931134223938\n",
      "Epoch : 3045 Loss: 4.356444261968136 Training error :0.8506274223327637\n",
      "Epoch : 3046 Loss: 4.354431733489037 Training error :0.8501731157302856\n",
      "Epoch : 3047 Loss: 4.352281764149666 Training error :0.8497427105903625\n",
      "Epoch : 3048 Loss: 4.350095264613628 Training error :0.8493148684501648\n",
      "Epoch : 3049 Loss: 4.347977750003338 Training error :0.8488767743110657\n",
      "Epoch : 3050 Loss: 4.3458458334207535 Training error :0.8484443426132202\n",
      "Epoch : 3051 Loss: 4.343759119510651 Training error :0.8480153679847717\n",
      "Epoch : 3052 Loss: 4.341634679585695 Training error :0.8475567102432251\n",
      "Epoch : 3053 Loss: 4.340081084519625 Training error :0.8469003438949585\n",
      "Epoch : 3054 Loss: 4.3367982394993305 Training error :0.8463960289955139\n",
      "Epoch : 3055 Loss: 4.333624325692654 Training error :0.8458545804023743\n",
      "Epoch : 3056 Loss: 4.330848254263401 Training error :0.8452408313751221\n",
      "Epoch : 3057 Loss: 4.328087147325277 Training error :0.8446331024169922\n",
      "Epoch : 3058 Loss: 4.325175952166319 Training error :0.8440564870834351\n",
      "Epoch : 3059 Loss: 4.32224977016449 Training error :0.8434769511222839\n",
      "Epoch : 3060 Loss: 4.31938236951828 Training error :0.8428969383239746\n",
      "Epoch : 3061 Loss: 4.316551264375448 Training error :0.8423170447349548\n",
      "Epoch : 3062 Loss: 4.313709236681461 Training error :0.8417842388153076\n",
      "Epoch : 3063 Loss: 4.311698365956545 Training error :0.8411235213279724\n",
      "Epoch : 3064 Loss: 4.308258764445782 Training error :0.8406344652175903\n",
      "Epoch : 3065 Loss: 4.305159404873848 Training error :0.8401116132736206\n",
      "Epoch : 3066 Loss: 4.30249098315835 Training error :0.8395208120346069\n",
      "Epoch : 3067 Loss: 4.299856528639793 Training error :0.8389401435852051\n",
      "Epoch : 3068 Loss: 4.297063134610653 Training error :0.8383868932723999\n",
      "Epoch : 3069 Loss: 4.294279962778091 Training error :0.8378336429595947\n",
      "Epoch : 3070 Loss: 4.291520915925503 Training error :0.8372771739959717\n",
      "Epoch : 3071 Loss: 4.288805931806564 Training error :0.8367385268211365\n",
      "Epoch : 3072 Loss: 4.286809425801039 Training error :0.8361238241195679\n",
      "Epoch : 3073 Loss: 4.283630009740591 Training error :0.8356600999832153\n",
      "Epoch : 3074 Loss: 4.280602425336838 Training error :0.8351600766181946\n",
      "Epoch : 3075 Loss: 4.278067205101252 Training error :0.8345859050750732\n",
      "Epoch : 3076 Loss: 4.275510933250189 Training error :0.8340295553207397\n",
      "Epoch : 3077 Loss: 4.272825099527836 Training error :0.8334956169128418\n",
      "Epoch : 3078 Loss: 4.27011202275753 Training error :0.8329641222953796\n",
      "Epoch : 3079 Loss: 4.267480291426182 Training error :0.8324299454689026\n",
      "Epoch : 3080 Loss: 4.264877710491419 Training error :0.8318986296653748\n",
      "Epoch : 3081 Loss: 4.262264013290405 Training error :0.831407904624939\n",
      "Epoch : 3082 Loss: 4.260501347482204 Training error :0.8307892084121704\n",
      "Epoch : 3083 Loss: 4.257339026778936 Training error :0.8303516507148743\n",
      "Epoch : 3084 Loss: 4.254438161849976 Training error :0.8298718333244324\n",
      "Epoch : 3085 Loss: 4.251966025680304 Training error :0.8293225169181824\n",
      "Epoch : 3086 Loss: 4.249561693519354 Training error :0.8287880420684814\n",
      "Epoch : 3087 Loss: 4.246968500316143 Training error :0.828279972076416\n",
      "Epoch : 3088 Loss: 4.2444158382713795 Training error :0.8277716040611267\n",
      "Epoch : 3089 Loss: 4.241877470165491 Training error :0.8272585272789001\n",
      "Epoch : 3090 Loss: 4.239376530051231 Training error :0.8267509341239929\n",
      "Epoch : 3091 Loss: 4.237565599381924 Training error :0.8261879682540894\n",
      "Epoch : 3092 Loss: 4.234645288437605 Training error :0.8257700204849243\n",
      "Epoch : 3093 Loss: 4.23183535784483 Training error :0.8253099322319031\n",
      "Epoch : 3094 Loss: 4.229489553719759 Training error :0.8247804045677185\n",
      "Epoch : 3095 Loss: 4.227182865142822 Training error :0.824262261390686\n",
      "Epoch : 3096 Loss: 4.224691707640886 Training error :0.8237761855125427\n",
      "Epoch : 3097 Loss: 4.222174979746342 Training error :0.8232870697975159\n",
      "Epoch : 3098 Loss: 4.219763249158859 Training error :0.8227927684783936\n",
      "Epoch : 3099 Loss: 4.217363324016333 Training error :0.8223032355308533\n",
      "Epoch : 3100 Loss: 4.214971907436848 Training error :0.821827232837677\n",
      "Epoch : 3101 Loss: 4.213284518569708 Training error :0.8212765455245972\n",
      "Epoch : 3102 Loss: 4.210432507097721 Training error :0.8208798766136169\n",
      "Epoch : 3103 Loss: 4.207740645855665 Training error :0.8204376101493835\n",
      "Epoch : 3104 Loss: 4.205483250319958 Training error :0.8199326992034912\n",
      "Epoch : 3105 Loss: 4.2032681703567505 Training error :0.8194342255592346\n",
      "Epoch : 3106 Loss: 4.200894840061665 Training error :0.8189675211906433\n",
      "Epoch : 3107 Loss: 4.19852264970541 Training error :0.8184987902641296\n",
      "Epoch : 3108 Loss: 4.196189843118191 Training error :0.8180274963378906\n",
      "Epoch : 3109 Loss: 4.193901263177395 Training error :0.8175582885742188\n",
      "Epoch : 3110 Loss: 4.191600665450096 Training error :0.8170998096466064\n",
      "Epoch : 3111 Loss: 4.1900103613734245 Training error :0.8165716528892517\n",
      "Epoch : 3112 Loss: 4.187291290611029 Training error :0.8161909580230713\n",
      "Epoch : 3113 Loss: 4.1846596747636795 Training error :0.8157724738121033\n",
      "Epoch : 3114 Loss: 4.182475067675114 Training error :0.8152859210968018\n",
      "Epoch : 3115 Loss: 4.1803894601762295 Training error :0.8148109912872314\n",
      "Epoch : 3116 Loss: 4.178141333162785 Training error :0.8143637180328369\n",
      "Epoch : 3117 Loss: 4.175844497978687 Training error :0.8139143586158752\n",
      "Epoch : 3118 Loss: 4.173609267920256 Training error :0.8134613037109375\n",
      "Epoch : 3119 Loss: 4.1714091822505 Training error :0.8130059838294983\n",
      "Epoch : 3120 Loss: 4.169189158827066 Training error :0.8125608563423157\n",
      "Epoch : 3121 Loss: 4.166983533650637 Training error :0.812156081199646\n",
      "Epoch : 3122 Loss: 4.165656331926584 Training error :0.8116204142570496\n",
      "Epoch : 3123 Loss: 4.162888690829277 Training error :0.8112618923187256\n",
      "Epoch : 3124 Loss: 4.160361558198929 Training error :0.8108612298965454\n",
      "Epoch : 3125 Loss: 4.158343862742186 Training error :0.8103967905044556\n",
      "Epoch : 3126 Loss: 4.156308516860008 Training error :0.8099452257156372\n",
      "Epoch : 3127 Loss: 4.15415957197547 Training error :0.8095165491104126\n",
      "Epoch : 3128 Loss: 4.151971206068993 Training error :0.8090904355049133\n",
      "Epoch : 3129 Loss: 4.14982857182622 Training error :0.808657169342041\n",
      "Epoch : 3130 Loss: 4.14773228764534 Training error :0.8082243204116821\n",
      "Epoch : 3131 Loss: 4.145605243742466 Training error :0.80781090259552\n",
      "Epoch : 3132 Loss: 4.144252855330706 Training error :0.8073173761367798\n",
      "Epoch : 3133 Loss: 4.141733810305595 Training error :0.8069816827774048\n",
      "Epoch : 3134 Loss: 4.139300357550383 Training error :0.8066002726554871\n",
      "Epoch : 3135 Loss: 4.137329496443272 Training error :0.8061507344245911\n",
      "Epoch : 3136 Loss: 4.135404571890831 Training error :0.8057136535644531\n",
      "Epoch : 3137 Loss: 4.133314799517393 Training error :0.8053082823753357\n",
      "Epoch : 3138 Loss: 4.131217762827873 Training error :0.8048980236053467\n",
      "Epoch : 3139 Loss: 4.129203073680401 Training error :0.8044835925102234\n",
      "Epoch : 3140 Loss: 4.127172213047743 Training error :0.8040708303451538\n",
      "Epoch : 3141 Loss: 4.12516675516963 Training error :0.8036600351333618\n",
      "Epoch : 3142 Loss: 4.123142410069704 Training error :0.8032793402671814\n",
      "Epoch : 3143 Loss: 4.1219182051718235 Training error :0.8027918934822083\n",
      "Epoch : 3144 Loss: 4.119455423206091 Training error :0.8024716973304749\n",
      "Epoch : 3145 Loss: 4.117093555629253 Training error :0.8021079301834106\n",
      "Epoch : 3146 Loss: 4.115225147455931 Training error :0.8016799092292786\n",
      "Epoch : 3147 Loss: 4.113385692238808 Training error :0.8012659549713135\n",
      "Epoch : 3148 Loss: 4.1113973967731 Training error :0.8008766174316406\n",
      "Epoch : 3149 Loss: 4.1094095185399055 Training error :0.8004868030548096\n",
      "Epoch : 3150 Loss: 4.107467204332352 Training error :0.800091028213501\n",
      "Epoch : 3151 Loss: 4.105573359876871 Training error :0.799694836139679\n",
      "Epoch : 3152 Loss: 4.103641714900732 Training error :0.7993032932281494\n",
      "Epoch : 3153 Loss: 4.10172450914979 Training error :0.7989400625228882\n",
      "Epoch : 3154 Loss: 4.100568328052759 Training error :0.7984697222709656\n",
      "Epoch : 3155 Loss: 4.098182633519173 Training error :0.7981694936752319\n",
      "Epoch : 3156 Loss: 4.095943309366703 Training error :0.7978267669677734\n",
      "Epoch : 3157 Loss: 4.094152886420488 Training error :0.7974128723144531\n",
      "Epoch : 3158 Loss: 4.092407088726759 Training error :0.7970190644264221\n",
      "Epoch : 3159 Loss: 4.090502358973026 Training error :0.7966461181640625\n",
      "Epoch : 3160 Loss: 4.088602248579264 Training error :0.7962737083435059\n",
      "Epoch : 3161 Loss: 4.0867675468325615 Training error :0.7958935499191284\n",
      "Epoch : 3162 Loss: 4.084940016269684 Training error :0.7955159544944763\n",
      "Epoch : 3163 Loss: 4.083065547049046 Training error :0.7951463460922241\n",
      "Epoch : 3164 Loss: 4.081240925937891 Training error :0.7947900891304016\n",
      "Epoch : 3165 Loss: 4.080176245421171 Training error :0.7943477630615234\n",
      "Epoch : 3166 Loss: 4.077917832881212 Training error :0.7940625548362732\n",
      "Epoch : 3167 Loss: 4.075739048421383 Training error :0.7937389612197876\n",
      "Epoch : 3168 Loss: 4.074036344885826 Training error :0.7933412790298462\n",
      "Epoch : 3169 Loss: 4.072396084666252 Training error :0.7929642200469971\n",
      "Epoch : 3170 Loss: 4.070576723664999 Training error :0.7926082611083984\n",
      "Epoch : 3171 Loss: 4.068754933774471 Training error :0.7922546863555908\n",
      "Epoch : 3172 Loss: 4.0669906586408615 Training error :0.7918945550918579\n",
      "Epoch : 3173 Loss: 4.065254546701908 Training error :0.7915330529212952\n",
      "Epoch : 3174 Loss: 4.063501577824354 Training error :0.7911777496337891\n",
      "Epoch : 3175 Loss: 4.06173674762249 Training error :0.7908233404159546\n",
      "Epoch : 3176 Loss: 4.060002040117979 Training error :0.790503740310669\n",
      "Epoch : 3177 Loss: 4.059089329093695 Training error :0.7900588512420654\n",
      "Epoch : 3178 Loss: 4.056836288422346 Training error :0.789794921875\n",
      "Epoch : 3179 Loss: 4.0547593012452126 Training error :0.7894830107688904\n",
      "Epoch : 3180 Loss: 4.053111668676138 Training error :0.7891114950180054\n",
      "Epoch : 3181 Loss: 4.051570605486631 Training error :0.788750410079956\n",
      "Epoch : 3182 Loss: 4.04985486716032 Training error :0.7884148359298706\n",
      "Epoch : 3183 Loss: 4.048137325793505 Training error :0.7880786657333374\n",
      "Epoch : 3184 Loss: 4.0464492328464985 Training error :0.78773033618927\n",
      "Epoch : 3185 Loss: 4.044788900762796 Training error :0.7873908281326294\n",
      "Epoch : 3186 Loss: 4.043127723038197 Training error :0.7870514392852783\n",
      "Epoch : 3187 Loss: 4.0414727330207825 Training error :0.7867177724838257\n",
      "Epoch : 3188 Loss: 4.040463596582413 Training error :0.7863191366195679\n",
      "Epoch : 3189 Loss: 4.0384450778365135 Training error :0.7860701084136963\n",
      "Epoch : 3190 Loss: 4.0364339128136635 Training error :0.78577721118927\n",
      "Epoch : 3191 Loss: 4.03489937260747 Training error :0.7854161262512207\n",
      "Epoch : 3192 Loss: 4.03340333327651 Training error :0.7850726246833801\n",
      "Epoch : 3193 Loss: 4.031766302883625 Training error :0.7847509980201721\n",
      "Epoch : 3194 Loss: 4.030128266662359 Training error :0.784431517124176\n",
      "Epoch : 3195 Loss: 4.02852687984705 Training error :0.7841007709503174\n",
      "Epoch : 3196 Loss: 4.026925850659609 Training error :0.7837746143341064\n",
      "Epoch : 3197 Loss: 4.025329578667879 Training error :0.7834513187408447\n",
      "Epoch : 3198 Loss: 4.023739628493786 Training error :0.78313148021698\n",
      "Epoch : 3199 Loss: 4.022154539823532 Training error :0.7828088402748108\n",
      "Epoch : 3200 Loss: 4.0205984339118 Training error :0.7825214266777039\n",
      "Epoch : 3201 Loss: 4.019860465079546 Training error :0.7821100354194641\n",
      "Epoch : 3202 Loss: 4.017782382667065 Training error :0.7818803191184998\n",
      "Epoch : 3203 Loss: 4.015842244029045 Training error :0.7816057801246643\n",
      "Epoch : 3204 Loss: 4.014406714588404 Training error :0.7812606692314148\n",
      "Epoch : 3205 Loss: 4.0130020678043365 Training error :0.7809355854988098\n",
      "Epoch : 3206 Loss: 4.011445887386799 Training error :0.780632495880127\n",
      "Epoch : 3207 Loss: 4.009859040379524 Training error :0.7803311347961426\n",
      "Epoch : 3208 Loss: 4.008358936756849 Training error :0.7800148725509644\n",
      "Epoch : 3209 Loss: 4.006866246461868 Training error :0.7797074317932129\n",
      "Epoch : 3210 Loss: 4.0053583681583405 Training error :0.7794023752212524\n",
      "Epoch : 3211 Loss: 4.003858461976051 Training error :0.7791004180908203\n",
      "Epoch : 3212 Loss: 4.0023501589894295 Training error :0.7788121104240417\n",
      "Epoch : 3213 Loss: 4.001634757965803 Training error :0.7784297466278076\n",
      "Epoch : 3214 Loss: 3.999693129211664 Training error :0.7782126665115356\n",
      "Epoch : 3215 Loss: 3.9978284500539303 Training error :0.7779538035392761\n",
      "Epoch : 3216 Loss: 3.9964262805879116 Training error :0.7776238918304443\n",
      "Epoch : 3217 Loss: 3.995132826268673 Training error :0.7773124575614929\n",
      "Epoch : 3218 Loss: 3.9936517849564552 Training error :0.7770260572433472\n",
      "Epoch : 3219 Loss: 3.9921630024909973 Training error :0.7767382860183716\n",
      "Epoch : 3220 Loss: 3.9907367825508118 Training error :0.7764427065849304\n",
      "Epoch : 3221 Loss: 3.9893037490546703 Training error :0.7761480212211609\n",
      "Epoch : 3222 Loss: 3.9878743179142475 Training error :0.7758570909500122\n",
      "Epoch : 3223 Loss: 3.986445624381304 Training error :0.7755680084228516\n",
      "Epoch : 3224 Loss: 3.9850367568433285 Training error :0.7752800583839417\n",
      "Epoch : 3225 Loss: 3.983605094254017 Training error :0.7750176787376404\n",
      "Epoch : 3226 Loss: 3.9830150566995144 Training error :0.7746431827545166\n",
      "Epoch : 3227 Loss: 3.9811021350324154 Training error :0.7744402885437012\n",
      "Epoch : 3228 Loss: 3.9793114326894283 Training error :0.7741954922676086\n",
      "Epoch : 3229 Loss: 3.9780173376202583 Training error :0.7738826274871826\n",
      "Epoch : 3230 Loss: 3.9767800979316235 Training error :0.7735901474952698\n",
      "Epoch : 3231 Loss: 3.975355613976717 Training error :0.7733173966407776\n",
      "Epoch : 3232 Loss: 3.9739329628646374 Training error :0.773044764995575\n",
      "Epoch : 3233 Loss: 3.972591269761324 Training error :0.7727615833282471\n",
      "Epoch : 3234 Loss: 3.971249181777239 Training error :0.7724847197532654\n",
      "Epoch : 3235 Loss: 3.969884764403105 Training error :0.7722083330154419\n",
      "Epoch : 3236 Loss: 3.9685472771525383 Training error :0.7719368934631348\n",
      "Epoch : 3237 Loss: 3.9671879932284355 Training error :0.7716606855392456\n",
      "Epoch : 3238 Loss: 3.9658536799252033 Training error :0.7714171409606934\n",
      "Epoch : 3239 Loss: 3.965331267565489 Training error :0.7710580229759216\n",
      "Epoch : 3240 Loss: 3.9635102935135365 Training error :0.7708736658096313\n",
      "Epoch : 3241 Loss: 3.9617993347346783 Training error :0.7706440091133118\n",
      "Epoch : 3242 Loss: 3.9605625942349434 Training error :0.77034592628479\n",
      "Epoch : 3243 Loss: 3.959381729364395 Training error :0.7700641751289368\n",
      "Epoch : 3244 Loss: 3.958043061196804 Training error :0.7698084115982056\n",
      "Epoch : 3245 Loss: 3.9567175917327404 Training error :0.7695501446723938\n",
      "Epoch : 3246 Loss: 3.9554278627038 Training error :0.7692800760269165\n",
      "Epoch : 3247 Loss: 3.9541642367839813 Training error :0.7690199017524719\n",
      "Epoch : 3248 Loss: 3.95287112519145 Training error :0.7687584757804871\n",
      "Epoch : 3249 Loss: 3.9515973031520844 Training error :0.7685012817382812\n",
      "Epoch : 3250 Loss: 3.9503155425190926 Training error :0.7682405114173889\n",
      "Epoch : 3251 Loss: 3.9490593671798706 Training error :0.7680003643035889\n",
      "Epoch : 3252 Loss: 3.9485452845692635 Training error :0.7676601409912109\n",
      "Epoch : 3253 Loss: 3.946874722838402 Training error :0.7674928903579712\n",
      "Epoch : 3254 Loss: 3.945198141038418 Training error :0.7672755718231201\n",
      "Epoch : 3255 Loss: 3.9440608732402325 Training error :0.7669910192489624\n",
      "Epoch : 3256 Loss: 3.9429394230246544 Training error :0.7667233347892761\n",
      "Epoch : 3257 Loss: 3.941683102399111 Training error :0.7664841413497925\n",
      "Epoch : 3258 Loss: 3.940416157245636 Training error :0.7662374973297119\n",
      "Epoch : 3259 Loss: 3.939189702272415 Training error :0.7659849524497986\n",
      "Epoch : 3260 Loss: 3.9379897341132164 Training error :0.7657358050346375\n",
      "Epoch : 3261 Loss: 3.9367809258401394 Training error :0.765488862991333\n",
      "Epoch : 3262 Loss: 3.9355590604245663 Training error :0.7652430534362793\n",
      "Epoch : 3263 Loss: 3.934364851564169 Training error :0.7649991512298584\n",
      "Epoch : 3264 Loss: 3.933159403502941 Training error :0.7647523283958435\n",
      "Epoch : 3265 Loss: 3.931974794715643 Training error :0.7645367383956909\n",
      "Epoch : 3266 Loss: 3.931580178439617 Training error :0.7642029523849487\n",
      "Epoch : 3267 Loss: 3.9299302101135254 Training error :0.764048159122467\n",
      "Epoch : 3268 Loss: 3.928357172757387 Training error :0.7638490796089172\n",
      "Epoch : 3269 Loss: 3.927239056676626 Training error :0.7635791897773743\n",
      "Epoch : 3270 Loss: 3.926194690167904 Training error :0.7633282542228699\n",
      "Epoch : 3271 Loss: 3.9250525683164597 Training error :0.7630990743637085\n",
      "Epoch : 3272 Loss: 3.923836085945368 Training error :0.762869119644165\n",
      "Epoch : 3273 Loss: 3.9226916693150997 Training error :0.7626299262046814\n",
      "Epoch : 3274 Loss: 3.9215415716171265 Training error :0.7623974084854126\n",
      "Epoch : 3275 Loss: 3.9203928373754025 Training error :0.7621631622314453\n",
      "Epoch : 3276 Loss: 3.9192672334611416 Training error :0.7619321942329407\n",
      "Epoch : 3277 Loss: 3.9181276448071003 Training error :0.7617003917694092\n",
      "Epoch : 3278 Loss: 3.9170058481395245 Training error :0.7614724636077881\n",
      "Epoch : 3279 Loss: 3.9158804453909397 Training error :0.7612632513046265\n",
      "Epoch : 3280 Loss: 3.9155093617737293 Training error :0.7609465718269348\n",
      "Epoch : 3281 Loss: 3.9139667004346848 Training error :0.7608045935630798\n",
      "Epoch : 3282 Loss: 3.9124360121786594 Training error :0.7606160640716553\n",
      "Epoch : 3283 Loss: 3.9114179015159607 Training error :0.7603604793548584\n",
      "Epoch : 3284 Loss: 3.910469312220812 Training error :0.7601243853569031\n",
      "Epoch : 3285 Loss: 3.9093372896313667 Training error :0.7599092125892639\n",
      "Epoch : 3286 Loss: 3.9082065522670746 Training error :0.7596933245658875\n",
      "Epoch : 3287 Loss: 3.9071370661258698 Training error :0.75946444272995\n",
      "Epoch : 3288 Loss: 3.906065821647644 Training error :0.7592454552650452\n",
      "Epoch : 3289 Loss: 3.9049918092787266 Training error :0.7590253353118896\n",
      "Epoch : 3290 Loss: 3.903898388147354 Training error :0.7588070034980774\n",
      "Epoch : 3291 Loss: 3.9028332345187664 Training error :0.7585908770561218\n",
      "Epoch : 3292 Loss: 3.901768036186695 Training error :0.7583746314048767\n",
      "Epoch : 3293 Loss: 3.900697898119688 Training error :0.758164644241333\n",
      "Epoch : 3294 Loss: 3.9003420881927013 Training error :0.7578722238540649\n",
      "Epoch : 3295 Loss: 3.8989286199212074 Training error :0.7577436566352844\n",
      "Epoch : 3296 Loss: 3.8974613659083843 Training error :0.7575692534446716\n",
      "Epoch : 3297 Loss: 3.896504484117031 Training error :0.7573245167732239\n",
      "Epoch : 3298 Loss: 3.895587518811226 Training error :0.7570996880531311\n",
      "Epoch : 3299 Loss: 3.894533272832632 Training error :0.7568974494934082\n",
      "Epoch : 3300 Loss: 3.893457282334566 Training error :0.7566940188407898\n",
      "Epoch : 3301 Loss: 3.8924411945044994 Training error :0.7564800381660461\n",
      "Epoch : 3302 Loss: 3.891444608569145 Training error :0.7562721371650696\n",
      "Epoch : 3303 Loss: 3.890434753149748 Training error :0.756065845489502\n",
      "Epoch : 3304 Loss: 3.889412257820368 Training error :0.7558634281158447\n",
      "Epoch : 3305 Loss: 3.8884031660854816 Training error :0.7556554675102234\n",
      "Epoch : 3306 Loss: 3.887403529137373 Training error :0.7554501295089722\n",
      "Epoch : 3307 Loss: 3.8863890320062637 Training error :0.7552470564842224\n",
      "Epoch : 3308 Loss: 3.8853907212615013 Training error :0.755053699016571\n",
      "Epoch : 3309 Loss: 3.885123297572136 Training error :0.7547719478607178\n",
      "Epoch : 3310 Loss: 3.883744701743126 Training error :0.7546550631523132\n",
      "Epoch : 3311 Loss: 3.8823503740131855 Training error :0.7544925212860107\n",
      "Epoch : 3312 Loss: 3.8814264461398125 Training error :0.7542632222175598\n",
      "Epoch : 3313 Loss: 3.8806157410144806 Training error :0.7540531158447266\n",
      "Epoch : 3314 Loss: 3.879615429788828 Training error :0.7538673281669617\n",
      "Epoch : 3315 Loss: 3.878606155514717 Training error :0.7536719441413879\n",
      "Epoch : 3316 Loss: 3.8776468969881535 Training error :0.7534710168838501\n",
      "Epoch : 3317 Loss: 3.8766940273344517 Training error :0.7532721757888794\n",
      "Epoch : 3318 Loss: 3.8757439740002155 Training error :0.7530805468559265\n",
      "Epoch : 3319 Loss: 3.8747870475053787 Training error :0.7528873682022095\n",
      "Epoch : 3320 Loss: 3.8738519065082073 Training error :0.7526942491531372\n",
      "Epoch : 3321 Loss: 3.872901104390621 Training error :0.7525003552436829\n",
      "Epoch : 3322 Loss: 3.8719742111861706 Training error :0.7523119449615479\n",
      "Epoch : 3323 Loss: 3.8710352890193462 Training error :0.7521215081214905\n",
      "Epoch : 3324 Loss: 3.8707969561219215 Training error :0.7518630027770996\n",
      "Epoch : 3325 Loss: 3.869515858590603 Training error :0.7517560720443726\n",
      "Epoch : 3326 Loss: 3.868160054087639 Training error :0.7516058087348938\n",
      "Epoch : 3327 Loss: 3.867318820208311 Training error :0.751385509967804\n",
      "Epoch : 3328 Loss: 3.8665484748780727 Training error :0.7511874437332153\n",
      "Epoch : 3329 Loss: 3.8655964247882366 Training error :0.7510093450546265\n",
      "Epoch : 3330 Loss: 3.8646458946168423 Training error :0.7508293986320496\n",
      "Epoch : 3331 Loss: 3.863748349249363 Training error :0.7506431341171265\n",
      "Epoch : 3332 Loss: 3.8628637194633484 Training error :0.7504567503929138\n",
      "Epoch : 3333 Loss: 3.8619675897061825 Training error :0.7502743005752563\n",
      "Epoch : 3334 Loss: 3.861060943454504 Training error :0.750092089176178\n",
      "Epoch : 3335 Loss: 3.860196053981781 Training error :0.7499127388000488\n",
      "Epoch : 3336 Loss: 3.8592967055737972 Training error :0.7497296333312988\n",
      "Epoch : 3337 Loss: 3.8584392853081226 Training error :0.7495507001876831\n",
      "Epoch : 3338 Loss: 3.8575521893799305 Training error :0.7493705153465271\n",
      "Epoch : 3339 Loss: 3.8566728457808495 Training error :0.7491990923881531\n",
      "Epoch : 3340 Loss: 3.8564887829124928 Training error :0.7489433288574219\n",
      "Epoch : 3341 Loss: 3.8552516289055347 Training error :0.7488530874252319\n",
      "Epoch : 3342 Loss: 3.853974997997284 Training error :0.7487130761146545\n",
      "Epoch : 3343 Loss: 3.853164605796337 Training error :0.7485082745552063\n",
      "Epoch : 3344 Loss: 3.85246928781271 Training error :0.7483174204826355\n",
      "Epoch : 3345 Loss: 3.8515836484730244 Training error :0.7481564283370972\n",
      "Epoch : 3346 Loss: 3.850679725408554 Training error :0.7479856610298157\n",
      "Epoch : 3347 Loss: 3.8498570099473 Training error :0.7478117346763611\n",
      "Epoch : 3348 Loss: 3.8490434288978577 Training error :0.7476373910903931\n",
      "Epoch : 3349 Loss: 3.848203118890524 Training error :0.7474693059921265\n",
      "Epoch : 3350 Loss: 3.8473373539745808 Training error :0.7472984194755554\n",
      "Epoch : 3351 Loss: 3.846524190157652 Training error :0.7471285462379456\n",
      "Epoch : 3352 Loss: 3.845685437321663 Training error :0.7469580769538879\n",
      "Epoch : 3353 Loss: 3.844862390309572 Training error :0.7467911243438721\n",
      "Epoch : 3354 Loss: 3.8440440632402897 Training error :0.7466217279434204\n",
      "Epoch : 3355 Loss: 3.8432406708598137 Training error :0.7464566826820374\n",
      "Epoch : 3356 Loss: 3.8431040421128273 Training error :0.7462194561958313\n",
      "Epoch : 3357 Loss: 3.8419745676219463 Training error :0.7461050152778625\n",
      "Epoch : 3358 Loss: 3.8406199999153614 Training error :0.7458810210227966\n",
      "Epoch : 3359 Loss: 3.839481022208929 Training error :0.7455880045890808\n",
      "Epoch : 3360 Loss: 3.838392172008753 Training error :0.7453124523162842\n",
      "Epoch : 3361 Loss: 3.837082915008068 Training error :0.7450583577156067\n",
      "Epoch : 3362 Loss: 3.835737034678459 Training error :0.7447946667671204\n",
      "Epoch : 3363 Loss: 3.834416873753071 Training error :0.744529664516449\n",
      "Epoch : 3364 Loss: 3.833142824470997 Training error :0.7442631721496582\n",
      "Epoch : 3365 Loss: 3.831842739135027 Training error :0.7440071702003479\n",
      "Epoch : 3366 Loss: 3.8305488862097263 Training error :0.743744432926178\n",
      "Epoch : 3367 Loss: 3.8292685113847256 Training error :0.7434842586517334\n",
      "Epoch : 3368 Loss: 3.8280047923326492 Training error :0.7432213425636292\n",
      "Epoch : 3369 Loss: 3.826718293130398 Training error :0.7429630160331726\n",
      "Epoch : 3370 Loss: 3.825455967336893 Training error :0.7427291870117188\n",
      "Epoch : 3371 Loss: 3.824966337531805 Training error :0.7423856854438782\n",
      "Epoch : 3372 Loss: 3.823297865688801 Training error :0.7422176599502563\n",
      "Epoch : 3373 Loss: 3.8215859048068523 Training error :0.7420015931129456\n",
      "Epoch : 3374 Loss: 3.820414647459984 Training error :0.7417172193527222\n",
      "Epoch : 3375 Loss: 3.819324564188719 Training error :0.7414568066596985\n",
      "Epoch : 3376 Loss: 3.818061877042055 Training error :0.7412147521972656\n",
      "Epoch : 3377 Loss: 3.816774148494005 Training error :0.7409703135490417\n",
      "Epoch : 3378 Loss: 3.8155700713396072 Training error :0.7407186627388\n",
      "Epoch : 3379 Loss: 3.814373254776001 Training error :0.7404683828353882\n",
      "Epoch : 3380 Loss: 3.8131480179727077 Training error :0.7402223944664001\n",
      "Epoch : 3381 Loss: 3.811940800398588 Training error :0.739976167678833\n",
      "Epoch : 3382 Loss: 3.8107367195189 Training error :0.739734947681427\n",
      "Epoch : 3383 Loss: 3.8095357455313206 Training error :0.7395017743110657\n",
      "Epoch : 3384 Loss: 3.809086885303259 Training error :0.7391824126243591\n",
      "Epoch : 3385 Loss: 3.807524371892214 Training error :0.7390261292457581\n",
      "Epoch : 3386 Loss: 3.8058957420289516 Training error :0.7388221025466919\n",
      "Epoch : 3387 Loss: 3.804763775318861 Training error :0.738554060459137\n",
      "Epoch : 3388 Loss: 3.8037413991987705 Training error :0.7383025884628296\n",
      "Epoch : 3389 Loss: 3.8025307655334473 Training error :0.7380769848823547\n",
      "Epoch : 3390 Loss: 3.801316536962986 Training error :0.7378427982330322\n",
      "Epoch : 3391 Loss: 3.800153858959675 Training error :0.7376031279563904\n",
      "Epoch : 3392 Loss: 3.799041587859392 Training error :0.7373660802841187\n",
      "Epoch : 3393 Loss: 3.797871071845293 Training error :0.737133264541626\n",
      "Epoch : 3394 Loss: 3.7967224791646004 Training error :0.7369030117988586\n",
      "Epoch : 3395 Loss: 3.795579843223095 Training error :0.7366693019866943\n",
      "Epoch : 3396 Loss: 3.7944514006376266 Training error :0.7364377379417419\n",
      "Epoch : 3397 Loss: 3.793309699743986 Training error :0.7362319827079773\n",
      "Epoch : 3398 Loss: 3.7929730601608753 Training error :0.7359120845794678\n",
      "Epoch : 3399 Loss: 3.7914433777332306 Training error :0.7357693314552307\n",
      "Epoch : 3400 Loss: 3.789872854948044 Training error :0.7355771660804749\n",
      "Epoch : 3401 Loss: 3.7888029627501965 Training error :0.7353196144104004\n",
      "Epoch : 3402 Loss: 3.78782606869936 Training error :0.7350834012031555\n",
      "Epoch : 3403 Loss: 3.7867025285959244 Training error :0.7348672747612\n",
      "Epoch : 3404 Loss: 3.7855509743094444 Training error :0.7346480488777161\n",
      "Epoch : 3405 Loss: 3.784450065344572 Training error :0.7344226241111755\n",
      "Epoch : 3406 Loss: 3.783382423222065 Training error :0.7341982126235962\n",
      "Epoch : 3407 Loss: 3.7822830751538277 Training error :0.7339773774147034\n",
      "Epoch : 3408 Loss: 3.7812031619250774 Training error :0.733761191368103\n",
      "Epoch : 3409 Loss: 3.780142806470394 Training error :0.7335408926010132\n",
      "Epoch : 3410 Loss: 3.7790594696998596 Training error :0.7333263158798218\n",
      "Epoch : 3411 Loss: 3.7786716856062412 Training error :0.7330348491668701\n",
      "Epoch : 3412 Loss: 3.7772766053676605 Training error :0.7329051494598389\n",
      "Epoch : 3413 Loss: 3.7757695615291595 Training error :0.7327229976654053\n",
      "Epoch : 3414 Loss: 3.7747712954878807 Training error :0.7324777245521545\n",
      "Epoch : 3415 Loss: 3.77386362105608 Training error :0.7322493195533752\n",
      "Epoch : 3416 Loss: 3.772786397486925 Training error :0.7320482134819031\n",
      "Epoch : 3417 Loss: 3.7716818153858185 Training error :0.7318381667137146\n",
      "Epoch : 3418 Loss: 3.770634710788727 Training error :0.7316227555274963\n",
      "Epoch : 3419 Loss: 3.7696220465004444 Training error :0.7314111590385437\n",
      "Epoch : 3420 Loss: 3.768577419221401 Training error :0.7311997413635254\n",
      "Epoch : 3421 Loss: 3.7675440683960915 Training error :0.7309941649436951\n",
      "Epoch : 3422 Loss: 3.766524713486433 Training error :0.7307829856872559\n",
      "Epoch : 3423 Loss: 3.7655132599174976 Training error :0.7305768728256226\n",
      "Epoch : 3424 Loss: 3.764505200088024 Training error :0.7303708791732788\n",
      "Epoch : 3425 Loss: 3.7641789950430393 Training error :0.730095386505127\n",
      "Epoch : 3426 Loss: 3.762850683182478 Training error :0.7299731969833374\n",
      "Epoch : 3427 Loss: 3.761397086083889 Training error :0.7298052906990051\n",
      "Epoch : 3428 Loss: 3.760426204651594 Training error :0.7295687794685364\n",
      "Epoch : 3429 Loss: 3.759584214538336 Training error :0.7293570637702942\n",
      "Epoch : 3430 Loss: 3.758544232696295 Training error :0.7291610836982727\n",
      "Epoch : 3431 Loss: 3.7575394175946712 Training error :0.7289646863937378\n",
      "Epoch : 3432 Loss: 3.7565333545207977 Training error :0.7287624478340149\n",
      "Epoch : 3433 Loss: 3.7555681504309177 Training error :0.7285622358322144\n",
      "Epoch : 3434 Loss: 3.7545827254652977 Training error :0.7283621430397034\n",
      "Epoch : 3435 Loss: 3.7536079809069633 Training error :0.7281658053398132\n",
      "Epoch : 3436 Loss: 3.7526475712656975 Training error :0.7279660701751709\n",
      "Epoch : 3437 Loss: 3.7516702339053154 Training error :0.7277685403823853\n",
      "Epoch : 3438 Loss: 3.7507135421037674 Training error :0.727573037147522\n",
      "Epoch : 3439 Loss: 3.7497505880892277 Training error :0.7273914813995361\n",
      "Epoch : 3440 Loss: 3.749532487243414 Training error :0.7271143198013306\n",
      "Epoch : 3441 Loss: 3.7482296638190746 Training error :0.7270039916038513\n",
      "Epoch : 3442 Loss: 3.7468238212168217 Training error :0.726845920085907\n",
      "Epoch : 3443 Loss: 3.7459167055785656 Training error :0.7266221642494202\n",
      "Epoch : 3444 Loss: 3.7451370023190975 Training error :0.7264214158058167\n",
      "Epoch : 3445 Loss: 3.744142558425665 Training error :0.7262395620346069\n",
      "Epoch : 3446 Loss: 3.7431664392352104 Training error :0.7260544300079346\n",
      "Epoch : 3447 Loss: 3.742242429405451 Training error :0.7258607745170593\n",
      "Epoch : 3448 Loss: 3.7413379848003387 Training error :0.7256733775138855\n",
      "Epoch : 3449 Loss: 3.740421675145626 Training error :0.7254841327667236\n",
      "Epoch : 3450 Loss: 3.7394943945109844 Training error :0.7252971529960632\n",
      "Epoch : 3451 Loss: 3.7385822758078575 Training error :0.725108802318573\n",
      "Epoch : 3452 Loss: 3.73765866830945 Training error :0.7249240875244141\n",
      "Epoch : 3453 Loss: 3.7367567494511604 Training error :0.7247393131256104\n",
      "Epoch : 3454 Loss: 3.7358202561736107 Training error :0.7245737314224243\n",
      "Epoch : 3455 Loss: 3.7356854788959026 Training error :0.7243003249168396\n",
      "Epoch : 3456 Loss: 3.734410285949707 Training error :0.7242009043693542\n",
      "Epoch : 3457 Loss: 3.7330676279962063 Training error :0.7240535616874695\n",
      "Epoch : 3458 Loss: 3.7322209142148495 Training error :0.7238397002220154\n",
      "Epoch : 3459 Loss: 3.73148275911808 Training error :0.7236489057540894\n",
      "Epoch : 3460 Loss: 3.7305591478943825 Training error :0.7234790921211243\n",
      "Epoch : 3461 Loss: 3.7296585850417614 Training error :0.7233058214187622\n",
      "Epoch : 3462 Loss: 3.7287623956799507 Training error :0.7231221795082092\n",
      "Epoch : 3463 Loss: 3.7279054187238216 Training error :0.7229434251785278\n",
      "Epoch : 3464 Loss: 3.727027304470539 Training error :0.7227674126625061\n",
      "Epoch : 3465 Loss: 3.726155787706375 Training error :0.7225914597511292\n",
      "Epoch : 3466 Loss: 3.7252899147570133 Training error :0.7224133014678955\n",
      "Epoch : 3467 Loss: 3.7244157940149307 Training error :0.7222366333007812\n",
      "Epoch : 3468 Loss: 3.7235490158200264 Training error :0.7220619916915894\n",
      "Epoch : 3469 Loss: 3.7226879596710205 Training error :0.7218987941741943\n",
      "Epoch : 3470 Loss: 3.7225513979792595 Training error :0.7216425538063049\n",
      "Epoch : 3471 Loss: 3.7213789224624634 Training error :0.7215520739555359\n",
      "Epoch : 3472 Loss: 3.7200683914124966 Training error :0.7214157581329346\n",
      "Epoch : 3473 Loss: 3.7192724719643593 Training error :0.7212101817131042\n",
      "Epoch : 3474 Loss: 3.718586679548025 Training error :0.721031665802002\n",
      "Epoch : 3475 Loss: 3.7177062556147575 Training error :0.7208706140518188\n",
      "Epoch : 3476 Loss: 3.7168111950159073 Training error :0.7207075357437134\n",
      "Epoch : 3477 Loss: 3.716003865003586 Training error :0.7205331921577454\n",
      "Epoch : 3478 Loss: 3.7151812463998795 Training error :0.7203657627105713\n",
      "Epoch : 3479 Loss: 3.7143594287335873 Training error :0.7201982736587524\n",
      "Epoch : 3480 Loss: 3.7135373279452324 Training error :0.7200311422348022\n",
      "Epoch : 3481 Loss: 3.712741147726774 Training error :0.7198643684387207\n",
      "Epoch : 3482 Loss: 3.711903341114521 Training error :0.7196958065032959\n",
      "Epoch : 3483 Loss: 3.711091212928295 Training error :0.7195312976837158\n",
      "Epoch : 3484 Loss: 3.7102672420442104 Training error :0.7193653583526611\n",
      "Epoch : 3485 Loss: 3.7094871439039707 Training error :0.7192229628562927\n",
      "Epoch : 3486 Loss: 3.7094118259847164 Training error :0.7189691662788391\n",
      "Epoch : 3487 Loss: 3.708273433148861 Training error :0.7188892960548401\n",
      "Epoch : 3488 Loss: 3.7070091627538204 Training error :0.7187612056732178\n",
      "Epoch : 3489 Loss: 3.706241175532341 Training error :0.7185710668563843\n",
      "Epoch : 3490 Loss: 3.705616995692253 Training error :0.7183973789215088\n",
      "Epoch : 3491 Loss: 3.7047698199748993 Training error :0.718247652053833\n",
      "Epoch : 3492 Loss: 3.703936729580164 Training error :0.7180901169776917\n",
      "Epoch : 3493 Loss: 3.7031599543988705 Training error :0.7179285287857056\n",
      "Epoch : 3494 Loss: 3.702435437589884 Training error :0.717766523361206\n",
      "Epoch : 3495 Loss: 3.701627593487501 Training error :0.7176085710525513\n",
      "Epoch : 3496 Loss: 3.7008509188890457 Training error :0.7174543738365173\n",
      "Epoch : 3497 Loss: 3.700087994337082 Training error :0.7172946333885193\n",
      "Epoch : 3498 Loss: 3.6993234753608704 Training error :0.7171409130096436\n",
      "Epoch : 3499 Loss: 3.6985458992421627 Training error :0.7169856429100037\n",
      "Epoch : 3500 Loss: 3.697813145816326 Training error :0.7168309092521667\n",
      "Epoch : 3501 Loss: 3.6970550380647182 Training error :0.7166908979415894\n",
      "Epoch : 3502 Loss: 3.6969944462180138 Training error :0.7164517045021057\n",
      "Epoch : 3503 Loss: 3.695926748216152 Training error :0.7163774967193604\n",
      "Epoch : 3504 Loss: 3.6946809701621532 Training error :0.71625816822052\n",
      "Epoch : 3505 Loss: 3.6939710937440395 Training error :0.7160739898681641\n",
      "Epoch : 3506 Loss: 3.6933792904019356 Training error :0.7159126400947571\n",
      "Epoch : 3507 Loss: 3.6925887800753117 Training error :0.7157723903656006\n",
      "Epoch : 3508 Loss: 3.6918153688311577 Training error :0.7156232595443726\n",
      "Epoch : 3509 Loss: 3.6911069452762604 Training error :0.7154731154441833\n",
      "Epoch : 3510 Loss: 3.69037077575922 Training error :0.7153206467628479\n",
      "Epoch : 3511 Loss: 3.689636681228876 Training error :0.7151745557785034\n",
      "Epoch : 3512 Loss: 3.6889301910996437 Training error :0.7150253057479858\n",
      "Epoch : 3513 Loss: 3.688197746872902 Training error :0.7148773670196533\n",
      "Epoch : 3514 Loss: 3.6874689757823944 Training error :0.7147290706634521\n",
      "Epoch : 3515 Loss: 3.6867454312741756 Training error :0.7145829200744629\n",
      "Epoch : 3516 Loss: 3.6860301718115807 Training error :0.7144367098808289\n",
      "Epoch : 3517 Loss: 3.685302320867777 Training error :0.7142959833145142\n",
      "Epoch : 3518 Loss: 3.685293275862932 Training error :0.7140741944313049\n",
      "Epoch : 3519 Loss: 3.684273112565279 Training error :0.7140110731124878\n",
      "Epoch : 3520 Loss: 3.6831241734325886 Training error :0.7139005064964294\n",
      "Epoch : 3521 Loss: 3.6824457310140133 Training error :0.7137271165847778\n",
      "Epoch : 3522 Loss: 3.681893851608038 Training error :0.7135764956474304\n",
      "Epoch : 3523 Loss: 3.6811479665338993 Training error :0.713441789150238\n",
      "Epoch : 3524 Loss: 3.680430918931961 Training error :0.7133050560951233\n",
      "Epoch : 3525 Loss: 3.6797373928129673 Training error :0.7131584286689758\n",
      "Epoch : 3526 Loss: 3.6790645495057106 Training error :0.7130153179168701\n",
      "Epoch : 3527 Loss: 3.6783804148435593 Training error :0.712877094745636\n",
      "Epoch : 3528 Loss: 3.677674699574709 Training error :0.7127373218536377\n",
      "Epoch : 3529 Loss: 3.6769967041909695 Training error :0.7125983238220215\n",
      "Epoch : 3530 Loss: 3.676314815878868 Training error :0.7124589681625366\n",
      "Epoch : 3531 Loss: 3.6756297685205936 Training error :0.7123203277587891\n",
      "Epoch : 3532 Loss: 3.674942087382078 Training error :0.712180495262146\n",
      "Epoch : 3533 Loss: 3.6742685958743095 Training error :0.7120441198348999\n",
      "Epoch : 3534 Loss: 3.6735878363251686 Training error :0.7119130492210388\n",
      "Epoch : 3535 Loss: 3.6736051440238953 Training error :0.7116973996162415\n",
      "Epoch : 3536 Loss: 3.672667533159256 Training error :0.7116451263427734\n",
      "Epoch : 3537 Loss: 3.671534411609173 Training error :0.7115410566329956\n",
      "Epoch : 3538 Loss: 3.6709074825048447 Training error :0.7113773226737976\n",
      "Epoch : 3539 Loss: 3.6703864224255085 Training error :0.7112309336662292\n",
      "Epoch : 3540 Loss: 3.6697079092264175 Training error :0.7111095190048218\n",
      "Epoch : 3541 Loss: 3.66900048032403 Training error :0.7109786868095398\n",
      "Epoch : 3542 Loss: 3.6683613881468773 Training error :0.7108438014984131\n",
      "Epoch : 3543 Loss: 3.667718667536974 Training error :0.7107102870941162\n",
      "Epoch : 3544 Loss: 3.6670843809843063 Training error :0.710578978061676\n",
      "Epoch : 3545 Loss: 3.6664290241897106 Training error :0.7104461193084717\n",
      "Epoch : 3546 Loss: 3.665779359638691 Training error :0.7103153467178345\n",
      "Epoch : 3547 Loss: 3.6651376001536846 Training error :0.7101855278015137\n",
      "Epoch : 3548 Loss: 3.6645002476871014 Training error :0.7100557088851929\n",
      "Epoch : 3549 Loss: 3.6638840287923813 Training error :0.7099260687828064\n",
      "Epoch : 3550 Loss: 3.6632318422198296 Training error :0.7097960710525513\n",
      "Epoch : 3551 Loss: 3.6626156382262707 Training error :0.7096666693687439\n",
      "Epoch : 3552 Loss: 3.66197656840086 Training error :0.7095538377761841\n",
      "Epoch : 3553 Loss: 3.662030577659607 Training error :0.7093385457992554\n",
      "Epoch : 3554 Loss: 3.6611206866800785 Training error :0.7092914581298828\n",
      "Epoch : 3555 Loss: 3.6600056923925877 Training error :0.709196925163269\n",
      "Epoch : 3556 Loss: 3.6594368554651737 Training error :0.709043025970459\n",
      "Epoch : 3557 Loss: 3.6589452140033245 Training error :0.7089056372642517\n",
      "Epoch : 3558 Loss: 3.658326033502817 Training error :0.7087931036949158\n",
      "Epoch : 3559 Loss: 3.6576545014977455 Training error :0.708672285079956\n",
      "Epoch : 3560 Loss: 3.6570557057857513 Training error :0.7085440158843994\n",
      "Epoch : 3561 Loss: 3.6564728878438473 Training error :0.7084165215492249\n",
      "Epoch : 3562 Loss: 3.6558736450970173 Training error :0.7082942128181458\n",
      "Epoch : 3563 Loss: 3.655259892344475 Training error :0.7081722021102905\n",
      "Epoch : 3564 Loss: 3.654660515487194 Training error :0.7080466747283936\n",
      "Epoch : 3565 Loss: 3.654053993523121 Training error :0.7079259753227234\n",
      "Epoch : 3566 Loss: 3.653450708836317 Training error :0.7078045606613159\n",
      "Epoch : 3567 Loss: 3.652870323508978 Training error :0.7076838612556458\n",
      "Epoch : 3568 Loss: 3.652286618947983 Training error :0.7075639367103577\n",
      "Epoch : 3569 Loss: 3.6516900584101677 Training error :0.7074388265609741\n",
      "Epoch : 3570 Loss: 3.6511056311428547 Training error :0.7073311805725098\n",
      "Epoch : 3571 Loss: 3.651192743331194 Training error :0.707126796245575\n",
      "Epoch : 3572 Loss: 3.6502802670001984 Training error :0.7070906162261963\n",
      "Epoch : 3573 Loss: 3.6492484100162983 Training error :0.7070029377937317\n",
      "Epoch : 3574 Loss: 3.6487066075205803 Training error :0.7068568468093872\n",
      "Epoch : 3575 Loss: 3.648254159837961 Training error :0.7067283391952515\n",
      "Epoch : 3576 Loss: 3.647666536271572 Training error :0.7066228985786438\n",
      "Epoch : 3577 Loss: 3.647071000188589 Training error :0.7065088152885437\n",
      "Epoch : 3578 Loss: 3.6464912369847298 Training error :0.7063886523246765\n",
      "Epoch : 3579 Loss: 3.645933374762535 Training error :0.7062689065933228\n",
      "Epoch : 3580 Loss: 3.64538062363863 Training error :0.706154465675354\n",
      "Epoch : 3581 Loss: 3.644794777035713 Training error :0.7060391306877136\n",
      "Epoch : 3582 Loss: 3.6442246176302433 Training error :0.7059223055839539\n",
      "Epoch : 3583 Loss: 3.6436680778861046 Training error :0.7058111429214478\n",
      "Epoch : 3584 Loss: 3.64311221241951 Training error :0.7056961059570312\n",
      "Epoch : 3585 Loss: 3.642560552805662 Training error :0.7055849432945251\n",
      "Epoch : 3586 Loss: 3.642019171267748 Training error :0.7054699063301086\n",
      "Epoch : 3587 Loss: 3.6414691731333733 Training error :0.7053533792495728\n",
      "Epoch : 3588 Loss: 3.640939861536026 Training error :0.7052443623542786\n",
      "Epoch : 3589 Loss: 3.6409434974193573 Training error :0.7050583362579346\n",
      "Epoch : 3590 Loss: 3.6402076706290245 Training error :0.7050250172615051\n",
      "Epoch : 3591 Loss: 3.639163341373205 Training error :0.7049440145492554\n",
      "Epoch : 3592 Loss: 3.6386466696858406 Training error :0.7048059701919556\n",
      "Epoch : 3593 Loss: 3.6382645331323147 Training error :0.7046867609024048\n",
      "Epoch : 3594 Loss: 3.6376971006393433 Training error :0.7045893669128418\n",
      "Epoch : 3595 Loss: 3.6371031887829304 Training error :0.704481303691864\n",
      "Epoch : 3596 Loss: 3.636574361473322 Training error :0.7043657898902893\n",
      "Epoch : 3597 Loss: 3.636091724038124 Training error :0.7042566537857056\n",
      "Epoch : 3598 Loss: 3.635518431663513 Training error :0.7041501402854919\n",
      "Epoch : 3599 Loss: 3.635004300624132 Training error :0.7040420770645142\n",
      "Epoch : 3600 Loss: 3.6344633139669895 Training error :0.7039334774017334\n",
      "Epoch : 3601 Loss: 3.6339550651609898 Training error :0.7038288712501526\n",
      "Epoch : 3602 Loss: 3.633424397557974 Training error :0.7037192583084106\n",
      "Epoch : 3603 Loss: 3.6329034380614758 Training error :0.7036119699478149\n",
      "Epoch : 3604 Loss: 3.632395140826702 Training error :0.7035057544708252\n",
      "Epoch : 3605 Loss: 3.63185378536582 Training error :0.7033982276916504\n",
      "Epoch : 3606 Loss: 3.631346981972456 Training error :0.7032939195632935\n",
      "Epoch : 3607 Loss: 3.63082205504179 Training error :0.7031880021095276\n",
      "Epoch : 3608 Loss: 3.6303074844181538 Training error :0.7030972838401794\n",
      "Epoch : 3609 Loss: 3.6304925493896008 Training error :0.7029086947441101\n",
      "Epoch : 3610 Loss: 3.6296694725751877 Training error :0.7028812170028687\n",
      "Epoch : 3611 Loss: 3.6286948770284653 Training error :0.7028100490570068\n",
      "Epoch : 3612 Loss: 3.6282092072069645 Training error :0.7026757597923279\n",
      "Epoch : 3613 Loss: 3.6278404891490936 Training error :0.7025656700134277\n",
      "Epoch : 3614 Loss: 3.627320047467947 Training error :0.7024723887443542\n",
      "Epoch : 3615 Loss: 3.626794718205929 Training error :0.7023731470108032\n",
      "Epoch : 3616 Loss: 3.626290123909712 Training error :0.7022685408592224\n",
      "Epoch : 3617 Loss: 3.6258083917200565 Training error :0.7021675705909729\n",
      "Epoch : 3618 Loss: 3.625319465994835 Training error :0.7020658850669861\n",
      "Epoch : 3619 Loss: 3.6248177625238895 Training error :0.7019639611244202\n",
      "Epoch : 3620 Loss: 3.624312650412321 Training error :0.701865553855896\n",
      "Epoch : 3621 Loss: 3.623810052871704 Training error :0.7017627954483032\n",
      "Epoch : 3622 Loss: 3.6233418993651867 Training error :0.7016656398773193\n",
      "Epoch : 3623 Loss: 3.6228730119764805 Training error :0.7015666365623474\n",
      "Epoch : 3624 Loss: 3.6223859637975693 Training error :0.7014650106430054\n",
      "Epoch : 3625 Loss: 3.6218842528760433 Training error :0.7013662457466125\n",
      "Epoch : 3626 Loss: 3.6213973090052605 Training error :0.7012658715248108\n",
      "Epoch : 3627 Loss: 3.620946254581213 Training error :0.7011722326278687\n",
      "Epoch : 3628 Loss: 3.621111609041691 Training error :0.7009997367858887\n",
      "Epoch : 3629 Loss: 3.6203688755631447 Training error :0.7009806036949158\n",
      "Epoch : 3630 Loss: 3.619423232972622 Training error :0.7009159922599792\n",
      "Epoch : 3631 Loss: 3.6189739517867565 Training error :0.7007920742034912\n",
      "Epoch : 3632 Loss: 3.6186504401266575 Training error :0.7006851434707642\n",
      "Epoch : 3633 Loss: 3.6181518360972404 Training error :0.7006001472473145\n",
      "Epoch : 3634 Loss: 3.617623630911112 Training error :0.7005064487457275\n",
      "Epoch : 3635 Loss: 3.6171672120690346 Training error :0.700406551361084\n",
      "Epoch : 3636 Loss: 3.616727638989687 Training error :0.7003132104873657\n",
      "Epoch : 3637 Loss: 3.6162449903786182 Training error :0.7002190351486206\n",
      "Epoch : 3638 Loss: 3.615811016410589 Training error :0.7001283764839172\n",
      "Epoch : 3639 Loss: 3.6153634339571 Training error :0.7000327110290527\n",
      "Epoch : 3640 Loss: 3.614915993064642 Training error :0.6999370455741882\n",
      "Epoch : 3641 Loss: 3.6144434362649918 Training error :0.6998438835144043\n",
      "Epoch : 3642 Loss: 3.6140051186084747 Training error :0.6997485160827637\n",
      "Epoch : 3643 Loss: 3.6135545782744884 Training error :0.6996602416038513\n",
      "Epoch : 3644 Loss: 3.613106142729521 Training error :0.6995662450790405\n",
      "Epoch : 3645 Loss: 3.6126377806067467 Training error :0.6994746923446655\n",
      "Epoch : 3646 Loss: 3.6121804267168045 Training error :0.699384331703186\n",
      "Epoch : 3647 Loss: 3.611727613955736 Training error :0.6992876529693604\n",
      "Epoch : 3648 Loss: 3.611286424100399 Training error :0.6992072463035583\n",
      "Epoch : 3649 Loss: 3.6114972420036793 Training error :0.6990381479263306\n",
      "Epoch : 3650 Loss: 3.6108030825853348 Training error :0.699024498462677\n",
      "Epoch : 3651 Loss: 3.6098714880645275 Training error :0.6989626288414001\n",
      "Epoch : 3652 Loss: 3.6094515584409237 Training error :0.6988480091094971\n",
      "Epoch : 3653 Loss: 3.609146699309349 Training error :0.6987475752830505\n",
      "Epoch : 3654 Loss: 3.608711637556553 Training error :0.6986709833145142\n",
      "Epoch : 3655 Loss: 3.6082074604928493 Training error :0.6985822916030884\n",
      "Epoch : 3656 Loss: 3.60777348279953 Training error :0.6984918117523193\n",
      "Epoch : 3657 Loss: 3.6073595955967903 Training error :0.6984043121337891\n",
      "Epoch : 3658 Loss: 3.6069506146013737 Training error :0.6983174085617065\n",
      "Epoch : 3659 Loss: 3.6065286211669445 Training error :0.6982321739196777\n",
      "Epoch : 3660 Loss: 3.6061019971966743 Training error :0.6981416940689087\n",
      "Epoch : 3661 Loss: 3.6056808084249496 Training error :0.6980518102645874\n",
      "Epoch : 3662 Loss: 3.605243518948555 Training error :0.6979675889015198\n",
      "Epoch : 3663 Loss: 3.604813791811466 Training error :0.6978809237480164\n",
      "Epoch : 3664 Loss: 3.604413866996765 Training error :0.6977972984313965\n",
      "Epoch : 3665 Loss: 3.6039941012859344 Training error :0.6977118849754333\n",
      "Epoch : 3666 Loss: 3.6035741716623306 Training error :0.6976273059844971\n",
      "Epoch : 3667 Loss: 3.6031692773103714 Training error :0.6975401043891907\n",
      "Epoch : 3668 Loss: 3.6027529165148735 Training error :0.6974522471427917\n",
      "Epoch : 3669 Loss: 3.602340240031481 Training error :0.6973758935928345\n",
      "Epoch : 3670 Loss: 3.6025794334709644 Training error :0.6972147226333618\n",
      "Epoch : 3671 Loss: 3.6019209809601307 Training error :0.6972050666809082\n",
      "Epoch : 3672 Loss: 3.6010308861732483 Training error :0.6971552968025208\n",
      "Epoch : 3673 Loss: 3.600614059716463 Training error :0.6970429420471191\n",
      "Epoch : 3674 Loss: 3.600365672260523 Training error :0.6969506144523621\n",
      "Epoch : 3675 Loss: 3.599944144487381 Training error :0.6968754529953003\n",
      "Epoch : 3676 Loss: 3.5994753390550613 Training error :0.6967981457710266\n",
      "Epoch : 3677 Loss: 3.5990826562047005 Training error :0.6967117190361023\n",
      "Epoch : 3678 Loss: 3.5987065695226192 Training error :0.6966301798820496\n",
      "Epoch : 3679 Loss: 3.5983071215450764 Training error :0.6965464949607849\n",
      "Epoch : 3680 Loss: 3.597898431122303 Training error :0.6964704394340515\n",
      "Epoch : 3681 Loss: 3.5975083522498608 Training error :0.6963876485824585\n",
      "Epoch : 3682 Loss: 3.597126856446266 Training error :0.6963051557540894\n",
      "Epoch : 3683 Loss: 3.596724733710289 Training error :0.6962261199951172\n",
      "Epoch : 3684 Loss: 3.5963056944310665 Training error :0.6961460709571838\n",
      "Epoch : 3685 Loss: 3.5959403552114964 Training error :0.6960657238960266\n",
      "Epoch : 3686 Loss: 3.5955495685338974 Training error :0.6959850788116455\n",
      "Epoch : 3687 Loss: 3.595152225345373 Training error :0.6959025859832764\n",
      "Epoch : 3688 Loss: 3.5947613418102264 Training error :0.6958258748054504\n",
      "Epoch : 3689 Loss: 3.5943627022206783 Training error :0.6957459449768066\n",
      "Epoch : 3690 Loss: 3.5940069518983364 Training error :0.6956702470779419\n",
      "Epoch : 3691 Loss: 3.593626145273447 Training error :0.6955966949462891\n",
      "Epoch : 3692 Loss: 3.5939049050211906 Training error :0.6954398155212402\n",
      "Epoch : 3693 Loss: 3.593277644366026 Training error :0.6954380869865417\n",
      "Epoch : 3694 Loss: 3.5924040153622627 Training error :0.6953914165496826\n",
      "Epoch : 3695 Loss: 3.5920451767742634 Training error :0.695284366607666\n",
      "Epoch : 3696 Loss: 3.591831497848034 Training error :0.6951969861984253\n",
      "Epoch : 3697 Loss: 3.591428104788065 Training error :0.6951313614845276\n",
      "Epoch : 3698 Loss: 3.5909722298383713 Training error :0.6950605511665344\n",
      "Epoch : 3699 Loss: 3.5906059071421623 Training error :0.6949803829193115\n",
      "Epoch : 3700 Loss: 3.5902745611965656 Training error :0.694902241230011\n",
      "Epoch : 3701 Loss: 3.5899176597595215 Training error :0.6948280334472656\n",
      "Epoch : 3702 Loss: 3.5895486436784267 Training error :0.6947522163391113\n",
      "Epoch : 3703 Loss: 3.589173100888729 Training error :0.6946758031845093\n",
      "Epoch : 3704 Loss: 3.588814042508602 Training error :0.6946021318435669\n",
      "Epoch : 3705 Loss: 3.588454097509384 Training error :0.6945263743400574\n",
      "Epoch : 3706 Loss: 3.588086321949959 Training error :0.6944541931152344\n",
      "Epoch : 3707 Loss: 3.5877066664397717 Training error :0.6943789720535278\n",
      "Epoch : 3708 Loss: 3.5873541459441185 Training error :0.6943035125732422\n",
      "Epoch : 3709 Loss: 3.5869931131601334 Training error :0.6942322254180908\n",
      "Epoch : 3710 Loss: 3.5866359882056713 Training error :0.6941580176353455\n",
      "Epoch : 3711 Loss: 3.586277339607477 Training error :0.6940860152244568\n",
      "Epoch : 3712 Loss: 3.5859383530914783 Training error :0.694012463092804\n",
      "Epoch : 3713 Loss: 3.5855806283652782 Training error :0.6939369440078735\n",
      "Epoch : 3714 Loss: 3.585234973579645 Training error :0.6938786506652832\n",
      "Epoch : 3715 Loss: 3.5855121165513992 Training error :0.693722128868103\n",
      "Epoch : 3716 Loss: 3.58490252494812 Training error :0.6937285661697388\n",
      "Epoch : 3717 Loss: 3.5841018110513687 Training error :0.6936853528022766\n",
      "Epoch : 3718 Loss: 3.5837146043777466 Training error :0.6935858130455017\n",
      "Epoch : 3719 Loss: 3.583542726933956 Training error :0.6935070753097534\n",
      "Epoch : 3720 Loss: 3.5831636153161526 Training error :0.6934440732002258\n",
      "Epoch : 3721 Loss: 3.5827630013227463 Training error :0.6933768391609192\n",
      "Epoch : 3722 Loss: 3.5824359096586704 Training error :0.6933026909828186\n",
      "Epoch : 3723 Loss: 3.5821234732866287 Training error :0.6932342648506165\n",
      "Epoch : 3724 Loss: 3.5817863568663597 Training error :0.6931648254394531\n",
      "Epoch : 3725 Loss: 3.581427402794361 Training error :0.693095862865448\n",
      "Epoch : 3726 Loss: 3.5810920111835003 Training error :0.6930259466171265\n",
      "Epoch : 3727 Loss: 3.5807792395353317 Training error :0.69295334815979\n",
      "Epoch : 3728 Loss: 3.580424554646015 Training error :0.6928861737251282\n",
      "Epoch : 3729 Loss: 3.5800697803497314 Training error :0.6928176879882812\n",
      "Epoch : 3730 Loss: 3.5797490179538727 Training error :0.6927497386932373\n",
      "Epoch : 3731 Loss: 3.5794133692979813 Training error :0.6926805377006531\n",
      "Epoch : 3732 Loss: 3.5790686681866646 Training error :0.6926089525222778\n",
      "Epoch : 3733 Loss: 3.5787264443933964 Training error :0.6925442218780518\n",
      "Epoch : 3734 Loss: 3.578391380608082 Training error :0.6924781203269958\n",
      "Epoch : 3735 Loss: 3.578094396740198 Training error :0.6924103498458862\n",
      "Epoch : 3736 Loss: 3.577766228467226 Training error :0.692341685295105\n",
      "Epoch : 3737 Loss: 3.5774332098662853 Training error :0.6922754645347595\n",
      "Epoch : 3738 Loss: 3.577755555510521 Training error :0.6921365261077881\n",
      "Epoch : 3739 Loss: 3.577196169644594 Training error :0.6921420097351074\n",
      "Epoch : 3740 Loss: 3.576392311602831 Training error :0.6921059489250183\n",
      "Epoch : 3741 Loss: 3.576064184308052 Training error :0.6920151710510254\n",
      "Epoch : 3742 Loss: 3.5758617222309113 Training error :0.691941499710083\n",
      "Epoch : 3743 Loss: 3.5755605697631836 Training error :0.6918846368789673\n",
      "Epoch : 3744 Loss: 3.5751673318445683 Training error :0.6918195486068726\n",
      "Epoch : 3745 Loss: 3.574841484427452 Training error :0.6917539238929749\n",
      "Epoch : 3746 Loss: 3.574550651013851 Training error :0.6916866898536682\n",
      "Epoch : 3747 Loss: 3.5742482990026474 Training error :0.6916269063949585\n",
      "Epoch : 3748 Loss: 3.573947425931692 Training error :0.6915595531463623\n",
      "Epoch : 3749 Loss: 3.5736332423985004 Training error :0.691494345664978\n",
      "Epoch : 3750 Loss: 3.573335323482752 Training error :0.6914320588111877\n",
      "Epoch : 3751 Loss: 3.5730074793100357 Training error :0.6913660168647766\n",
      "Epoch : 3752 Loss: 3.5726972855627537 Training error :0.6913059949874878\n",
      "Epoch : 3753 Loss: 3.572401400655508 Training error :0.6912381649017334\n",
      "Epoch : 3754 Loss: 3.5720854476094246 Training error :0.6911749839782715\n",
      "Epoch : 3755 Loss: 3.5717713721096516 Training error :0.6911119818687439\n",
      "Epoch : 3756 Loss: 3.57147504016757 Training error :0.691049337387085\n",
      "Epoch : 3757 Loss: 3.571170821785927 Training error :0.6909893751144409\n",
      "Epoch : 3758 Loss: 3.5708573386073112 Training error :0.6909261345863342\n",
      "Epoch : 3759 Loss: 3.5705663189291954 Training error :0.6908647418022156\n",
      "Epoch : 3760 Loss: 3.57026357203722 Training error :0.6908011436462402\n",
      "Epoch : 3761 Loss: 3.5699532367289066 Training error :0.6907394528388977\n",
      "Epoch : 3762 Loss: 3.569659322500229 Training error :0.6906861662864685\n",
      "Epoch : 3763 Loss: 3.56999259442091 Training error :0.6905435919761658\n",
      "Epoch : 3764 Loss: 3.5694466829299927 Training error :0.6905577182769775\n",
      "Epoch : 3765 Loss: 3.568677119910717 Training error :0.6905244588851929\n",
      "Epoch : 3766 Loss: 3.568388346582651 Training error :0.6904388666152954\n",
      "Epoch : 3767 Loss: 3.5682114847004414 Training error :0.690369188785553\n",
      "Epoch : 3768 Loss: 3.567890826612711 Training error :0.6903154253959656\n",
      "Epoch : 3769 Loss: 3.5675667077302933 Training error :0.6902619004249573\n",
      "Epoch : 3770 Loss: 3.5672953873872757 Training error :0.6901986002922058\n",
      "Epoch : 3771 Loss: 3.5670375488698483 Training error :0.6901386976242065\n",
      "Epoch : 3772 Loss: 3.5667348988354206 Training error :0.6900805830955505\n",
      "Epoch : 3773 Loss: 3.56643733009696 Training error :0.6900190711021423\n",
      "Epoch : 3774 Loss: 3.5661483891308308 Training error :0.6899616718292236\n",
      "Epoch : 3775 Loss: 3.5658473260700703 Training error :0.6898988485336304\n",
      "Epoch : 3776 Loss: 3.5655575916171074 Training error :0.6898436546325684\n",
      "Epoch : 3777 Loss: 3.565296895802021 Training error :0.6897832751274109\n",
      "Epoch : 3778 Loss: 3.5650095231831074 Training error :0.6897247433662415\n",
      "Epoch : 3779 Loss: 3.564726557582617 Training error :0.6896686553955078\n",
      "Epoch : 3780 Loss: 3.5644529163837433 Training error :0.6896092295646667\n",
      "Epoch : 3781 Loss: 3.5641729570925236 Training error :0.689551830291748\n",
      "Epoch : 3782 Loss: 3.5638949424028397 Training error :0.6894951462745667\n",
      "Epoch : 3783 Loss: 3.5636157244443893 Training error :0.6894362568855286\n",
      "Epoch : 3784 Loss: 3.563323810696602 Training error :0.6893794536590576\n",
      "Epoch : 3785 Loss: 3.5630337856709957 Training error :0.6893214583396912\n",
      "Epoch : 3786 Loss: 3.5627725273370743 Training error :0.6892663836479187\n",
      "Epoch : 3787 Loss: 3.562496177852154 Training error :0.689209520816803\n",
      "Epoch : 3788 Loss: 3.5622283704578876 Training error :0.6891574263572693\n",
      "Epoch : 3789 Loss: 3.562586337327957 Training error :0.6890250444412231\n",
      "Epoch : 3790 Loss: 3.562093511223793 Training error :0.6890400052070618\n",
      "Epoch : 3791 Loss: 3.561357330530882 Training error :0.6890133023262024\n",
      "Epoch : 3792 Loss: 3.5610369257628918 Training error :0.6889297962188721\n",
      "Epoch : 3793 Loss: 3.560931481420994 Training error :0.6888684034347534\n",
      "Epoch : 3794 Loss: 3.5606341175734997 Training error :0.6888211965560913\n",
      "Epoch : 3795 Loss: 3.560321167111397 Training error :0.6887716054916382\n",
      "Epoch : 3796 Loss: 3.5600418262183666 Training error :0.6887108683586121\n",
      "Epoch : 3797 Loss: 3.559812732040882 Training error :0.6886566877365112\n",
      "Epoch : 3798 Loss: 3.559555310755968 Training error :0.6886047124862671\n",
      "Epoch : 3799 Loss: 3.5592889301478863 Training error :0.6885482668876648\n",
      "Epoch : 3800 Loss: 3.5590138882398605 Training error :0.6884931325912476\n",
      "Epoch : 3801 Loss: 3.558736987411976 Training error :0.6884381771087646\n",
      "Epoch : 3802 Loss: 3.5584864132106304 Training error :0.688382625579834\n",
      "Epoch : 3803 Loss: 3.558235265314579 Training error :0.6883318424224854\n",
      "Epoch : 3804 Loss: 3.5579634308815002 Training error :0.6882787942886353\n",
      "Epoch : 3805 Loss: 3.5576938688755035 Training error :0.6882255673408508\n",
      "Epoch : 3806 Loss: 3.5574532374739647 Training error :0.6881713271141052\n",
      "Epoch : 3807 Loss: 3.5572018697857857 Training error :0.6881161332130432\n",
      "Epoch : 3808 Loss: 3.556936040520668 Training error :0.6880629658699036\n",
      "Epoch : 3809 Loss: 3.556675862520933 Training error :0.6880096197128296\n",
      "Epoch : 3810 Loss: 3.55642481893301 Training error :0.6879612803459167\n",
      "Epoch : 3811 Loss: 3.5561550557613373 Training error :0.6879086494445801\n",
      "Epoch : 3812 Loss: 3.5559091977775097 Training error :0.6878530383110046\n",
      "Epoch : 3813 Loss: 3.555661480873823 Training error :0.6878004670143127\n",
      "Epoch : 3814 Loss: 3.555383674800396 Training error :0.6877484321594238\n",
      "Epoch : 3815 Loss: 3.5551369562745094 Training error :0.6877037882804871\n",
      "Epoch : 3816 Loss: 3.5555147230625153 Training error :0.6875752806663513\n",
      "Epoch : 3817 Loss: 3.555020086467266 Training error :0.6875958442687988\n",
      "Epoch : 3818 Loss: 3.5543233305215836 Training error :0.6875733137130737\n",
      "Epoch : 3819 Loss: 3.554058827459812 Training error :0.6874958276748657\n",
      "Epoch : 3820 Loss: 3.5539308078587055 Training error :0.6874370574951172\n",
      "Epoch : 3821 Loss: 3.553670696914196 Training error :0.6873983144760132\n",
      "Epoch : 3822 Loss: 3.5533782802522182 Training error :0.6873496770858765\n",
      "Epoch : 3823 Loss: 3.5531453378498554 Training error :0.6872926354408264\n",
      "Epoch : 3824 Loss: 3.5528961457312107 Training error :0.6872404217720032\n",
      "Epoch : 3825 Loss: 3.552658375352621 Training error :0.6871929168701172\n",
      "Epoch : 3826 Loss: 3.5524151735007763 Training error :0.6871441006660461\n",
      "Epoch : 3827 Loss: 3.5521905422210693 Training error :0.6870953440666199\n",
      "Epoch : 3828 Loss: 3.551966518163681 Training error :0.6870445013046265\n",
      "Epoch : 3829 Loss: 3.5517262518405914 Training error :0.686994731426239\n",
      "Epoch : 3830 Loss: 3.551480393856764 Training error :0.6869460344314575\n",
      "Epoch : 3831 Loss: 3.551225494593382 Training error :0.6868975758552551\n",
      "Epoch : 3832 Loss: 3.5510052740573883 Training error :0.686849057674408\n",
      "Epoch : 3833 Loss: 3.5507535189390182 Training error :0.6868003606796265\n",
      "Epoch : 3834 Loss: 3.550523269921541 Training error :0.6867506504058838\n",
      "Epoch : 3835 Loss: 3.5502956807613373 Training error :0.6867032051086426\n",
      "Epoch : 3836 Loss: 3.550055854022503 Training error :0.6866546869277954\n",
      "Epoch : 3837 Loss: 3.549822047352791 Training error :0.686606228351593\n",
      "Epoch : 3838 Loss: 3.5495946668088436 Training error :0.6865575909614563\n",
      "Epoch : 3839 Loss: 3.5493459664285183 Training error :0.6865066289901733\n",
      "Epoch : 3840 Loss: 3.549108821898699 Training error :0.6864627003669739\n",
      "Epoch : 3841 Loss: 3.5488778203725815 Training error :0.6864122748374939\n",
      "Epoch : 3842 Loss: 3.54863940179348 Training error :0.6863685846328735\n",
      "Epoch : 3843 Loss: 3.5484086722135544 Training error :0.6863219738006592\n",
      "Epoch : 3844 Loss: 3.548807065933943 Training error :0.6862018704414368\n",
      "Epoch : 3845 Loss: 3.5483603477478027 Training error :0.6862249970436096\n",
      "Epoch : 3846 Loss: 3.5476758517324924 Training error :0.6862040162086487\n",
      "Epoch : 3847 Loss: 3.5474348552525043 Training error :0.6861355304718018\n",
      "Epoch : 3848 Loss: 3.5473234467208385 Training error :0.6860788464546204\n",
      "Epoch : 3849 Loss: 3.5470979064702988 Training error :0.6860441565513611\n",
      "Epoch : 3850 Loss: 3.5468185022473335 Training error :0.6859998106956482\n",
      "Epoch : 3851 Loss: 3.5465871952474117 Training error :0.6859495639801025\n",
      "Epoch : 3852 Loss: 3.5463743545114994 Training error :0.6859032511711121\n",
      "Epoch : 3853 Loss: 3.546156581491232 Training error :0.685858964920044\n",
      "Epoch : 3854 Loss: 3.545939203351736 Training error :0.6858150362968445\n",
      "Epoch : 3855 Loss: 3.5457169115543365 Training error :0.685768187046051\n",
      "Epoch : 3856 Loss: 3.545518323779106 Training error :0.6857212781906128\n",
      "Epoch : 3857 Loss: 3.545288559049368 Training error :0.6856791973114014\n",
      "Epoch : 3858 Loss: 3.5450602173805237 Training error :0.6856338381767273\n",
      "Epoch : 3859 Loss: 3.5448436364531517 Training error :0.6855887770652771\n",
      "Epoch : 3860 Loss: 3.544628232717514 Training error :0.6855403184890747\n",
      "Epoch : 3861 Loss: 3.5444104559719563 Training error :0.6854961514472961\n",
      "Epoch : 3862 Loss: 3.544205032289028 Training error :0.6854525804519653\n",
      "Epoch : 3863 Loss: 3.5439817905426025 Training error :0.6854102611541748\n",
      "Epoch : 3864 Loss: 3.543772380799055 Training error :0.6853671073913574\n",
      "Epoch : 3865 Loss: 3.543551567941904 Training error :0.6853209733963013\n",
      "Epoch : 3866 Loss: 3.5433146581053734 Training error :0.6852747201919556\n",
      "Epoch : 3867 Loss: 3.543134156614542 Training error :0.6852337121963501\n",
      "Epoch : 3868 Loss: 3.5429157316684723 Training error :0.6851871609687805\n",
      "Epoch : 3869 Loss: 3.542699933052063 Training error :0.6851454377174377\n",
      "Epoch : 3870 Loss: 3.542493764311075 Training error :0.6851029992103577\n",
      "Epoch : 3871 Loss: 3.5422921925783157 Training error :0.685056209564209\n",
      "Epoch : 3872 Loss: 3.542078100144863 Training error :0.6850171685218811\n",
      "Epoch : 3873 Loss: 3.541868958622217 Training error :0.6849758625030518\n",
      "Epoch : 3874 Loss: 3.542272102087736 Training error :0.6848597526550293\n",
      "Epoch : 3875 Loss: 3.541852328926325 Training error :0.6848857998847961\n",
      "Epoch : 3876 Loss: 3.541162855923176 Training error :0.6848716139793396\n",
      "Epoch : 3877 Loss: 3.540942870080471 Training error :0.6848006248474121\n",
      "Epoch : 3878 Loss: 3.5408537313342094 Training error :0.6847504377365112\n",
      "Epoch : 3879 Loss: 3.540657088160515 Training error :0.6847185492515564\n",
      "Epoch : 3880 Loss: 3.540410812944174 Training error :0.6846808791160583\n",
      "Epoch : 3881 Loss: 3.540185507386923 Training error :0.6846354007720947\n",
      "Epoch : 3882 Loss: 3.540021378546953 Training error :0.6845912933349609\n",
      "Epoch : 3883 Loss: 3.5398113057017326 Training error :0.6845489740371704\n",
      "Epoch : 3884 Loss: 3.5396152921020985 Training error :0.6845084428787231\n",
      "Epoch : 3885 Loss: 3.5394010469317436 Training error :0.6844686269760132\n",
      "Epoch : 3886 Loss: 3.539217744022608 Training error :0.6844286322593689\n",
      "Epoch : 3887 Loss: 3.539003722369671 Training error :0.6843858957290649\n",
      "Epoch : 3888 Loss: 3.5388161465525627 Training error :0.6843435764312744\n",
      "Epoch : 3889 Loss: 3.538611702620983 Training error :0.6843051314353943\n",
      "Epoch : 3890 Loss: 3.5384081341326237 Training error :0.6842634677886963\n",
      "Epoch : 3891 Loss: 3.5382286198437214 Training error :0.6842234134674072\n",
      "Epoch : 3892 Loss: 3.5380080081522465 Training error :0.6841820478439331\n",
      "Epoch : 3893 Loss: 3.537818443030119 Training error :0.6841408610343933\n",
      "Epoch : 3894 Loss: 3.5376355573534966 Training error :0.6840823292732239\n",
      "Epoch : 3895 Loss: 3.53741192817688 Training error :0.6839948296546936\n",
      "Epoch : 3896 Loss: 3.5370994806289673 Training error :0.6839036345481873\n",
      "Epoch : 3897 Loss: 3.536700613796711 Training error :0.6838123798370361\n",
      "Epoch : 3898 Loss: 3.536284014582634 Training error :0.6837212443351746\n",
      "Epoch : 3899 Loss: 3.535852812230587 Training error :0.6836342811584473\n",
      "Epoch : 3900 Loss: 3.5354175865650177 Training error :0.6835443377494812\n",
      "Epoch : 3901 Loss: 3.5349877811968327 Training error :0.6834592223167419\n",
      "Epoch : 3902 Loss: 3.5345351733267307 Training error :0.6833778619766235\n",
      "Epoch : 3903 Loss: 3.5347582772374153 Training error :0.6832108497619629\n",
      "Epoch : 3904 Loss: 3.534040469676256 Training error :0.6831938028335571\n",
      "Epoch : 3905 Loss: 3.5331790409982204 Training error :0.6831291913986206\n",
      "Epoch : 3906 Loss: 3.5327232033014297 Training error :0.683021605014801\n",
      "Epoch : 3907 Loss: 3.5324230790138245 Training error :0.6829270720481873\n",
      "Epoch : 3908 Loss: 3.5320098735392094 Training error :0.6828509569168091\n",
      "Epoch : 3909 Loss: 3.531517945230007 Training error :0.6827669739723206\n",
      "Epoch : 3910 Loss: 3.5311253555119038 Training error :0.6826768517494202\n",
      "Epoch : 3911 Loss: 3.530719842761755 Training error :0.6825908422470093\n",
      "Epoch : 3912 Loss: 3.5302963815629482 Training error :0.68250572681427\n",
      "Epoch : 3913 Loss: 3.529871303588152 Training error :0.6824234127998352\n",
      "Epoch : 3914 Loss: 3.529458452016115 Training error :0.682338535785675\n",
      "Epoch : 3915 Loss: 3.5290538035333157 Training error :0.6822509169578552\n",
      "Epoch : 3916 Loss: 3.528624467551708 Training error :0.6821672320365906\n",
      "Epoch : 3917 Loss: 3.528196342289448 Training error :0.6820841431617737\n",
      "Epoch : 3918 Loss: 3.5277913846075535 Training error :0.6820030808448792\n",
      "Epoch : 3919 Loss: 3.5273879170417786 Training error :0.6819173693656921\n",
      "Epoch : 3920 Loss: 3.526976816356182 Training error :0.6818315386772156\n",
      "Epoch : 3921 Loss: 3.526579514145851 Training error :0.6817514300346375\n",
      "Epoch : 3922 Loss: 3.526155486702919 Training error :0.6816673874855042\n",
      "Epoch : 3923 Loss: 3.526334051042795 Training error :0.6815209984779358\n",
      "Epoch : 3924 Loss: 3.5257772020995617 Training error :0.6815044283866882\n",
      "Epoch : 3925 Loss: 3.524903815239668 Training error :0.6814529299736023\n",
      "Epoch : 3926 Loss: 3.5245058946311474 Training error :0.6813452839851379\n",
      "Epoch : 3927 Loss: 3.5242308638989925 Training error :0.6812563538551331\n",
      "Epoch : 3928 Loss: 3.5237834844738245 Training error :0.6811856627464294\n",
      "Epoch : 3929 Loss: 3.523358765989542 Training error :0.6811059713363647\n",
      "Epoch : 3930 Loss: 3.5229799151420593 Training error :0.6810227036476135\n",
      "Epoch : 3931 Loss: 3.522608073428273 Training error :0.6809423565864563\n",
      "Epoch : 3932 Loss: 3.5222268775105476 Training error :0.6808620691299438\n",
      "Epoch : 3933 Loss: 3.5218271985650063 Training error :0.6807830929756165\n",
      "Epoch : 3934 Loss: 3.5214351024478674 Training error :0.6807042956352234\n",
      "Epoch : 3935 Loss: 3.521052109077573 Training error :0.6806256771087646\n",
      "Epoch : 3936 Loss: 3.5206630043685436 Training error :0.6805479526519775\n",
      "Epoch : 3937 Loss: 3.5202694088220596 Training error :0.6804687976837158\n",
      "Epoch : 3938 Loss: 3.519889187067747 Training error :0.6803893446922302\n",
      "Epoch : 3939 Loss: 3.5194886550307274 Training error :0.6803085207939148\n",
      "Epoch : 3940 Loss: 3.51910319365561 Training error :0.6802322268486023\n",
      "Epoch : 3941 Loss: 3.5187300872057676 Training error :0.6801550984382629\n",
      "Epoch : 3942 Loss: 3.5183586720377207 Training error :0.6800809502601624\n",
      "Epoch : 3943 Loss: 3.5179944150149822 Training error :0.680004894733429\n",
      "Epoch : 3944 Loss: 3.518173884600401 Training error :0.6798536777496338\n",
      "Epoch : 3945 Loss: 3.5176404528319836 Training error :0.6798490881919861\n",
      "Epoch : 3946 Loss: 3.51680332608521 Training error :0.6797970533370972\n",
      "Epoch : 3947 Loss: 3.5164078660309315 Training error :0.6796979308128357\n",
      "Epoch : 3948 Loss: 3.5161538012325764 Training error :0.6796147227287292\n",
      "Epoch : 3949 Loss: 3.5157735515385866 Training error :0.6795487403869629\n",
      "Epoch : 3950 Loss: 3.515361549332738 Training error :0.6794752478599548\n",
      "Epoch : 3951 Loss: 3.5149751994758844 Training error :0.6793950796127319\n",
      "Epoch : 3952 Loss: 3.5146472975611687 Training error :0.6793226599693298\n",
      "Epoch : 3953 Loss: 3.5142713114619255 Training error :0.6792485117912292\n",
      "Epoch : 3954 Loss: 3.513913484290242 Training error :0.6791740655899048\n",
      "Epoch : 3955 Loss: 3.51354568451643 Training error :0.6790977716445923\n",
      "Epoch : 3956 Loss: 3.513188013806939 Training error :0.6790213584899902\n",
      "Epoch : 3957 Loss: 3.512810742482543 Training error :0.6789487600326538\n",
      "Epoch : 3958 Loss: 3.5124372877180576 Training error :0.6788771152496338\n",
      "Epoch : 3959 Loss: 3.5120873246341944 Training error :0.678802490234375\n",
      "Epoch : 3960 Loss: 3.5117150731384754 Training error :0.6787306070327759\n",
      "Epoch : 3961 Loss: 3.511361377313733 Training error :0.6786559224128723\n",
      "Epoch : 3962 Loss: 3.5110145322978497 Training error :0.6785857677459717\n",
      "Epoch : 3963 Loss: 3.510672889649868 Training error :0.6785098314285278\n",
      "Epoch : 3964 Loss: 3.5103136654943228 Training error :0.6784405708312988\n",
      "Epoch : 3965 Loss: 3.5105489939451218 Training error :0.6783009171485901\n",
      "Epoch : 3966 Loss: 3.5100297778844833 Training error :0.6782934665679932\n",
      "Epoch : 3967 Loss: 3.5092046912759542 Training error :0.6782500743865967\n",
      "Epoch : 3968 Loss: 3.5088347494602203 Training error :0.678153395652771\n",
      "Epoch : 3969 Loss: 3.5086251832544804 Training error :0.6780763864517212\n",
      "Epoch : 3970 Loss: 3.508237410336733 Training error :0.6780129075050354\n",
      "Epoch : 3971 Loss: 3.507839672267437 Training error :0.6779474020004272\n",
      "Epoch : 3972 Loss: 3.507510693743825 Training error :0.6778723001480103\n",
      "Epoch : 3973 Loss: 3.5071956161409616 Training error :0.6778018474578857\n",
      "Epoch : 3974 Loss: 3.506843402981758 Training error :0.6777343153953552\n",
      "Epoch : 3975 Loss: 3.5065030101686716 Training error :0.6776642799377441\n",
      "Epoch : 3976 Loss: 3.5061763264238834 Training error :0.6775926351547241\n",
      "Epoch : 3977 Loss: 3.5058158729225397 Training error :0.6775251626968384\n",
      "Epoch : 3978 Loss: 3.505468001589179 Training error :0.6774553656578064\n",
      "Epoch : 3979 Loss: 3.505121424794197 Training error :0.6773877739906311\n",
      "Epoch : 3980 Loss: 3.5048002284020185 Training error :0.6773172616958618\n",
      "Epoch : 3981 Loss: 3.5044608674943447 Training error :0.6772474646568298\n",
      "Epoch : 3982 Loss: 3.5041315499693155 Training error :0.6771785020828247\n",
      "Epoch : 3983 Loss: 3.5037771090865135 Training error :0.6771118640899658\n",
      "Epoch : 3984 Loss: 3.5034684110432863 Training error :0.6770442724227905\n",
      "Epoch : 3985 Loss: 3.503128919750452 Training error :0.676974356174469\n",
      "Epoch : 3986 Loss: 3.502795021981001 Training error :0.6769058108329773\n",
      "Epoch : 3987 Loss: 3.502452317625284 Training error :0.6768490076065063\n",
      "Epoch : 3988 Loss: 3.502733651548624 Training error :0.6767051815986633\n",
      "Epoch : 3989 Loss: 3.502204392105341 Training error :0.6767098903656006\n",
      "Epoch : 3990 Loss: 3.501450588926673 Training error :0.676666259765625\n",
      "Epoch : 3991 Loss: 3.501079997047782 Training error :0.6765739917755127\n",
      "Epoch : 3992 Loss: 3.500878954306245 Training error :0.6765008568763733\n",
      "Epoch : 3993 Loss: 3.5005459375679493 Training error :0.676442563533783\n",
      "Epoch : 3994 Loss: 3.500162236392498 Training error :0.6763817667961121\n",
      "Epoch : 3995 Loss: 3.4998532235622406 Training error :0.6763126254081726\n",
      "Epoch : 3996 Loss: 3.4995469581335783 Training error :0.6762480735778809\n",
      "Epoch : 3997 Loss: 3.499238945543766 Training error :0.6761815547943115\n",
      "Epoch : 3998 Loss: 3.4989187102764845 Training error :0.6761158108711243\n",
      "Epoch : 3999 Loss: 3.4986130464822054 Training error :0.6760531067848206\n",
      "Epoch : 4000 Loss: 3.498278345912695 Training error :0.6759862303733826\n",
      "Epoch : 4001 Loss: 3.4979675468057394 Training error :0.6759200096130371\n",
      "Epoch : 4002 Loss: 3.497641585767269 Training error :0.675856351852417\n",
      "Epoch : 4003 Loss: 3.4973323699086905 Training error :0.6757940053939819\n",
      "Epoch : 4004 Loss: 3.4970260336995125 Training error :0.6757292747497559\n",
      "Epoch : 4005 Loss: 3.496721174567938 Training error :0.6756666302680969\n",
      "Epoch : 4006 Loss: 3.496416937559843 Training error :0.6756002902984619\n",
      "Epoch : 4007 Loss: 3.496097818017006 Training error :0.6755374670028687\n",
      "Epoch : 4008 Loss: 3.495772313326597 Training error :0.6754733920097351\n",
      "Epoch : 4009 Loss: 3.4954779520630836 Training error :0.6754099130630493\n",
      "Epoch : 4010 Loss: 3.495155096054077 Training error :0.6753543019294739\n",
      "Epoch : 4011 Loss: 3.495430050417781 Training error :0.6752164959907532\n",
      "Epoch : 4012 Loss: 3.4949240796267986 Training error :0.6752199530601501\n",
      "Epoch : 4013 Loss: 3.494181690737605 Training error :0.6751858592033386\n",
      "Epoch : 4014 Loss: 3.4938194043934345 Training error :0.6751015186309814\n",
      "Epoch : 4015 Loss: 3.4936742577701807 Training error :0.6750307679176331\n",
      "Epoch : 4016 Loss: 3.493357004597783 Training error :0.6749770641326904\n",
      "Epoch : 4017 Loss: 3.4930041320621967 Training error :0.6749191284179688\n",
      "Epoch : 4018 Loss: 3.4926880709826946 Training error :0.6748532056808472\n",
      "Epoch : 4019 Loss: 3.4924423452466726 Training error :0.6747925281524658\n",
      "Epoch : 4020 Loss: 3.492127802222967 Training error :0.6747302412986755\n",
      "Epoch : 4021 Loss: 3.4917802065610886 Training error :0.6746689081192017\n",
      "Epoch : 4022 Loss: 3.491510981693864 Training error :0.6746092438697815\n",
      "Epoch : 4023 Loss: 3.4912143629044294 Training error :0.6745491027832031\n",
      "Epoch : 4024 Loss: 3.4909357856959105 Training error :0.6744900345802307\n",
      "Epoch : 4025 Loss: 3.490625459700823 Training error :0.6744270324707031\n",
      "Epoch : 4026 Loss: 3.4903315249830484 Training error :0.6743659377098083\n",
      "Epoch : 4027 Loss: 3.490034507587552 Training error :0.6743070483207703\n",
      "Epoch : 4028 Loss: 3.489735584706068 Training error :0.6742496490478516\n",
      "Epoch : 4029 Loss: 3.4894414730370045 Training error :0.6741889119148254\n",
      "Epoch : 4030 Loss: 3.489144917577505 Training error :0.6741273403167725\n",
      "Epoch : 4031 Loss: 3.488861370831728 Training error :0.6740661859512329\n",
      "Epoch : 4032 Loss: 3.488579172641039 Training error :0.674009382724762\n",
      "Epoch : 4033 Loss: 3.488273646682501 Training error :0.6739523410797119\n",
      "Epoch : 4034 Loss: 3.4885599724948406 Training error :0.6738256812095642\n",
      "Epoch : 4035 Loss: 3.488131146878004 Training error :0.6738336086273193\n",
      "Epoch : 4036 Loss: 3.487368417903781 Training error :0.673798680305481\n",
      "Epoch : 4037 Loss: 3.4870413541793823 Training error :0.6737175583839417\n",
      "Epoch : 4038 Loss: 3.4868747778236866 Training error :0.673656165599823\n",
      "Epoch : 4039 Loss: 3.4866167437285185 Training error :0.6736039519309998\n",
      "Epoch : 4040 Loss: 3.486278124153614 Training error :0.6735495924949646\n",
      "Epoch : 4041 Loss: 3.485977800562978 Training error :0.6734880208969116\n",
      "Epoch : 4042 Loss: 3.485728606581688 Training error :0.6734281778335571\n",
      "Epoch : 4043 Loss: 3.4854313377290964 Training error :0.673371434211731\n",
      "Epoch : 4044 Loss: 3.4851700849831104 Training error :0.6733172535896301\n",
      "Epoch : 4045 Loss: 3.4848803654313087 Training error :0.67325758934021\n",
      "Epoch : 4046 Loss: 3.4845956303179264 Training error :0.6732019782066345\n",
      "Epoch : 4047 Loss: 3.484328906983137 Training error :0.6731454730033875\n",
      "Epoch : 4048 Loss: 3.4840517602860928 Training error :0.6730883717536926\n",
      "Epoch : 4049 Loss: 3.483754960820079 Training error :0.6730299592018127\n",
      "Epoch : 4050 Loss: 3.4834795221686363 Training error :0.6729764342308044\n",
      "Epoch : 4051 Loss: 3.4832310806959867 Training error :0.6729202270507812\n",
      "Epoch : 4052 Loss: 3.4829525295645 Training error :0.6728678941726685\n",
      "Epoch : 4053 Loss: 3.4826769027858973 Training error :0.6728096008300781\n",
      "Epoch : 4054 Loss: 3.482423271983862 Training error :0.672752320766449\n",
      "Epoch : 4055 Loss: 3.482137192040682 Training error :0.6726984977722168\n",
      "Epoch : 4056 Loss: 3.48186619207263 Training error :0.6726415157318115\n",
      "Epoch : 4057 Loss: 3.481589013710618 Training error :0.67258620262146\n",
      "Epoch : 4058 Loss: 3.4813309274613857 Training error :0.672538161277771\n",
      "Epoch : 4059 Loss: 3.4816599786281586 Training error :0.672410249710083\n",
      "Epoch : 4060 Loss: 3.481221329420805 Training error :0.672420859336853\n",
      "Epoch : 4061 Loss: 3.4804910235106945 Training error :0.672391414642334\n",
      "Epoch : 4062 Loss: 3.4801930971443653 Training error :0.6723169088363647\n",
      "Epoch : 4063 Loss: 3.4800457470119 Training error :0.6722543835639954\n",
      "Epoch : 4064 Loss: 3.479788290336728 Training error :0.672208309173584\n",
      "Epoch : 4065 Loss: 3.4794717207551003 Training error :0.6721577644348145\n",
      "Epoch : 4066 Loss: 3.4791984483599663 Training error :0.6721017360687256\n",
      "Epoch : 4067 Loss: 3.478981252759695 Training error :0.6720479726791382\n",
      "Epoch : 4068 Loss: 3.478696310892701 Training error :0.6719943881034851\n",
      "Epoch : 4069 Loss: 3.478421077132225 Training error :0.6719407439231873\n",
      "Epoch : 4070 Loss: 3.478167500346899 Training error :0.6718887090682983\n",
      "Epoch : 4071 Loss: 3.4779166728258133 Training error :0.6718348860740662\n",
      "Epoch : 4072 Loss: 3.4776654466986656 Training error :0.6717835068702698\n",
      "Epoch : 4073 Loss: 3.477396849542856 Training error :0.6717286705970764\n",
      "Epoch : 4074 Loss: 3.4771481920033693 Training error :0.6716758608818054\n",
      "Epoch : 4075 Loss: 3.47689388692379 Training error :0.6716265082359314\n",
      "Epoch : 4076 Loss: 3.4766381066292524 Training error :0.6715728044509888\n",
      "Epoch : 4077 Loss: 3.47636547498405 Training error :0.6715194582939148\n",
      "Epoch : 4078 Loss: 3.4761209841817617 Training error :0.6714701652526855\n",
      "Epoch : 4079 Loss: 3.47586627304554 Training error :0.6714179515838623\n",
      "Epoch : 4080 Loss: 3.475642738863826 Training error :0.67136549949646\n",
      "Epoch : 4081 Loss: 3.475363912060857 Training error :0.6713129281997681\n",
      "Epoch : 4082 Loss: 3.4751151129603386 Training error :0.6712591648101807\n",
      "Epoch : 4083 Loss: 3.4748595356941223 Training error :0.671215832233429\n",
      "Epoch : 4084 Loss: 3.4751964695751667 Training error :0.6710960268974304\n",
      "Epoch : 4085 Loss: 3.4747557155787945 Training error :0.6711074113845825\n",
      "Epoch : 4086 Loss: 3.474063090980053 Training error :0.6710822582244873\n",
      "Epoch : 4087 Loss: 3.4737956896424294 Training error :0.671008825302124\n",
      "Epoch : 4088 Loss: 3.473668560385704 Training error :0.6709497570991516\n",
      "Epoch : 4089 Loss: 3.473406484350562 Training error :0.6709083318710327\n",
      "Epoch : 4090 Loss: 3.4731219839304686 Training error :0.6708621382713318\n",
      "Epoch : 4091 Loss: 3.4728750120848417 Training error :0.6708068251609802\n",
      "Epoch : 4092 Loss: 3.4726495146751404 Training error :0.6707556843757629\n",
      "Epoch : 4093 Loss: 3.4724038280546665 Training error :0.6707063317298889\n",
      "Epoch : 4094 Loss: 3.472145464271307 Training error :0.6706585884094238\n",
      "Epoch : 4095 Loss: 3.47190560400486 Training error :0.6706094145774841\n",
      "Epoch : 4096 Loss: 3.47167512960732 Training error :0.6705614328384399\n",
      "Epoch : 4097 Loss: 3.4714358150959015 Training error :0.6705106496810913\n",
      "Epoch : 4098 Loss: 3.4711899794638157 Training error :0.6704622507095337\n",
      "Epoch : 4099 Loss: 3.4709592573344707 Training error :0.6704122424125671\n",
      "Epoch : 4100 Loss: 3.4707236606627703 Training error :0.6703614592552185\n",
      "Epoch : 4101 Loss: 3.470492694526911 Training error :0.670315146446228\n",
      "Epoch : 4102 Loss: 3.470245622098446 Training error :0.6702644228935242\n",
      "Epoch : 4103 Loss: 3.4700148664414883 Training error :0.670218825340271\n",
      "Epoch : 4104 Loss: 3.4697611946612597 Training error :0.6701686382293701\n",
      "Epoch : 4105 Loss: 3.469533897936344 Training error :0.6701209545135498\n",
      "Epoch : 4106 Loss: 3.46929844468832 Training error :0.6700711846351624\n",
      "Epoch : 4107 Loss: 3.469077229499817 Training error :0.6700226664543152\n",
      "Epoch : 4108 Loss: 3.468819873407483 Training error :0.6699760556221008\n",
      "Epoch : 4109 Loss: 3.4686092771589756 Training error :0.6699333190917969\n",
      "Epoch : 4110 Loss: 3.4689289685338736 Training error :0.6698133945465088\n",
      "Epoch : 4111 Loss: 3.468532558530569 Training error :0.6698316931724548\n",
      "Epoch : 4112 Loss: 3.467833708971739 Training error :0.6698120832443237\n",
      "Epoch : 4113 Loss: 3.467595688998699 Training error :0.6697417497634888\n",
      "Epoch : 4114 Loss: 3.4674732219427824 Training error :0.6696856021881104\n",
      "Epoch : 4115 Loss: 3.467240186408162 Training error :0.6696452498435974\n",
      "Epoch : 4116 Loss: 3.4669582303613424 Training error :0.6696024537086487\n",
      "Epoch : 4117 Loss: 3.466748610138893 Training error :0.6695539951324463\n",
      "Epoch : 4118 Loss: 3.466553505510092 Training error :0.6695083379745483\n",
      "Epoch : 4119 Loss: 3.4663365371525288 Training error :0.6694629788398743\n",
      "Epoch : 4120 Loss: 3.4660871475934982 Training error :0.6694120764732361\n",
      "Epoch : 4121 Loss: 3.465849695727229 Training error :0.6693692207336426\n",
      "Epoch : 4122 Loss: 3.4656401108950377 Training error :0.6693229079246521\n",
      "Epoch : 4123 Loss: 3.465411487966776 Training error :0.6692745685577393\n",
      "Epoch : 4124 Loss: 3.465184036642313 Training error :0.669228732585907\n",
      "Epoch : 4125 Loss: 3.4649631902575493 Training error :0.6691845655441284\n",
      "Epoch : 4126 Loss: 3.464767722412944 Training error :0.6691418290138245\n",
      "Epoch : 4127 Loss: 3.464519701898098 Training error :0.6690946221351624\n",
      "Epoch : 4128 Loss: 3.464298142120242 Training error :0.6690467000007629\n",
      "Epoch : 4129 Loss: 3.4640793539583683 Training error :0.6690011024475098\n",
      "Epoch : 4130 Loss: 3.463834762573242 Training error :0.6689565181732178\n",
      "Epoch : 4131 Loss: 3.4636233374476433 Training error :0.6689139604568481\n",
      "Epoch : 4132 Loss: 3.4634239953011274 Training error :0.668865442276001\n",
      "Epoch : 4133 Loss: 3.4631879962980747 Training error :0.6688226461410522\n",
      "Epoch : 4134 Loss: 3.462978171184659 Training error :0.6687790751457214\n",
      "Epoch : 4135 Loss: 3.4627583269029856 Training error :0.668731153011322\n",
      "Epoch : 4136 Loss: 3.462534461170435 Training error :0.6686908006668091\n",
      "Epoch : 4137 Loss: 3.4628645591437817 Training error :0.6685805320739746\n",
      "Epoch : 4138 Loss: 3.4624852277338505 Training error :0.6685972809791565\n",
      "Epoch : 4139 Loss: 3.4618454538285732 Training error :0.6685798764228821\n",
      "Epoch : 4140 Loss: 3.461561545729637 Training error :0.6685104966163635\n",
      "Epoch : 4141 Loss: 3.461497673764825 Training error :0.668465256690979\n",
      "Epoch : 4142 Loss: 3.461274703964591 Training error :0.6684265732765198\n",
      "Epoch : 4143 Loss: 3.46102930419147 Training error :0.6683852672576904\n",
      "Epoch : 4144 Loss: 3.460806768387556 Training error :0.6683422327041626\n",
      "Epoch : 4145 Loss: 3.460619915276766 Training error :0.6682973504066467\n",
      "Epoch : 4146 Loss: 3.460402123630047 Training error :0.6682528853416443\n",
      "Epoch : 4147 Loss: 3.4601867254823446 Training error :0.6682104468345642\n",
      "Epoch : 4148 Loss: 3.459979908540845 Training error :0.6681687831878662\n",
      "Epoch : 4149 Loss: 3.459781441837549 Training error :0.6681283712387085\n",
      "Epoch : 4150 Loss: 3.459569728001952 Training error :0.6680814623832703\n",
      "Epoch : 4151 Loss: 3.459364205598831 Training error :0.6680391430854797\n",
      "Epoch : 4152 Loss: 3.459148682653904 Training error :0.6679966449737549\n",
      "Epoch : 4153 Loss: 3.458935931324959 Training error :0.6679545640945435\n",
      "Epoch : 4154 Loss: 3.458726041018963 Training error :0.6679112911224365\n",
      "Epoch : 4155 Loss: 3.4585068970918655 Training error :0.6678670644760132\n",
      "Epoch : 4156 Loss: 3.458327956497669 Training error :0.667826771736145\n",
      "Epoch : 4157 Loss: 3.4581096842885017 Training error :0.6677852869033813\n",
      "Epoch : 4158 Loss: 3.457924544811249 Training error :0.6677419543266296\n",
      "Epoch : 4159 Loss: 3.457711884751916 Training error :0.6676995158195496\n",
      "Epoch : 4160 Loss: 3.4575085490942 Training error :0.6676586866378784\n",
      "Epoch : 4161 Loss: 3.4572956692427397 Training error :0.6676173210144043\n",
      "Epoch : 4162 Loss: 3.457098301500082 Training error :0.6675770282745361\n",
      "Epoch : 4163 Loss: 3.4568883441388607 Training error :0.6675359606742859\n",
      "Epoch : 4164 Loss: 3.4567044489085674 Training error :0.6674928069114685\n",
      "Epoch : 4165 Loss: 3.4565121233463287 Training error :0.667460560798645\n",
      "Epoch : 4166 Loss: 3.456830883398652 Training error :0.6673459410667419\n",
      "Epoch : 4167 Loss: 3.4564815126359463 Training error :0.6673682928085327\n",
      "Epoch : 4168 Loss: 3.455859435722232 Training error :0.6673523783683777\n",
      "Epoch : 4169 Loss: 3.4555834643542767 Training error :0.6672870516777039\n",
      "Epoch : 4170 Loss: 3.45555305108428 Training error :0.6672406792640686\n",
      "Epoch : 4171 Loss: 3.4553257264196873 Training error :0.6672078967094421\n",
      "Epoch : 4172 Loss: 3.4550447650253773 Training error :0.667173445224762\n",
      "Epoch : 4173 Loss: 3.4548746440559626 Training error :0.6671293377876282\n",
      "Epoch : 4174 Loss: 3.4547073785215616 Training error :0.6670851707458496\n",
      "Epoch : 4175 Loss: 3.4545260164886713 Training error :0.6670469045639038\n",
      "Epoch : 4176 Loss: 3.4543186221271753 Training error :0.6670079827308655\n",
      "Epoch : 4177 Loss: 3.454141788184643 Training error :0.6669684052467346\n",
      "Epoch : 4178 Loss: 3.4539405461400747 Training error :0.6669281721115112\n",
      "Epoch : 4179 Loss: 3.4537345096468925 Training error :0.6668897867202759\n",
      "Epoch : 4180 Loss: 3.4535278901457787 Training error :0.6668513417243958\n",
      "Epoch : 4181 Loss: 3.453355509787798 Training error :0.6668107509613037\n",
      "Epoch : 4182 Loss: 3.453153323382139 Training error :0.6667693257331848\n",
      "Epoch : 4183 Loss: 3.45296841673553 Training error :0.666731059551239\n",
      "Epoch : 4184 Loss: 3.452779907733202 Training error :0.6666925549507141\n",
      "Epoch : 4185 Loss: 3.452582724392414 Training error :0.6666545271873474\n",
      "Epoch : 4186 Loss: 3.452381931245327 Training error :0.6666148900985718\n",
      "Epoch : 4187 Loss: 3.4521929435431957 Training error :0.6665737628936768\n",
      "Epoch : 4188 Loss: 3.451986502856016 Training error :0.666536808013916\n",
      "Epoch : 4189 Loss: 3.4518126510083675 Training error :0.6664977073669434\n",
      "Epoch : 4190 Loss: 3.451620388776064 Training error :0.6664567589759827\n",
      "Epoch : 4191 Loss: 3.4514309745281935 Training error :0.6664196252822876\n",
      "Epoch : 4192 Loss: 3.4512489456683397 Training error :0.6663805246353149\n",
      "Epoch : 4193 Loss: 3.451066030189395 Training error :0.6663436889648438\n",
      "Epoch : 4194 Loss: 3.4508664682507515 Training error :0.6663061380386353\n",
      "Epoch : 4195 Loss: 3.451211193576455 Training error :0.6662033200263977\n",
      "Epoch : 4196 Loss: 3.4508925061672926 Training error :0.666229248046875\n",
      "Epoch : 4197 Loss: 3.45028780028224 Training error :0.6662125587463379\n",
      "Epoch : 4198 Loss: 3.450036458671093 Training error :0.666155219078064\n",
      "Epoch : 4199 Loss: 3.4499617367982864 Training error :0.666109025478363\n",
      "Epoch : 4200 Loss: 3.4498122483491898 Training error :0.6660792827606201\n",
      "Epoch : 4201 Loss: 3.4495399557054043 Training error :0.6660438776016235\n",
      "Epoch : 4202 Loss: 3.4493625685572624 Training error :0.6660059094429016\n",
      "Epoch : 4203 Loss: 3.4492140524089336 Training error :0.6659678816795349\n",
      "Epoch : 4204 Loss: 3.449014462530613 Training error :0.6659317016601562\n",
      "Epoch : 4205 Loss: 3.448831368237734 Training error :0.6658937931060791\n",
      "Epoch : 4206 Loss: 3.448665253818035 Training error :0.6658588647842407\n",
      "Epoch : 4207 Loss: 3.4484791345894337 Training error :0.665822446346283\n",
      "Epoch : 4208 Loss: 3.448304522782564 Training error :0.6657823920249939\n",
      "Epoch : 4209 Loss: 3.448124099522829 Training error :0.6657477617263794\n",
      "Epoch : 4210 Loss: 3.447941217571497 Training error :0.6657109260559082\n",
      "Epoch : 4211 Loss: 3.447777282446623 Training error :0.6656755208969116\n",
      "Epoch : 4212 Loss: 3.447573171928525 Training error :0.6656380295753479\n",
      "Epoch : 4213 Loss: 3.4474094100296497 Training error :0.6656001806259155\n",
      "Epoch : 4214 Loss: 3.4472628589719534 Training error :0.6655641794204712\n",
      "Epoch : 4215 Loss: 3.4470686446875334 Training error :0.6655263304710388\n",
      "Epoch : 4216 Loss: 3.4468920417129993 Training error :0.6654910445213318\n",
      "Epoch : 4217 Loss: 3.446720378473401 Training error :0.665455162525177\n",
      "Epoch : 4218 Loss: 3.446545096114278 Training error :0.6654210686683655\n",
      "Epoch : 4219 Loss: 3.4463643953204155 Training error :0.6653850078582764\n",
      "Epoch : 4220 Loss: 3.4461969565600157 Training error :0.6653479933738708\n",
      "Epoch : 4221 Loss: 3.446031656116247 Training error :0.6653118133544922\n",
      "Epoch : 4222 Loss: 3.4458520226180553 Training error :0.6652768850326538\n",
      "Epoch : 4223 Loss: 3.445684300735593 Training error :0.6652411818504333\n",
      "Epoch : 4224 Loss: 3.4454966969788074 Training error :0.6652052998542786\n",
      "Epoch : 4225 Loss: 3.4453342482447624 Training error :0.6651766300201416\n",
      "Epoch : 4226 Loss: 3.4456630684435368 Training error :0.6650701761245728\n",
      "Epoch : 4227 Loss: 3.445375395938754 Training error :0.6650955080986023\n",
      "Epoch : 4228 Loss: 3.4447644911706448 Training error :0.6650845408439636\n",
      "Epoch : 4229 Loss: 3.4445641711354256 Training error :0.665028989315033\n",
      "Epoch : 4230 Loss: 3.444483345374465 Training error :0.6649860143661499\n",
      "Epoch : 4231 Loss: 3.444338483735919 Training error :0.6649588942527771\n",
      "Epoch : 4232 Loss: 3.4441214352846146 Training error :0.6649301052093506\n",
      "Epoch : 4233 Loss: 3.443955186754465 Training error :0.6648930311203003\n",
      "Epoch : 4234 Loss: 3.4437963105738163 Training error :0.664855420589447\n",
      "Epoch : 4235 Loss: 3.4436253532767296 Training error :0.6648202538490295\n",
      "Epoch : 4236 Loss: 3.443447068333626 Training error :0.6647875905036926\n",
      "Epoch : 4237 Loss: 3.4432960394769907 Training error :0.6647533178329468\n",
      "Epoch : 4238 Loss: 3.4431306552141905 Training error :0.6647191643714905\n",
      "Epoch : 4239 Loss: 3.4429683312773705 Training error :0.664686381816864\n",
      "Epoch : 4240 Loss: 3.442819472402334 Training error :0.6646520495414734\n",
      "Epoch : 4241 Loss: 3.442641755566001 Training error :0.6646192073822021\n",
      "Epoch : 4242 Loss: 3.4424953311681747 Training error :0.6645845770835876\n",
      "Epoch : 4243 Loss: 3.442312177270651 Training error :0.6645496487617493\n",
      "Epoch : 4244 Loss: 3.4421416260302067 Training error :0.6645147204399109\n",
      "Epoch : 4245 Loss: 3.441996833309531 Training error :0.6644849181175232\n",
      "Epoch : 4246 Loss: 3.441831763833761 Training error :0.6644498705863953\n",
      "Epoch : 4247 Loss: 3.4416760988533497 Training error :0.6644155979156494\n",
      "Epoch : 4248 Loss: 3.441502207890153 Training error :0.6643834114074707\n",
      "Epoch : 4249 Loss: 3.441340757533908 Training error :0.6643478870391846\n",
      "Epoch : 4250 Loss: 3.441183654591441 Training error :0.6643158197402954\n",
      "Epoch : 4251 Loss: 3.4409923516213894 Training error :0.6642813086509705\n",
      "Epoch : 4252 Loss: 3.4408466331660748 Training error :0.6642467975616455\n",
      "Epoch : 4253 Loss: 3.4406863655894995 Training error :0.6642186045646667\n",
      "Epoch : 4254 Loss: 3.4405339285731316 Training error :0.6641845107078552\n",
      "Epoch : 4255 Loss: 3.4403787031769753 Training error :0.6641488671302795\n",
      "Epoch : 4256 Loss: 3.4402050375938416 Training error :0.6641187071800232\n",
      "Epoch : 4257 Loss: 3.4400350376963615 Training error :0.6640905141830444\n",
      "Epoch : 4258 Loss: 3.440409068018198 Training error :0.6639916896820068\n",
      "Epoch : 4259 Loss: 3.4401038493961096 Training error :0.6640162467956543\n",
      "Epoch : 4260 Loss: 3.439509879797697 Training error :0.6640064120292664\n",
      "Epoch : 4261 Loss: 3.439326224848628 Training error :0.6639559268951416\n",
      "Epoch : 4262 Loss: 3.4392631724476814 Training error :0.6639153957366943\n",
      "Epoch : 4263 Loss: 3.4391021635383368 Training error :0.6638937592506409\n",
      "Epoch : 4264 Loss: 3.438907193019986 Training error :0.6638625860214233\n",
      "Epoch : 4265 Loss: 3.4387580640614033 Training error :0.6638280749320984\n",
      "Epoch : 4266 Loss: 3.438621526584029 Training error :0.663794755935669\n",
      "Epoch : 4267 Loss: 3.438458252698183 Training error :0.6637649536132812\n",
      "Epoch : 4268 Loss: 3.438321281224489 Training error :0.6637352705001831\n",
      "Epoch : 4269 Loss: 3.4381578359752893 Training error :0.6637011766433716\n",
      "Epoch : 4270 Loss: 3.438026251271367 Training error :0.6636689901351929\n",
      "Epoch : 4271 Loss: 3.437878381460905 Training error :0.66363924741745\n",
      "Epoch : 4272 Loss: 3.43772161193192 Training error :0.6636064052581787\n",
      "Epoch : 4273 Loss: 3.437547130510211 Training error :0.6635741591453552\n",
      "Epoch : 4274 Loss: 3.437381774187088 Training error :0.6635434031486511\n",
      "Epoch : 4275 Loss: 3.4372404869645834 Training error :0.6635133624076843\n",
      "Epoch : 4276 Loss: 3.437089905142784 Training error :0.663483202457428\n",
      "Epoch : 4277 Loss: 3.436945144087076 Training error :0.6634511351585388\n",
      "Epoch : 4278 Loss: 3.4368025697767735 Training error :0.6634196639060974\n",
      "Epoch : 4279 Loss: 3.4366339314728975 Training error :0.663390040397644\n",
      "Epoch : 4280 Loss: 3.436484843492508 Training error :0.6633580923080444\n",
      "Epoch : 4281 Loss: 3.4363404773175716 Training error :0.6633260250091553\n",
      "Epoch : 4282 Loss: 3.4361974764615297 Training error :0.6632987260818481\n",
      "Epoch : 4283 Loss: 3.43604170717299 Training error :0.6632660031318665\n",
      "Epoch : 4284 Loss: 3.435903949663043 Training error :0.663235068321228\n",
      "Epoch : 4285 Loss: 3.435747219249606 Training error :0.6632020473480225\n",
      "Epoch : 4286 Loss: 3.4355950839817524 Training error :0.6631707549095154\n",
      "Epoch : 4287 Loss: 3.4354517105966806 Training error :0.6631431579589844\n",
      "Epoch : 4288 Loss: 3.4353058394044638 Training error :0.6631107926368713\n",
      "Epoch : 4289 Loss: 3.43515750952065 Training error :0.6630811095237732\n",
      "Epoch : 4290 Loss: 3.4350197352468967 Training error :0.663054883480072\n",
      "Epoch : 4291 Loss: 3.435358766466379 Training error :0.6629591584205627\n",
      "Epoch : 4292 Loss: 3.435137014836073 Training error :0.6629894375801086\n",
      "Epoch : 4293 Loss: 3.434520773589611 Training error :0.6629824042320251\n",
      "Epoch : 4294 Loss: 3.434329394251108 Training error :0.6629290580749512\n",
      "Epoch : 4295 Loss: 3.434287877753377 Training error :0.6628925800323486\n",
      "Epoch : 4296 Loss: 3.434134665876627 Training error :0.6628724932670593\n",
      "Epoch : 4297 Loss: 3.4339373633265495 Training error :0.6628470420837402\n",
      "Epoch : 4298 Loss: 3.433827180415392 Training error :0.6628148555755615\n",
      "Epoch : 4299 Loss: 3.4336950443685055 Training error :0.6627829074859619\n",
      "Epoch : 4300 Loss: 3.4335564337670803 Training error :0.6627543568611145\n",
      "Epoch : 4301 Loss: 3.433407787233591 Training error :0.662726640701294\n",
      "Epoch : 4302 Loss: 3.4332833327353 Training error :0.6626970767974854\n",
      "Epoch : 4303 Loss: 3.433140380308032 Training error :0.6626664996147156\n",
      "Epoch : 4304 Loss: 3.432977668941021 Training error :0.662636935710907\n",
      "Epoch : 4305 Loss: 3.4328636843711138 Training error :0.6626110672950745\n",
      "Epoch : 4306 Loss: 3.432721484452486 Training error :0.6625815033912659\n",
      "Epoch : 4307 Loss: 3.432562429457903 Training error :0.6625513434410095\n",
      "Epoch : 4308 Loss: 3.432434279471636 Training error :0.6625238656997681\n",
      "Epoch : 4309 Loss: 3.432286899536848 Training error :0.662493109703064\n",
      "Epoch : 4310 Loss: 3.4321612883359194 Training error :0.6624657511711121\n",
      "Epoch : 4311 Loss: 3.43200870603323 Training error :0.6624366641044617\n",
      "Epoch : 4312 Loss: 3.431862497702241 Training error :0.6624067425727844\n",
      "Epoch : 4313 Loss: 3.431731529533863 Training error :0.6623820662498474\n",
      "Epoch : 4314 Loss: 3.4315847158432007 Training error :0.662351131439209\n",
      "Epoch : 4315 Loss: 3.4314667843282223 Training error :0.6623194813728333\n",
      "Epoch : 4316 Loss: 3.4313151836395264 Training error :0.6622931957244873\n",
      "Epoch : 4317 Loss: 3.431164685636759 Training error :0.662264883518219\n",
      "Epoch : 4318 Loss: 3.431034632027149 Training error :0.662238597869873\n",
      "Epoch : 4319 Loss: 3.430901166051626 Training error :0.6622080206871033\n",
      "Epoch : 4320 Loss: 3.430771393701434 Training error :0.6621793508529663\n",
      "Epoch : 4321 Loss: 3.430634304881096 Training error :0.6621522307395935\n",
      "Epoch : 4322 Loss: 3.4305047150701284 Training error :0.6621236205101013\n",
      "Epoch : 4323 Loss: 3.430362416431308 Training error :0.6620936393737793\n",
      "Epoch : 4324 Loss: 3.4302074294537306 Training error :0.6620670557022095\n",
      "Epoch : 4325 Loss: 3.4300794899463654 Training error :0.6620430946350098\n",
      "Epoch : 4326 Loss: 3.430401209741831 Training error :0.6619515419006348\n",
      "Epoch : 4327 Loss: 3.430177679285407 Training error :0.6619791388511658\n",
      "Epoch : 4328 Loss: 3.4295928925275803 Training error :0.661974310874939\n",
      "Epoch : 4329 Loss: 3.429432362318039 Training error :0.6619290113449097\n",
      "Epoch : 4330 Loss: 3.429412854835391 Training error :0.6618940234184265\n",
      "Epoch : 4331 Loss: 3.4292919356375933 Training error :0.6618751287460327\n",
      "Epoch : 4332 Loss: 3.429097319021821 Training error :0.6618493795394897\n",
      "Epoch : 4333 Loss: 3.4289488960057497 Training error :0.6618175506591797\n",
      "Epoch : 4334 Loss: 3.4288412984460592 Training error :0.6617946028709412\n",
      "Epoch : 4335 Loss: 3.428738344460726 Training error :0.6617669463157654\n",
      "Epoch : 4336 Loss: 3.428581802174449 Training error :0.6617379188537598\n",
      "Epoch : 4337 Loss: 3.428468883037567 Training error :0.6617137789726257\n",
      "Epoch : 4338 Loss: 3.4283350128680468 Training error :0.6616852283477783\n",
      "Epoch : 4339 Loss: 3.4281926825642586 Training error :0.6616619229316711\n",
      "Epoch : 4340 Loss: 3.4280810579657555 Training error :0.6616325974464417\n",
      "Epoch : 4341 Loss: 3.427934568375349 Training error :0.6616058349609375\n",
      "Epoch : 4342 Loss: 3.427804756909609 Training error :0.6615810394287109\n",
      "Epoch : 4343 Loss: 3.427683465182781 Training error :0.6615515947341919\n",
      "Epoch : 4344 Loss: 3.427549947053194 Training error :0.6615238785743713\n",
      "Epoch : 4345 Loss: 3.4274072907865047 Training error :0.6614983081817627\n",
      "Epoch : 4346 Loss: 3.4272757787257433 Training error :0.6614718437194824\n",
      "Epoch : 4347 Loss: 3.427164549008012 Training error :0.66144859790802\n",
      "Epoch : 4348 Loss: 3.4270115960389376 Training error :0.661419689655304\n",
      "Epoch : 4349 Loss: 3.4268940407782793 Training error :0.6613910794258118\n",
      "Epoch : 4350 Loss: 3.4267589282244444 Training error :0.6613684296607971\n",
      "Epoch : 4351 Loss: 3.426642306149006 Training error :0.6613406538963318\n",
      "Epoch : 4352 Loss: 3.4265181124210358 Training error :0.6613131165504456\n",
      "Epoch : 4353 Loss: 3.4263795372098684 Training error :0.6612889766693115\n",
      "Epoch : 4354 Loss: 3.4262486584484577 Training error :0.6612642407417297\n",
      "Epoch : 4355 Loss: 3.4261232055723667 Training error :0.6612361073493958\n",
      "Epoch : 4356 Loss: 3.4259939193725586 Training error :0.6612107753753662\n",
      "Epoch : 4357 Loss: 3.425860095769167 Training error :0.6611813306808472\n",
      "Epoch : 4358 Loss: 3.4257292076945305 Training error :0.661156415939331\n",
      "Epoch : 4359 Loss: 3.425607716664672 Training error :0.6611308455467224\n",
      "Epoch : 4360 Loss: 3.4254837054759264 Training error :0.6611052751541138\n",
      "Epoch : 4361 Loss: 3.425348026677966 Training error :0.6610809564590454\n",
      "Epoch : 4362 Loss: 3.425222584977746 Training error :0.6610574126243591\n",
      "Epoch : 4363 Loss: 3.425629297271371 Training error :0.6609698534011841\n",
      "Epoch : 4364 Loss: 3.4253801200538874 Training error :0.66100013256073\n",
      "Epoch : 4365 Loss: 3.424866158515215 Training error :0.660999059677124\n",
      "Epoch : 4366 Loss: 3.4246897026896477 Training error :0.6609510779380798\n",
      "Epoch : 4367 Loss: 3.4246593825519085 Training error :0.6609197854995728\n",
      "Epoch : 4368 Loss: 3.424524415284395 Training error :0.6609033942222595\n",
      "Epoch : 4369 Loss: 3.4243470542132854 Training error :0.6608813405036926\n",
      "Epoch : 4370 Loss: 3.42421961016953 Training error :0.6608529686927795\n",
      "Epoch : 4371 Loss: 3.4241504184901714 Training error :0.6608276963233948\n",
      "Epoch : 4372 Loss: 3.4240268990397453 Training error :0.6608045101165771\n",
      "Epoch : 4373 Loss: 3.4239117987453938 Training error :0.6607780456542969\n",
      "Epoch : 4374 Loss: 3.423781977966428 Training error :0.6607529520988464\n",
      "Epoch : 4375 Loss: 3.423666186630726 Training error :0.6607270836830139\n",
      "Epoch : 4376 Loss: 3.423553094267845 Training error :0.6607072949409485\n",
      "Epoch : 4377 Loss: 3.4234297685325146 Training error :0.6606788635253906\n",
      "Epoch : 4378 Loss: 3.423301622271538 Training error :0.6606542468070984\n",
      "Epoch : 4379 Loss: 3.4231974203139544 Training error :0.6606297492980957\n",
      "Epoch : 4380 Loss: 3.4230692088603973 Training error :0.660605251789093\n",
      "Epoch : 4381 Loss: 3.422982394695282 Training error :0.6605817675590515\n",
      "Epoch : 4382 Loss: 3.4228292629122734 Training error :0.6605572700500488\n",
      "Epoch : 4383 Loss: 3.422715874388814 Training error :0.6605308651924133\n",
      "Epoch : 4384 Loss: 3.4225944075733423 Training error :0.6605092883110046\n",
      "Epoch : 4385 Loss: 3.4224650133401155 Training error :0.6604834794998169\n",
      "Epoch : 4386 Loss: 3.4223741106688976 Training error :0.660459041595459\n",
      "Epoch : 4387 Loss: 3.4222565181553364 Training error :0.660434365272522\n",
      "Epoch : 4388 Loss: 3.4221252612769604 Training error :0.6604106426239014\n",
      "Epoch : 4389 Loss: 3.422007445245981 Training error :0.6603879332542419\n",
      "Epoch : 4390 Loss: 3.421893933787942 Training error :0.6603636145591736\n",
      "Epoch : 4391 Loss: 3.421773897483945 Training error :0.6603407263755798\n",
      "Epoch : 4392 Loss: 3.421671289950609 Training error :0.6603153944015503\n",
      "Epoch : 4393 Loss: 3.4215447828173637 Training error :0.6602920293807983\n",
      "Epoch : 4394 Loss: 3.4214385244995356 Training error :0.6602660417556763\n",
      "Epoch : 4395 Loss: 3.421288611367345 Training error :0.6602457165718079\n",
      "Epoch : 4396 Loss: 3.4211862422525883 Training error :0.6602200269699097\n",
      "Epoch : 4397 Loss: 3.4210652709007263 Training error :0.6601957678794861\n",
      "Epoch : 4398 Loss: 3.4209328796714544 Training error :0.6601725816726685\n",
      "Epoch : 4399 Loss: 3.4208363257348537 Training error :0.6601489186286926\n",
      "Epoch : 4400 Loss: 3.420735750347376 Training error :0.6601272821426392\n",
      "Epoch : 4401 Loss: 3.4210962653160095 Training error :0.6600428223609924\n",
      "Epoch : 4402 Loss: 3.420867659151554 Training error :0.6600717306137085\n",
      "Epoch : 4403 Loss: 3.42033221013844 Training error :0.6600742340087891\n",
      "Epoch : 4404 Loss: 3.4201699942350388 Training error :0.6600327491760254\n",
      "Epoch : 4405 Loss: 3.420163018628955 Training error :0.6600019335746765\n",
      "Epoch : 4406 Loss: 3.4200431890785694 Training error :0.65998375415802\n",
      "Epoch : 4407 Loss: 3.419887436553836 Training error :0.6599639654159546\n",
      "Epoch : 4408 Loss: 3.4197840951383114 Training error :0.6599377989768982\n",
      "Epoch : 4409 Loss: 3.419665463268757 Training error :0.6599152088165283\n",
      "Epoch : 4410 Loss: 3.4195856861770153 Training error :0.6598950028419495\n",
      "Epoch : 4411 Loss: 3.4194504357874393 Training error :0.6598724126815796\n",
      "Epoch : 4412 Loss: 3.4193417821079493 Training error :0.6598467230796814\n",
      "Epoch : 4413 Loss: 3.4192242361605167 Training error :0.6598268151283264\n",
      "Epoch : 4414 Loss: 3.4191246647387743 Training error :0.6598023176193237\n",
      "Epoch : 4415 Loss: 3.419008718803525 Training error :0.659777820110321\n",
      "Epoch : 4416 Loss: 3.4188976902514696 Training error :0.659756600856781\n",
      "Epoch : 4417 Loss: 3.4187898095697165 Training error :0.6597344875335693\n",
      "Epoch : 4418 Loss: 3.4186838883906603 Training error :0.6597126722335815\n",
      "Epoch : 4419 Loss: 3.4185716584324837 Training error :0.6596895456314087\n",
      "Epoch : 4420 Loss: 3.4184565357863903 Training error :0.659667432308197\n",
      "Epoch : 4421 Loss: 3.4183679297566414 Training error :0.6596462726593018\n",
      "Epoch : 4422 Loss: 3.4182431921362877 Training error :0.6596229672431946\n",
      "Epoch : 4423 Loss: 3.4181421529501677 Training error :0.6595976948738098\n",
      "Epoch : 4424 Loss: 3.4180300887674093 Training error :0.6595799922943115\n",
      "Epoch : 4425 Loss: 3.417909398674965 Training error :0.6595561504364014\n",
      "Epoch : 4426 Loss: 3.4178140871226788 Training error :0.6595330834388733\n",
      "Epoch : 4427 Loss: 3.417690299451351 Training error :0.6595108509063721\n",
      "Epoch : 4428 Loss: 3.4175744969397783 Training error :0.6594880819320679\n",
      "Epoch : 4429 Loss: 3.417489791288972 Training error :0.6594682931900024\n",
      "Epoch : 4430 Loss: 3.417385356500745 Training error :0.6594448089599609\n",
      "Epoch : 4431 Loss: 3.4172774087637663 Training error :0.6594234704971313\n",
      "Epoch : 4432 Loss: 3.4171596355736256 Training error :0.6594008803367615\n",
      "Epoch : 4433 Loss: 3.4170649275183678 Training error :0.6593772172927856\n",
      "Epoch : 4434 Loss: 3.416959322988987 Training error :0.6593554615974426\n",
      "Epoch : 4435 Loss: 3.4168346635997295 Training error :0.6593343615531921\n",
      "Epoch : 4436 Loss: 3.4167361557483673 Training error :0.6593132019042969\n",
      "Epoch : 4437 Loss: 3.4166463017463684 Training error :0.65929114818573\n",
      "Epoch : 4438 Loss: 3.416518537327647 Training error :0.6592703461647034\n",
      "Epoch : 4439 Loss: 3.4164197165519 Training error :0.6592480540275574\n",
      "Epoch : 4440 Loss: 3.4163115583360195 Training error :0.6592270135879517\n",
      "Epoch : 4441 Loss: 3.416194874793291 Training error :0.6592060923576355\n",
      "Epoch : 4442 Loss: 3.416615840047598 Training error :0.6591213941574097\n",
      "Epoch : 4443 Loss: 3.416372800245881 Training error :0.6591566205024719\n",
      "Epoch : 4444 Loss: 3.415873119607568 Training error :0.6591557264328003\n",
      "Epoch : 4445 Loss: 3.41569884121418 Training error :0.6591199636459351\n",
      "Epoch : 4446 Loss: 3.41570414789021 Training error :0.659089982509613\n",
      "Epoch : 4447 Loss: 3.41560629196465 Training error :0.6590743064880371\n",
      "Epoch : 4448 Loss: 3.4154646173119545 Training error :0.6590549349784851\n",
      "Epoch : 4449 Loss: 3.4153478611260653 Training error :0.6590328216552734\n",
      "Epoch : 4450 Loss: 3.4152694176882505 Training error :0.6590118408203125\n",
      "Epoch : 4451 Loss: 3.415171029046178 Training error :0.6589913368225098\n",
      "Epoch : 4452 Loss: 3.4150657672435045 Training error :0.6589691042900085\n",
      "Epoch : 4453 Loss: 3.414965257048607 Training error :0.6589511632919312\n",
      "Epoch : 4454 Loss: 3.414869209751487 Training error :0.6589280962944031\n",
      "Epoch : 4455 Loss: 3.4147501196712255 Training error :0.6589064002037048\n",
      "Epoch : 4456 Loss: 3.414664076641202 Training error :0.6588870286941528\n",
      "Epoch : 4457 Loss: 3.414557918906212 Training error :0.6588659882545471\n",
      "Epoch : 4458 Loss: 3.414454374462366 Training error :0.6588472723960876\n",
      "Epoch : 4459 Loss: 3.4143564328551292 Training error :0.6588263511657715\n",
      "Epoch : 4460 Loss: 3.4142692256718874 Training error :0.6588041186332703\n",
      "Epoch : 4461 Loss: 3.4141748175024986 Training error :0.6587841510772705\n",
      "Epoch : 4462 Loss: 3.4140561148524284 Training error :0.6587631702423096\n",
      "Epoch : 4463 Loss: 3.413956295698881 Training error :0.6587383151054382\n",
      "Epoch : 4464 Loss: 3.413866225630045 Training error :0.6587194204330444\n",
      "Epoch : 4465 Loss: 3.4137526024132967 Training error :0.6587006449699402\n",
      "Epoch : 4466 Loss: 3.4136654902249575 Training error :0.6586802005767822\n",
      "Epoch : 4467 Loss: 3.4135614447295666 Training error :0.6586601734161377\n",
      "Epoch : 4468 Loss: 3.41345146112144 Training error :0.6586388945579529\n",
      "Epoch : 4469 Loss: 3.413361791521311 Training error :0.6586204767227173\n",
      "Epoch : 4470 Loss: 3.413263265043497 Training error :0.6585977077484131\n",
      "Epoch : 4471 Loss: 3.4131632819771767 Training error :0.658576250076294\n",
      "Epoch : 4472 Loss: 3.4130715746432543 Training error :0.6585582494735718\n",
      "Epoch : 4473 Loss: 3.412953408434987 Training error :0.6585395336151123\n",
      "Epoch : 4474 Loss: 3.412860233336687 Training error :0.6585189700126648\n",
      "Epoch : 4475 Loss: 3.4127591978758574 Training error :0.6584963202476501\n",
      "Epoch : 4476 Loss: 3.412664107978344 Training error :0.6584751605987549\n",
      "Epoch : 4477 Loss: 3.4125778917223215 Training error :0.658456027507782\n",
      "Epoch : 4478 Loss: 3.412469107657671 Training error :0.6584369540214539\n",
      "Epoch : 4479 Loss: 3.4123767986893654 Training error :0.6584161520004272\n",
      "Epoch : 4480 Loss: 3.4122798200696707 Training error :0.658397376537323\n",
      "Epoch : 4481 Loss: 3.4121792167425156 Training error :0.6583752632141113\n",
      "Epoch : 4482 Loss: 3.412071816623211 Training error :0.6583566069602966\n",
      "Epoch : 4483 Loss: 3.411991449072957 Training error :0.6583359241485596\n",
      "Epoch : 4484 Loss: 3.4118925612419844 Training error :0.6583200693130493\n",
      "Epoch : 4485 Loss: 3.412303738296032 Training error :0.6582375168800354\n",
      "Epoch : 4486 Loss: 3.4120701141655445 Training error :0.6582701802253723\n",
      "Epoch : 4487 Loss: 3.4115447774529457 Training error :0.6582761406898499\n",
      "Epoch : 4488 Loss: 3.411415286362171 Training error :0.6582350134849548\n",
      "Epoch : 4489 Loss: 3.4114160649478436 Training error :0.6582088470458984\n",
      "Epoch : 4490 Loss: 3.411336986348033 Training error :0.6581989526748657\n",
      "Epoch : 4491 Loss: 3.411178071051836 Training error :0.6581841111183167\n",
      "Epoch : 4492 Loss: 3.4111020993441343 Training error :0.6581586599349976\n",
      "Epoch : 4493 Loss: 3.4110312052071095 Training error :0.6581398844718933\n",
      "Epoch : 4494 Loss: 3.4109159633517265 Training error :0.6581200361251831\n",
      "Epoch : 4495 Loss: 3.4108400903642178 Training error :0.6581031680107117\n",
      "Epoch : 4496 Loss: 3.4107312578707933 Training error :0.6580811142921448\n",
      "Epoch : 4497 Loss: 3.4106318317353725 Training error :0.6580609083175659\n",
      "Epoch : 4498 Loss: 3.410550305619836 Training error :0.6580456495285034\n",
      "Epoch : 4499 Loss: 3.4104589987546206 Training error :0.6580237746238708\n",
      "Epoch : 4500 Loss: 3.410362880676985 Training error :0.658003568649292\n",
      "Epoch : 4501 Loss: 3.410262292250991 Training error :0.657985508441925\n",
      "Epoch : 4502 Loss: 3.410173263400793 Training error :0.6579664349555969\n",
      "Epoch : 4503 Loss: 3.410083819180727 Training error :0.657948911190033\n",
      "Epoch : 4504 Loss: 3.4099848456680775 Training error :0.6579287052154541\n",
      "Epoch : 4505 Loss: 3.4099004436284304 Training error :0.6579079627990723\n",
      "Epoch : 4506 Loss: 3.4098148941993713 Training error :0.6578931212425232\n",
      "Epoch : 4507 Loss: 3.4097171928733587 Training error :0.6578742861747742\n",
      "Epoch : 4508 Loss: 3.409624855965376 Training error :0.6578530669212341\n",
      "Epoch : 4509 Loss: 3.4095445703715086 Training error :0.6578347086906433\n",
      "Epoch : 4510 Loss: 3.4094270542263985 Training error :0.6578150987625122\n",
      "Epoch : 4511 Loss: 3.4093433283269405 Training error :0.6577963829040527\n",
      "Epoch : 4512 Loss: 3.4092599637806416 Training error :0.6577783823013306\n",
      "Epoch : 4513 Loss: 3.4091572668403387 Training error :0.6577590703964233\n",
      "Epoch : 4514 Loss: 3.409081406891346 Training error :0.6577434539794922\n",
      "Epoch : 4515 Loss: 3.408996446058154 Training error :0.6577219367027283\n",
      "Epoch : 4516 Loss: 3.4088925942778587 Training error :0.6577009558677673\n",
      "Epoch : 4517 Loss: 3.4087960608303547 Training error :0.6576854586601257\n",
      "Epoch : 4518 Loss: 3.408714536577463 Training error :0.6576663255691528\n",
      "Epoch : 4519 Loss: 3.408626027405262 Training error :0.657646656036377\n",
      "Epoch : 4520 Loss: 3.4085348788648844 Training error :0.6576312184333801\n",
      "Epoch : 4521 Loss: 3.4084301367402077 Training error :0.6576127409934998\n",
      "Epoch : 4522 Loss: 3.4083671886473894 Training error :0.6575922966003418\n",
      "Epoch : 4523 Loss: 3.4082457032054663 Training error :0.657573938369751\n",
      "Epoch : 4524 Loss: 3.40815956145525 Training error :0.6575526595115662\n",
      "Epoch : 4525 Loss: 3.408073566854 Training error :0.6575378179550171\n",
      "Epoch : 4526 Loss: 3.4079816173762083 Training error :0.657519519329071\n",
      "Epoch : 4527 Loss: 3.407907247543335 Training error :0.6574990153312683\n",
      "Epoch : 4528 Loss: 3.407820101827383 Training error :0.6574830412864685\n",
      "Epoch : 4529 Loss: 3.4077061247080564 Training error :0.657463788986206\n",
      "Epoch : 4530 Loss: 3.408101499080658 Training error :0.6573892831802368\n",
      "Epoch : 4531 Loss: 3.4079352635890245 Training error :0.6574212908744812\n",
      "Epoch : 4532 Loss: 3.4074126668274403 Training error :0.6574263572692871\n",
      "Epoch : 4533 Loss: 3.4072769209742546 Training error :0.6573871374130249\n",
      "Epoch : 4534 Loss: 3.407295446842909 Training error :0.6573641896247864\n",
      "Epoch : 4535 Loss: 3.4072185792028904 Training error :0.6573583483695984\n",
      "Epoch : 4536 Loss: 3.40707466006279 Training error :0.657339334487915\n",
      "Epoch : 4537 Loss: 3.406972587108612 Training error :0.6573183536529541\n",
      "Epoch : 4538 Loss: 3.406920447945595 Training error :0.6573026180267334\n",
      "Epoch : 4539 Loss: 3.4068293683230877 Training error :0.6572834849357605\n",
      "Epoch : 4540 Loss: 3.406750861555338 Training error :0.6572677493095398\n",
      "Epoch : 4541 Loss: 3.406667632982135 Training error :0.6572514176368713\n",
      "Epoch : 4542 Loss: 3.4065772518515587 Training error :0.6572331190109253\n",
      "Epoch : 4543 Loss: 3.406507156789303 Training error :0.6572158932685852\n",
      "Epoch : 4544 Loss: 3.4064073637127876 Training error :0.6571954488754272\n",
      "Epoch : 4545 Loss: 3.4063250850886106 Training error :0.6571769118309021\n",
      "Epoch : 4546 Loss: 3.406227830797434 Training error :0.6571632623672485\n",
      "Epoch : 4547 Loss: 3.406155513599515 Training error :0.6571440696716309\n",
      "Epoch : 4548 Loss: 3.4060715157538652 Training error :0.65712571144104\n",
      "Epoch : 4549 Loss: 3.4059817790985107 Training error :0.6571097373962402\n",
      "Epoch : 4550 Loss: 3.4058905858546495 Training error :0.6570906639099121\n",
      "Epoch : 4551 Loss: 3.40580328181386 Training error :0.6570755243301392\n",
      "Epoch : 4552 Loss: 3.405716996639967 Training error :0.6570559144020081\n",
      "Epoch : 4553 Loss: 3.405634507536888 Training error :0.6570380330085754\n",
      "Epoch : 4554 Loss: 3.4055682756006718 Training error :0.6570238471031189\n",
      "Epoch : 4555 Loss: 3.405470348894596 Training error :0.6570065021514893\n",
      "Epoch : 4556 Loss: 3.4053889345377684 Training error :0.6569849252700806\n",
      "Epoch : 4557 Loss: 3.405284868553281 Training error :0.6569702625274658\n",
      "Epoch : 4558 Loss: 3.405204340815544 Training error :0.6569515466690063\n",
      "Epoch : 4559 Loss: 3.4051305316388607 Training error :0.6569331884384155\n",
      "Epoch : 4560 Loss: 3.4050464928150177 Training error :0.6569175124168396\n",
      "Epoch : 4561 Loss: 3.4049486611038446 Training error :0.6569002270698547\n",
      "Epoch : 4562 Loss: 3.404872413724661 Training error :0.6568865180015564\n",
      "Epoch : 4563 Loss: 3.404788749292493 Training error :0.6568672060966492\n",
      "Epoch : 4564 Loss: 3.4046950303018093 Training error :0.6568489670753479\n",
      "Epoch : 4565 Loss: 3.404625726863742 Training error :0.6568355560302734\n",
      "Epoch : 4566 Loss: 3.4045498818159103 Training error :0.656816303730011\n",
      "Epoch : 4567 Loss: 3.4044682662934065 Training error :0.6567968130111694\n",
      "Epoch : 4568 Loss: 3.404396664351225 Training error :0.6567818522453308\n",
      "Epoch : 4569 Loss: 3.404296835884452 Training error :0.6567630171775818\n",
      "Epoch : 4570 Loss: 3.4042346011847258 Training error :0.6567475199699402\n",
      "Epoch : 4571 Loss: 3.404124364256859 Training error :0.6567285656929016\n",
      "Epoch : 4572 Loss: 3.404054082930088 Training error :0.6567133069038391\n",
      "Epoch : 4573 Loss: 3.40397484973073 Training error :0.6566972136497498\n",
      "Epoch : 4574 Loss: 3.4038856495171785 Training error :0.6566791534423828\n",
      "Epoch : 4575 Loss: 3.4038002230226994 Training error :0.6566612720489502\n",
      "Epoch : 4576 Loss: 3.4037340711802244 Training error :0.6566460132598877\n",
      "Epoch : 4577 Loss: 3.403611619025469 Training error :0.6566292643547058\n",
      "Epoch : 4578 Loss: 3.4035603553056717 Training error :0.6566144824028015\n",
      "Epoch : 4579 Loss: 3.403468556702137 Training error :0.6566003561019897\n",
      "Epoch : 4580 Loss: 3.40387474372983 Training error :0.6565191745758057\n",
      "Epoch : 4581 Loss: 3.4036908950656652 Training error :0.6565572619438171\n",
      "Epoch : 4582 Loss: 3.403197655454278 Training error :0.6565636396408081\n",
      "Epoch : 4583 Loss: 3.40307928994298 Training error :0.656526505947113\n",
      "Epoch : 4584 Loss: 3.4030784387141466 Training error :0.6565041542053223\n",
      "Epoch : 4585 Loss: 3.403020404279232 Training error :0.6564954519271851\n",
      "Epoch : 4586 Loss: 3.4028839878737926 Training error :0.6564837098121643\n",
      "Epoch : 4587 Loss: 3.4028047043830156 Training error :0.656463086605072\n",
      "Epoch : 4588 Loss: 3.4027787894010544 Training error :0.6564455628395081\n",
      "Epoch : 4589 Loss: 3.4026765413582325 Training error :0.6564313769340515\n",
      "Epoch : 4590 Loss: 3.4025808945298195 Training error :0.6564140319824219\n",
      "Epoch : 4591 Loss: 3.4025129042565823 Training error :0.6563984751701355\n",
      "Epoch : 4592 Loss: 3.402444563806057 Training error :0.65638267993927\n",
      "Epoch : 4593 Loss: 3.402347169816494 Training error :0.6563666462898254\n",
      "Epoch : 4594 Loss: 3.4022867381572723 Training error :0.6563513278961182\n",
      "Epoch : 4595 Loss: 3.4022030364722013 Training error :0.6563341617584229\n",
      "Epoch : 4596 Loss: 3.402110191062093 Training error :0.6563172936439514\n",
      "Epoch : 4597 Loss: 3.402032621204853 Training error :0.6563017964363098\n",
      "Epoch : 4598 Loss: 3.4019562173634768 Training error :0.6562860608100891\n",
      "Epoch : 4599 Loss: 3.40189060382545 Training error :0.6562698483467102\n",
      "Epoch : 4600 Loss: 3.401810295879841 Training error :0.6562540531158447\n",
      "Epoch : 4601 Loss: 3.4017308820039034 Training error :0.6562391519546509\n",
      "Epoch : 4602 Loss: 3.4016392156481743 Training error :0.6562212109565735\n",
      "Epoch : 4603 Loss: 3.401571098715067 Training error :0.6562045812606812\n",
      "Epoch : 4604 Loss: 3.4014803878962994 Training error :0.6561886072158813\n",
      "Epoch : 4605 Loss: 3.4014140032231808 Training error :0.6561736464500427\n",
      "Epoch : 4606 Loss: 3.401330392807722 Training error :0.6561563014984131\n",
      "Epoch : 4607 Loss: 3.4012673180550337 Training error :0.6561440229415894\n",
      "Epoch : 4608 Loss: 3.401184257119894 Training error :0.6561246514320374\n",
      "Epoch : 4609 Loss: 3.4011003375053406 Training error :0.6561071276664734\n",
      "Epoch : 4610 Loss: 3.401022221893072 Training error :0.6560949087142944\n",
      "Epoch : 4611 Loss: 3.4009643755853176 Training error :0.6560778617858887\n",
      "Epoch : 4612 Loss: 3.400877594947815 Training error :0.656063437461853\n",
      "Epoch : 4613 Loss: 3.4007999934256077 Training error :0.6560465097427368\n",
      "Epoch : 4614 Loss: 3.400717446580529 Training error :0.6560332775115967\n",
      "Epoch : 4615 Loss: 3.4006433747708797 Training error :0.6560180187225342\n",
      "Epoch : 4616 Loss: 3.400569461286068 Training error :0.6559999585151672\n",
      "Epoch : 4617 Loss: 3.400489116087556 Training error :0.6559810638427734\n",
      "Epoch : 4618 Loss: 3.40040161088109 Training error :0.6559706926345825\n",
      "Epoch : 4619 Loss: 3.4003304578363895 Training error :0.6559536457061768\n",
      "Epoch : 4620 Loss: 3.400260217487812 Training error :0.6559380292892456\n",
      "Epoch : 4621 Loss: 3.4001817032694817 Training error :0.6559232473373413\n",
      "Epoch : 4622 Loss: 3.4001065641641617 Training error :0.6559056639671326\n",
      "Epoch : 4623 Loss: 3.4000235497951508 Training error :0.6558910608291626\n",
      "Epoch : 4624 Loss: 3.399947887286544 Training error :0.6558772921562195\n",
      "Epoch : 4625 Loss: 3.3998865727335215 Training error :0.6558611989021301\n",
      "Epoch : 4626 Loss: 3.3998257536441088 Training error :0.6558457016944885\n",
      "Epoch : 4627 Loss: 3.3997251242399216 Training error :0.6558294892311096\n",
      "Epoch : 4628 Loss: 3.3996587023139 Training error :0.6558127403259277\n",
      "Epoch : 4629 Loss: 3.3995642215013504 Training error :0.6557998061180115\n",
      "Epoch : 4630 Loss: 3.399506349116564 Training error :0.6557825207710266\n",
      "Epoch : 4631 Loss: 3.3994152322411537 Training error :0.6557701826095581\n",
      "Epoch : 4632 Loss: 3.3998359702527523 Training error :0.6556961536407471\n",
      "Epoch : 4633 Loss: 3.39963972941041 Training error :0.6557325124740601\n",
      "Epoch : 4634 Loss: 3.399169858545065 Training error :0.6557398438453674\n",
      "Epoch : 4635 Loss: 3.3990594558417797 Training error :0.655707597732544\n",
      "Epoch : 4636 Loss: 3.399073425680399 Training error :0.655687689781189\n",
      "Epoch : 4637 Loss: 3.398993380367756 Training error :0.6556762456893921\n",
      "Epoch : 4638 Loss: 3.398907346650958 Training error :0.6556623578071594\n",
      "Epoch : 4639 Loss: 3.3988354094326496 Training error :0.6556483507156372\n",
      "Epoch : 4640 Loss: 3.3987578433007 Training error :0.6556320786476135\n",
      "Epoch : 4641 Loss: 3.3986953776329756 Training error :0.6556184887886047\n",
      "Epoch : 4642 Loss: 3.398624360561371 Training error :0.655605137348175\n",
      "Epoch : 4643 Loss: 3.3985233195126057 Training error :0.6555906534194946\n",
      "Epoch : 4644 Loss: 3.3985042814165354 Training error :0.6555765271186829\n",
      "Epoch : 4645 Loss: 3.3984192050993443 Training error :0.6555591821670532\n",
      "Epoch : 4646 Loss: 3.398348368704319 Training error :0.6555445194244385\n",
      "Epoch : 4647 Loss: 3.3982711657881737 Training error :0.6555320620536804\n",
      "Epoch : 4648 Loss: 3.398193074390292 Training error :0.6555161476135254\n",
      "Epoch : 4649 Loss: 3.3981028366833925 Training error :0.6554985642433167\n",
      "Epoch : 4650 Loss: 3.398042043671012 Training error :0.6554849147796631\n",
      "Epoch : 4651 Loss: 3.397967576980591 Training error :0.655468761920929\n",
      "Epoch : 4652 Loss: 3.397901013493538 Training error :0.6554579138755798\n",
      "Epoch : 4653 Loss: 3.3978281170129776 Training error :0.6554415225982666\n",
      "Epoch : 4654 Loss: 3.397763041779399 Training error :0.6554256677627563\n",
      "Epoch : 4655 Loss: 3.3976960331201553 Training error :0.6554133892059326\n",
      "Epoch : 4656 Loss: 3.3976073134690523 Training error :0.6553974151611328\n",
      "Epoch : 4657 Loss: 3.3975292332470417 Training error :0.6553805470466614\n",
      "Epoch : 4658 Loss: 3.3974698949605227 Training error :0.6553699374198914\n",
      "Epoch : 4659 Loss: 3.3973924834281206 Training error :0.6553540229797363\n",
      "Epoch : 4660 Loss: 3.3973243590444326 Training error :0.6553376913070679\n",
      "Epoch : 4661 Loss: 3.397263005375862 Training error :0.6553248763084412\n",
      "Epoch : 4662 Loss: 3.397164348512888 Training error :0.6553102135658264\n",
      "Epoch : 4663 Loss: 3.3971040658652782 Training error :0.6552969217300415\n",
      "Epoch : 4664 Loss: 3.3970320522785187 Training error :0.6552815437316895\n",
      "Epoch : 4665 Loss: 3.3969602808356285 Training error :0.6552668809890747\n",
      "Epoch : 4666 Loss: 3.3969003167003393 Training error :0.655253529548645\n",
      "Epoch : 4667 Loss: 3.3968280404806137 Training error :0.6552382111549377\n",
      "Epoch : 4668 Loss: 3.3967433907091618 Training error :0.6552224159240723\n",
      "Epoch : 4669 Loss: 3.3966717831790447 Training error :0.6552075147628784\n",
      "Epoch : 4670 Loss: 3.3966155257076025 Training error :0.655195951461792\n",
      "Epoch : 4671 Loss: 3.3965406976640224 Training error :0.6551835536956787\n",
      "Epoch : 4672 Loss: 3.39647595025599 Training error :0.6551662683486938\n",
      "Epoch : 4673 Loss: 3.3964110482484102 Training error :0.655152440071106\n",
      "Epoch : 4674 Loss: 3.3963362108916044 Training error :0.6551380157470703\n",
      "Epoch : 4675 Loss: 3.3962549064308405 Training error :0.6551207900047302\n",
      "Epoch : 4676 Loss: 3.396195612847805 Training error :0.6551072001457214\n",
      "Epoch : 4677 Loss: 3.3961194530129433 Training error :0.6550971865653992\n",
      "Epoch : 4678 Loss: 3.3960646018385887 Training error :0.655081570148468\n",
      "Epoch : 4679 Loss: 3.3959857262670994 Training error :0.6550670862197876\n",
      "Epoch : 4680 Loss: 3.395912878215313 Training error :0.6550529599189758\n",
      "Epoch : 4681 Loss: 3.3958332762122154 Training error :0.6550369262695312\n",
      "Epoch : 4682 Loss: 3.3957907240837812 Training error :0.6550265550613403\n",
      "Epoch : 4683 Loss: 3.3957123067229986 Training error :0.6550121903419495\n",
      "Epoch : 4684 Loss: 3.3956442195922136 Training error :0.6549960374832153\n",
      "Epoch : 4685 Loss: 3.3955761063843966 Training error :0.6549860239028931\n",
      "Epoch : 4686 Loss: 3.395956616848707 Training error :0.6549124717712402\n",
      "Epoch : 4687 Loss: 3.3957950063049793 Training error :0.6549508571624756\n",
      "Epoch : 4688 Loss: 3.3953282721340656 Training error :0.6549537777900696\n",
      "Epoch : 4689 Loss: 3.3952112961560488 Training error :0.6549224853515625\n",
      "Epoch : 4690 Loss: 3.395257690921426 Training error :0.6549049615859985\n",
      "Epoch : 4691 Loss: 3.3951776921749115 Training error :0.6548979878425598\n",
      "Epoch : 4692 Loss: 3.395057711750269 Training error :0.6548886895179749\n",
      "Epoch : 4693 Loss: 3.3949934225529432 Training error :0.6548725962638855\n",
      "Epoch : 4694 Loss: 3.3949570637196302 Training error :0.6548540592193604\n",
      "Epoch : 4695 Loss: 3.3948861435055733 Training error :0.6548445224761963\n",
      "Epoch : 4696 Loss: 3.3947928231209517 Training error :0.6548285484313965\n",
      "Epoch : 4697 Loss: 3.394738782197237 Training error :0.654815673828125\n",
      "Epoch : 4698 Loss: 3.3946881499141455 Training error :0.6548025012016296\n",
      "Epoch : 4699 Loss: 3.394594458863139 Training error :0.6547897458076477\n",
      "Epoch : 4700 Loss: 3.3945519365370274 Training error :0.6547744870185852\n",
      "Epoch : 4701 Loss: 3.3944677244871855 Training error :0.654763400554657\n",
      "Epoch : 4702 Loss: 3.39440774358809 Training error :0.6547478437423706\n",
      "Epoch : 4703 Loss: 3.3943521194159985 Training error :0.6547348499298096\n",
      "Epoch : 4704 Loss: 3.394295535981655 Training error :0.6547225713729858\n",
      "Epoch : 4705 Loss: 3.394215904176235 Training error :0.6547075510025024\n",
      "Epoch : 4706 Loss: 3.3941554445773363 Training error :0.6546931266784668\n",
      "Epoch : 4707 Loss: 3.3940614499151707 Training error :0.6546789407730103\n",
      "Epoch : 4708 Loss: 3.3940228782594204 Training error :0.6546679139137268\n",
      "Epoch : 4709 Loss: 3.393958367407322 Training error :0.6546505689620972\n",
      "Epoch : 4710 Loss: 3.3939072377979755 Training error :0.6546390652656555\n",
      "Epoch : 4711 Loss: 3.3938328195363283 Training error :0.6546264886856079\n",
      "Epoch : 4712 Loss: 3.3937569819390774 Training error :0.654613196849823\n",
      "Epoch : 4713 Loss: 3.3936928547918797 Training error :0.6545976996421814\n",
      "Epoch : 4714 Loss: 3.393606474623084 Training error :0.6545863151550293\n",
      "Epoch : 4715 Loss: 3.393555600196123 Training error :0.6545717120170593\n",
      "Epoch : 4716 Loss: 3.393498722463846 Training error :0.6545574069023132\n",
      "Epoch : 4717 Loss: 3.39342412725091 Training error :0.6545466780662537\n",
      "Epoch : 4718 Loss: 3.393363671377301 Training error :0.6545336246490479\n",
      "Epoch : 4719 Loss: 3.3933011889457703 Training error :0.6545190215110779\n",
      "Epoch : 4720 Loss: 3.393226008862257 Training error :0.6545042395591736\n",
      "Epoch : 4721 Loss: 3.3931697607040405 Training error :0.6544904112815857\n",
      "Epoch : 4722 Loss: 3.39309555105865 Training error :0.6544793844223022\n",
      "Epoch : 4723 Loss: 3.3930325768887997 Training error :0.654463529586792\n",
      "Epoch : 4724 Loss: 3.392957679927349 Training error :0.6544498205184937\n",
      "Epoch : 4725 Loss: 3.392898766323924 Training error :0.6544385552406311\n",
      "Epoch : 4726 Loss: 3.3928308356553316 Training error :0.6544252038002014\n",
      "Epoch : 4727 Loss: 3.392769169062376 Training error :0.6544119119644165\n",
      "Epoch : 4728 Loss: 3.3927097246050835 Training error :0.6543987989425659\n",
      "Epoch : 4729 Loss: 3.392657656222582 Training error :0.6543851494789124\n",
      "Epoch : 4730 Loss: 3.39257856272161 Training error :0.6543748378753662\n",
      "Epoch : 4731 Loss: 3.392514295876026 Training error :0.6543582081794739\n",
      "Epoch : 4732 Loss: 3.3924507424235344 Training error :0.6543439626693726\n",
      "Epoch : 4733 Loss: 3.3923914190381765 Training error :0.6543330550193787\n",
      "Epoch : 4734 Loss: 3.392319720238447 Training error :0.654319167137146\n",
      "Epoch : 4735 Loss: 3.3922543302178383 Training error :0.6543088555335999\n",
      "Epoch : 4736 Loss: 3.392180908471346 Training error :0.6542955040931702\n",
      "Epoch : 4737 Loss: 3.3921289946883917 Training error :0.6542797088623047\n",
      "Epoch : 4738 Loss: 3.3920750860124826 Training error :0.654268205165863\n",
      "Epoch : 4739 Loss: 3.391977660357952 Training error :0.654254138469696\n",
      "Epoch : 4740 Loss: 3.391937118023634 Training error :0.6542410850524902\n",
      "Epoch : 4741 Loss: 3.3918754309415817 Training error :0.654228925704956\n",
      "Epoch : 4742 Loss: 3.391789609566331 Training error :0.6542174816131592\n",
      "Epoch : 4743 Loss: 3.3917477503418922 Training error :0.6542055606842041\n",
      "Epoch : 4744 Loss: 3.3916685301810503 Training error :0.6541904211044312\n",
      "Epoch : 4745 Loss: 3.3915894124656916 Training error :0.6541745066642761\n",
      "Epoch : 4746 Loss: 3.3919691778719425 Training error :0.6541084051132202\n",
      "Epoch : 4747 Loss: 3.3918445520102978 Training error :0.6541458368301392\n",
      "Epoch : 4748 Loss: 3.391382407397032 Training error :0.6541551947593689\n",
      "Epoch : 4749 Loss: 3.391289748251438 Training error :0.6541233062744141\n",
      "Epoch : 4750 Loss: 3.39129295386374 Training error :0.6541028022766113\n",
      "Epoch : 4751 Loss: 3.391257930546999 Training error :0.6541004180908203\n",
      "Epoch : 4752 Loss: 3.391142174601555 Training error :0.6540887355804443\n",
      "Epoch : 4753 Loss: 3.391080219298601 Training error :0.6540713310241699\n",
      "Epoch : 4754 Loss: 3.391044743359089 Training error :0.6540598273277283\n",
      "Epoch : 4755 Loss: 3.390965197235346 Training error :0.6540497541427612\n",
      "Epoch : 4756 Loss: 3.390921665355563 Training error :0.6540380716323853\n",
      "Epoch : 4757 Loss: 3.3908545579761267 Training error :0.6540229320526123\n",
      "Epoch : 4758 Loss: 3.390781845897436 Training error :0.6540080904960632\n",
      "Epoch : 4759 Loss: 3.390718122944236 Training error :0.6540003418922424\n",
      "Epoch : 4760 Loss: 3.3906765300780535 Training error :0.653984785079956\n",
      "Epoch : 4761 Loss: 3.390608850866556 Training error :0.6539720296859741\n",
      "Epoch : 4762 Loss: 3.390546841546893 Training error :0.6539624929428101\n",
      "Epoch : 4763 Loss: 3.390484808012843 Training error :0.6539486646652222\n",
      "Epoch : 4764 Loss: 3.390410454943776 Training error :0.6539379358291626\n",
      "Epoch : 4765 Loss: 3.3903594333678484 Training error :0.6539244055747986\n",
      "Epoch : 4766 Loss: 3.390306646004319 Training error :0.6539077758789062\n",
      "Epoch : 4767 Loss: 3.3902325071394444 Training error :0.6538979411125183\n",
      "Epoch : 4768 Loss: 3.39016130939126 Training error :0.6538863182067871\n",
      "Epoch : 4769 Loss: 3.3901141341775656 Training error :0.6538715958595276\n",
      "Epoch : 4770 Loss: 3.3900311328470707 Training error :0.6538602709770203\n",
      "Epoch : 4771 Loss: 3.3899854253977537 Training error :0.6538477540016174\n",
      "Epoch : 4772 Loss: 3.389932269230485 Training error :0.6538357734680176\n",
      "Epoch : 4773 Loss: 3.3898635245859623 Training error :0.6538243293762207\n",
      "Epoch : 4774 Loss: 3.3898063004016876 Training error :0.6538102030754089\n",
      "Epoch : 4775 Loss: 3.3897504694759846 Training error :0.6537995338439941\n",
      "Epoch : 4776 Loss: 3.38967096619308 Training error :0.6537867784500122\n",
      "Epoch : 4777 Loss: 3.389629576355219 Training error :0.6537726521492004\n",
      "Epoch : 4778 Loss: 3.38956299982965 Training error :0.653759777545929\n",
      "Epoch : 4779 Loss: 3.389507420361042 Training error :0.6537489295005798\n",
      "Epoch : 4780 Loss: 3.389442529529333 Training error :0.6537394523620605\n",
      "Epoch : 4781 Loss: 3.3893955051898956 Training error :0.6537266373634338\n",
      "Epoch : 4782 Loss: 3.3893374241888523 Training error :0.6537120938301086\n",
      "Epoch : 4783 Loss: 3.389262767508626 Training error :0.6537010073661804\n",
      "Epoch : 4784 Loss: 3.389216994866729 Training error :0.6536873579025269\n",
      "Epoch : 4785 Loss: 3.389149185270071 Training error :0.653674304485321\n",
      "Epoch : 4786 Loss: 3.389086242765188 Training error :0.65366530418396\n",
      "Epoch : 4787 Loss: 3.3890283964574337 Training error :0.6536529660224915\n",
      "Epoch : 4788 Loss: 3.388978149741888 Training error :0.6536408066749573\n",
      "Epoch : 4789 Loss: 3.3888956271111965 Training error :0.6536274552345276\n",
      "Epoch : 4790 Loss: 3.3888485319912434 Training error :0.6536129713058472\n",
      "Epoch : 4791 Loss: 3.388784172013402 Training error :0.6536015272140503\n",
      "Epoch : 4792 Loss: 3.3887245282530785 Training error :0.6535910964012146\n",
      "Epoch : 4793 Loss: 3.3886684514582157 Training error :0.6535783410072327\n",
      "Epoch : 4794 Loss: 3.3886146564036608 Training error :0.653565526008606\n",
      "Epoch : 4795 Loss: 3.3885331079363823 Training error :0.6535534858703613\n",
      "Epoch : 4796 Loss: 3.3884993456304073 Training error :0.6535447835922241\n",
      "Epoch : 4797 Loss: 3.3884347043931484 Training error :0.6535298228263855\n",
      "Epoch : 4798 Loss: 3.3883647676557302 Training error :0.6535173654556274\n",
      "Epoch : 4799 Loss: 3.3883204460144043 Training error :0.6535075902938843\n",
      "Epoch : 4800 Loss: 3.3882534001022577 Training error :0.6534935235977173\n",
      "Epoch : 4801 Loss: 3.3881776574999094 Training error :0.6534805297851562\n",
      "Epoch : 4802 Loss: 3.388140555471182 Training error :0.653472363948822\n",
      "Epoch : 4803 Loss: 3.388072518631816 Training error :0.6534562110900879\n",
      "Epoch : 4804 Loss: 3.388006815686822 Training error :0.6534443497657776\n",
      "Epoch : 4805 Loss: 3.387955065816641 Training error :0.6534351110458374\n",
      "Epoch : 4806 Loss: 3.3878955021500587 Training error :0.6534209251403809\n",
      "Epoch : 4807 Loss: 3.3878320455551147 Training error :0.6534111499786377\n",
      "Epoch : 4808 Loss: 3.3877849839627743 Training error :0.6533990502357483\n",
      "Epoch : 4809 Loss: 3.3877208456397057 Training error :0.6533877849578857\n",
      "Epoch : 4810 Loss: 3.387672943994403 Training error :0.6533765196800232\n",
      "Epoch : 4811 Loss: 3.387600716203451 Training error :0.6533645391464233\n",
      "Epoch : 4812 Loss: 3.387974062934518 Training error :0.6532938480377197\n",
      "Epoch : 4813 Loss: 3.387839024886489 Training error :0.6533327698707581\n",
      "Epoch : 4814 Loss: 3.38737022690475 Training error :0.6533402800559998\n",
      "Epoch : 4815 Loss: 3.3873018492013216 Training error :0.6533141136169434\n",
      "Epoch : 4816 Loss: 3.387312177568674 Training error :0.6532951593399048\n",
      "Epoch : 4817 Loss: 3.3872674461454153 Training error :0.6532880663871765\n",
      "Epoch : 4818 Loss: 3.3871521800756454 Training error :0.6532801985740662\n",
      "Epoch : 4819 Loss: 3.387094570323825 Training error :0.6532673835754395\n",
      "Epoch : 4820 Loss: 3.387059725821018 Training error :0.6532562971115112\n",
      "Epoch : 4821 Loss: 3.387007774785161 Training error :0.6532454490661621\n",
      "Epoch : 4822 Loss: 3.3869609255343676 Training error :0.6532303094863892\n",
      "Epoch : 4823 Loss: 3.3869069442152977 Training error :0.6532213091850281\n",
      "Epoch : 4824 Loss: 3.3868212029337883 Training error :0.6532097458839417\n",
      "Epoch : 4825 Loss: 3.386778112500906 Training error :0.6531985998153687\n",
      "Epoch : 4826 Loss: 3.38671251013875 Training error :0.6531878709793091\n",
      "Epoch : 4827 Loss: 3.3866599425673485 Training error :0.6531760692596436\n",
      "Epoch : 4828 Loss: 3.3866128716617823 Training error :0.6531635522842407\n",
      "Epoch : 4829 Loss: 3.38656971976161 Training error :0.6531527042388916\n",
      "Epoch : 4830 Loss: 3.38648553006351 Training error :0.6531417965888977\n",
      "Epoch : 4831 Loss: 3.3864534590393305 Training error :0.6531322598457336\n",
      "Epoch : 4832 Loss: 3.3863844610750675 Training error :0.6531184911727905\n",
      "Epoch : 4833 Loss: 3.3863377403467894 Training error :0.653104841709137\n",
      "Epoch : 4834 Loss: 3.3862704150378704 Training error :0.653093159198761\n",
      "Epoch : 4835 Loss: 3.3862265665084124 Training error :0.6530839800834656\n",
      "Epoch : 4836 Loss: 3.3861572481691837 Training error :0.6530702114105225\n",
      "Epoch : 4837 Loss: 3.3860978186130524 Training error :0.6530609130859375\n",
      "Epoch : 4838 Loss: 3.38604149967432 Training error :0.6530503034591675\n",
      "Epoch : 4839 Loss: 3.3859984390437603 Training error :0.6530377864837646\n",
      "Epoch : 4840 Loss: 3.3859459925442934 Training error :0.6530256867408752\n",
      "Epoch : 4841 Loss: 3.3858893010765314 Training error :0.6530128717422485\n",
      "Epoch : 4842 Loss: 3.3858240619301796 Training error :0.6530037522315979\n",
      "Epoch : 4843 Loss: 3.3857682943344116 Training error :0.6529914140701294\n",
      "Epoch : 4844 Loss: 3.3857224211096764 Training error :0.6529805064201355\n",
      "Epoch : 4845 Loss: 3.3856468945741653 Training error :0.6529698967933655\n",
      "Epoch : 4846 Loss: 3.3855992890894413 Training error :0.6529579758644104\n",
      "Epoch : 4847 Loss: 3.3855502232909203 Training error :0.6529472470283508\n",
      "Epoch : 4848 Loss: 3.385487135499716 Training error :0.6529353857040405\n",
      "Epoch : 4849 Loss: 3.385445434600115 Training error :0.6529238820075989\n",
      "Epoch : 4850 Loss: 3.3853806406259537 Training error :0.6529124975204468\n",
      "Epoch : 4851 Loss: 3.3853021785616875 Training error :0.6529017090797424\n",
      "Epoch : 4852 Loss: 3.3852711878716946 Training error :0.65288907289505\n",
      "Epoch : 4853 Loss: 3.385210059583187 Training error :0.6528812646865845\n",
      "Epoch : 4854 Loss: 3.3851602599024773 Training error :0.6528668999671936\n",
      "Epoch : 4855 Loss: 3.3851018641144037 Training error :0.6528565883636475\n",
      "Epoch : 4856 Loss: 3.3850497100502253 Training error :0.6528465151786804\n",
      "Epoch : 4857 Loss: 3.3849752955138683 Training error :0.6528352499008179\n",
      "Epoch : 4858 Loss: 3.38493924587965 Training error :0.6528214812278748\n",
      "Epoch : 4859 Loss: 3.3848871774971485 Training error :0.6528120636940002\n",
      "Epoch : 4860 Loss: 3.3848183285444975 Training error :0.652798056602478\n",
      "Epoch : 4861 Loss: 3.3847718983888626 Training error :0.6527906656265259\n",
      "Epoch : 4862 Loss: 3.384711315855384 Training error :0.6527777910232544\n",
      "Epoch : 4863 Loss: 3.384637515991926 Training error :0.6527655124664307\n",
      "Epoch : 4864 Loss: 3.3846017196774483 Training error :0.6527550220489502\n",
      "Epoch : 4865 Loss: 3.3845458775758743 Training error :0.6527437567710876\n",
      "Epoch : 4866 Loss: 3.384493511170149 Training error :0.652733325958252\n",
      "Epoch : 4867 Loss: 3.3844319209456444 Training error :0.6527221202850342\n",
      "Epoch : 4868 Loss: 3.384378420189023 Training error :0.6527103185653687\n",
      "Epoch : 4869 Loss: 3.384309096261859 Training error :0.6527023315429688\n",
      "Epoch : 4870 Loss: 3.384267272427678 Training error :0.6526892185211182\n",
      "Epoch : 4871 Loss: 3.384220961481333 Training error :0.652676522731781\n",
      "Epoch : 4872 Loss: 3.3841571398079395 Training error :0.6526651978492737\n",
      "Epoch : 4873 Loss: 3.384096894413233 Training error :0.6526545882225037\n",
      "Epoch : 4874 Loss: 3.384046036750078 Training error :0.6526436805725098\n",
      "Epoch : 4875 Loss: 3.383983029052615 Training error :0.6526341438293457\n",
      "Epoch : 4876 Loss: 3.3839336186647415 Training error :0.6526233553886414\n",
      "Epoch : 4877 Loss: 3.383901983499527 Training error :0.6526119709014893\n",
      "Epoch : 4878 Loss: 3.3838235568255186 Training error :0.6525992751121521\n",
      "Epoch : 4879 Loss: 3.3837854973971844 Training error :0.6525903940200806\n",
      "Epoch : 4880 Loss: 3.3837262224406004 Training error :0.652578592300415\n",
      "Epoch : 4881 Loss: 3.38366661593318 Training error :0.6525675058364868\n",
      "Epoch : 4882 Loss: 3.383634742349386 Training error :0.6525552868843079\n",
      "Epoch : 4883 Loss: 3.3835755828768015 Training error :0.6525448560714722\n",
      "Epoch : 4884 Loss: 3.383514389395714 Training error :0.6525344252586365\n",
      "Epoch : 4885 Loss: 3.383462868630886 Training error :0.6525240540504456\n",
      "Epoch : 4886 Loss: 3.3834081310778856 Training error :0.6525126695632935\n",
      "Epoch : 4887 Loss: 3.3833277709782124 Training error :0.6525014638900757\n",
      "Epoch : 4888 Loss: 3.3833129964768887 Training error :0.6524932384490967\n",
      "Epoch : 4889 Loss: 3.3836934696882963 Training error :0.6524247527122498\n",
      "Epoch : 4890 Loss: 3.383564565330744 Training error :0.652465283870697\n",
      "Epoch : 4891 Loss: 3.383109960705042 Training error :0.6524707078933716\n",
      "Epoch : 4892 Loss: 3.3830140363425016 Training error :0.6524420976638794\n",
      "Epoch : 4893 Loss: 3.3830495551228523 Training error :0.6524263620376587\n",
      "Epoch : 4894 Loss: 3.382961867377162 Training error :0.6524221301078796\n",
      "Epoch : 4895 Loss: 3.382914375513792 Training error :0.6524165272712708\n",
      "Epoch : 4896 Loss: 3.3828602097928524 Training error :0.6524019837379456\n",
      "Epoch : 4897 Loss: 3.3828136548399925 Training error :0.6523892283439636\n",
      "Epoch : 4898 Loss: 3.3827669844031334 Training error :0.6523830890655518\n",
      "Epoch : 4899 Loss: 3.382700849324465 Training error :0.6523714661598206\n",
      "Epoch : 4900 Loss: 3.382633298635483 Training error :0.6523581743240356\n",
      "Epoch : 4901 Loss: 3.3826093710958958 Training error :0.6523498296737671\n",
      "Epoch : 4902 Loss: 3.3825394231826067 Training error :0.6523373126983643\n",
      "Epoch : 4903 Loss: 3.3825007248669863 Training error :0.6523258686065674\n",
      "Epoch : 4904 Loss: 3.3824478946626186 Training error :0.6523163318634033\n",
      "Epoch : 4905 Loss: 3.382387137040496 Training error :0.6523069739341736\n",
      "Epoch : 4906 Loss: 3.3823300283402205 Training error :0.6522971987724304\n",
      "Epoch : 4907 Loss: 3.382295224815607 Training error :0.6522843837738037\n",
      "Epoch : 4908 Loss: 3.382226809859276 Training error :0.652273416519165\n",
      "Epoch : 4909 Loss: 3.3821780495345592 Training error :0.6522641181945801\n",
      "Epoch : 4910 Loss: 3.3821351565420628 Training error :0.6522533893585205\n",
      "Epoch : 4911 Loss: 3.382071830332279 Training error :0.6522446274757385\n",
      "Epoch : 4912 Loss: 3.3820184748619795 Training error :0.6522342562675476\n",
      "Epoch : 4913 Loss: 3.3819640148431063 Training error :0.6522233486175537\n",
      "Epoch : 4914 Loss: 3.381910152733326 Training error :0.6522111296653748\n",
      "Epoch : 4915 Loss: 3.381861463189125 Training error :0.6521987318992615\n",
      "Epoch : 4916 Loss: 3.381805080920458 Training error :0.652189314365387\n",
      "Epoch : 4917 Loss: 3.38174332305789 Training error :0.6521802544593811\n",
      "Epoch : 4918 Loss: 3.381710959598422 Training error :0.652167797088623\n",
      "Epoch : 4919 Loss: 3.3816492781043053 Training error :0.652157723903656\n",
      "Epoch : 4920 Loss: 3.3816055692732334 Training error :0.6521486639976501\n",
      "Epoch : 4921 Loss: 3.3815565407276154 Training error :0.6521365642547607\n",
      "Epoch : 4922 Loss: 3.3815131708979607 Training error :0.652128279209137\n",
      "Epoch : 4923 Loss: 3.3814495764672756 Training error :0.6521169543266296\n",
      "Epoch : 4924 Loss: 3.381386447697878 Training error :0.6521047353744507\n",
      "Epoch : 4925 Loss: 3.381338521838188 Training error :0.6520969271659851\n",
      "Epoch : 4926 Loss: 3.3812879845499992 Training error :0.6520842909812927\n",
      "Epoch : 4927 Loss: 3.3812397308647633 Training error :0.6520721316337585\n",
      "Epoch : 4928 Loss: 3.38119600340724 Training error :0.6520636081695557\n",
      "Epoch : 4929 Loss: 3.3811266273260117 Training error :0.6520549654960632\n",
      "Epoch : 4930 Loss: 3.38108560629189 Training error :0.6520435214042664\n",
      "Epoch : 4931 Loss: 3.3810252249240875 Training error :0.6520327925682068\n",
      "Epoch : 4932 Loss: 3.3809706792235374 Training error :0.6520218849182129\n",
      "Epoch : 4933 Loss: 3.380935488268733 Training error :0.6520129442214966\n",
      "Epoch : 4934 Loss: 3.380882291123271 Training error :0.6519996523857117\n",
      "Epoch : 4935 Loss: 3.380819195881486 Training error :0.6519917249679565\n",
      "Epoch : 4936 Loss: 3.3807751778513193 Training error :0.6519811153411865\n",
      "Epoch : 4937 Loss: 3.380723722279072 Training error :0.6519706845283508\n",
      "Epoch : 4938 Loss: 3.380669830366969 Training error :0.6519601345062256\n",
      "Epoch : 4939 Loss: 3.380613086745143 Training error :0.6519486904144287\n",
      "Epoch : 4940 Loss: 3.380573209375143 Training error :0.6519373059272766\n",
      "Epoch : 4941 Loss: 3.380502175539732 Training error :0.6519297957420349\n",
      "Epoch : 4942 Loss: 3.380468213930726 Training error :0.651917040348053\n",
      "Epoch : 4943 Loss: 3.3804049249738455 Training error :0.6519067287445068\n",
      "Epoch : 4944 Loss: 3.3803625516593456 Training error :0.651897668838501\n",
      "Epoch : 4945 Loss: 3.380303608253598 Training error :0.6518871188163757\n",
      "Epoch : 4946 Loss: 3.3802767489105463 Training error :0.6518780589103699\n",
      "Epoch : 4947 Loss: 3.380202267318964 Training error :0.6518682241439819\n",
      "Epoch : 4948 Loss: 3.3801692947745323 Training error :0.6518577337265015\n",
      "Epoch : 4949 Loss: 3.380115259438753 Training error :0.6518489122390747\n",
      "Epoch : 4950 Loss: 3.3800735399127007 Training error :0.6518363356590271\n",
      "Epoch : 4951 Loss: 3.380014020949602 Training error :0.6518266797065735\n",
      "Epoch : 4952 Loss: 3.3799633234739304 Training error :0.6518176198005676\n",
      "Epoch : 4953 Loss: 3.3799120504409075 Training error :0.6518056392669678\n",
      "Epoch : 4954 Loss: 3.379870470613241 Training error :0.6517951488494873\n",
      "Epoch : 4955 Loss: 3.379806362092495 Training error :0.651786208152771\n",
      "Epoch : 4956 Loss: 3.3797652535140514 Training error :0.6517745852470398\n",
      "Epoch : 4957 Loss: 3.3797152750194073 Training error :0.6517669558525085\n",
      "Epoch : 4958 Loss: 3.37966282106936 Training error :0.6517559885978699\n",
      "Epoch : 4959 Loss: 3.379608605057001 Training error :0.651745080947876\n",
      "Epoch : 4960 Loss: 3.379565354436636 Training error :0.6517363786697388\n",
      "Epoch : 4961 Loss: 3.3794887363910675 Training error :0.6517244577407837\n",
      "Epoch : 4962 Loss: 3.3794621378183365 Training error :0.6517114043235779\n",
      "Epoch : 4963 Loss: 3.379407225176692 Training error :0.6517037153244019\n",
      "Epoch : 4964 Loss: 3.379353754222393 Training error :0.6516937017440796\n",
      "Epoch : 4965 Loss: 3.3793095909059048 Training error :0.6516851782798767\n",
      "Epoch : 4966 Loss: 3.3792615830898285 Training error :0.6516733765602112\n",
      "Epoch : 4967 Loss: 3.3791983257979155 Training error :0.6516623497009277\n",
      "Epoch : 4968 Loss: 3.379174128174782 Training error :0.6516538262367249\n",
      "Epoch : 4969 Loss: 3.3791150897741318 Training error :0.6516429781913757\n",
      "Epoch : 4970 Loss: 3.379057751968503 Training error :0.6516319513320923\n",
      "Epoch : 4971 Loss: 3.3790124971419573 Training error :0.6516233682632446\n",
      "Epoch : 4972 Loss: 3.3789629209786654 Training error :0.6516145467758179\n",
      "Epoch : 4973 Loss: 3.3793043717741966 Training error :0.6515470743179321\n",
      "Epoch : 4974 Loss: 3.379223445430398 Training error :0.6515876054763794\n",
      "Epoch : 4975 Loss: 3.3787473756819963 Training error :0.6515939831733704\n",
      "Epoch : 4976 Loss: 3.37868139334023 Training error :0.6515693068504333\n",
      "Epoch : 4977 Loss: 3.3787072096019983 Training error :0.6515540480613708\n",
      "Epoch : 4978 Loss: 3.3786982391029596 Training error :0.651549756526947\n",
      "Epoch : 4979 Loss: 3.378567971289158 Training error :0.6515412926673889\n",
      "Epoch : 4980 Loss: 3.3785115629434586 Training error :0.6515293121337891\n",
      "Epoch : 4981 Loss: 3.37850359454751 Training error :0.6515225172042847\n",
      "Epoch : 4982 Loss: 3.378455977886915 Training error :0.6515125036239624\n",
      "Epoch : 4983 Loss: 3.378402851521969 Training error :0.6515028476715088\n",
      "Epoch : 4984 Loss: 3.3783555049449205 Training error :0.6514938473701477\n",
      "Epoch : 4985 Loss: 3.378290466964245 Training error :0.651482105255127\n",
      "Epoch : 4986 Loss: 3.378260374069214 Training error :0.6514739990234375\n",
      "Epoch : 4987 Loss: 3.378220919519663 Training error :0.6514618396759033\n",
      "Epoch : 4988 Loss: 3.3781632352620363 Training error :0.651451051235199\n",
      "Epoch : 4989 Loss: 3.3781283479183912 Training error :0.6514437198638916\n",
      "Epoch : 4990 Loss: 3.3780760429799557 Training error :0.6514331102371216\n",
      "Epoch : 4991 Loss: 3.3780221324414015 Training error :0.65142422914505\n",
      "Epoch : 4992 Loss: 3.377975793555379 Training error :0.6514151692390442\n",
      "Epoch : 4993 Loss: 3.377927787601948 Training error :0.6514052748680115\n",
      "Epoch : 4994 Loss: 3.3778941370546818 Training error :0.651394784450531\n",
      "Epoch : 4995 Loss: 3.3778381291776896 Training error :0.6513826251029968\n",
      "Epoch : 4996 Loss: 3.3777806032449007 Training error :0.6513732075691223\n",
      "Epoch : 4997 Loss: 3.3777292631566525 Training error :0.6513674259185791\n",
      "Epoch : 4998 Loss: 3.37767980620265 Training error :0.6513549089431763\n",
      "Epoch : 4999 Loss: 3.3776345141232014 Training error :0.6513450741767883\n",
      "Epoch : 5000 Loss: 3.3775790482759476 Training error :0.6513350009918213\n",
      "Epoch : 5001 Loss: 3.3775269016623497 Training error :0.6513254046440125\n",
      "Epoch : 5002 Loss: 3.3774975035339594 Training error :0.6513171195983887\n",
      "Epoch : 5003 Loss: 3.377427648752928 Training error :0.6513063311576843\n",
      "Epoch : 5004 Loss: 3.37737450376153 Training error :0.6512962579727173\n",
      "Epoch : 5005 Loss: 3.377352388575673 Training error :0.6512889266014099\n",
      "Epoch : 5006 Loss: 3.377291513606906 Training error :0.6512769460678101\n",
      "Epoch : 5007 Loss: 3.3772451393306255 Training error :0.6512652039527893\n",
      "Epoch : 5008 Loss: 3.377194382250309 Training error :0.6512574553489685\n",
      "Epoch : 5009 Loss: 3.3771362230181694 Training error :0.6512480974197388\n",
      "Epoch : 5010 Loss: 3.3770827688276768 Training error :0.6512356400489807\n",
      "Epoch : 5011 Loss: 3.3770466465502977 Training error :0.6512271761894226\n",
      "Epoch : 5012 Loss: 3.376998282968998 Training error :0.6512187719345093\n",
      "Epoch : 5013 Loss: 3.3769601825624704 Training error :0.6512079834938049\n",
      "Epoch : 5014 Loss: 3.3769063632935286 Training error :0.6511979103088379\n",
      "Epoch : 5015 Loss: 3.376843435689807 Training error :0.6511902809143066\n",
      "Epoch : 5016 Loss: 3.3768062740564346 Training error :0.6511813998222351\n",
      "Epoch : 5017 Loss: 3.3767595011740923 Training error :0.6511691808700562\n",
      "Epoch : 5018 Loss: 3.376710679382086 Training error :0.6511577367782593\n",
      "Epoch : 5019 Loss: 3.3766584657132626 Training error :0.651150107383728\n",
      "Epoch : 5020 Loss: 3.3765997663140297 Training error :0.6511399149894714\n",
      "Epoch : 5021 Loss: 3.3765551447868347 Training error :0.6511282920837402\n",
      "Epoch : 5022 Loss: 3.376503050327301 Training error :0.6511208415031433\n",
      "Epoch : 5023 Loss: 3.3764373548328876 Training error :0.6511123776435852\n",
      "Epoch : 5024 Loss: 3.3764196299016476 Training error :0.651103675365448\n",
      "Epoch : 5025 Loss: 3.376365803182125 Training error :0.6510917544364929\n",
      "Epoch : 5026 Loss: 3.37630863673985 Training error :0.6510830521583557\n",
      "Epoch : 5027 Loss: 3.376261133700609 Training error :0.6510726809501648\n",
      "Epoch : 5028 Loss: 3.3762004394084215 Training error :0.651062548160553\n",
      "Epoch : 5029 Loss: 3.376157723367214 Training error :0.6510518789291382\n",
      "Epoch : 5030 Loss: 3.3761243149638176 Training error :0.6510434150695801\n",
      "Epoch : 5031 Loss: 3.3760700952261686 Training error :0.6510308980941772\n",
      "Epoch : 5032 Loss: 3.3760088440030813 Training error :0.6510213613510132\n",
      "Epoch : 5033 Loss: 3.375972794368863 Training error :0.6510137319564819\n",
      "Epoch : 5034 Loss: 3.3759052380919456 Training error :0.6510049700737\n",
      "Epoch : 5035 Loss: 3.375867174938321 Training error :0.6509957909584045\n",
      "Epoch : 5036 Loss: 3.3758335411548615 Training error :0.6509854793548584\n",
      "Epoch : 5037 Loss: 3.3757963441312313 Training error :0.6509751677513123\n",
      "Epoch : 5038 Loss: 3.375737275928259 Training error :0.6509661674499512\n",
      "Epoch : 5039 Loss: 3.375693541020155 Training error :0.6509552001953125\n",
      "Epoch : 5040 Loss: 3.3756338991224766 Training error :0.6509473919868469\n",
      "Epoch : 5041 Loss: 3.375589605420828 Training error :0.6509395837783813\n",
      "Epoch : 5042 Loss: 3.375549925491214 Training error :0.6509279012680054\n",
      "Epoch : 5043 Loss: 3.3755067624151707 Training error :0.650916337966919\n",
      "Epoch : 5044 Loss: 3.3754440136253834 Training error :0.6509078741073608\n",
      "Epoch : 5045 Loss: 3.3754050843417645 Training error :0.6508990526199341\n",
      "Epoch : 5046 Loss: 3.375353978946805 Training error :0.6508891582489014\n",
      "Epoch : 5047 Loss: 3.375304240733385 Training error :0.650881290435791\n",
      "Epoch : 5048 Loss: 3.375257633626461 Training error :0.6508707404136658\n",
      "Epoch : 5049 Loss: 3.375232119113207 Training error :0.6508623361587524\n",
      "Epoch : 5050 Loss: 3.3751706201583147 Training error :0.6508499979972839\n",
      "Epoch : 5051 Loss: 3.375123143196106 Training error :0.6508423089981079\n",
      "Epoch : 5052 Loss: 3.3750651832669973 Training error :0.6508325934410095\n",
      "Epoch : 5053 Loss: 3.375017346814275 Training error :0.6508232355117798\n",
      "Epoch : 5054 Loss: 3.374979306012392 Training error :0.6508134603500366\n",
      "Epoch : 5055 Loss: 3.3749342150986195 Training error :0.6508055329322815\n",
      "Epoch : 5056 Loss: 3.374878006055951 Training error :0.6507949233055115\n",
      "Epoch : 5057 Loss: 3.3748393319547176 Training error :0.6507846713066101\n",
      "Epoch : 5058 Loss: 3.374771408736706 Training error :0.6507768034934998\n",
      "Epoch : 5059 Loss: 3.3747450467199087 Training error :0.6507676839828491\n",
      "Epoch : 5060 Loss: 3.3747109342366457 Training error :0.650757372379303\n",
      "Epoch : 5061 Loss: 3.374639831483364 Training error :0.6507468223571777\n",
      "Epoch : 5062 Loss: 3.374601749703288 Training error :0.6507389545440674\n",
      "Epoch : 5063 Loss: 3.3745598699897528 Training error :0.650729775428772\n",
      "Epoch : 5064 Loss: 3.3744990937411785 Training error :0.6507189273834229\n",
      "Epoch : 5065 Loss: 3.374875186011195 Training error :0.6506550312042236\n",
      "Epoch : 5066 Loss: 3.3747561778873205 Training error :0.6506944894790649\n",
      "Epoch : 5067 Loss: 3.3743256218731403 Training error :0.6507039070129395\n",
      "Epoch : 5068 Loss: 3.374246910214424 Training error :0.6506799459457397\n",
      "Epoch : 5069 Loss: 3.374283544719219 Training error :0.650662362575531\n",
      "Epoch : 5070 Loss: 3.374239206314087 Training error :0.6506592035293579\n",
      "Epoch : 5071 Loss: 3.374158103018999 Training error :0.6506553888320923\n",
      "Epoch : 5072 Loss: 3.3741050343960524 Training error :0.6506437659263611\n",
      "Epoch : 5073 Loss: 3.3740729801356792 Training error :0.6506315469741821\n",
      "Epoch : 5074 Loss: 3.3740259241312742 Training error :0.6506244540214539\n",
      "Epoch : 5075 Loss: 3.373983684927225 Training error :0.6506162285804749\n",
      "Epoch : 5076 Loss: 3.373936392366886 Training error :0.650606632232666\n",
      "Epoch : 5077 Loss: 3.373893801122904 Training error :0.6505966782569885\n",
      "Epoch : 5078 Loss: 3.3738563619554043 Training error :0.6505879759788513\n",
      "Epoch : 5079 Loss: 3.3738104831427336 Training error :0.6505804657936096\n",
      "Epoch : 5080 Loss: 3.3737642727792263 Training error :0.6505680680274963\n",
      "Epoch : 5081 Loss: 3.3737371675670147 Training error :0.6505596041679382\n",
      "Epoch : 5082 Loss: 3.3736764527857304 Training error :0.6505506038665771\n",
      "Epoch : 5083 Loss: 3.373616334050894 Training error :0.650542140007019\n",
      "Epoch : 5084 Loss: 3.373595777899027 Training error :0.6505348086357117\n",
      "Epoch : 5085 Loss: 3.373529937118292 Training error :0.6505246758460999\n",
      "Epoch : 5086 Loss: 3.373499382287264 Training error :0.6505122184753418\n",
      "Epoch : 5087 Loss: 3.373437862843275 Training error :0.6505077481269836\n",
      "Epoch : 5088 Loss: 3.3733902741223574 Training error :0.6504949331283569\n",
      "Epoch : 5089 Loss: 3.37333039753139 Training error :0.6504852771759033\n",
      "Epoch : 5090 Loss: 3.3733027800917625 Training error :0.6504790782928467\n",
      "Epoch : 5091 Loss: 3.373249936848879 Training error :0.6504684686660767\n",
      "Epoch : 5092 Loss: 3.3732326831668615 Training error :0.6504589915275574\n",
      "Epoch : 5093 Loss: 3.3731745034456253 Training error :0.6504509449005127\n",
      "Epoch : 5094 Loss: 3.3731226716190577 Training error :0.650440514087677\n",
      "Epoch : 5095 Loss: 3.3730654288083315 Training error :0.6504334807395935\n",
      "Epoch : 5096 Loss: 3.373041220009327 Training error :0.6504243612289429\n",
      "Epoch : 5097 Loss: 3.3729850463569164 Training error :0.6504116654396057\n",
      "Epoch : 5098 Loss: 3.372942991554737 Training error :0.6504043340682983\n",
      "Epoch : 5099 Loss: 3.372900363057852 Training error :0.6503944396972656\n",
      "Epoch : 5100 Loss: 3.3728448897600174 Training error :0.6503891944885254\n",
      "Epoch : 5101 Loss: 3.3727955110371113 Training error :0.6503763794898987\n",
      "Epoch : 5102 Loss: 3.3727573789656162 Training error :0.6503667235374451\n",
      "Epoch : 5103 Loss: 3.3727091290056705 Training error :0.6503589749336243\n",
      "Epoch : 5104 Loss: 3.3726733289659023 Training error :0.6503489017486572\n",
      "Epoch : 5105 Loss: 3.3726101107895374 Training error :0.6503404378890991\n",
      "Epoch : 5106 Loss: 3.3725773002952337 Training error :0.6503317952156067\n",
      "Epoch : 5107 Loss: 3.3725068792700768 Training error :0.6503238677978516\n",
      "Epoch : 5108 Loss: 3.3724893778562546 Training error :0.6503126621246338\n",
      "Epoch : 5109 Loss: 3.3724421206861734 Training error :0.6503040194511414\n",
      "Epoch : 5110 Loss: 3.3724060114473104 Training error :0.6502944231033325\n",
      "Epoch : 5111 Loss: 3.372347690165043 Training error :0.6502864956855774\n",
      "Epoch : 5112 Loss: 3.3722927533090115 Training error :0.6502776145935059\n",
      "Epoch : 5113 Loss: 3.3722431175410748 Training error :0.6502676606178284\n",
      "Epoch : 5114 Loss: 3.372205352410674 Training error :0.6502606272697449\n",
      "Epoch : 5115 Loss: 3.372160194441676 Training error :0.6502484083175659\n",
      "Epoch : 5116 Loss: 3.3721320554614067 Training error :0.6502397656440735\n",
      "Epoch : 5117 Loss: 3.372074728831649 Training error :0.6502318382263184\n",
      "Epoch : 5118 Loss: 3.3720208536833525 Training error :0.6502227187156677\n",
      "Epoch : 5119 Loss: 3.3720027580857277 Training error :0.650216281414032\n",
      "Epoch : 5120 Loss: 3.3719298411160707 Training error :0.6502060890197754\n",
      "Epoch : 5121 Loss: 3.3718949127942324 Training error :0.6501941680908203\n",
      "Epoch : 5122 Loss: 3.371855966746807 Training error :0.6501876711845398\n",
      "Epoch : 5123 Loss: 3.371803777292371 Training error :0.6501775979995728\n",
      "Epoch : 5124 Loss: 3.3717551678419113 Training error :0.65016770362854\n",
      "Epoch : 5125 Loss: 3.3717280477285385 Training error :0.6501608490943909\n",
      "Epoch : 5126 Loss: 3.371655819937587 Training error :0.6501500010490417\n",
      "Epoch : 5127 Loss: 3.3716419022530317 Training error :0.6501435041427612\n",
      "Epoch : 5128 Loss: 3.371593775227666 Training error :0.6501327753067017\n",
      "Epoch : 5129 Loss: 3.3715330325067043 Training error :0.6501237750053406\n",
      "Epoch : 5130 Loss: 3.3714903090149164 Training error :0.6501173377037048\n",
      "Epoch : 5131 Loss: 3.371453057974577 Training error :0.6501075029373169\n",
      "Epoch : 5132 Loss: 3.3714056983590126 Training error :0.6500974297523499\n",
      "Epoch : 5133 Loss: 3.3713723104447126 Training error :0.6500873565673828\n",
      "Epoch : 5134 Loss: 3.371322074905038 Training error :0.6500803232192993\n",
      "Epoch : 5135 Loss: 3.3712750524282455 Training error :0.6500712633132935\n",
      "Epoch : 5136 Loss: 3.371234493330121 Training error :0.6500610113143921\n",
      "Epoch : 5137 Loss: 3.3711727634072304 Training error :0.6500514149665833\n",
      "Epoch : 5138 Loss: 3.371153010055423 Training error :0.6500445604324341\n",
      "Epoch : 5139 Loss: 3.371109466999769 Training error :0.650035560131073\n",
      "Epoch : 5140 Loss: 3.371053583920002 Training error :0.6500241756439209\n",
      "Epoch : 5141 Loss: 3.3710191138088703 Training error :0.6500192880630493\n",
      "Epoch : 5142 Loss: 3.3709701783955097 Training error :0.6500094532966614\n",
      "Epoch : 5143 Loss: 3.37091095559299 Training error :0.6499980688095093\n",
      "Epoch : 5144 Loss: 3.3708797693252563 Training error :0.6499893069267273\n",
      "Epoch : 5145 Loss: 3.370826743543148 Training error :0.6499816179275513\n",
      "Epoch : 5146 Loss: 3.3707911670207977 Training error :0.6499736309051514\n",
      "Epoch : 5147 Loss: 3.37074763700366 Training error :0.6499623656272888\n",
      "Epoch : 5148 Loss: 3.370697094127536 Training error :0.6499524116516113\n",
      "Epoch : 5149 Loss: 3.3706307988613844 Training error :0.649946928024292\n",
      "Epoch : 5150 Loss: 3.3706191070377827 Training error :0.6499363780021667\n",
      "Epoch : 5151 Loss: 3.3705633487552404 Training error :0.6499261260032654\n",
      "Epoch : 5152 Loss: 3.3705231230705976 Training error :0.6499179601669312\n",
      "Epoch : 5153 Loss: 3.370478928089142 Training error :0.649909257888794\n",
      "Epoch : 5154 Loss: 3.3704444244503975 Training error :0.6499010324478149\n",
      "Epoch : 5155 Loss: 3.3703701123595238 Training error :0.649893581867218\n",
      "Epoch : 5156 Loss: 3.3703442998230457 Training error :0.649881899356842\n",
      "Epoch : 5157 Loss: 3.3702942430973053 Training error :0.6498739123344421\n",
      "Epoch : 5158 Loss: 3.3702513854950666 Training error :0.6498645544052124\n",
      "Epoch : 5159 Loss: 3.370208326727152 Training error :0.6498541831970215\n",
      "Epoch : 5160 Loss: 3.370169896632433 Training error :0.6498478055000305\n",
      "Epoch : 5161 Loss: 3.3701081294566393 Training error :0.6498396992683411\n",
      "Epoch : 5162 Loss: 3.370083784684539 Training error :0.64983069896698\n",
      "Epoch : 5163 Loss: 3.370035408064723 Training error :0.6498218774795532\n",
      "Epoch : 5164 Loss: 3.3699929993599653 Training error :0.6498119235038757\n",
      "Epoch : 5165 Loss: 3.3699509911239147 Training error :0.6498051285743713\n",
      "Epoch : 5166 Loss: 3.369896862655878 Training error :0.6497960090637207\n",
      "Epoch : 5167 Loss: 3.3702431209385395 Training error :0.6497293710708618\n",
      "Epoch : 5168 Loss: 3.3701340183615685 Training error :0.6497721076011658\n",
      "Epoch : 5169 Loss: 3.369710974395275 Training error :0.649781346321106\n",
      "Epoch : 5170 Loss: 3.3696362264454365 Training error :0.6497555375099182\n",
      "Epoch : 5171 Loss: 3.3696888610720634 Training error :0.6497417688369751\n",
      "Epoch : 5172 Loss: 3.3696304000914097 Training error :0.6497374773025513\n",
      "Epoch : 5173 Loss: 3.3695488777011633 Training error :0.6497344374656677\n",
      "Epoch : 5174 Loss: 3.3695149421691895 Training error :0.6497228741645813\n",
      "Epoch : 5175 Loss: 3.369495678693056 Training error :0.6497136950492859\n",
      "Epoch : 5176 Loss: 3.369465745985508 Training error :0.6497063636779785\n",
      "Epoch : 5177 Loss: 3.369406297802925 Training error :0.6496951580047607\n",
      "Epoch : 5178 Loss: 3.3693664260208607 Training error :0.6496893167495728\n",
      "Epoch : 5179 Loss: 3.3693121541291475 Training error :0.649679958820343\n",
      "Epoch : 5180 Loss: 3.369295824319124 Training error :0.6496706604957581\n",
      "Epoch : 5181 Loss: 3.3692368492484093 Training error :0.6496639847755432\n",
      "Epoch : 5182 Loss: 3.36920215934515 Training error :0.6496518850326538\n",
      "Epoch : 5183 Loss: 3.369170017540455 Training error :0.6496458649635315\n",
      "Epoch : 5184 Loss: 3.369100783020258 Training error :0.649636447429657\n",
      "Epoch : 5185 Loss: 3.369051592424512 Training error :0.6496279835700989\n",
      "Epoch : 5186 Loss: 3.369022335857153 Training error :0.6496211290359497\n",
      "Epoch : 5187 Loss: 3.368979411199689 Training error :0.6496107578277588\n",
      "Epoch : 5188 Loss: 3.3689518347382545 Training error :0.6496009826660156\n",
      "Epoch : 5189 Loss: 3.368893561884761 Training error :0.6495934128761292\n",
      "Epoch : 5190 Loss: 3.368819208815694 Training error :0.6495829820632935\n",
      "Epoch : 5191 Loss: 3.3687850199639797 Training error :0.6495762467384338\n",
      "Epoch : 5192 Loss: 3.368766551837325 Training error :0.6495688557624817\n",
      "Epoch : 5193 Loss: 3.3687024042010307 Training error :0.6495568156242371\n",
      "Epoch : 5194 Loss: 3.368661690503359 Training error :0.6495519280433655\n",
      "Epoch : 5195 Loss: 3.36862226203084 Training error :0.6495404839515686\n",
      "Epoch : 5196 Loss: 3.368567366153002 Training error :0.6495313048362732\n",
      "Epoch : 5197 Loss: 3.368519749492407 Training error :0.6495246291160583\n",
      "Epoch : 5198 Loss: 3.3684932105243206 Training error :0.6495155096054077\n",
      "Epoch : 5199 Loss: 3.3684513941407204 Training error :0.6495075821876526\n",
      "Epoch : 5200 Loss: 3.368409361690283 Training error :0.6494966745376587\n",
      "Epoch : 5201 Loss: 3.3683497086167336 Training error :0.6494897603988647\n",
      "Epoch : 5202 Loss: 3.3683214504271746 Training error :0.649482011795044\n",
      "Epoch : 5203 Loss: 3.3682617526501417 Training error :0.6494718194007874\n",
      "Epoch : 5204 Loss: 3.3682201635092497 Training error :0.6494603753089905\n",
      "Epoch : 5205 Loss: 3.368192121386528 Training error :0.6494565606117249\n",
      "Epoch : 5206 Loss: 3.368158096447587 Training error :0.649448037147522\n",
      "Epoch : 5207 Loss: 3.368099920451641 Training error :0.6494364142417908\n",
      "Epoch : 5208 Loss: 3.3680639136582613 Training error :0.6494290232658386\n",
      "Epoch : 5209 Loss: 3.3680011052638292 Training error :0.6494218707084656\n",
      "Epoch : 5210 Loss: 3.3679772038012743 Training error :0.6494137644767761\n",
      "Epoch : 5211 Loss: 3.367937780916691 Training error :0.649402916431427\n",
      "Epoch : 5212 Loss: 3.367888616397977 Training error :0.6493926048278809\n",
      "Epoch : 5213 Loss: 3.367835922166705 Training error :0.6493861675262451\n",
      "Epoch : 5214 Loss: 3.3678008876740932 Training error :0.6493775248527527\n",
      "Epoch : 5215 Loss: 3.367756262421608 Training error :0.6493688821792603\n",
      "Epoch : 5216 Loss: 3.3677139412611723 Training error :0.6493600606918335\n",
      "Epoch : 5217 Loss: 3.3676561694592237 Training error :0.6493503451347351\n",
      "Epoch : 5218 Loss: 3.367633495479822 Training error :0.6493440866470337\n",
      "Epoch : 5219 Loss: 3.367593001574278 Training error :0.6493340134620667\n",
      "Epoch : 5220 Loss: 3.3675262089818716 Training error :0.6493229866027832\n",
      "Epoch : 5221 Loss: 3.367493998259306 Training error :0.649319052696228\n",
      "Epoch : 5222 Loss: 3.367444697767496 Training error :0.6493096947669983\n",
      "Epoch : 5223 Loss: 3.367425251752138 Training error :0.6492993235588074\n",
      "Epoch : 5224 Loss: 3.3673592191189528 Training error :0.649289608001709\n",
      "Epoch : 5225 Loss: 3.367308558896184 Training error :0.6492826342582703\n",
      "Epoch : 5226 Loss: 3.3672748245298862 Training error :0.6492743492126465\n",
      "Epoch : 5227 Loss: 3.367225641384721 Training error :0.649265468120575\n",
      "Epoch : 5228 Loss: 3.367199093103409 Training error :0.649258017539978\n",
      "Epoch : 5229 Loss: 3.367163997143507 Training error :0.6492506265640259\n",
      "Epoch : 5230 Loss: 3.367105070501566 Training error :0.6492396593093872\n",
      "Epoch : 5231 Loss: 3.367067836225033 Training error :0.6492296457290649\n",
      "Epoch : 5232 Loss: 3.367014978080988 Training error :0.6492247581481934\n",
      "Epoch : 5233 Loss: 3.366977024823427 Training error :0.6492167711257935\n",
      "Epoch : 5234 Loss: 3.3669489305466413 Training error :0.6492059826850891\n",
      "Epoch : 5235 Loss: 3.366908086463809 Training error :0.649196982383728\n",
      "Epoch : 5236 Loss: 3.3668406922370195 Training error :0.6491895914077759\n",
      "Epoch : 5237 Loss: 3.3668100386857986 Training error :0.6491801738739014\n",
      "Epoch : 5238 Loss: 3.366755897179246 Training error :0.6491711139678955\n",
      "Epoch : 5239 Loss: 3.366708302870393 Training error :0.6491624712944031\n",
      "Epoch : 5240 Loss: 3.366682769730687 Training error :0.6491562724113464\n",
      "Epoch : 5241 Loss: 3.3666362650692463 Training error :0.649147629737854\n",
      "Epoch : 5242 Loss: 3.3665958531200886 Training error :0.6491363048553467\n",
      "Epoch : 5243 Loss: 3.3665383607149124 Training error :0.6491293907165527\n",
      "Epoch : 5244 Loss: 3.3665137961506844 Training error :0.6491220593452454\n",
      "Epoch : 5245 Loss: 3.366473177447915 Training error :0.6491151452064514\n",
      "Epoch : 5246 Loss: 3.3664367496967316 Training error :0.6491023898124695\n",
      "Epoch : 5247 Loss: 3.3663884308189154 Training error :0.6490941047668457\n",
      "Epoch : 5248 Loss: 3.366341780871153 Training error :0.6490870714187622\n",
      "Epoch : 5249 Loss: 3.3662779182195663 Training error :0.6490802764892578\n",
      "Epoch : 5250 Loss: 3.366255072876811 Training error :0.6490710973739624\n",
      "Epoch : 5251 Loss: 3.3662083949893713 Training error :0.6490633487701416\n",
      "Epoch : 5252 Loss: 3.3661767095327377 Training error :0.6490526795387268\n",
      "Epoch : 5253 Loss: 3.3661256339401007 Training error :0.6490470767021179\n",
      "Epoch : 5254 Loss: 3.3660849891602993 Training error :0.6490356922149658\n",
      "Epoch : 5255 Loss: 3.3660245314240456 Training error :0.649029552936554\n",
      "Epoch : 5256 Loss: 3.3659902680665255 Training error :0.6490222215652466\n",
      "Epoch : 5257 Loss: 3.365965012460947 Training error :0.6490107774734497\n",
      "Epoch : 5258 Loss: 3.3659192249178886 Training error :0.6490027904510498\n",
      "Epoch : 5259 Loss: 3.3658607490360737 Training error :0.6489944458007812\n",
      "Epoch : 5260 Loss: 3.36581920273602 Training error :0.6489855051040649\n",
      "Epoch : 5261 Loss: 3.3657791689038277 Training error :0.6489797830581665\n",
      "Epoch : 5262 Loss: 3.3657329455018044 Training error :0.6489701271057129\n",
      "Epoch : 5263 Loss: 3.3657035659998655 Training error :0.648959755897522\n",
      "Epoch : 5264 Loss: 3.3656649198383093 Training error :0.6489540338516235\n",
      "Epoch : 5265 Loss: 3.3656067065894604 Training error :0.6489440202713013\n",
      "Epoch : 5266 Loss: 3.3655602671205997 Training error :0.648934543132782\n",
      "Epoch : 5267 Loss: 3.3655399158596992 Training error :0.6489303708076477\n",
      "Epoch : 5268 Loss: 3.3654786720871925 Training error :0.6489204168319702\n",
      "Epoch : 5269 Loss: 3.365459246560931 Training error :0.6489095091819763\n",
      "Epoch : 5270 Loss: 3.365415455773473 Training error :0.648902416229248\n",
      "Epoch : 5271 Loss: 3.365360412746668 Training error :0.6488950848579407\n",
      "Epoch : 5272 Loss: 3.365314830094576 Training error :0.6488870978355408\n",
      "Epoch : 5273 Loss: 3.365279581397772 Training error :0.6488783359527588\n",
      "Epoch : 5274 Loss: 3.3652328569442034 Training error :0.6488680839538574\n",
      "Epoch : 5275 Loss: 3.3652010820806026 Training error :0.6488619446754456\n",
      "Epoch : 5276 Loss: 3.365160120651126 Training error :0.6488519906997681\n",
      "Epoch : 5277 Loss: 3.3655293751507998 Training error :0.6487919688224792\n",
      "Epoch : 5278 Loss: 3.3653920516371727 Training error :0.6488292217254639\n",
      "Epoch : 5279 Loss: 3.365007746964693 Training error :0.6488394141197205\n",
      "Epoch : 5280 Loss: 3.3649213649332523 Training error :0.6488168239593506\n",
      "Epoch : 5281 Loss: 3.3649627417325974 Training error :0.6488033533096313\n",
      "Epoch : 5282 Loss: 3.3649191074073315 Training error :0.6488006711006165\n",
      "Epoch : 5283 Loss: 3.364827139303088 Training error :0.648794412612915\n",
      "Epoch : 5284 Loss: 3.364790741354227 Training error :0.6487841010093689\n",
      "Epoch : 5285 Loss: 3.3647862784564495 Training error :0.6487759947776794\n",
      "Epoch : 5286 Loss: 3.3647321574389935 Training error :0.6487683653831482\n",
      "Epoch : 5287 Loss: 3.364684810861945 Training error :0.6487590074539185\n",
      "Epoch : 5288 Loss: 3.364652168005705 Training error :0.6487529277801514\n",
      "Epoch : 5289 Loss: 3.3646076880395412 Training error :0.6487441062927246\n",
      "Epoch : 5290 Loss: 3.3645505271852016 Training error :0.6487345099449158\n",
      "Epoch : 5291 Loss: 3.3645213805139065 Training error :0.6487260460853577\n",
      "Epoch : 5292 Loss: 3.3644799701869488 Training error :0.6487177014350891\n",
      "Epoch : 5293 Loss: 3.3644582107663155 Training error :0.6487137079238892\n",
      "Epoch : 5294 Loss: 3.364391950890422 Training error :0.6487036943435669\n",
      "Epoch : 5295 Loss: 3.364368449896574 Training error :0.648692786693573\n",
      "Epoch : 5296 Loss: 3.3643043395131826 Training error :0.6486861705780029\n",
      "Epoch : 5297 Loss: 3.364263331517577 Training error :0.6486780643463135\n",
      "Epoch : 5298 Loss: 3.364257413893938 Training error :0.6486683487892151\n",
      "Epoch : 5299 Loss: 3.3641967102885246 Training error :0.6486627459526062\n",
      "Epoch : 5300 Loss: 3.3641420025378466 Training error :0.6486549377441406\n",
      "Epoch : 5301 Loss: 3.364123985171318 Training error :0.6486461758613586\n",
      "Epoch : 5302 Loss: 3.3640651032328606 Training error :0.6486360430717468\n",
      "Epoch : 5303 Loss: 3.364020921289921 Training error :0.648627519607544\n",
      "Epoch : 5304 Loss: 3.3639978729188442 Training error :0.6486205458641052\n",
      "Epoch : 5305 Loss: 3.363954471424222 Training error :0.6486137509346008\n",
      "Epoch : 5306 Loss: 3.3638924546539783 Training error :0.6486020684242249\n",
      "Epoch : 5307 Loss: 3.3638681322336197 Training error :0.6485961079597473\n",
      "Epoch : 5308 Loss: 3.3638355378061533 Training error :0.6485884785652161\n",
      "Epoch : 5309 Loss: 3.363785047084093 Training error :0.6485777497291565\n",
      "Epoch : 5310 Loss: 3.3637458067387342 Training error :0.6485726237297058\n",
      "Epoch : 5311 Loss: 3.3636962193995714 Training error :0.6485641598701477\n",
      "Epoch : 5312 Loss: 3.3636658638715744 Training error :0.6485554575920105\n",
      "Epoch : 5313 Loss: 3.3636011984199286 Training error :0.6485470533370972\n",
      "Epoch : 5314 Loss: 3.36357594281435 Training error :0.6485368609428406\n",
      "Epoch : 5315 Loss: 3.36352407746017 Training error :0.6485304832458496\n",
      "Epoch : 5316 Loss: 3.3635042998939753 Training error :0.6485228538513184\n",
      "Epoch : 5317 Loss: 3.363426558673382 Training error :0.6485122442245483\n",
      "Epoch : 5318 Loss: 3.3633997570723295 Training error :0.6485047340393066\n",
      "Epoch : 5319 Loss: 3.3633391074836254 Training error :0.6484969258308411\n",
      "Epoch : 5320 Loss: 3.363317895680666 Training error :0.6484876275062561\n",
      "Epoch : 5321 Loss: 3.3632935509085655 Training error :0.6484825611114502\n",
      "Epoch : 5322 Loss: 3.3632333744317293 Training error :0.6484751105308533\n",
      "Epoch : 5323 Loss: 3.363192507997155 Training error :0.6484660506248474\n",
      "Epoch : 5324 Loss: 3.3631702698767185 Training error :0.6484559774398804\n",
      "Epoch : 5325 Loss: 3.3631044179201126 Training error :0.6484445333480835\n",
      "Epoch : 5326 Loss: 3.3630625400692225 Training error :0.6484400033950806\n",
      "Epoch : 5327 Loss: 3.363028572872281 Training error :0.6484301686286926\n",
      "Epoch : 5328 Loss: 3.3629835434257984 Training error :0.6484227776527405\n",
      "Epoch : 5329 Loss: 3.3629372119903564 Training error :0.6484151482582092\n",
      "Epoch : 5330 Loss: 3.3628976810723543 Training error :0.6484055519104004\n",
      "Epoch : 5331 Loss: 3.3628775663673878 Training error :0.6483988761901855\n",
      "Epoch : 5332 Loss: 3.3628262169659138 Training error :0.6483911275863647\n",
      "Epoch : 5333 Loss: 3.362782096490264 Training error :0.6483803391456604\n",
      "Epoch : 5334 Loss: 3.362751141190529 Training error :0.6483758687973022\n",
      "Epoch : 5335 Loss: 3.362700793892145 Training error :0.6483657956123352\n",
      "Epoch : 5336 Loss: 3.362649420276284 Training error :0.6483578681945801\n",
      "Epoch : 5337 Loss: 3.362630335614085 Training error :0.648348331451416\n",
      "Epoch : 5338 Loss: 3.362568449229002 Training error :0.6483404636383057\n",
      "Epoch : 5339 Loss: 3.362532451748848 Training error :0.6483314633369446\n",
      "Epoch : 5340 Loss: 3.3624835275113583 Training error :0.648324728012085\n",
      "Epoch : 5341 Loss: 3.3624531105160713 Training error :0.6483178734779358\n",
      "Epoch : 5342 Loss: 3.362408436834812 Training error :0.6483104825019836\n",
      "Epoch : 5343 Loss: 3.3623595647513866 Training error :0.6483005285263062\n",
      "Epoch : 5344 Loss: 3.36234163120389 Training error :0.6482915282249451\n",
      "Epoch : 5345 Loss: 3.3622889388352633 Training error :0.6482869982719421\n",
      "Epoch : 5346 Loss: 3.362244714051485 Training error :0.6482775211334229\n",
      "Epoch : 5347 Loss: 3.362216791138053 Training error :0.6482677459716797\n",
      "Epoch : 5348 Loss: 3.362158128991723 Training error :0.6482606530189514\n",
      "Epoch : 5349 Loss: 3.362117577344179 Training error :0.6482526659965515\n",
      "Epoch : 5350 Loss: 3.362089116126299 Training error :0.6482434272766113\n",
      "Epoch : 5351 Loss: 3.362040573731065 Training error :0.6482357978820801\n",
      "Epoch : 5352 Loss: 3.3619873449206352 Training error :0.6482293009757996\n",
      "Epoch : 5353 Loss: 3.3619678635150194 Training error :0.6482193470001221\n",
      "Epoch : 5354 Loss: 3.361908745020628 Training error :0.6482110023498535\n",
      "Epoch : 5355 Loss: 3.361858481541276 Training error :0.6482033729553223\n",
      "Epoch : 5356 Loss: 3.3618371058255434 Training error :0.6481963396072388\n",
      "Epoch : 5357 Loss: 3.3617960680276155 Training error :0.6481886506080627\n",
      "Epoch : 5358 Loss: 3.3617310244590044 Training error :0.6481795310974121\n",
      "Epoch : 5359 Loss: 3.361690528690815 Training error :0.6481727361679077\n",
      "Epoch : 5360 Loss: 3.3616588432341814 Training error :0.6481601595878601\n",
      "Epoch : 5361 Loss: 3.3616220485419035 Training error :0.6481567621231079\n",
      "Epoch : 5362 Loss: 3.3615778274834156 Training error :0.6481464505195618\n",
      "Epoch : 5363 Loss: 3.361521642655134 Training error :0.6481385827064514\n",
      "Epoch : 5364 Loss: 3.3614968210458755 Training error :0.6481311917304993\n",
      "Epoch : 5365 Loss: 3.3614398799836636 Training error :0.6481242179870605\n",
      "Epoch : 5366 Loss: 3.3614085353910923 Training error :0.6481143832206726\n",
      "Epoch : 5367 Loss: 3.361381171271205 Training error :0.6481066346168518\n",
      "Epoch : 5368 Loss: 3.3613323345780373 Training error :0.648099958896637\n",
      "Epoch : 5369 Loss: 3.3612942434847355 Training error :0.6480906009674072\n",
      "Epoch : 5370 Loss: 3.3612578604370356 Training error :0.6480837464332581\n",
      "Epoch : 5371 Loss: 3.3612062707543373 Training error :0.6480749249458313\n",
      "Epoch : 5372 Loss: 3.3611716348677874 Training error :0.6480682492256165\n",
      "Epoch : 5373 Loss: 3.361134923994541 Training error :0.648057758808136\n",
      "Epoch : 5374 Loss: 3.361091934144497 Training error :0.6480480432510376\n",
      "Epoch : 5375 Loss: 3.361039999872446 Training error :0.6480443477630615\n",
      "Epoch : 5376 Loss: 3.3610126711428165 Training error :0.6480342149734497\n",
      "Epoch : 5377 Loss: 3.3609612211585045 Training error :0.6480252146720886\n",
      "Epoch : 5378 Loss: 3.3609125446528196 Training error :0.648020327091217\n",
      "Epoch : 5379 Loss: 3.3608739133924246 Training error :0.6480110883712769\n",
      "Epoch : 5380 Loss: 3.3608577381819487 Training error :0.6480032205581665\n",
      "Epoch : 5381 Loss: 3.360805530101061 Training error :0.6479948163032532\n",
      "Epoch : 5382 Loss: 3.3607404734939337 Training error :0.6479864716529846\n",
      "Epoch : 5383 Loss: 3.360721191391349 Training error :0.6479785442352295\n",
      "Epoch : 5384 Loss: 3.360679551959038 Training error :0.6479700207710266\n",
      "Epoch : 5385 Loss: 3.3606404662132263 Training error :0.6479610204696655\n",
      "Epoch : 5386 Loss: 3.36059314198792 Training error :0.6479539275169373\n",
      "Epoch : 5387 Loss: 3.3605500776320696 Training error :0.6479440331459045\n",
      "Epoch : 5388 Loss: 3.3605082258582115 Training error :0.6479361653327942\n",
      "Epoch : 5389 Loss: 3.3604805059731007 Training error :0.6479300856590271\n",
      "Epoch : 5390 Loss: 3.360441228374839 Training error :0.6479219198226929\n",
      "Epoch : 5391 Loss: 3.3604091871529818 Training error :0.6479149460792542\n",
      "Epoch : 5392 Loss: 3.3603570368140936 Training error :0.6479073166847229\n",
      "Epoch : 5393 Loss: 3.3603099901229143 Training error :0.6478971838951111\n",
      "Epoch : 5394 Loss: 3.3602781128138304 Training error :0.6478908658027649\n",
      "Epoch : 5395 Loss: 3.360228566452861 Training error :0.6478800773620605\n",
      "Epoch : 5396 Loss: 3.360192069783807 Training error :0.6478720307350159\n",
      "Epoch : 5397 Loss: 3.3601593412458897 Training error :0.6478636264801025\n",
      "Epoch : 5398 Loss: 3.360099956393242 Training error :0.6478573083877563\n",
      "Epoch : 5399 Loss: 3.360056959092617 Training error :0.6478521823883057\n",
      "Epoch : 5400 Loss: 3.36045946367085 Training error :0.647789478302002\n",
      "Epoch : 5401 Loss: 3.360323341563344 Training error :0.6478273272514343\n",
      "Epoch : 5402 Loss: 3.359937870875001 Training error :0.6478382349014282\n",
      "Epoch : 5403 Loss: 3.3598352260887623 Training error :0.6478142738342285\n",
      "Epoch : 5404 Loss: 3.3598813451826572 Training error :0.6478025913238525\n",
      "Epoch : 5405 Loss: 3.359843386337161 Training error :0.6478002071380615\n",
      "Epoch : 5406 Loss: 3.3597670309245586 Training error :0.6477938294410706\n",
      "Epoch : 5407 Loss: 3.3597115017473698 Training error :0.6477863788604736\n",
      "Epoch : 5408 Loss: 3.3597063440829515 Training error :0.6477758884429932\n",
      "Epoch : 5409 Loss: 3.3596809078007936 Training error :0.6477674841880798\n",
      "Epoch : 5410 Loss: 3.3596161119639874 Training error :0.6477628350257874\n",
      "Epoch : 5411 Loss: 3.3595850598067045 Training error :0.6477528214454651\n",
      "Epoch : 5412 Loss: 3.359539944678545 Training error :0.6477466821670532\n",
      "Epoch : 5413 Loss: 3.3595015928149223 Training error :0.6477388739585876\n",
      "Epoch : 5414 Loss: 3.3594752475619316 Training error :0.6477298736572266\n",
      "Epoch : 5415 Loss: 3.3594201765954494 Training error :0.6477223038673401\n",
      "Epoch : 5416 Loss: 3.359388107433915 Training error :0.6477137804031372\n",
      "Epoch : 5417 Loss: 3.359340079128742 Training error :0.6477048397064209\n",
      "Epoch : 5418 Loss: 3.359297038987279 Training error :0.6476982235908508\n",
      "Epoch : 5419 Loss: 3.359265062958002 Training error :0.6476892828941345\n",
      "Epoch : 5420 Loss: 3.359252417460084 Training error :0.6476842761039734\n",
      "Epoch : 5421 Loss: 3.359184432774782 Training error :0.647675096988678\n",
      "Epoch : 5422 Loss: 3.359148044139147 Training error :0.6476661562919617\n",
      "Epoch : 5423 Loss: 3.359104273840785 Training error :0.6476604342460632\n",
      "Epoch : 5424 Loss: 3.359069086611271 Training error :0.6476519107818604\n",
      "Epoch : 5425 Loss: 3.359029423445463 Training error :0.6476420760154724\n",
      "Epoch : 5426 Loss: 3.358994074165821 Training error :0.6476350426673889\n",
      "Epoch : 5427 Loss: 3.3589309826493263 Training error :0.6476278901100159\n",
      "Epoch : 5428 Loss: 3.358921492472291 Training error :0.6476196050643921\n",
      "Epoch : 5429 Loss: 3.358859794214368 Training error :0.6476113796234131\n",
      "Epoch : 5430 Loss: 3.3588205315172672 Training error :0.6476026773452759\n",
      "Epoch : 5431 Loss: 3.358805924654007 Training error :0.6475963592529297\n",
      "Epoch : 5432 Loss: 3.3587537929415703 Training error :0.6475881338119507\n",
      "Epoch : 5433 Loss: 3.358704650774598 Training error :0.6475792527198792\n",
      "Epoch : 5434 Loss: 3.358674492686987 Training error :0.647572934627533\n",
      "Epoch : 5435 Loss: 3.358615145087242 Training error :0.647565484046936\n",
      "Epoch : 5436 Loss: 3.3586067464202642 Training error :0.6475562453269958\n",
      "Epoch : 5437 Loss: 3.3585476633161306 Training error :0.6475496888160706\n",
      "Epoch : 5438 Loss: 3.3584976121783257 Training error :0.6475409865379333\n",
      "Epoch : 5439 Loss: 3.3584687020629644 Training error :0.6475324034690857\n",
      "Epoch : 5440 Loss: 3.358420178294182 Training error :0.6475239396095276\n",
      "Epoch : 5441 Loss: 3.35837029106915 Training error :0.6475154757499695\n",
      "Epoch : 5442 Loss: 3.3583444990217686 Training error :0.6475096344947815\n",
      "Epoch : 5443 Loss: 3.3583101015537977 Training error :0.6475017666816711\n",
      "Epoch : 5444 Loss: 3.3582496773451567 Training error :0.6474936604499817\n",
      "Epoch : 5445 Loss: 3.3582282662391663 Training error :0.6474846601486206\n",
      "Epoch : 5446 Loss: 3.358172921463847 Training error :0.6474770307540894\n",
      "Epoch : 5447 Loss: 3.3581449575722218 Training error :0.6474716663360596\n",
      "Epoch : 5448 Loss: 3.3581120036542416 Training error :0.6474637389183044\n",
      "Epoch : 5449 Loss: 3.3580680452287197 Training error :0.6474555134773254\n",
      "Epoch : 5450 Loss: 3.3580363877117634 Training error :0.6474473476409912\n",
      "Epoch : 5451 Loss: 3.3579880855977535 Training error :0.6474385857582092\n",
      "Epoch : 5452 Loss: 3.357935592532158 Training error :0.6474288702011108\n",
      "Epoch : 5453 Loss: 3.357901645824313 Training error :0.6474213600158691\n",
      "Epoch : 5454 Loss: 3.3578543849289417 Training error :0.6474156379699707\n",
      "Epoch : 5455 Loss: 3.357831936329603 Training error :0.647408664226532\n",
      "Epoch : 5456 Loss: 3.3577890396118164 Training error :0.647399365901947\n",
      "Epoch : 5457 Loss: 3.3577527943998575 Training error :0.6473909020423889\n",
      "Epoch : 5458 Loss: 3.3576956912875175 Training error :0.6473862528800964\n",
      "Epoch : 5459 Loss: 3.3576724007725716 Training error :0.6473761796951294\n",
      "Epoch : 5460 Loss: 3.3576324861496687 Training error :0.6473671197891235\n",
      "Epoch : 5461 Loss: 3.357586206868291 Training error :0.6473599076271057\n",
      "Epoch : 5462 Loss: 3.3575346395373344 Training error :0.6473506689071655\n",
      "Epoch : 5463 Loss: 3.357502795755863 Training error :0.6473425030708313\n",
      "Epoch : 5464 Loss: 3.357464414089918 Training error :0.6473382711410522\n",
      "Epoch : 5465 Loss: 3.3574315439909697 Training error :0.6473283171653748\n",
      "Epoch : 5466 Loss: 3.357403328642249 Training error :0.6473225355148315\n",
      "Epoch : 5467 Loss: 3.357352614402771 Training error :0.6473132967948914\n",
      "Epoch : 5468 Loss: 3.357306709513068 Training error :0.6473051309585571\n",
      "Epoch : 5469 Loss: 3.3572554923594 Training error :0.6472991108894348\n",
      "Epoch : 5470 Loss: 3.3572330959141254 Training error :0.647289514541626\n",
      "Epoch : 5471 Loss: 3.357199903577566 Training error :0.6472821831703186\n",
      "Epoch : 5472 Loss: 3.3571519032120705 Training error :0.6472761034965515\n",
      "Epoch : 5473 Loss: 3.357118856161833 Training error :0.6472657918930054\n",
      "Epoch : 5474 Loss: 3.3570739682763815 Training error :0.6472593545913696\n",
      "Epoch : 5475 Loss: 3.3570090997964144 Training error :0.6472523212432861\n",
      "Epoch : 5476 Loss: 3.3569906130433083 Training error :0.6472413539886475\n",
      "Epoch : 5477 Loss: 3.356958592310548 Training error :0.6472371816635132\n",
      "Epoch : 5478 Loss: 3.356912573799491 Training error :0.6472277641296387\n",
      "Epoch : 5479 Loss: 3.356874655932188 Training error :0.6472176313400269\n",
      "Epoch : 5480 Loss: 3.3568345829844475 Training error :0.6472101807594299\n",
      "Epoch : 5481 Loss: 3.3567699193954468 Training error :0.6472060680389404\n",
      "Epoch : 5482 Loss: 3.356763491407037 Training error :0.6471989154815674\n",
      "Epoch : 5483 Loss: 3.3567237704992294 Training error :0.6471887826919556\n",
      "Epoch : 5484 Loss: 3.3566831834614277 Training error :0.6471794247627258\n",
      "Epoch : 5485 Loss: 3.356640202924609 Training error :0.6471742391586304\n",
      "Epoch : 5486 Loss: 3.356595914810896 Training error :0.6471645832061768\n",
      "Epoch : 5487 Loss: 3.356546837836504 Training error :0.6471554040908813\n",
      "Epoch : 5488 Loss: 3.3565211445093155 Training error :0.6471499800682068\n",
      "Epoch : 5489 Loss: 3.3564717322587967 Training error :0.6471429467201233\n",
      "Epoch : 5490 Loss: 3.3564440831542015 Training error :0.6471325159072876\n",
      "Epoch : 5491 Loss: 3.356405409052968 Training error :0.6471256017684937\n",
      "Epoch : 5492 Loss: 3.3563609272241592 Training error :0.6471171379089355\n",
      "Epoch : 5493 Loss: 3.3563234452158213 Training error :0.6471113562583923\n",
      "Epoch : 5494 Loss: 3.3562922663986683 Training error :0.6471027135848999\n",
      "Epoch : 5495 Loss: 3.35624448210001 Training error :0.6470933556556702\n",
      "Epoch : 5496 Loss: 3.3561976552009583 Training error :0.6470862627029419\n",
      "Epoch : 5497 Loss: 3.3561595268547535 Training error :0.6470777988433838\n",
      "Epoch : 5498 Loss: 3.3561073187738657 Training error :0.6470710039138794\n",
      "Epoch : 5499 Loss: 3.3560846131294966 Training error :0.6470642685890198\n",
      "Epoch : 5500 Loss: 3.3560436218976974 Training error :0.6470548510551453\n",
      "Epoch : 5501 Loss: 3.35601438395679 Training error :0.6470494270324707\n",
      "Epoch : 5502 Loss: 3.3559669591486454 Training error :0.6470411419868469\n",
      "Epoch : 5503 Loss: 3.3559184204787016 Training error :0.6470298767089844\n",
      "Epoch : 5504 Loss: 3.3558776266872883 Training error :0.6470269560813904\n",
      "Epoch : 5505 Loss: 3.355837984010577 Training error :0.6470187306404114\n",
      "Epoch : 5506 Loss: 3.3558069579303265 Training error :0.6470096707344055\n",
      "Epoch : 5507 Loss: 3.3557728808373213 Training error :0.6470016241073608\n",
      "Epoch : 5508 Loss: 3.355719594284892 Training error :0.6469941735267639\n",
      "Epoch : 5509 Loss: 3.355682058259845 Training error :0.6469852924346924\n",
      "Epoch : 5510 Loss: 3.355632634833455 Training error :0.6469792723655701\n",
      "Epoch : 5511 Loss: 3.355603836476803 Training error :0.6469707489013672\n",
      "Epoch : 5512 Loss: 3.355574779212475 Training error :0.6469653844833374\n",
      "Epoch : 5513 Loss: 3.3555248100310564 Training error :0.6469535231590271\n",
      "Epoch : 5514 Loss: 3.3554955776780844 Training error :0.6469457745552063\n",
      "Epoch : 5515 Loss: 3.355456056073308 Training error :0.646941602230072\n",
      "Epoch : 5516 Loss: 3.3554074317216873 Training error :0.6469336152076721\n",
      "Epoch : 5517 Loss: 3.35537944547832 Training error :0.6469246745109558\n",
      "Epoch : 5518 Loss: 3.3553362749516964 Training error :0.6469169855117798\n",
      "Epoch : 5519 Loss: 3.3552929405122995 Training error :0.6469082832336426\n",
      "Epoch : 5520 Loss: 3.355249982327223 Training error :0.6468988656997681\n",
      "Epoch : 5521 Loss: 3.3552048932760954 Training error :0.6468952298164368\n",
      "Epoch : 5522 Loss: 3.3551740385591984 Training error :0.6468857526779175\n",
      "Epoch : 5523 Loss: 3.3551440723240376 Training error :0.6468800902366638\n",
      "Epoch : 5524 Loss: 3.3551080767065287 Training error :0.6468704342842102\n",
      "Epoch : 5525 Loss: 3.3550544418394566 Training error :0.6468613147735596\n",
      "Epoch : 5526 Loss: 3.355028009042144 Training error :0.6468558311462402\n",
      "Epoch : 5527 Loss: 3.354971081018448 Training error :0.6468476057052612\n",
      "Epoch : 5528 Loss: 3.3549525309354067 Training error :0.6468394994735718\n",
      "Epoch : 5529 Loss: 3.354920057579875 Training error :0.6468328833580017\n",
      "Epoch : 5530 Loss: 3.3548715990036726 Training error :0.646823525428772\n",
      "Epoch : 5531 Loss: 3.3548276126384735 Training error :0.6468179225921631\n",
      "Epoch : 5532 Loss: 3.3547867704182863 Training error :0.6468073129653931\n",
      "Epoch : 5533 Loss: 3.354734782129526 Training error :0.6468002796173096\n",
      "Epoch : 5534 Loss: 3.354706335812807 Training error :0.6467947959899902\n",
      "Epoch : 5535 Loss: 3.3546903170645237 Training error :0.6467862725257874\n",
      "Epoch : 5536 Loss: 3.3546504992991686 Training error :0.6467767357826233\n",
      "Epoch : 5537 Loss: 3.3545916341245174 Training error :0.6467729806900024\n",
      "Epoch : 5538 Loss: 3.3545599058270454 Training error :0.6467638611793518\n",
      "Epoch : 5539 Loss: 3.3545178417116404 Training error :0.6467559337615967\n",
      "Epoch : 5540 Loss: 3.3544747456908226 Training error :0.6467474102973938\n",
      "Epoch : 5541 Loss: 3.3544409424066544 Training error :0.6467394828796387\n",
      "Epoch : 5542 Loss: 3.3544129263609648 Training error :0.6467304825782776\n",
      "Epoch : 5543 Loss: 3.354352306574583 Training error :0.6467239856719971\n",
      "Epoch : 5544 Loss: 3.354307834059 Training error :0.646717369556427\n",
      "Epoch : 5545 Loss: 3.354288022965193 Training error :0.6467102766036987\n",
      "Epoch : 5546 Loss: 3.354237597435713 Training error :0.6467012763023376\n",
      "Epoch : 5547 Loss: 3.3542087245732546 Training error :0.6466943621635437\n",
      "Epoch : 5548 Loss: 3.3546089585870504 Training error :0.6466355919837952\n",
      "Epoch : 5549 Loss: 3.354464067146182 Training error :0.6466710567474365\n",
      "Epoch : 5550 Loss: 3.3540520556271076 Training error :0.6466819047927856\n",
      "Epoch : 5551 Loss: 3.353941895067692 Training error :0.646657407283783\n",
      "Epoch : 5552 Loss: 3.354002207517624 Training error :0.6466436982154846\n",
      "Epoch : 5553 Loss: 3.353980014100671 Training error :0.6466450691223145\n",
      "Epoch : 5554 Loss: 3.3538722675293684 Training error :0.6466405987739563\n",
      "Epoch : 5555 Loss: 3.353848038241267 Training error :0.6466318368911743\n",
      "Epoch : 5556 Loss: 3.353816147893667 Training error :0.6466227173805237\n",
      "Epoch : 5557 Loss: 3.3537883926182985 Training error :0.6466131806373596\n",
      "Epoch : 5558 Loss: 3.3537422996014357 Training error :0.6466097235679626\n",
      "Epoch : 5559 Loss: 3.3537284452468157 Training error :0.6466012597084045\n",
      "Epoch : 5560 Loss: 3.3536799289286137 Training error :0.6465923190116882\n",
      "Epoch : 5561 Loss: 3.3536391928792 Training error :0.6465840339660645\n",
      "Epoch : 5562 Loss: 3.353591552004218 Training error :0.6465787291526794\n",
      "Epoch : 5563 Loss: 3.353562816977501 Training error :0.6465709209442139\n",
      "Epoch : 5564 Loss: 3.3535218462347984 Training error :0.6465628743171692\n",
      "Epoch : 5565 Loss: 3.3534815814346075 Training error :0.6465559601783752\n",
      "Epoch : 5566 Loss: 3.353445602580905 Training error :0.6465484499931335\n",
      "Epoch : 5567 Loss: 3.3534210845828056 Training error :0.6465379595756531\n",
      "Epoch : 5568 Loss: 3.35337452031672 Training error :0.646531879901886\n",
      "Epoch : 5569 Loss: 3.353332918137312 Training error :0.6465252041816711\n",
      "Epoch : 5570 Loss: 3.3533006384968758 Training error :0.6465176343917847\n",
      "Epoch : 5571 Loss: 3.353258192539215 Training error :0.6465094089508057\n",
      "Epoch : 5572 Loss: 3.3532160874456167 Training error :0.6465012431144714\n",
      "Epoch : 5573 Loss: 3.353173343464732 Training error :0.646492063999176\n",
      "Epoch : 5574 Loss: 3.3531542643904686 Training error :0.6464858055114746\n",
      "Epoch : 5575 Loss: 3.353094808757305 Training error :0.6464784741401672\n",
      "Epoch : 5576 Loss: 3.3530632108449936 Training error :0.6464704275131226\n",
      "Epoch : 5577 Loss: 3.353024085983634 Training error :0.6464649438858032\n",
      "Epoch : 5578 Loss: 3.352986726909876 Training error :0.6464565396308899\n",
      "Epoch : 5579 Loss: 3.352942096069455 Training error :0.6464486718177795\n",
      "Epoch : 5580 Loss: 3.352923206984997 Training error :0.646441638469696\n",
      "Epoch : 5581 Loss: 3.3528853189200163 Training error :0.6464335918426514\n",
      "Epoch : 5582 Loss: 3.352873818948865 Training error :0.6464233994483948\n",
      "Epoch : 5583 Loss: 3.3528173211961985 Training error :0.6464168429374695\n",
      "Epoch : 5584 Loss: 3.352770183235407 Training error :0.6464083194732666\n",
      "Epoch : 5585 Loss: 3.352723741903901 Training error :0.6464022994041443\n",
      "Epoch : 5586 Loss: 3.3526788353919983 Training error :0.6463949084281921\n",
      "Epoch : 5587 Loss: 3.352639328688383 Training error :0.6463871598243713\n",
      "Epoch : 5588 Loss: 3.3526228349655867 Training error :0.6463805437088013\n",
      "Epoch : 5589 Loss: 3.352565936744213 Training error :0.6463726758956909\n",
      "Epoch : 5590 Loss: 3.352528665214777 Training error :0.6463627815246582\n",
      "Epoch : 5591 Loss: 3.352484367787838 Training error :0.6463558077812195\n",
      "Epoch : 5592 Loss: 3.352456670254469 Training error :0.6463494300842285\n",
      "Epoch : 5593 Loss: 3.35241430811584 Training error :0.646342933177948\n",
      "Epoch : 5594 Loss: 3.352368250489235 Training error :0.6463332176208496\n",
      "Epoch : 5595 Loss: 3.3523423951119184 Training error :0.6463244557380676\n",
      "Epoch : 5596 Loss: 3.3522934038192034 Training error :0.6463181376457214\n",
      "Epoch : 5597 Loss: 3.352249179035425 Training error :0.6463090777397156\n",
      "Epoch : 5598 Loss: 3.35221341624856 Training error :0.646301805973053\n",
      "Epoch : 5599 Loss: 3.3521892447024584 Training error :0.6462969183921814\n",
      "Epoch : 5600 Loss: 3.3521330319344997 Training error :0.6462884545326233\n",
      "Epoch : 5601 Loss: 3.352100981399417 Training error :0.6462793946266174\n",
      "Epoch : 5602 Loss: 3.3520642668008804 Training error :0.6462721824645996\n",
      "Epoch : 5603 Loss: 3.3520175367593765 Training error :0.6462650299072266\n",
      "Epoch : 5604 Loss: 3.351992480456829 Training error :0.6462578773498535\n",
      "Epoch : 5605 Loss: 3.3519592210650444 Training error :0.6462483406066895\n",
      "Epoch : 5606 Loss: 3.3519112318754196 Training error :0.6462390422821045\n",
      "Epoch : 5607 Loss: 3.3518756851553917 Training error :0.6462348699569702\n",
      "Epoch : 5608 Loss: 3.35182031057775 Training error :0.6462252736091614\n",
      "Epoch : 5609 Loss: 3.351786579936743 Training error :0.6462161540985107\n",
      "Epoch : 5610 Loss: 3.351751623675227 Training error :0.6462113261222839\n",
      "Epoch : 5611 Loss: 3.3517027385532856 Training error :0.6462018489837646\n",
      "Epoch : 5612 Loss: 3.351677408441901 Training error :0.6461938619613647\n",
      "Epoch : 5613 Loss: 3.3516260348260403 Training error :0.646187424659729\n",
      "Epoch : 5614 Loss: 3.3515872843563557 Training error :0.6461808681488037\n",
      "Epoch : 5615 Loss: 3.3515614587813616 Training error :0.6461746096611023\n",
      "Epoch : 5616 Loss: 3.3515216317027807 Training error :0.6461655497550964\n",
      "Epoch : 5617 Loss: 3.3514829501509666 Training error :0.6461557745933533\n",
      "Epoch : 5618 Loss: 3.351436845958233 Training error :0.6461486220359802\n",
      "Epoch : 5619 Loss: 3.3513848427683115 Training error :0.6461389064788818\n",
      "Epoch : 5620 Loss: 3.3513483107089996 Training error :0.6461328864097595\n",
      "Epoch : 5621 Loss: 3.3513230085372925 Training error :0.6461277008056641\n",
      "Epoch : 5622 Loss: 3.351280463859439 Training error :0.6461200714111328\n",
      "Epoch : 5623 Loss: 3.3512333631515503 Training error :0.6461116671562195\n",
      "Epoch : 5624 Loss: 3.3512100744992495 Training error :0.6461037993431091\n",
      "Epoch : 5625 Loss: 3.3511472661048174 Training error :0.6460950374603271\n",
      "Epoch : 5626 Loss: 3.351121488958597 Training error :0.6460895538330078\n",
      "Epoch : 5627 Loss: 3.351091966032982 Training error :0.6460817456245422\n",
      "Epoch : 5628 Loss: 3.3510452322661877 Training error :0.6460742950439453\n",
      "Epoch : 5629 Loss: 3.3510132897645235 Training error :0.646064817905426\n",
      "Epoch : 5630 Loss: 3.350954294204712 Training error :0.6460569500923157\n",
      "Epoch : 5631 Loss: 3.3509224709123373 Training error :0.6460506916046143\n",
      "Epoch : 5632 Loss: 3.3508868888020515 Training error :0.6460440158843994\n",
      "Epoch : 5633 Loss: 3.3508613035082817 Training error :0.6460355520248413\n",
      "Epoch : 5634 Loss: 3.350811345502734 Training error :0.6460288763046265\n",
      "Epoch : 5635 Loss: 3.3507863394916058 Training error :0.6460189819335938\n",
      "Epoch : 5636 Loss: 3.3507253620773554 Training error :0.6460098028182983\n",
      "Epoch : 5637 Loss: 3.350690111517906 Training error :0.6460066437721252\n",
      "Epoch : 5638 Loss: 3.3506513703614473 Training error :0.6459980607032776\n",
      "Epoch : 5639 Loss: 3.350610764697194 Training error :0.6459895968437195\n",
      "Epoch : 5640 Loss: 3.3505839332938194 Training error :0.6459829807281494\n",
      "Epoch : 5641 Loss: 3.350538693368435 Training error :0.6459731459617615\n",
      "Epoch : 5642 Loss: 3.3504892084747553 Training error :0.6459670066833496\n",
      "Epoch : 5643 Loss: 3.3504605311900377 Training error :0.6459590196609497\n",
      "Epoch : 5644 Loss: 3.350423565134406 Training error :0.6459514498710632\n",
      "Epoch : 5645 Loss: 3.350381899625063 Training error :0.6459454894065857\n",
      "Epoch : 5646 Loss: 3.3503579646348953 Training error :0.6459358930587769\n",
      "Epoch : 5647 Loss: 3.350316420197487 Training error :0.6459279656410217\n",
      "Epoch : 5648 Loss: 3.3502625711262226 Training error :0.6459235548973083\n",
      "Epoch : 5649 Loss: 3.3502377662807703 Training error :0.6459153890609741\n",
      "Epoch : 5650 Loss: 3.3502021804451942 Training error :0.645905613899231\n",
      "Epoch : 5651 Loss: 3.3501423373818398 Training error :0.6458988785743713\n",
      "Epoch : 5652 Loss: 3.3501088339835405 Training error :0.6458906531333923\n",
      "Epoch : 5653 Loss: 3.3500622771680355 Training error :0.645884096622467\n",
      "Epoch : 5654 Loss: 3.350026736035943 Training error :0.6458762884140015\n",
      "Epoch : 5655 Loss: 3.350000562146306 Training error :0.645867109298706\n",
      "Epoch : 5656 Loss: 3.349955014884472 Training error :0.6458629369735718\n",
      "Epoch : 5657 Loss: 3.3499157335609198 Training error :0.6458547115325928\n",
      "Epoch : 5658 Loss: 3.3498854525387287 Training error :0.6458464860916138\n",
      "Epoch : 5659 Loss: 3.3498321771621704 Training error :0.6458393335342407\n",
      "Epoch : 5660 Loss: 3.349800579249859 Training error :0.6458297967910767\n",
      "Epoch : 5661 Loss: 3.3497862815856934 Training error :0.6458225250244141\n",
      "Epoch : 5662 Loss: 3.3497331012040377 Training error :0.6458151340484619\n",
      "Epoch : 5663 Loss: 3.349680334329605 Training error :0.6458074450492859\n",
      "Epoch : 5664 Loss: 3.3496549762785435 Training error :0.645801305770874\n",
      "Epoch : 5665 Loss: 3.349600838497281 Training error :0.6457927227020264\n",
      "Epoch : 5666 Loss: 3.349578795954585 Training error :0.6457833051681519\n",
      "Epoch : 5667 Loss: 3.349543271586299 Training error :0.6457788348197937\n",
      "Epoch : 5668 Loss: 3.3494951725006104 Training error :0.645771861076355\n",
      "Epoch : 5669 Loss: 3.3494643289595842 Training error :0.645761251449585\n",
      "Epoch : 5670 Loss: 3.3493928126990795 Training error :0.6457560062408447\n",
      "Epoch : 5671 Loss: 3.349372636526823 Training error :0.6457481980323792\n",
      "Epoch : 5672 Loss: 3.349353240802884 Training error :0.6457414031028748\n",
      "Epoch : 5673 Loss: 3.3493012078106403 Training error :0.6457350850105286\n",
      "Epoch : 5674 Loss: 3.3492723274976015 Training error :0.6457238793373108\n",
      "Epoch : 5675 Loss: 3.3492257073521614 Training error :0.6457183957099915\n",
      "Epoch : 5676 Loss: 3.349158190190792 Training error :0.6457093954086304\n",
      "Epoch : 5677 Loss: 3.3491439316421747 Training error :0.6457006335258484\n",
      "Epoch : 5678 Loss: 3.3491214960813522 Training error :0.6456957459449768\n",
      "Epoch : 5679 Loss: 3.349066596478224 Training error :0.6456887125968933\n",
      "Epoch : 5680 Loss: 3.349040834233165 Training error :0.6456789374351501\n",
      "Epoch : 5681 Loss: 3.3490029498934746 Training error :0.6456729769706726\n",
      "Epoch : 5682 Loss: 3.3489478304982185 Training error :0.6456644535064697\n",
      "Epoch : 5683 Loss: 3.348924696445465 Training error :0.6456605792045593\n",
      "Epoch : 5684 Loss: 3.3488786183297634 Training error :0.6456498503684998\n",
      "Epoch : 5685 Loss: 3.348842542618513 Training error :0.6456435918807983\n",
      "Epoch : 5686 Loss: 3.3488117419183254 Training error :0.6456374526023865\n",
      "Epoch : 5687 Loss: 3.3487519808113575 Training error :0.6456276774406433\n",
      "Epoch : 5688 Loss: 3.348727049306035 Training error :0.6456162333488464\n",
      "Epoch : 5689 Loss: 3.3486863300204277 Training error :0.6456111073493958\n",
      "Epoch : 5690 Loss: 3.3486447762697935 Training error :0.6456072330474854\n",
      "Epoch : 5691 Loss: 3.3486212380230427 Training error :0.645598292350769\n",
      "Epoch : 5692 Loss: 3.3485771026462317 Training error :0.6455913782119751\n",
      "Epoch : 5693 Loss: 3.3485221583396196 Training error :0.645582377910614\n",
      "Epoch : 5694 Loss: 3.348507434129715 Training error :0.645577609539032\n",
      "Epoch : 5695 Loss: 3.348472870886326 Training error :0.6455684900283813\n",
      "Epoch : 5696 Loss: 3.348422883078456 Training error :0.645561695098877\n",
      "Epoch : 5697 Loss: 3.3483867924660444 Training error :0.6455538272857666\n",
      "Epoch : 5698 Loss: 3.348349004983902 Training error :0.6455439925193787\n",
      "Epoch : 5699 Loss: 3.348289893940091 Training error :0.6455366611480713\n",
      "Epoch : 5700 Loss: 3.3482650481164455 Training error :0.6455301642417908\n",
      "Epoch : 5701 Loss: 3.348223738372326 Training error :0.6455231308937073\n",
      "Epoch : 5702 Loss: 3.348188016563654 Training error :0.645514726638794\n",
      "Epoch : 5703 Loss: 3.3481541462242603 Training error :0.6455085873603821\n",
      "Epoch : 5704 Loss: 3.348094128072262 Training error :0.6454986333847046\n",
      "Epoch : 5705 Loss: 3.3480835147202015 Training error :0.645493745803833\n",
      "Epoch : 5706 Loss: 3.3480428475886583 Training error :0.6454854011535645\n",
      "Epoch : 5707 Loss: 3.347989307716489 Training error :0.6454775929450989\n",
      "Epoch : 5708 Loss: 3.347971634939313 Training error :0.6454715132713318\n",
      "Epoch : 5709 Loss: 3.347929110750556 Training error :0.645464301109314\n",
      "Epoch : 5710 Loss: 3.347872545942664 Training error :0.6454527378082275\n",
      "Epoch : 5711 Loss: 3.3478439673781395 Training error :0.6454479694366455\n",
      "Epoch : 5712 Loss: 3.3478220012038946 Training error :0.6454405784606934\n",
      "Epoch : 5713 Loss: 3.347777396440506 Training error :0.6454328894615173\n",
      "Epoch : 5714 Loss: 3.3477325700223446 Training error :0.6454257369041443\n",
      "Epoch : 5715 Loss: 3.347691947594285 Training error :0.6454164385795593\n",
      "Epoch : 5716 Loss: 3.3476575519889593 Training error :0.6454113125801086\n",
      "Epoch : 5717 Loss: 3.3476277366280556 Training error :0.6454031467437744\n",
      "Epoch : 5718 Loss: 3.3475791718810797 Training error :0.6453936100006104\n",
      "Epoch : 5719 Loss: 3.3475448079407215 Training error :0.6453884243965149\n",
      "Epoch : 5720 Loss: 3.347502676770091 Training error :0.6453816294670105\n",
      "Epoch : 5721 Loss: 3.347459115087986 Training error :0.6453720927238464\n",
      "Epoch : 5722 Loss: 3.3474254850298166 Training error :0.6453649401664734\n",
      "Epoch : 5723 Loss: 3.3473863564431667 Training error :0.6453571915626526\n",
      "Epoch : 5724 Loss: 3.3473672568798065 Training error :0.6453513503074646\n",
      "Epoch : 5725 Loss: 3.347323577851057 Training error :0.6453433036804199\n",
      "Epoch : 5726 Loss: 3.3472707755863667 Training error :0.6453350186347961\n",
      "Epoch : 5727 Loss: 3.347218818962574 Training error :0.645328164100647\n",
      "Epoch : 5728 Loss: 3.347189949825406 Training error :0.6453202962875366\n",
      "Epoch : 5729 Loss: 3.3471560701727867 Training error :0.6453124284744263\n",
      "Epoch : 5730 Loss: 3.3471274450421333 Training error :0.6453063488006592\n",
      "Epoch : 5731 Loss: 3.3470832128077745 Training error :0.6453003883361816\n",
      "Epoch : 5732 Loss: 3.3470453992486 Training error :0.6452913880348206\n",
      "Epoch : 5733 Loss: 3.3469943273812532 Training error :0.645282506942749\n",
      "Epoch : 5734 Loss: 3.346973454579711 Training error :0.645272970199585\n",
      "Epoch : 5735 Loss: 3.3469272032380104 Training error :0.6452697515487671\n",
      "Epoch : 5736 Loss: 3.346891086548567 Training error :0.6452606320381165\n",
      "Epoch : 5737 Loss: 3.3468563612550497 Training error :0.6452530026435852\n",
      "Epoch : 5738 Loss: 3.3468108773231506 Training error :0.645246684551239\n",
      "Epoch : 5739 Loss: 3.3467717114835978 Training error :0.6452388167381287\n",
      "Epoch : 5740 Loss: 3.3467483799904585 Training error :0.6452290415763855\n",
      "Epoch : 5741 Loss: 3.346700483933091 Training error :0.6452246308326721\n",
      "Epoch : 5742 Loss: 3.3466573245823383 Training error :0.6452175974845886\n",
      "Epoch : 5743 Loss: 3.3466332219541073 Training error :0.6452118754386902\n",
      "Epoch : 5744 Loss: 3.3465888276696205 Training error :0.6452007293701172\n",
      "Epoch : 5745 Loss: 3.346555443480611 Training error :0.6451935768127441\n",
      "Epoch : 5746 Loss: 3.3465044517070055 Training error :0.6451871395111084\n",
      "Epoch : 5747 Loss: 3.346483336761594 Training error :0.6451771259307861\n",
      "Epoch : 5748 Loss: 3.3464360423386097 Training error :0.645170271396637\n",
      "Epoch : 5749 Loss: 3.3463839795440435 Training error :0.6451644897460938\n",
      "Epoch : 5750 Loss: 3.3463467992842197 Training error :0.6451566815376282\n",
      "Epoch : 5751 Loss: 3.3463368602097034 Training error :0.6451500058174133\n",
      "Epoch : 5752 Loss: 3.3462845496833324 Training error :0.6451443433761597\n",
      "Epoch : 5753 Loss: 3.346698835492134 Training error :0.6450811624526978\n",
      "Epoch : 5754 Loss: 3.3465435914695263 Training error :0.6451208591461182\n",
      "Epoch : 5755 Loss: 3.3461334835737944 Training error :0.6451301574707031\n",
      "Epoch : 5756 Loss: 3.3460476119071245 Training error :0.6451078653335571\n",
      "Epoch : 5757 Loss: 3.346106680110097 Training error :0.645094633102417\n",
      "Epoch : 5758 Loss: 3.346067978069186 Training error :0.6450923085212708\n",
      "Epoch : 5759 Loss: 3.3459880594164133 Training error :0.6450889706611633\n",
      "Epoch : 5760 Loss: 3.3459597378969193 Training error :0.6450785994529724\n",
      "Epoch : 5761 Loss: 3.345925237983465 Training error :0.6450709700584412\n",
      "Epoch : 5762 Loss: 3.345895618200302 Training error :0.6450653672218323\n",
      "Epoch : 5763 Loss: 3.3458613324910402 Training error :0.6450592279434204\n",
      "Epoch : 5764 Loss: 3.3458171021193266 Training error :0.6450512409210205\n",
      "Epoch : 5765 Loss: 3.345776379108429 Training error :0.645041823387146\n",
      "Epoch : 5766 Loss: 3.345730699598789 Training error :0.6450350880622864\n",
      "Epoch : 5767 Loss: 3.345694875344634 Training error :0.6450306177139282\n",
      "Epoch : 5768 Loss: 3.3456689100712538 Training error :0.645022988319397\n",
      "Epoch : 5769 Loss: 3.345628574490547 Training error :0.6450132727622986\n",
      "Epoch : 5770 Loss: 3.3456024676561356 Training error :0.6450070738792419\n",
      "Epoch : 5771 Loss: 3.345560658723116 Training error :0.6449990272521973\n",
      "Epoch : 5772 Loss: 3.3455207608640194 Training error :0.6449941396713257\n",
      "Epoch : 5773 Loss: 3.345470802858472 Training error :0.6449854969978333\n",
      "Epoch : 5774 Loss: 3.3454570211470127 Training error :0.6449791193008423\n",
      "Epoch : 5775 Loss: 3.345415160059929 Training error :0.6449716091156006\n",
      "Epoch : 5776 Loss: 3.34536893106997 Training error :0.6449614763259888\n",
      "Epoch : 5777 Loss: 3.345343519002199 Training error :0.6449537873268127\n",
      "Epoch : 5778 Loss: 3.3452901504933834 Training error :0.6449482440948486\n",
      "Epoch : 5779 Loss: 3.3452505581080914 Training error :0.6449415683746338\n",
      "Epoch : 5780 Loss: 3.345220286399126 Training error :0.644931435585022\n",
      "Epoch : 5781 Loss: 3.3451900612562895 Training error :0.6449280381202698\n",
      "Epoch : 5782 Loss: 3.3451486565172672 Training error :0.6449196338653564\n",
      "Epoch : 5783 Loss: 3.3451087046414614 Training error :0.6449135541915894\n",
      "Epoch : 5784 Loss: 3.345045790076256 Training error :0.6449036598205566\n",
      "Epoch : 5785 Loss: 3.345026645809412 Training error :0.6448960304260254\n",
      "Epoch : 5786 Loss: 3.344986166805029 Training error :0.6448889374732971\n",
      "Epoch : 5787 Loss: 3.34494798630476 Training error :0.6448808908462524\n",
      "Epoch : 5788 Loss: 3.344919838011265 Training error :0.6448730230331421\n",
      "Epoch : 5789 Loss: 3.344876855611801 Training error :0.6448677182197571\n",
      "Epoch : 5790 Loss: 3.3448178935796022 Training error :0.6448577642440796\n",
      "Epoch : 5791 Loss: 3.344816830009222 Training error :0.6448527574539185\n",
      "Epoch : 5792 Loss: 3.3447549678385258 Training error :0.6448444128036499\n",
      "Epoch : 5793 Loss: 3.344724966213107 Training error :0.6448372006416321\n",
      "Epoch : 5794 Loss: 3.344685636460781 Training error :0.6448306441307068\n",
      "Epoch : 5795 Loss: 3.3446417953819036 Training error :0.6448236107826233\n",
      "Epoch : 5796 Loss: 3.344623703509569 Training error :0.6448149085044861\n",
      "Epoch : 5797 Loss: 3.3445667270570993 Training error :0.644809901714325\n",
      "Epoch : 5798 Loss: 3.3445436786860228 Training error :0.6448005437850952\n",
      "Epoch : 5799 Loss: 3.344511665403843 Training error :0.6447917222976685\n",
      "Epoch : 5800 Loss: 3.3444565646350384 Training error :0.6447830200195312\n",
      "Epoch : 5801 Loss: 3.344407605007291 Training error :0.6447793841362\n",
      "Epoch : 5802 Loss: 3.3443969413638115 Training error :0.6447721123695374\n",
      "Epoch : 5803 Loss: 3.344349894672632 Training error :0.6447641849517822\n",
      "Epoch : 5804 Loss: 3.3443094305694103 Training error :0.6447548866271973\n",
      "Epoch : 5805 Loss: 3.344282116740942 Training error :0.6447489857673645\n",
      "Epoch : 5806 Loss: 3.3442316092550755 Training error :0.6447417736053467\n",
      "Epoch : 5807 Loss: 3.3441957402974367 Training error :0.6447315812110901\n",
      "Epoch : 5808 Loss: 3.3442279547452927 Training error :0.6447085738182068\n",
      "Epoch : 5809 Loss: 3.344240617007017 Training error :0.6446822881698608\n",
      "Epoch : 5810 Loss: 3.3441898431628942 Training error :0.6446640491485596\n",
      "Epoch : 5811 Loss: 3.3440773598849773 Training error :0.6446409821510315\n",
      "Epoch : 5812 Loss: 3.34398852661252 Training error :0.6446189880371094\n",
      "Epoch : 5813 Loss: 3.3438855446875095 Training error :0.6445999145507812\n",
      "Epoch : 5814 Loss: 3.343786356970668 Training error :0.6445769667625427\n",
      "Epoch : 5815 Loss: 3.343674972653389 Training error :0.6445555686950684\n",
      "Epoch : 5816 Loss: 3.343584965914488 Training error :0.6445363759994507\n",
      "Epoch : 5817 Loss: 3.34345755726099 Training error :0.644514799118042\n",
      "Epoch : 5818 Loss: 3.343362905085087 Training error :0.6444966197013855\n",
      "Epoch : 5819 Loss: 3.34324773773551 Training error :0.6444743871688843\n",
      "Epoch : 5820 Loss: 3.343154478818178 Training error :0.6444524526596069\n",
      "Epoch : 5821 Loss: 3.343056218698621 Training error :0.644432008266449\n",
      "Epoch : 5822 Loss: 3.3429519068449736 Training error :0.6444107294082642\n",
      "Epoch : 5823 Loss: 3.3428733237087727 Training error :0.6443892121315002\n",
      "Epoch : 5824 Loss: 3.342753581702709 Training error :0.6443705558776855\n",
      "Epoch : 5825 Loss: 3.34263763576746 Training error :0.6443495750427246\n",
      "Epoch : 5826 Loss: 3.342573458328843 Training error :0.6443304419517517\n",
      "Epoch : 5827 Loss: 3.3424630146473646 Training error :0.644309401512146\n",
      "Epoch : 5828 Loss: 3.342358846217394 Training error :0.644288182258606\n",
      "Epoch : 5829 Loss: 3.3422630093991756 Training error :0.6442703008651733\n",
      "Epoch : 5830 Loss: 3.3421544898301363 Training error :0.644249439239502\n",
      "Epoch : 5831 Loss: 3.3420845698565245 Training error :0.6442275047302246\n",
      "Epoch : 5832 Loss: 3.341973192989826 Training error :0.6442105174064636\n",
      "Epoch : 5833 Loss: 3.3418783731758595 Training error :0.6441900134086609\n",
      "Epoch : 5834 Loss: 3.341804414987564 Training error :0.6441689133644104\n",
      "Epoch : 5835 Loss: 3.341698056086898 Training error :0.6441486477851868\n",
      "Epoch : 5836 Loss: 3.3415756542235613 Training error :0.6441268920898438\n",
      "Epoch : 5837 Loss: 3.341481324285269 Training error :0.6441099643707275\n",
      "Epoch : 5838 Loss: 3.341385442763567 Training error :0.6440880298614502\n",
      "Epoch : 5839 Loss: 3.3412960190325975 Training error :0.6440686583518982\n",
      "Epoch : 5840 Loss: 3.3412127178162336 Training error :0.644051730632782\n",
      "Epoch : 5841 Loss: 3.341503079980612 Training error :0.6439816951751709\n",
      "Epoch : 5842 Loss: 3.341348709538579 Training error :0.6440094113349915\n",
      "Epoch : 5843 Loss: 3.340893153101206 Training error :0.6440058350563049\n",
      "Epoch : 5844 Loss: 3.3407593574374914 Training error :0.643968403339386\n",
      "Epoch : 5845 Loss: 3.34073743596673 Training error :0.6439465284347534\n",
      "Epoch : 5846 Loss: 3.340622615069151 Training error :0.6439368724822998\n",
      "Epoch : 5847 Loss: 3.3405108861625195 Training error :0.6439189314842224\n",
      "Epoch : 5848 Loss: 3.3404341265559196 Training error :0.6438955068588257\n",
      "Epoch : 5849 Loss: 3.3403519056737423 Training error :0.6438748836517334\n",
      "Epoch : 5850 Loss: 3.3402546159923077 Training error :0.6438589692115784\n",
      "Epoch : 5851 Loss: 3.3401703629642725 Training error :0.6438378691673279\n",
      "Epoch : 5852 Loss: 3.34005518630147 Training error :0.6438180804252625\n",
      "Epoch : 5853 Loss: 3.339976206421852 Training error :0.6438031792640686\n",
      "Epoch : 5854 Loss: 3.3399040326476097 Training error :0.6437814831733704\n",
      "Epoch : 5855 Loss: 3.3398113176226616 Training error :0.643764317035675\n",
      "Epoch : 5856 Loss: 3.3397061340510845 Training error :0.6437431573867798\n",
      "Epoch : 5857 Loss: 3.3396161291748285 Training error :0.6437209844589233\n",
      "Epoch : 5858 Loss: 3.339518815279007 Training error :0.6437056660652161\n",
      "Epoch : 5859 Loss: 3.3394244238734245 Training error :0.6436870098114014\n",
      "Epoch : 5860 Loss: 3.3393341340124607 Training error :0.6436682343482971\n",
      "Epoch : 5861 Loss: 3.3392643351107836 Training error :0.6436494588851929\n",
      "Epoch : 5862 Loss: 3.339149136096239 Training error :0.6436301469802856\n",
      "Epoch : 5863 Loss: 3.339078087359667 Training error :0.6436139941215515\n",
      "Epoch : 5864 Loss: 3.338977750390768 Training error :0.643593430519104\n",
      "Epoch : 5865 Loss: 3.3388580605387688 Training error :0.6435745358467102\n",
      "Epoch : 5866 Loss: 3.33879985101521 Training error :0.6435559988021851\n",
      "Epoch : 5867 Loss: 3.3386981785297394 Training error :0.6435377597808838\n",
      "Epoch : 5868 Loss: 3.3386017978191376 Training error :0.6435179114341736\n",
      "Epoch : 5869 Loss: 3.338513370603323 Training error :0.6434991955757141\n",
      "Epoch : 5870 Loss: 3.3384222723543644 Training error :0.6434814929962158\n",
      "Epoch : 5871 Loss: 3.3383314833045006 Training error :0.6434610486030579\n",
      "Epoch : 5872 Loss: 3.33822762966156 Training error :0.6434440612792969\n",
      "Epoch : 5873 Loss: 3.338154535740614 Training error :0.6434282064437866\n",
      "Epoch : 5874 Loss: 3.3380698207765818 Training error :0.6434068083763123\n",
      "Epoch : 5875 Loss: 3.3379669077694416 Training error :0.6433885097503662\n",
      "Epoch : 5876 Loss: 3.3378715496510267 Training error :0.6433696746826172\n",
      "Epoch : 5877 Loss: 3.3377831168472767 Training error :0.6433538198471069\n",
      "Epoch : 5878 Loss: 3.3376815784722567 Training error :0.643334150314331\n",
      "Epoch : 5879 Loss: 3.33759306371212 Training error :0.6433144211769104\n",
      "Epoch : 5880 Loss: 3.3375194277614355 Training error :0.6432985663414001\n",
      "Epoch : 5881 Loss: 3.3374208584427834 Training error :0.6432814002037048\n",
      "Epoch : 5882 Loss: 3.3377166762948036 Training error :0.6432093977928162\n",
      "Epoch : 5883 Loss: 3.337557405233383 Training error :0.6432359218597412\n",
      "Epoch : 5884 Loss: 3.337137872353196 Training error :0.6432361602783203\n",
      "Epoch : 5885 Loss: 3.3369531240314245 Training error :0.6432042121887207\n",
      "Epoch : 5886 Loss: 3.3369968701153994 Training error :0.6431816816329956\n",
      "Epoch : 5887 Loss: 3.336898596957326 Training error :0.6431693434715271\n",
      "Epoch : 5888 Loss: 3.33678213134408 Training error :0.6431515216827393\n",
      "Epoch : 5889 Loss: 3.336695432662964 Training error :0.6431332230567932\n",
      "Epoch : 5890 Loss: 3.336622975766659 Training error :0.6431159973144531\n",
      "Epoch : 5891 Loss: 3.336524283513427 Training error :0.6430971622467041\n",
      "Epoch : 5892 Loss: 3.336427640169859 Training error :0.6430789828300476\n",
      "Epoch : 5893 Loss: 3.336350319907069 Training error :0.6430639028549194\n",
      "Epoch : 5894 Loss: 3.336265677586198 Training error :0.6430462002754211\n",
      "Epoch : 5895 Loss: 3.33617708273232 Training error :0.643025279045105\n",
      "Epoch : 5896 Loss: 3.336081186309457 Training error :0.6430096626281738\n",
      "Epoch : 5897 Loss: 3.3359793853014708 Training error :0.6429920792579651\n",
      "Epoch : 5898 Loss: 3.335893027484417 Training error :0.642975389957428\n",
      "Epoch : 5899 Loss: 3.335822768509388 Training error :0.6429567933082581\n",
      "Epoch : 5900 Loss: 3.335711093619466 Training error :0.6429396271705627\n",
      "Epoch : 5901 Loss: 3.335652120411396 Training error :0.642921507358551\n",
      "Epoch : 5902 Loss: 3.3355617858469486 Training error :0.6429032683372498\n",
      "Epoch : 5903 Loss: 3.3354560397565365 Training error :0.6428852081298828\n",
      "Epoch : 5904 Loss: 3.33537751249969 Training error :0.6428691148757935\n",
      "Epoch : 5905 Loss: 3.335288729518652 Training error :0.6428478360176086\n",
      "Epoch : 5906 Loss: 3.335193293169141 Training error :0.6428320407867432\n",
      "Epoch : 5907 Loss: 3.3351268898695707 Training error :0.6428169012069702\n",
      "Epoch : 5908 Loss: 3.335052950307727 Training error :0.6427981853485107\n",
      "Epoch : 5909 Loss: 3.334949241951108 Training error :0.6427829265594482\n",
      "Epoch : 5910 Loss: 3.334873514249921 Training error :0.6427636742591858\n",
      "Epoch : 5911 Loss: 3.3347848746925592 Training error :0.6427468657493591\n",
      "Epoch : 5912 Loss: 3.334698535501957 Training error :0.6427310705184937\n",
      "Epoch : 5913 Loss: 3.334617540240288 Training error :0.642712414264679\n",
      "Epoch : 5914 Loss: 3.3345303554087877 Training error :0.6426944136619568\n",
      "Epoch : 5915 Loss: 3.334455816075206 Training error :0.6426796913146973\n",
      "Epoch : 5916 Loss: 3.334370892494917 Training error :0.6426593661308289\n",
      "Epoch : 5917 Loss: 3.3342842925339937 Training error :0.642641007900238\n",
      "Epoch : 5918 Loss: 3.334193117916584 Training error :0.6426244378089905\n",
      "Epoch : 5919 Loss: 3.334090607240796 Training error :0.6426096558570862\n",
      "Epoch : 5920 Loss: 3.3340274896472692 Training error :0.6425923109054565\n",
      "Epoch : 5921 Loss: 3.3339471854269505 Training error :0.6425743103027344\n",
      "Epoch : 5922 Loss: 3.333856489509344 Training error :0.6425565481185913\n",
      "Epoch : 5923 Loss: 3.3337839264422655 Training error :0.6425400376319885\n",
      "Epoch : 5924 Loss: 3.33408578671515 Training error :0.6424754858016968\n",
      "Epoch : 5925 Loss: 3.3339379858225584 Training error :0.6424990892410278\n",
      "Epoch : 5926 Loss: 3.3335345704108477 Training error :0.6425011157989502\n",
      "Epoch : 5927 Loss: 3.3333536963909864 Training error :0.6424667835235596\n",
      "Epoch : 5928 Loss: 3.3334062974900007 Training error :0.6424487829208374\n",
      "Epoch : 5929 Loss: 3.333323422819376 Training error :0.6424374580383301\n",
      "Epoch : 5930 Loss: 3.333189018070698 Training error :0.6424224972724915\n",
      "Epoch : 5931 Loss: 3.3330899998545647 Training error :0.6424038410186768\n",
      "Epoch : 5932 Loss: 3.333024987950921 Training error :0.642386794090271\n",
      "Epoch : 5933 Loss: 3.332948811352253 Training error :0.6423702836036682\n",
      "Epoch : 5934 Loss: 3.3328722640872 Training error :0.642352283000946\n",
      "Epoch : 5935 Loss: 3.33277122490108 Training error :0.6423380374908447\n",
      "Epoch : 5936 Loss: 3.332708064466715 Training error :0.6423203349113464\n",
      "Epoch : 5937 Loss: 3.3326351772993803 Training error :0.6423057317733765\n",
      "Epoch : 5938 Loss: 3.3325256519019604 Training error :0.6422868967056274\n",
      "Epoch : 5939 Loss: 3.3324698228389025 Training error :0.6422708630561829\n",
      "Epoch : 5940 Loss: 3.3323847614228725 Training error :0.6422533392906189\n",
      "Epoch : 5941 Loss: 3.332286324352026 Training error :0.6422359943389893\n",
      "Epoch : 5942 Loss: 3.3322084061801434 Training error :0.6422226428985596\n",
      "Epoch : 5943 Loss: 3.3321363050490618 Training error :0.6422046422958374\n",
      "Epoch : 5944 Loss: 3.3320528492331505 Training error :0.6421873569488525\n",
      "Epoch : 5945 Loss: 3.3319611866027117 Training error :0.6421711444854736\n",
      "Epoch : 5946 Loss: 3.331880060955882 Training error :0.642153799533844\n",
      "Epoch : 5947 Loss: 3.331815470010042 Training error :0.6421363949775696\n",
      "Epoch : 5948 Loss: 3.3317295368760824 Training error :0.6421217918395996\n",
      "Epoch : 5949 Loss: 3.331650622189045 Training error :0.6421075463294983\n",
      "Epoch : 5950 Loss: 3.3315527103841305 Training error :0.6420890092849731\n",
      "Epoch : 5951 Loss: 3.331478279083967 Training error :0.6420710682868958\n",
      "Epoch : 5952 Loss: 3.331419374793768 Training error :0.6420541405677795\n",
      "Epoch : 5953 Loss: 3.33132291957736 Training error :0.6420389413833618\n",
      "Epoch : 5954 Loss: 3.3312558978796005 Training error :0.6420235633850098\n",
      "Epoch : 5955 Loss: 3.3311854619532824 Training error :0.6420064568519592\n",
      "Epoch : 5956 Loss: 3.3310866206884384 Training error :0.6419898867607117\n",
      "Epoch : 5957 Loss: 3.331012509763241 Training error :0.6419758796691895\n",
      "Epoch : 5958 Loss: 3.3309459760785103 Training error :0.6419573426246643\n",
      "Epoch : 5959 Loss: 3.330868750810623 Training error :0.6419402360916138\n",
      "Epoch : 5960 Loss: 3.330772589892149 Training error :0.6419263482093811\n",
      "Epoch : 5961 Loss: 3.3306956961750984 Training error :0.6419108510017395\n",
      "Epoch : 5962 Loss: 3.330613600090146 Training error :0.6418942213058472\n",
      "Epoch : 5963 Loss: 3.330531731247902 Training error :0.641874372959137\n",
      "Epoch : 5964 Loss: 3.3304333221167326 Training error :0.6418607234954834\n",
      "Epoch : 5965 Loss: 3.3303882032632828 Training error :0.6418430805206299\n",
      "Epoch : 5966 Loss: 3.33028918877244 Training error :0.6418297290802002\n",
      "Epoch : 5967 Loss: 3.3302137833088636 Training error :0.6418126821517944\n",
      "Epoch : 5968 Loss: 3.3301381301134825 Training error :0.6417985558509827\n",
      "Epoch : 5969 Loss: 3.330451400950551 Training error :0.6417341828346252\n",
      "Epoch : 5970 Loss: 3.3303031139075756 Training error :0.641761839389801\n",
      "Epoch : 5971 Loss: 3.329871414229274 Training error :0.6417601108551025\n",
      "Epoch : 5972 Loss: 3.3297248482704163 Training error :0.6417301893234253\n",
      "Epoch : 5973 Loss: 3.329762602224946 Training error :0.6417089700698853\n",
      "Epoch : 5974 Loss: 3.329695673659444 Training error :0.6417002081871033\n",
      "Epoch : 5975 Loss: 3.329572716727853 Training error :0.6416863203048706\n",
      "Epoch : 5976 Loss: 3.32948212698102 Training error :0.6416702270507812\n",
      "Epoch : 5977 Loss: 3.3294107541441917 Training error :0.6416530013084412\n",
      "Epoch : 5978 Loss: 3.3293542098253965 Training error :0.6416399478912354\n",
      "Epoch : 5979 Loss: 3.3292693067342043 Training error :0.641623854637146\n",
      "Epoch : 5980 Loss: 3.3292045313864946 Training error :0.6416079998016357\n",
      "Epoch : 5981 Loss: 3.3291418477892876 Training error :0.641592264175415\n",
      "Epoch : 5982 Loss: 3.3290349934250116 Training error :0.6415761709213257\n",
      "Epoch : 5983 Loss: 3.328979082405567 Training error :0.6415632367134094\n",
      "Epoch : 5984 Loss: 3.32888082228601 Training error :0.6415481567382812\n",
      "Epoch : 5985 Loss: 3.328807920217514 Training error :0.6415272355079651\n",
      "Epoch : 5986 Loss: 3.3287374544888735 Training error :0.6415141224861145\n",
      "Epoch : 5987 Loss: 3.3286696262657642 Training error :0.6414998769760132\n",
      "Epoch : 5988 Loss: 3.328590426594019 Training error :0.6414858102798462\n",
      "Epoch : 5989 Loss: 3.328520007431507 Training error :0.6414667367935181\n",
      "Epoch : 5990 Loss: 3.328425047919154 Training error :0.6414528489112854\n",
      "Epoch : 5991 Loss: 3.328355384990573 Training error :0.6414389610290527\n",
      "Epoch : 5992 Loss: 3.3282786440104246 Training error :0.641421377658844\n",
      "Epoch : 5993 Loss: 3.328215854242444 Training error :0.6414054036140442\n",
      "Epoch : 5994 Loss: 3.3281211741268635 Training error :0.6413947343826294\n",
      "Epoch : 5995 Loss: 3.3280542362481356 Training error :0.6413758993148804\n",
      "Epoch : 5996 Loss: 3.3279728200286627 Training error :0.641358494758606\n",
      "Epoch : 5997 Loss: 3.3278929758816957 Training error :0.6413449048995972\n",
      "Epoch : 5998 Loss: 3.327826051041484 Training error :0.6413306593894958\n",
      "Epoch : 5999 Loss: 3.3277572467923164 Training error :0.6413164734840393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_of_epochs=6000\n",
    "batch_size=10\n",
    "sum_loss_list=[]\n",
    "\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "\n",
    "    for b in range(0,X_variable.size(0),batch_size):\n",
    "        out = net(X_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, Y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "    \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "    print(\"Epoch :\",t, \"Loss:\",sum_loss, \"Training error :{}\".format((net(X_variable)-Y_variable).pow_(2).sum().div_(Y_variable.shape[0]).data[0]))\n",
    "    sum_loss_list.append(sum_loss)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e86f141c18>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e86f46c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out=net(X_variable)\n",
    "differ=out-Y_variable\n",
    "differ.pow_(2).sum().div_(Y_variable.shape[0]).data[0]\n",
    "plt.scatter(np.arange(0,X_variable.shape[0]),out.data[:,0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##import matplotlib.pyplot as plt\n",
    "##import numpy as np\n",
    "## \n",
    "##x = np.linspace(0, 10*np.pi, 100)\n",
    "##y = np.sin(x)\n",
    "## \n",
    "##plt.ion()\n",
    "##fig = plt.figure()\n",
    "##ax = fig.add_subplot(111)\n",
    "##line1, = ax.plot(x, y, 'b-') \n",
    "## \n",
    "##for phase in np.linspace(0, 10*np.pi, 1000):\n",
    "##    \n",
    "##    line1.set_ydata(np.sin(0.5 * x + phase))\n",
    "##\n",
    "##    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-91.2550289604\n"
     ]
    }
   ],
   "source": [
    "# Testing the network\n",
    "b4=Bouncing_ball(v0=112,theta=55,height=20)\n",
    "\n",
    "for i in range(1): \n",
    "    current_velocity_y4,current_velocity_x4,previous_velocity_x4, previous_velocity_y4,y4,x4,time4=b4.advance_step(0.05)\n",
    "    print(current_velocity_y4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " -90.8953\n",
       "  62.5780\n",
       " [torch.FloatTensor of size 2],\n",
       " -91.255028960367085,\n",
       " 64.240560871317157,\n",
       " 24.575001448018355)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#net.eval()\n",
    "\n",
    "previous_velocity_y=previous_velocity_y4\n",
    "previous_velocity_x=previous_velocity_x4\n",
    "height=y4\n",
    "\n",
    "\n",
    "\n",
    "X_test=np.array([previous_velocity_y,previous_velocity_x,height])\n",
    "X_test_variable=Variable(torch.from_numpy(X_test).type(torch.FloatTensor))\n",
    "out=net(X_test_variable)\n",
    "\n",
    "\n",
    "out,current_velocity_y4,current_velocity_x4,y4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.6052374236\n",
      "84.7307152495\n",
      "165.146233508\n",
      "326.309090704\n",
      "649.300560176\n",
      "1296.61844518\n",
      "-0.583094215393\n",
      "-5.02605361938\n",
      "-13.5988250732\n",
      "-30.2545820236\n",
      "-62.9364732742\n",
      "-127.541042519\n",
      "-255.805198002\n",
      "-511.968071413\n",
      "-1024.45121975\n",
      "-2049.88306875\n",
      "-4101.70195351\n",
      "-8207.25471992\n",
      "-16422.1926178\n",
      "-32859.7368179\n"
     ]
    }
   ],
   "source": [
    "nb_test_steps=20\n",
    "velocities_y=np.empty([nb_test_steps])\n",
    "velocities_x=np.empty([nb_test_steps])\n",
    "y_position=np.empty([nb_test_steps])\n",
    "\n",
    "for i in range(nb_test_steps):\n",
    "    X_test=np.array([previous_velocity_y,previous_velocity_x,height])\n",
    "    X_test_variable=Variable(torch.from_numpy(X_test).type(torch.FloatTensor))\n",
    "    out=net(X_test_variable)\n",
    "    velocities=out.data.numpy()\n",
    "    velocity_y,velocity_x=velocities[0],velocities[1]\n",
    "    velocity_y=velocities[0]\n",
    "    if velocity_y>=0: height-=height+velocity_y*0.05\n",
    "    else: height+=height+velocity_y*0.05 \n",
    "    previous_velocity_y=velocity_y\n",
    "    previous_velocity_x=velocity_x\n",
    "    velocities_y[i]=velocity_y\n",
    "    velocities_x[i]=velocity_x\n",
    "    np.append(y_position,height)\n",
    "    print(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e86ee7fb00>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwBcRiRL/H0to184LoYhLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e86f2d16a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e86eff7da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure=plt.figure()\n",
    "ax1=figure.add_subplot(111)\n",
    "real_velocity=ax1.plot(Y_train_data[:,0][:6],label='Real Velocity')\n",
    "predictions=ax1.plot(velocities_y[:6],label='Predicted Velocities')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "figure=plt.figure()\n",
    "ax2=figure.add_subplot(111)\n",
    "real_velocity=ax2.plot(X_train_data[:,2][:6],'r',label='Real Height')\n",
    "predictions=ax2.plot(y_position[:6],'g',label='Calculated Height')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " -613.9114\n",
       "-1430.5164\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=np.array([previous_velocity_y,previous_velocity_x,height])\n",
    "X_test_variable=Variable(torch.from_numpy(X_test).type(torch.FloatTensor))\n",
    "net(X_test_variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_velocity_y=[]\n",
    "real_velocity_x=[]\n",
    "\n",
    "for i in range(100):\n",
    "    current_velocity_y4,current_velocity_x4,previous_velocity_x4, previous_velocity_y4,y4,x4,time4=b4.advance_step(0.5)\n",
    "    real_velocity_y.append(current_velocity_y4)   \n",
    "    real_velocity_x.append(current_velocity_x4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -90.89530945,  -89.59519196,  -86.30393982,  -79.66752625,\n",
       "        -66.35242462,  -39.65350342,   11.66188431,  -77.19730377,\n",
       "        -70.93435669,  -61.13863754,  -48.54618454,  -33.3619194 ,\n",
       "        -14.46225929,   -7.15350819,  -10.30153847,  -19.61258507,\n",
       "        -38.71632004,  -77.01625824, -153.66355896, -307.03164673])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e86eba16a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(real_velocity_x)\n",
    "velocities_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-310-67247a250eb9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-310-67247a250eb9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    plt.plot(y_position\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "plt.plot(X_train_data[:,2])\n",
    "plt.plot(y_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
