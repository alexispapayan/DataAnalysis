{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/Dimensionality_reduction/')\n",
    "\n",
    "from Deformation_gradient import get_eigenvalues\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "\n",
    "from IPython.core.debugger  import Tracer\n",
    "\n",
    "%matplotlib tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nb_of_triangles=16000\n",
    "nb_of_samples=500\n",
    "\n",
    "Y=np.empty([nb_of_triangles,2])\n",
    "X=np.empty([nb_of_triangles,4])\n",
    "\n",
    "x_test=np.empty([nb_of_samples,4])\n",
    "y_test=np.empty([nb_of_samples,2])\n",
    "#X=np.empty([nb_of_triangles,5])\n",
    "\n",
    "for i in range(nb_of_triangles):\n",
    "    triangle=np.random.normal(0,1,(3,2))\n",
    "    #triangle=np.array([[-1,0],[1,0],[0,sqrt(3)]])\n",
    "    triangle=np.hstack((triangle,[[0],[0],[0]]))\n",
    "    \n",
    "    #fig=plt.figure()\n",
    "    #ax=fig.add_subplot(111)\n",
    "    #plt.triplot(triangle[:,0],triangle[:,1])\n",
    "    \n",
    "    eig,F,U,angle=get_eigenvalues(triangle)\n",
    "    X[i]=F.flatten()\n",
    "    #X[i]=np.append(F.flatten(),angle)\n",
    "    Y[i]=np.array(eig)\n",
    "    \n",
    "for j in range(nb_of_samples):\n",
    "    triangle=np.random.normal(0,1,(3,2))\n",
    "    #triangle=np.array([[-1,0],[1,0],[0,sqrt(3)]])\n",
    "    triangle=np.hstack((triangle,[[0],[0],[0]]))\n",
    "    \n",
    "    #fig=plt.figure()\n",
    "    #ax=fig.add_subplot(111)\n",
    "    #plt.triplot(triangle[:,0],triangle[:,1])\n",
    "    \n",
    "    eig,F,U,angle=get_eigenvalues(triangle)\n",
    "    x_test[j]=F.flatten()\n",
    "    #X[i]=np.append(F.flatten(),angle)\n",
    "    y_test[j]=np.array(eig)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training data\n",
    "x_tensor=torch.from_numpy(X).type(torch.FloatTensor)\n",
    "y_tensor=torch.from_numpy(Y).type(torch.FloatTensor)\n",
    "\n",
    "x_tensor_test=torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "# Convert to pytorch variables\n",
    "x_variable=Variable(x_tensor,requires_grad=False)\n",
    "y_variable=Variable(y_tensor,requires_grad=False)\n",
    "\n",
    "# Convert to pytorch variables\n",
    "x_variable_test=Variable(x_tensor_test,requires_grad=False)\n",
    "y_variable_test=Variable(y_tensor_test,requires_grad=False)\n",
    "\n",
    "# Normalize data\n",
    "#mu,std=x_variable.mean(0),x_variable.std(0)\n",
    "#x_variable.sub_(mu).div_(std)\n",
    "#x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(n_feature, n_hidden,bias=True)   # hidden layer\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc2_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "        self.fc3 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc3_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        self.fc4 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc4_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        self.fc5 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.fc5_bn=torch.nn.BatchNorm1d(n_hidden)\n",
    "\n",
    "        self.fc6 = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        #dropout=nn.Dropout(p=0.5)\n",
    "\n",
    "        x = Func.relu(self.fc1(x))# activation function for hidden layer\n",
    "        #x=dropout(x)\n",
    "        \n",
    "        x = Func.relu(self.fc2(x))\n",
    "        x=self.fc2_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = Func.relu(self.fc3(x))\n",
    "        x=self.fc3_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = Func.relu(self.fc4(x))\n",
    "        x=self.fc4_bn(x)\n",
    "        #x=dropout(x)\n",
    "        x = Func.relu(self.fc5(x))\n",
    "        x=self.fc5_bn(x)        \n",
    "        #x=dropout(x)\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=4, out_features=108)\n",
      "  (fc2): Linear(in_features=108, out_features=108)\n",
      "  (fc2_bn): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc3): Linear(in_features=108, out_features=108)\n",
      "  (fc3_bn): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc4): Linear(in_features=108, out_features=108)\n",
      "  (fc4_bn): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc5): Linear(in_features=108, out_features=108)\n",
      "  (fc5_bn): BatchNorm1d(108, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc6): Linear(in_features=108, out_features=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters #\n",
    "net = Net(n_feature=4, n_hidden=108\n",
    "          , n_output=2)     # define the network\n",
    "print(net)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.SGD(net.parameters(), lr=1e-7,momentum=0.9)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4, weight_decay=0.9)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False)  \n",
    "#loss_func=torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Training loss: 0.93812724635005 Test loss: 13.737913131713867\n",
      "Epoch : 1 Training loss: 1.0649432983249425 Test loss: 23.18625831604004\n",
      "Epoch : 2 Training loss: 2.99012386405468 Test loss: 22.90634536743164\n",
      "Epoch : 3 Training loss: 5.742697579860687 Test loss: 14.670475959777832\n",
      "Epoch : 4 Training loss: 1.7953347301483153 Test loss: 15.280519485473633\n",
      "Epoch : 5 Training loss: 1.2650786291956901 Test loss: 14.20820426940918\n",
      "Epoch : 6 Training loss: 1.1066072690784932 Test loss: 13.984732627868652\n",
      "Epoch : 7 Training loss: 1.034726929873228 Test loss: 14.114143371582031\n",
      "Epoch : 8 Training loss: 0.9814717056453228 Test loss: 14.64558219909668\n",
      "Epoch : 9 Training loss: 0.9455744668543339 Test loss: 14.106894493103027\n",
      "Epoch : 10 Training loss: 0.8913947528004647 Test loss: 13.673588752746582\n",
      "Epoch : 11 Training loss: 0.8583110829591751 Test loss: 13.803516387939453\n",
      "Epoch : 12 Training loss: 0.8339670728445053 Test loss: 13.797268867492676\n",
      "Epoch : 13 Training loss: 0.8531631723344326 Test loss: 13.766899108886719\n",
      "Epoch : 14 Training loss: 0.8770500264167785 Test loss: 13.815248489379883\n",
      "Epoch : 15 Training loss: 1.149428263515234 Test loss: 14.653639793395996\n",
      "Epoch : 16 Training loss: 1.3563986924886704 Test loss: 14.942179679870605\n",
      "Epoch : 17 Training loss: 1.5416915062367915 Test loss: 16.310482025146484\n",
      "Epoch : 18 Training loss: 1.1406070354878903 Test loss: 14.282233238220215\n",
      "Epoch : 19 Training loss: 0.9315867395401001 Test loss: 15.108489990234375\n",
      "Epoch : 20 Training loss: 0.8635308652520179 Test loss: 14.23984432220459\n",
      "Epoch : 21 Training loss: 0.8319712323099375 Test loss: 14.146040916442871\n",
      "Epoch : 22 Training loss: 1.143496621876955 Test loss: 16.45142936706543\n",
      "Epoch : 23 Training loss: 2.168130334496498 Test loss: 14.344158172607422\n",
      "Epoch : 24 Training loss: 1.450331631809473 Test loss: 14.281525611877441\n",
      "Epoch : 25 Training loss: 1.1881638873815537 Test loss: 14.45724105834961\n",
      "Epoch : 26 Training loss: 1.1550629996657371 Test loss: 14.215194702148438\n",
      "Epoch : 27 Training loss: 0.9599932553172111 Test loss: 14.696712493896484\n",
      "Epoch : 28 Training loss: 0.8702332306206226 Test loss: 14.408479690551758\n",
      "Epoch : 29 Training loss: 0.8310452934801579 Test loss: 14.752640724182129\n",
      "Epoch : 30 Training loss: 1.6505494597554207 Test loss: 14.90748119354248\n",
      "Epoch : 31 Training loss: 1.348380742907524 Test loss: 14.26700496673584\n",
      "Epoch : 32 Training loss: 1.2617605598270893 Test loss: 15.413198471069336\n",
      "Epoch : 33 Training loss: 1.0057950729727745 Test loss: 14.146492958068848\n",
      "Epoch : 34 Training loss: 0.9680698826014995 Test loss: 15.418673515319824\n",
      "Epoch : 35 Training loss: 0.9260213330090046 Test loss: 14.934846878051758\n",
      "Epoch : 36 Training loss: 0.8460583890974521 Test loss: 14.515576362609863\n",
      "Epoch : 37 Training loss: 0.9675765336751938 Test loss: 14.355243682861328\n",
      "Epoch : 38 Training loss: 1.214262533634901 Test loss: 15.075525283813477\n",
      "Epoch : 39 Training loss: 1.0251244699656963 Test loss: 15.505753517150879\n",
      "Epoch : 40 Training loss: 1.0518627843856811 Test loss: 15.062652587890625\n",
      "Epoch : 41 Training loss: 1.0917933336198329 Test loss: 14.904265403747559\n",
      "Epoch : 42 Training loss: 1.1545121544003487 Test loss: 14.802939414978027\n",
      "Epoch : 43 Training loss: 1.4076380659341812 Test loss: 14.568584442138672\n",
      "Epoch : 44 Training loss: 1.0444483253359795 Test loss: 17.486568450927734\n",
      "Epoch : 45 Training loss: 1.1634608038961887 Test loss: 14.567015647888184\n",
      "Epoch : 46 Training loss: 1.0307026447951793 Test loss: 14.506514549255371\n",
      "Epoch : 47 Training loss: 1.0505979911386967 Test loss: 13.014220237731934\n",
      "Epoch : 48 Training loss: 1.2726560264825821 Test loss: 14.17841911315918\n",
      "Epoch : 49 Training loss: 1.045432490170002 Test loss: 14.182028770446777\n",
      "Epoch : 50 Training loss: 1.2678825149238109 Test loss: 13.458298683166504\n",
      "Epoch : 51 Training loss: 0.9950419961214065 Test loss: 13.334439277648926\n",
      "Epoch : 52 Training loss: 0.8622105810940266 Test loss: 13.897026062011719\n",
      "Epoch : 53 Training loss: 0.8159716224074364 Test loss: 14.194314956665039\n",
      "Epoch : 54 Training loss: 0.7881478234529495 Test loss: 13.71354866027832\n",
      "Epoch : 55 Training loss: 0.8682189536690712 Test loss: 13.812006950378418\n",
      "Epoch : 56 Training loss: 1.0173338856995107 Test loss: 14.747884750366211\n",
      "Epoch : 57 Training loss: 3.143983541727066 Test loss: 14.496916770935059\n",
      "Epoch : 58 Training loss: 3.755891736090183 Test loss: 16.34099578857422\n",
      "Epoch : 59 Training loss: 2.0117084316313267 Test loss: 14.779977798461914\n",
      "Epoch : 60 Training loss: 1.2136180675029755 Test loss: 14.24008846282959\n",
      "Epoch : 61 Training loss: 1.0464581505060195 Test loss: 14.322649955749512\n",
      "Epoch : 62 Training loss: 0.9646928995847702 Test loss: 14.285823822021484\n",
      "Epoch : 63 Training loss: 0.8867444539964199 Test loss: 14.279973983764648\n",
      "Epoch : 64 Training loss: 0.8461922711730003 Test loss: 14.024048805236816\n",
      "Epoch : 65 Training loss: 0.8338167686462402 Test loss: 13.906394004821777\n",
      "Epoch : 66 Training loss: 0.8223877868950367 Test loss: 13.438840866088867\n",
      "Epoch : 67 Training loss: 0.8103571840822696 Test loss: 14.022400856018066\n",
      "Epoch : 68 Training loss: 0.7979339576065541 Test loss: 13.992537498474121\n",
      "Epoch : 69 Training loss: 0.8641708111166954 Test loss: 13.966989517211914\n",
      "Epoch : 70 Training loss: 0.9485339059829712 Test loss: 13.73703670501709\n",
      "Epoch : 71 Training loss: 1.8924375599920749 Test loss: 16.378690719604492\n",
      "Epoch : 72 Training loss: 2.4997302349209787 Test loss: 14.911459922790527\n",
      "Epoch : 73 Training loss: 1.861651540338993 Test loss: 14.511037826538086\n",
      "Epoch : 74 Training loss: 1.05708651638031 Test loss: 14.18366527557373\n",
      "Epoch : 75 Training loss: 0.9133528417050838 Test loss: 14.033044815063477\n",
      "Epoch : 76 Training loss: 0.8509019461274147 Test loss: 14.407477378845215\n",
      "Epoch : 77 Training loss: 0.8627391359359026 Test loss: 14.476250648498535\n",
      "Epoch : 78 Training loss: 0.8311224115788937 Test loss: 14.511226654052734\n",
      "Epoch : 79 Training loss: 0.8183323230445385 Test loss: 14.367780685424805\n",
      "Epoch : 80 Training loss: 0.8507318300902843 Test loss: 14.750650405883789\n",
      "Epoch : 81 Training loss: 0.7822428986728192 Test loss: 13.978825569152832\n",
      "Epoch : 82 Training loss: 0.7963640441596508 Test loss: 14.581947326660156\n",
      "Epoch : 83 Training loss: 0.8658421596288681 Test loss: 14.000709533691406\n",
      "Epoch : 84 Training loss: 0.9542276908606291 Test loss: 13.562451362609863\n",
      "Epoch : 85 Training loss: 1.0208293680548668 Test loss: 18.623231887817383\n",
      "Epoch : 86 Training loss: 1.176592711687088 Test loss: 14.336627006530762\n",
      "Epoch : 87 Training loss: 1.213209592461586 Test loss: 14.325127601623535\n",
      "Epoch : 88 Training loss: 1.238267487436533 Test loss: 15.170278549194336\n",
      "Epoch : 89 Training loss: 1.742706694126129 Test loss: 13.95410442352295\n",
      "Epoch : 90 Training loss: 1.0546436074376107 Test loss: 14.408088684082031\n",
      "Epoch : 91 Training loss: 0.8700635005235672 Test loss: 13.839420318603516\n",
      "Epoch : 92 Training loss: 0.8059601310193538 Test loss: 14.244904518127441\n",
      "Epoch : 93 Training loss: 0.7744460465461016 Test loss: 14.061816215515137\n",
      "Epoch : 94 Training loss: 0.8042922118306159 Test loss: 14.53512954711914\n",
      "Epoch : 95 Training loss: 0.8350895017683506 Test loss: 15.235169410705566\n",
      "Epoch : 96 Training loss: 1.1372881056368351 Test loss: 14.51362133026123\n",
      "Epoch : 97 Training loss: 1.996065605521202 Test loss: 13.906949996948242\n",
      "Epoch : 98 Training loss: 1.662061844944954 Test loss: 14.286514282226562\n",
      "Epoch : 99 Training loss: 1.8842375742793083 Test loss: 14.11056137084961\n",
      "Epoch : 100 Training loss: 1.3401991145312786 Test loss: 13.409887313842773\n",
      "Epoch : 101 Training loss: 1.0519713296592235 Test loss: 14.003877639770508\n",
      "Epoch : 102 Training loss: 0.9469430154561996 Test loss: 14.108963012695312\n",
      "Epoch : 103 Training loss: 0.8613083897531033 Test loss: 14.141570091247559\n",
      "Epoch : 104 Training loss: 0.7900430937707424 Test loss: 14.092528343200684\n",
      "Epoch : 105 Training loss: 0.7677846348583698 Test loss: 13.94101619720459\n",
      "Epoch : 106 Training loss: 0.7402661710828543 Test loss: 14.654382705688477\n",
      "Epoch : 107 Training loss: 0.7478001185655594 Test loss: 13.685598373413086\n",
      "Epoch : 108 Training loss: 0.7323046363294125 Test loss: 14.037863731384277\n",
      "Epoch : 109 Training loss: 0.7182546838670969 Test loss: 14.1305570602417\n",
      "Epoch : 110 Training loss: 0.7142749047875404 Test loss: 13.95273208618164\n",
      "Epoch : 111 Training loss: 0.7656647605001926 Test loss: 14.505706787109375\n",
      "Epoch : 112 Training loss: 0.835834111303091 Test loss: 14.159989356994629\n",
      "Epoch : 113 Training loss: 1.1373314823210239 Test loss: 13.563308715820312\n",
      "Epoch : 114 Training loss: 2.0774599791765214 Test loss: 14.875856399536133\n",
      "Epoch : 115 Training loss: 1.5331844300329684 Test loss: 14.976944923400879\n",
      "Epoch : 116 Training loss: 1.1400724469125272 Test loss: 14.799684524536133\n",
      "Epoch : 117 Training loss: 1.1059777310192584 Test loss: 13.812487602233887\n",
      "Epoch : 118 Training loss: 0.9503380858451128 Test loss: 14.059073448181152\n",
      "Epoch : 119 Training loss: 0.8325013876259327 Test loss: 13.978537559509277\n",
      "Epoch : 120 Training loss: 0.7981751699149608 Test loss: 13.640785217285156\n",
      "Epoch : 121 Training loss: 0.7937713994234801 Test loss: 14.117148399353027\n",
      "Epoch : 122 Training loss: 0.7468615995496511 Test loss: 14.820911407470703\n",
      "Epoch : 123 Training loss: 0.7948341343849897 Test loss: 15.185588836669922\n",
      "Epoch : 124 Training loss: 1.0148781743943691 Test loss: 16.916828155517578\n",
      "Epoch : 125 Training loss: 0.8027172789871693 Test loss: 14.446464538574219\n",
      "Epoch : 126 Training loss: 0.7556363582611084 Test loss: 14.500905990600586\n",
      "Epoch : 127 Training loss: 0.7387059383690358 Test loss: 13.879131317138672\n",
      "Epoch : 128 Training loss: 0.7362125079929829 Test loss: 15.226120948791504\n",
      "Epoch : 129 Training loss: 1.3373418409228326 Test loss: 32.138824462890625\n",
      "Epoch : 130 Training loss: 1.7178914712071418 Test loss: 16.022140502929688\n",
      "Epoch : 131 Training loss: 1.072808899998665 Test loss: 14.782398223876953\n",
      "Epoch : 132 Training loss: 0.9176085858345032 Test loss: 13.657130241394043\n",
      "Epoch : 133 Training loss: 0.9181207130402327 Test loss: 13.90566635131836\n",
      "Epoch : 134 Training loss: 0.8731088021993637 Test loss: 14.01876449584961\n",
      "Epoch : 135 Training loss: 1.017373863697052 Test loss: 13.907966613769531\n",
      "Epoch : 136 Training loss: 0.9338368156850338 Test loss: 14.593901634216309\n",
      "Epoch : 137 Training loss: 0.887432322204113 Test loss: 15.460671424865723\n",
      "Epoch : 138 Training loss: 0.8351440072655678 Test loss: 14.182403564453125\n",
      "Epoch : 139 Training loss: 0.7372047791182995 Test loss: 13.787956237792969\n",
      "Epoch : 140 Training loss: 0.7148644751757384 Test loss: 14.610288619995117\n",
      "Epoch : 141 Training loss: 0.7193926324248314 Test loss: 15.52009105682373\n",
      "Epoch : 142 Training loss: 0.7303526290506125 Test loss: 14.767258644104004\n",
      "Epoch : 143 Training loss: 0.7204215764403343 Test loss: 14.755522727966309\n",
      "Epoch : 144 Training loss: 0.7259110385477543 Test loss: 14.84689712524414\n",
      "Epoch : 145 Training loss: 0.9287433625459671 Test loss: 21.841835021972656\n",
      "Epoch : 146 Training loss: 1.5261342292428017 Test loss: 14.452574729919434\n",
      "Epoch : 147 Training loss: 1.6560231692194938 Test loss: 17.25939178466797\n",
      "Epoch : 148 Training loss: 1.2441033778190613 Test loss: 20.930891036987305\n",
      "Epoch : 149 Training loss: 2.232254205137491 Test loss: 14.397124290466309\n",
      "Epoch : 150 Training loss: 2.864461127400398 Test loss: 16.947282791137695\n",
      "Epoch : 151 Training loss: 1.3876170638799668 Test loss: 19.438961029052734\n",
      "Epoch : 152 Training loss: 1.0026472574174403 Test loss: 18.4834041595459\n",
      "Epoch : 153 Training loss: 0.8751208297908306 Test loss: 18.05507469177246\n",
      "Epoch : 154 Training loss: 0.826389845430851 Test loss: 18.817934036254883\n",
      "Epoch : 155 Training loss: 0.7917274781763554 Test loss: 18.206552505493164\n",
      "Epoch : 156 Training loss: 0.7749063723832369 Test loss: 17.4239559173584\n",
      "Epoch : 157 Training loss: 0.7605188520997763 Test loss: 17.28143882751465\n",
      "Epoch : 158 Training loss: 0.7349944589436054 Test loss: 24.605409622192383\n",
      "Epoch : 159 Training loss: 0.748617429703474 Test loss: 18.788143157958984\n",
      "Epoch : 160 Training loss: 0.7589310236126184 Test loss: 17.807209014892578\n",
      "Epoch : 161 Training loss: 0.7658752701282501 Test loss: 18.234146118164062\n",
      "Epoch : 162 Training loss: 0.9799297473579646 Test loss: 16.816307067871094\n",
      "Epoch : 163 Training loss: 0.9176426750421524 Test loss: 14.873866081237793\n",
      "Epoch : 164 Training loss: 0.8129394307732583 Test loss: 15.005573272705078\n",
      "Epoch : 165 Training loss: 0.7833062740564346 Test loss: 14.836213111877441\n",
      "Epoch : 166 Training loss: 1.0628126529455184 Test loss: 14.70109748840332\n",
      "Epoch : 167 Training loss: 0.8392260718941689 Test loss: 15.785886764526367\n",
      "Epoch : 168 Training loss: 0.8763244461417198 Test loss: 14.253045082092285\n",
      "Epoch : 169 Training loss: 0.8008820225149393 Test loss: 13.93869400024414\n",
      "Epoch : 170 Training loss: 0.7265764400362968 Test loss: 14.529105186462402\n",
      "Epoch : 171 Training loss: 0.7063988636136055 Test loss: 14.735629081726074\n",
      "Epoch : 172 Training loss: 0.7195586965680122 Test loss: 14.076761245727539\n",
      "Epoch : 173 Training loss: 0.6997069663703441 Test loss: 14.337088584899902\n",
      "Epoch : 174 Training loss: 0.7018658641427755 Test loss: 14.599394798278809\n",
      "Epoch : 175 Training loss: 1.2604863854944706 Test loss: 26.976844787597656\n",
      "Epoch : 176 Training loss: 5.877254949212074 Test loss: 27.639995574951172\n",
      "Epoch : 177 Training loss: 3.005506409049034 Test loss: 11.607172012329102\n",
      "Epoch : 178 Training loss: 1.509544960796833 Test loss: 12.524134635925293\n",
      "Epoch : 179 Training loss: 1.1067723326683045 Test loss: 12.494144439697266\n",
      "Epoch : 180 Training loss: 0.929635273873806 Test loss: 12.067152976989746\n",
      "Epoch : 181 Training loss: 0.8568089908063412 Test loss: 12.410581588745117\n",
      "Epoch : 182 Training loss: 0.8334922691583634 Test loss: 12.201854705810547\n",
      "Epoch : 183 Training loss: 0.7959720114022494 Test loss: 13.554771423339844\n",
      "Epoch : 184 Training loss: 0.7886841200292111 Test loss: 12.693414688110352\n",
      "Epoch : 185 Training loss: 0.7620293810218572 Test loss: 13.583830833435059\n",
      "Epoch : 186 Training loss: 0.7191631751209497 Test loss: 13.192377090454102\n",
      "Epoch : 187 Training loss: 0.7060672229528427 Test loss: 13.202027320861816\n",
      "Epoch : 188 Training loss: 0.7184951326847077 Test loss: 13.619967460632324\n",
      "Epoch : 189 Training loss: 0.9060012942105532 Test loss: 15.225798606872559\n",
      "Epoch : 190 Training loss: 0.9656610776782035 Test loss: 14.925736427307129\n",
      "Epoch : 191 Training loss: 1.0117422030568124 Test loss: 14.007428169250488\n",
      "Epoch : 192 Training loss: 1.130335158199072 Test loss: 13.482746124267578\n",
      "Epoch : 193 Training loss: 1.0529347273409366 Test loss: 14.20556354522705\n",
      "Epoch : 194 Training loss: 1.166469695210457 Test loss: 14.326013565063477\n",
      "Epoch : 195 Training loss: 0.9242392403781414 Test loss: 14.065889358520508\n",
      "Epoch : 196 Training loss: 0.9885403556227684 Test loss: 13.530221939086914\n",
      "Epoch : 197 Training loss: 0.8195642018318177 Test loss: 13.651047706604004\n",
      "Epoch : 198 Training loss: 0.7604175192415714 Test loss: 13.86570930480957\n",
      "Epoch : 199 Training loss: 0.7380383566319942 Test loss: 13.839812278747559\n",
      "Epoch : 200 Training loss: 0.7210598318129778 Test loss: 13.110486030578613\n",
      "Epoch : 201 Training loss: 0.7141708303838968 Test loss: 13.7736234664917\n",
      "Epoch : 202 Training loss: 0.7871896759867668 Test loss: 13.076581954956055\n",
      "Epoch : 203 Training loss: 0.7450237069129944 Test loss: 13.500568389892578\n",
      "Epoch : 204 Training loss: 0.7682663051784039 Test loss: 13.977153778076172\n",
      "Epoch : 205 Training loss: 1.2786300602853298 Test loss: 35.600929260253906\n",
      "Epoch : 206 Training loss: 1.4294707679748535 Test loss: 14.722946166992188\n",
      "Epoch : 207 Training loss: 1.3820544382184743 Test loss: 23.911531448364258\n",
      "Epoch : 208 Training loss: 1.2561682212352752 Test loss: 12.472665786743164\n",
      "Epoch : 209 Training loss: 0.8720385207533836 Test loss: 13.064818382263184\n",
      "Epoch : 210 Training loss: 0.7657768490612507 Test loss: 13.696640968322754\n",
      "Epoch : 211 Training loss: 0.7219233054965735 Test loss: 13.764439582824707\n",
      "Epoch : 212 Training loss: 0.6963660000264644 Test loss: 13.751728057861328\n",
      "Epoch : 213 Training loss: 0.6930377424359322 Test loss: 13.67387866973877\n",
      "Epoch : 214 Training loss: 0.8004912819564343 Test loss: 13.8041353225708\n",
      "Epoch : 215 Training loss: 1.3502795183956624 Test loss: 15.425862312316895\n",
      "Epoch : 216 Training loss: 1.436677754610777 Test loss: 14.880538940429688\n",
      "Epoch : 217 Training loss: 1.202386516660452 Test loss: 13.795928955078125\n",
      "Epoch : 218 Training loss: 0.8692574794590473 Test loss: 13.6189603805542\n",
      "Epoch : 219 Training loss: 0.7409135968983174 Test loss: 13.94736385345459\n",
      "Epoch : 220 Training loss: 0.7030769917964935 Test loss: 14.147822380065918\n",
      "Epoch : 221 Training loss: 0.6969541182219983 Test loss: 13.421015739440918\n",
      "Epoch : 222 Training loss: 0.7027257191687822 Test loss: 15.815216064453125\n",
      "Epoch : 223 Training loss: 0.7087497262507677 Test loss: 14.652509689331055\n",
      "Epoch : 224 Training loss: 0.771722386226058 Test loss: 13.970661163330078\n",
      "Epoch : 225 Training loss: 0.7211617705225944 Test loss: 14.744512557983398\n",
      "Epoch : 226 Training loss: 0.7678819670081138 Test loss: 14.009817123413086\n",
      "Epoch : 227 Training loss: 0.8248193309009075 Test loss: 13.544559478759766\n",
      "Epoch : 228 Training loss: 1.3426715496778487 Test loss: 16.20842170715332\n",
      "Epoch : 229 Training loss: 2.2546882098317145 Test loss: 14.104344367980957\n",
      "Epoch : 230 Training loss: 1.5109376822113991 Test loss: 13.913803100585938\n",
      "Epoch : 231 Training loss: 1.8476360404193402 Test loss: 13.132132530212402\n",
      "Epoch : 232 Training loss: 1.0945771466195584 Test loss: 13.411063194274902\n",
      "Epoch : 233 Training loss: 0.804045540779829 Test loss: 13.57197093963623\n",
      "Epoch : 234 Training loss: 0.7331872381865978 Test loss: 13.568354606628418\n",
      "Epoch : 235 Training loss: 0.7076235675662756 Test loss: 13.425769805908203\n",
      "Epoch : 236 Training loss: 0.6797493239641189 Test loss: 13.663020133972168\n",
      "Epoch : 237 Training loss: 0.6781614618599415 Test loss: 13.60629940032959\n",
      "Epoch : 238 Training loss: 0.667114671945572 Test loss: 13.600991249084473\n",
      "Epoch : 239 Training loss: 0.6754337432682515 Test loss: 13.276758193969727\n",
      "Epoch : 240 Training loss: 0.663402615815401 Test loss: 12.982800483703613\n",
      "Epoch : 241 Training loss: 0.7312455658763647 Test loss: 13.012441635131836\n",
      "Epoch : 242 Training loss: 0.8060607295334339 Test loss: 13.958781242370605\n",
      "Epoch : 243 Training loss: 0.7364538512378931 Test loss: 13.259087562561035\n",
      "Epoch : 244 Training loss: 2.31144103705883 Test loss: 14.094467163085938\n",
      "Epoch : 245 Training loss: 1.5107696814239024 Test loss: 13.740788459777832\n",
      "Epoch : 246 Training loss: 0.9949804218411445 Test loss: 13.671504020690918\n",
      "Epoch : 247 Training loss: 0.8614739874899388 Test loss: 13.747499465942383\n",
      "Epoch : 248 Training loss: 0.752326472967863 Test loss: 13.790987014770508\n",
      "Epoch : 249 Training loss: 0.7059885218292474 Test loss: 13.477067947387695\n",
      "Epoch : 250 Training loss: 0.6671759493649005 Test loss: 13.758475303649902\n",
      "Epoch : 251 Training loss: 0.6553010519295931 Test loss: 13.573010444641113\n",
      "Epoch : 252 Training loss: 0.650494121298194 Test loss: 13.650205612182617\n",
      "Epoch : 253 Training loss: 0.6419165401607752 Test loss: 13.935250282287598\n",
      "Epoch : 254 Training loss: 0.866859363168478 Test loss: 13.864801406860352\n",
      "Epoch : 255 Training loss: 0.9749763938486576 Test loss: 14.571205139160156\n",
      "Epoch : 256 Training loss: 1.808028876632452 Test loss: 15.017498970031738\n",
      "Epoch : 257 Training loss: 1.3207021122276783 Test loss: 14.126167297363281\n",
      "Epoch : 258 Training loss: 1.0221074421554803 Test loss: 14.210750579833984\n",
      "Epoch : 259 Training loss: 0.8068343459218741 Test loss: 13.725685119628906\n",
      "Epoch : 260 Training loss: 0.7180747425854206 Test loss: 13.720810890197754\n",
      "Epoch : 261 Training loss: 0.6772065826952457 Test loss: 13.629150390625\n",
      "Epoch : 262 Training loss: 0.6678806091845035 Test loss: 13.829856872558594\n",
      "Epoch : 263 Training loss: 0.6475495063066482 Test loss: 14.07164478302002\n",
      "Epoch : 264 Training loss: 0.6450544029772282 Test loss: 13.541566848754883\n",
      "Epoch : 265 Training loss: 0.665681888923049 Test loss: 14.778934478759766\n",
      "Epoch : 266 Training loss: 0.6756066376417875 Test loss: 16.808414459228516\n",
      "Epoch : 267 Training loss: 0.9305214941799641 Test loss: 13.976569175720215\n",
      "Epoch : 268 Training loss: 1.3036866660416127 Test loss: 16.607683181762695\n",
      "Epoch : 269 Training loss: 1.4376226384937763 Test loss: 17.50884437561035\n",
      "Epoch : 270 Training loss: 3.759392388880253 Test loss: 15.062145233154297\n",
      "Epoch : 271 Training loss: 1.4371999616622926 Test loss: 13.870214462280273\n",
      "Epoch : 272 Training loss: 0.949122299939394 Test loss: 13.824295997619629\n",
      "Epoch : 273 Training loss: 0.8011820610761643 Test loss: 13.78541088104248\n",
      "Epoch : 274 Training loss: 0.756357519865036 Test loss: 13.740754127502441\n",
      "Epoch : 275 Training loss: 0.7232679933011532 Test loss: 13.687739372253418\n",
      "Epoch : 276 Training loss: 0.7114984800070524 Test loss: 13.675925254821777\n",
      "Epoch : 277 Training loss: 0.6937392354309558 Test loss: 13.71678352355957\n",
      "Epoch : 278 Training loss: 0.6944740274697542 Test loss: 14.039860725402832\n",
      "Epoch : 279 Training loss: 0.6938602422922849 Test loss: 13.598196029663086\n",
      "Epoch : 280 Training loss: 0.6812781901657581 Test loss: 13.531510353088379\n",
      "Epoch : 281 Training loss: 0.6687927227318287 Test loss: 13.916390419006348\n",
      "Epoch : 282 Training loss: 0.6884144065529108 Test loss: 14.482085227966309\n",
      "Epoch : 283 Training loss: 0.7424392572343349 Test loss: 13.715570449829102\n",
      "Epoch : 284 Training loss: 1.1311794829666615 Test loss: 13.086625099182129\n",
      "Epoch : 285 Training loss: 1.0365517408251763 Test loss: 14.192906379699707\n",
      "Epoch : 286 Training loss: 1.1706062891483306 Test loss: 14.988612174987793\n",
      "Epoch : 287 Training loss: 1.0413156558275223 Test loss: 13.722110748291016\n",
      "Epoch : 288 Training loss: 0.8006861562430858 Test loss: 14.206873893737793\n",
      "Epoch : 289 Training loss: 0.6990890502631665 Test loss: 13.47258186340332\n",
      "Epoch : 290 Training loss: 0.6953276336193085 Test loss: 13.794285774230957\n",
      "Epoch : 291 Training loss: 0.6937900507301092 Test loss: 13.547647476196289\n",
      "Epoch : 292 Training loss: 0.6637485874593257 Test loss: 13.668742179870605\n",
      "Epoch : 293 Training loss: 0.6420209017544984 Test loss: 13.383785247802734\n",
      "Epoch : 294 Training loss: 0.7225809813737869 Test loss: 13.839574813842773\n",
      "Epoch : 295 Training loss: 2.0932192927747963 Test loss: 15.107215881347656\n",
      "Epoch : 296 Training loss: 1.8127873816490174 Test loss: 14.63764476776123\n",
      "Epoch : 297 Training loss: 1.3426080375909806 Test loss: 13.196645736694336\n",
      "Epoch : 298 Training loss: 0.9978458488881587 Test loss: 13.691008567810059\n",
      "Epoch : 299 Training loss: 0.8974020776450634 Test loss: 13.985706329345703\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_of_epochs=300\n",
    "batch_size=50\n",
    "sum_loss_list=[]\n",
    "\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out,y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "    \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "    sum_loss_list.append(sum_loss/x_variable.shape[0])\n",
    "    net.eval()\n",
    "    loss_test=loss_func(net(x_variable_test),y_variable_test)/(x_variable_test.shape[0])\n",
    "    print(\"Epoch :\",t, \"Training loss:\",sum_loss/x_variable.shape[0],\"Test loss:\",loss_test.data[0])\n",
    "    net.train()\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "triangle_test=np.random.normal(0,1,(3,2))\n",
    "#triangle=np.array([-1,0],[1,0],[0,sqrt(3)]])\n",
    "triangle_test=np.hstack((triangle_test,[[0],[0],[0]]))\n",
    "#fig=plt.figure()\n",
    "#ax=fig.add_subplot(111)\n",
    "#plt.triplot(triangle_test[:,0],triangle_test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "eig,F,U,angle=get_eigenvalues(triangle_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test=F.flatten()\n",
    "X_tensor_test=torch.from_numpy(X_test).float().type(torch.FloatTensor)\n",
    "X_variable_test=Variable(X_tensor_test,requires_grad=False)\n",
    "X_variable_test=X_variable_test.resize(1,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#net.train(False)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "net(X_variable_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "triangle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.triplot(triangle[0,:],triangle[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "View more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "Dependencies:\n",
    "torch: 0.1.11\n",
    "matplotlib\n",
    "numpy\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "# np.random.seed(1)\n",
    "\n",
    "# Hyper parameters\n",
    "N_SAMPLES = 2000\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 12\n",
    "LR = 0.03\n",
    "N_HIDDEN = 8\n",
    "ACTIVATION = F.tanh\n",
    "B_INIT = -0.2   # use a bad bias constant initializer\n",
    "\n",
    "# training data\n",
    "x = np.linspace(-7, 10, N_SAMPLES)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 2, x.shape)\n",
    "y = np.square(x) - 5 + noise\n",
    "\n",
    "# test data\n",
    "test_x = np.linspace(-7, 10, 200)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 2, test_x.shape)\n",
    "test_y = np.square(test_x) - 5 + noise\n",
    "\n",
    "train_x, train_y = torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "test_x = Variable(torch.from_numpy(test_x).float(), volatile=True)  # not for computing gradients\n",
    "test_y = Variable(torch.from_numpy(test_y).float(), volatile=True)\n",
    "\n",
    "train_dataset = Data.TensorDataset(data_tensor=train_x, target_tensor=train_y)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n",
    "\n",
    "# show data\n",
    "plt.scatter(train_x.numpy(), train_y.numpy(), c='#FF9359', s=50, alpha=0.2, label='train')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, batch_normalization=False):\n",
    "        super(Net, self).__init__()\n",
    "        self.do_bn = batch_normalization\n",
    "        self.fcs = []\n",
    "        self.bns = []\n",
    "        self.bn_input = nn.BatchNorm1d(1, momentum=0.5)   # for input data\n",
    "\n",
    "        for i in range(N_HIDDEN):               # build hidden layers and BN layers\n",
    "            input_size = 1 if i == 0 else 10\n",
    "            fc = nn.Linear(input_size, 10)\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self._set_init(fc)                  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(10, momentum=0.5)\n",
    "                setattr(self, 'bn%i' % i, bn)   # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "\n",
    "        self.predict = nn.Linear(10, 1)         # output layer\n",
    "        self._set_init(self.predict)            # parameters initialization\n",
    "\n",
    "    def _set_init(self, layer):\n",
    "        init.normal(layer.weight, mean=0., std=.1)\n",
    "        init.constant(layer.bias, B_INIT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pre_activation = [x]\n",
    "        if self.do_bn: x = self.bn_input(x)     # input batch normalization\n",
    "        layer_input = [x]\n",
    "        for i in range(N_HIDDEN):\n",
    "            x = self.fcs[i](x)\n",
    "            pre_activation.append(x)\n",
    "            if self.do_bn: x = self.bns[i](x)   # batch normalization\n",
    "            x = ACTIVATION(x)\n",
    "            layer_input.append(x)\n",
    "        out = self.predict(x)\n",
    "        return out, layer_input, pre_activation\n",
    "\n",
    "nets = [Net(batch_normalization=False), Net(batch_normalization=True)]\n",
    "\n",
    "print(*nets)    # print net architecture\n",
    "\n",
    "opts = [torch.optim.Adam(net.parameters(), lr=LR) for net in nets]\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "f, axs = plt.subplots(4, N_HIDDEN+1, figsize=(10, 5))\n",
    "plt.ion()   # something about plotting\n",
    "plt.show()\n",
    "def plot_histogram(l_in, l_in_bn, pre_ac, pre_ac_bn):\n",
    "    for i, (ax_pa, ax_pa_bn, ax,  ax_bn) in enumerate(zip(axs[0, :], axs[1, :], axs[2, :], axs[3, :])):\n",
    "        [a.clear() for a in [ax_pa, ax_pa_bn, ax, ax_bn]]\n",
    "        if i == 0: p_range = (-7, 10);the_range = (-7, 10)\n",
    "        else:p_range = (-4, 4);the_range = (-1, 1)\n",
    "        ax_pa.set_title('L' + str(i))\n",
    "        ax_pa.hist(pre_ac[i].data.numpy().ravel(), bins=10, range=p_range, color='#FF9359', alpha=0.5);ax_pa_bn.hist(pre_ac_bn[i].data.numpy().ravel(), bins=10, range=p_range, color='#74BCFF', alpha=0.5)\n",
    "        ax.hist(l_in[i].data.numpy().ravel(), bins=10, range=the_range, color='#FF9359');ax_bn.hist(l_in_bn[i].data.numpy().ravel(), bins=10, range=the_range, color='#74BCFF')\n",
    "        for a in [ax_pa, ax, ax_pa_bn, ax_bn]: a.set_yticks(());a.set_xticks(())\n",
    "        ax_pa_bn.set_xticks(p_range);ax_bn.set_xticks(the_range)\n",
    "        axs[0, 0].set_ylabel('PreAct');axs[1, 0].set_ylabel('BN PreAct');axs[2, 0].set_ylabel('Act');axs[3, 0].set_ylabel('BN Act')\n",
    "    plt.pause(0.01)\n",
    "\n",
    "# training\n",
    "losses = [[], []]  # recode loss for two networks\n",
    "for epoch in range(EPOCH):\n",
    "    print('Epoch: ', epoch)\n",
    "    layer_inputs, pre_acts = [], []\n",
    "    for net, l in zip(nets, losses):\n",
    "        net.eval()              # set eval mode to fix moving_mean and moving_var\n",
    "        pred, layer_input, pre_act = net(test_x)\n",
    "        l.append(loss_func(pred, test_y).data[0])\n",
    "        layer_inputs.append(layer_input)\n",
    "        pre_acts.append(pre_act)\n",
    "        net.train()             # free moving_mean and moving_var\n",
    "    plot_histogram(*layer_inputs, *pre_acts)     # plot histogram\n",
    "\n",
    "    for step, (b_x, b_y) in enumerate(train_loader):\n",
    "        b_x, b_y = Variable(b_x), Variable(b_y)\n",
    "        for net, opt in zip(nets, opts):     # train for each network\n",
    "            pred, _, _ = net(b_x)\n",
    "            loss = loss_func(pred, b_y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()    # it will also learns the parameters in Batch Normalization\n",
    "\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "# plot training loss\n",
    "plt.figure(2)\n",
    "plt.plot(losses[0], c='#FF9359', lw=3, label='Original')\n",
    "plt.plot(losses[1], c='#74BCFF', lw=3, label='Batch Normalization')\n",
    "plt.xlabel('step');plt.ylabel('test loss');plt.ylim((0, 2000));plt.legend(loc='best')\n",
    "\n",
    "# evaluation\n",
    "# set net to eval mode to freeze the parameters in batch normalization layers\n",
    "[net.eval() for net in nets]    # set eval mode to fix moving_mean and moving_var\n",
    "preds = [net(test_x)[0] for net in nets]\n",
    "plt.figure(3)\n",
    "plt.plot(test_x.data.numpy(), preds[0].data.numpy(), c='#FF9359', lw=4, label='Original')\n",
    "plt.plot(test_x.data.numpy(), preds[1].data.numpy(), c='#74BCFF', lw=4, label='Batch Normalization')\n",
    "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='r', s=50, alpha=0.2, label='train')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
