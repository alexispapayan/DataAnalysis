{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "B_INIT = -0.2 # use a bad bias constant initializer\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_features_dimension,out_features_dimension,nb_of_hidden_layers,nb_of_hidden_nodes,\n",
    "                 batch_normalization=False):\n",
    "        \n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.nb_hidden_layers=nb_of_hidden_layers\n",
    "        self.do_bn=batch_normalization\n",
    "        self.fcs=[]\n",
    "        self.bns=[]\n",
    "        self.bn_input=nn.BatchNorm1d(in_features_dimension,momentum=0.5) #for input data\n",
    "        \n",
    "        for i in range(nb_of_hidden_layers):                              # build hidden layers and BN layers\n",
    "            \n",
    "            input_size=in_features_dimension if i==0 else nb_of_hidden_nodes\n",
    "            fc=nn.Linear(input_size,nb_of_hidden_nodes)\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self._set_init(fc)                  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "            \n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(nb_of_hidden_nodes, momentum=0.5)\n",
    "                setattr(self, 'bn%i' % i, bn)                         # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "    \n",
    "            self.predict = nn.Linear(nb_of_hidden_nodes,out_features_dimension)         # output layer\n",
    "            self._set_init(self.predict)                                              # parameters initialization\n",
    "    \n",
    "    \n",
    "    def _set_init(self, layer):\n",
    "            init.normal(layer.weight, mean=0., std=.1)\n",
    "            init.constant(layer.bias, B_INIT)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ACTIVATION=F.relu\n",
    "        pre_activation = [x]\n",
    "        if self.do_bn: x = self.bn_input(x)     # input batch normalization\n",
    "        layer_input = [x]\n",
    "        for i in range(self.nb_hidden_layers):\n",
    "            x = self.fcs[i](x)\n",
    "            pre_activation.append(x)\n",
    "            if self.do_bn: x = self.bns[i](x)   # batch normalization\n",
    "            x = ACTIVATION(x)\n",
    "            layer_input.append(x)\n",
    "        out = self.predict(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
