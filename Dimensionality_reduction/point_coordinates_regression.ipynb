{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Triangulation import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from  Neural_network import *\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_nb_of_points(point_coordinates):\n",
    "    set_of_numbers=set()\n",
    "    for index,_ in enumerate(point_coordinates):\n",
    "        set_of_numbers.add(len(point_coordinates[index][0]))\n",
    "    return set_of_numbers\n",
    "\n",
    "\n",
    "def get_indices_nb_of_points(set_of_numbers,number_of_points,point_coordinates):\n",
    "    indices=[]\n",
    "    if number_of_points not in set_of_numbers:\n",
    "        return \"No such number of points for sample\"\n",
    "    else:\n",
    "        for index,_ in enumerate(point_coordinates):\n",
    "            if len(point_coordinates[index][0])==number_of_points:\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "    \n",
    "def get_polygons_nb_of_points(indices):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons=load_dataset('12_polygons.pkl')\n",
    "polygons=[i for i in polygons for j in range(10)]\n",
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(2,12))\n",
    "\n",
    "polygons_reshaped=np.array(polygons_reshaped)\n",
    "#polygons_reshaped=polygons_reshaped.reshape(60000,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=load_dataset('12_point_coordinates')\n",
    "number_of_insertion_points=load_dataset('12_nb_of_points.pkl')\n",
    "centers_of_mass=load_dataset('12_centers_of_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_points=get_set_nb_of_points(point_coordinates)        \n",
    "indices=get_indices_nb_of_points(set_of_points,1,point_coordinates)\n",
    "indices=np.asarray(indices)\n",
    "number_of_insertion_points=np.array(number_of_insertion_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped.resize(len(point_coordinates),2*12)\n",
    "\n",
    "polygons_reshaped=np.hstack([polygons_reshaped[indices],number_of_insertion_points[indices,1].reshape(len(indices),1) ])\n",
    "#polygons_reshaped=polygons_reshaped[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=[ point_coordinates[i][0] for i in indices]\n",
    "point_coordinates=np.array(point_coordinates)\n",
    "#barycenters,point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(polygons_reshaped),len(indices)\n",
    "#point_coordinates.shape\n",
    "centers_of_mass=centers_of_mass.reshape(60000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(polygons_reshaped[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(polygons_reshaped[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n",
    "\n",
    "y_tensor=torch.from_numpy(point_coordinates[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(point_coordinates[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "shuffle=torch.randperm(x_variable.shape[0])\n",
    "x_variable = x_variable[shuffle]\n",
    "y_variable=y_variable[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 174978\n"
     ]
    }
   ],
   "source": [
    "my_net=Net(2*12+1,1*2,nb_of_hidden_layers=2, nb_of_hidden_nodes=40,batch_normalization=True)\n",
    "print(\"Training data length:\",x_variable.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-3,weight_decay=0)\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "max_distance=0.6108970818704328\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=myLossfunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    x_variable , y_variable=x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test,y_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 0.03597994311472371 Test Loss: 0.04591718449065872\n",
      "Epoch: 10 Training Loss: 0.03595471533385625 Test Loss: 0.04589256057795069\n",
      "Epoch: 20 Training Loss: 0.03594138357973931 Test Loss: 0.04591361257222419\n",
      "Epoch: 30 Training Loss: 0.03593050609621281 Test Loss: 0.045934031109087815\n",
      "Epoch: 40 Training Loss: 0.03591936490765982 Test Loss: 0.04592797838167806\n",
      "Epoch: 50 Training Loss: 0.03590778560599404 Test Loss: 0.045925185029839805\n",
      "Epoch: 60 Training Loss: 0.03589508590678185 Test Loss: 0.04591705891540126\n",
      "Epoch: 70 Training Loss: 0.03588216156995451 Test Loss: 0.04591509994138482\n",
      "Epoch: 80 Training Loss: 0.035868235433079446 Test Loss: 0.0459132721237484\n",
      "Epoch: 90 Training Loss: 0.03585296007524875 Test Loss: 0.045934519457311286\n",
      "Epoch: 100 Training Loss: 0.03583402354287039 Test Loss: 0.04594240000235746\n",
      "Epoch: 110 Training Loss: 0.03581583068657203 Test Loss: 0.045958989889149086\n",
      "Epoch: 120 Training Loss: 0.03579958701504618 Test Loss: 0.04595384130359306\n",
      "Epoch: 130 Training Loss: 0.03578420840762839 Test Loss: 0.04596699042833017\n",
      "Epoch: 140 Training Loss: 0.03576778614222217 Test Loss: 0.04599072694255212\n",
      "Epoch: 150 Training Loss: 0.03575126062722885 Test Loss: 0.045994767675281184\n",
      "Epoch: 160 Training Loss: 0.03573366773474737 Test Loss: 0.04603198818159347\n",
      "Epoch: 170 Training Loss: 0.03571543441577298 Test Loss: 0.04602291885744331\n",
      "Epoch: 180 Training Loss: 0.035698191734728796 Test Loss: 0.046023150474029294\n",
      "Epoch: 190 Training Loss: 0.03568158389911875 Test Loss: 0.046049136180640486\n",
      "Epoch: 200 Training Loss: 0.03566711081848504 Test Loss: 0.04602602754270586\n",
      "Epoch: 210 Training Loss: 0.03565362977104622 Test Loss: 0.046041082620795135\n",
      "Epoch: 220 Training Loss: 0.03563965759043637 Test Loss: 0.046077025050042575\n",
      "Epoch: 230 Training Loss: 0.03562672906781495 Test Loss: 0.04605728182900798\n",
      "Epoch: 240 Training Loss: 0.035614051692837816 Test Loss: 0.046064205211536154\n",
      "Epoch: 250 Training Loss: 0.03560113572759861 Test Loss: 0.04609100018091767\n",
      "Epoch: 260 Training Loss: 0.035576732548163195 Test Loss: 0.046102405204856665\n",
      "Epoch: 270 Training Loss: 0.035543230847681566 Test Loss: 0.04607563814108792\n",
      "Epoch: 280 Training Loss: 0.03551316428886774 Test Loss: 0.046044367111418134\n",
      "Epoch: 290 Training Loss: 0.03549214044051252 Test Loss: 0.046083158703729366\n",
      "Epoch: 300 Training Loss: 0.03547200677102979 Test Loss: 0.04610028158772489\n",
      "Epoch: 310 Training Loss: 0.035453388359000845 Test Loss: 0.04611612918521683\n",
      "Epoch: 320 Training Loss: 0.03543441694500523 Test Loss: 0.04613430411081377\n",
      "Epoch: 330 Training Loss: 0.03541775190354288 Test Loss: 0.046153514334644465\n",
      "Epoch: 340 Training Loss: 0.03540282396662009 Test Loss: 0.046145229158213126\n",
      "Epoch: 350 Training Loss: 0.03538824205534053 Test Loss: 0.04609597017055196\n",
      "Epoch: 360 Training Loss: 0.035373616890855566 Test Loss: 0.04610118014845607\n",
      "Epoch: 370 Training Loss: 0.03535904056063476 Test Loss: 0.0460987411979\n",
      "Epoch: 380 Training Loss: 0.03534425075491633 Test Loss: 0.046116330105628774\n",
      "Epoch: 390 Training Loss: 0.03533085481862363 Test Loss: 0.046160284236302404\n",
      "Epoch: 400 Training Loss: 0.03531545946802955 Test Loss: 0.046148773171034885\n",
      "Epoch: 410 Training Loss: 0.035296938724528944 Test Loss: 0.04612089825443918\n",
      "Epoch: 420 Training Loss: 0.035281634066139746 Test Loss: 0.046136837940453264\n",
      "Epoch: 430 Training Loss: 0.035265715491286764 Test Loss: 0.046134094818718\n",
      "Epoch: 440 Training Loss: 0.035251374960798644 Test Loss: 0.046089546298492365\n",
      "Epoch: 450 Training Loss: 0.0352378241501253 Test Loss: 0.04609441303735941\n",
      "Epoch: 460 Training Loss: 0.03522573697211221 Test Loss: 0.04610227125791537\n",
      "Epoch: 470 Training Loss: 0.035213852107479224 Test Loss: 0.046112875390767874\n",
      "Epoch: 480 Training Loss: 0.035201268215236343 Test Loss: 0.046123504638671875\n",
      "Epoch: 490 Training Loss: 0.035189090345018374 Test Loss: 0.046064141028626784\n",
      "Epoch: 500 Training Loss: 0.03517751383388197 Test Loss: 0.04609046718371377\n",
      "Epoch: 510 Training Loss: 0.03516665309353176 Test Loss: 0.0460884635607169\n",
      "Epoch: 520 Training Loss: 0.035156534633996844 Test Loss: 0.046091815024810544\n",
      "Epoch: 530 Training Loss: 0.03514596829449631 Test Loss: 0.046050983532205846\n",
      "Epoch: 540 Training Loss: 0.035134337368036984 Test Loss: 0.046023114196732696\n",
      "Epoch: 550 Training Loss: 0.03512182184376393 Test Loss: 0.04602908041674287\n",
      "Epoch: 560 Training Loss: 0.0351088082099964 Test Loss: 0.04603418714387973\n",
      "Epoch: 570 Training Loss: 0.035096143392401476 Test Loss: 0.04600514856323153\n",
      "Epoch: 580 Training Loss: 0.03508318138342749 Test Loss: 0.046005134610425144\n",
      "Epoch: 590 Training Loss: 0.03507386938688364 Test Loss: 0.0459931658931082\n",
      "Epoch: 600 Training Loss: 0.035064569947722 Test Loss: 0.045950648901492204\n",
      "Epoch: 610 Training Loss: 0.035052248365240915 Test Loss: 0.04592786954978826\n",
      "Epoch: 620 Training Loss: 0.03504302008457849 Test Loss: 0.04595360131532324\n",
      "Epoch: 630 Training Loss: 0.035034421068291445 Test Loss: 0.045934137150416346\n",
      "Epoch: 640 Training Loss: 0.03502722708354765 Test Loss: 0.045897075706096836\n",
      "Epoch: 650 Training Loss: 0.03501899781240356 Test Loss: 0.045941479117136065\n",
      "Epoch: 660 Training Loss: 0.03501202707001023 Test Loss: 0.04592098523511796\n",
      "Epoch: 670 Training Loss: 0.035000262198140626 Test Loss: 0.04590310052789383\n",
      "Epoch: 680 Training Loss: 0.034992060832290346 Test Loss: 0.04592052479250726\n",
      "Epoch: 690 Training Loss: 0.03498077314094486 Test Loss: 0.045891773639670584\n",
      "Epoch: 700 Training Loss: 0.034974007502461026 Test Loss: 0.045861549070479665\n",
      "Epoch: 710 Training Loss: 0.03496201938824095 Test Loss: 0.04588121415579851\n",
      "Epoch: 720 Training Loss: 0.03495409847059345 Test Loss: 0.0458372935118602\n",
      "Epoch: 730 Training Loss: 0.03494463439019834 Test Loss: 0.04582750980402312\n",
      "Epoch: 740 Training Loss: 0.034938516154530856 Test Loss: 0.04581668242626846\n",
      "Epoch: 750 Training Loss: 0.034930345484503765 Test Loss: 0.04581831769517677\n",
      "Epoch: 760 Training Loss: 0.03492285849417498 Test Loss: 0.04584926501973842\n",
      "Epoch: 770 Training Loss: 0.0349162742401009 Test Loss: 0.04581757819643837\n",
      "Epoch: 780 Training Loss: 0.034910970839012616 Test Loss: 0.045840028261911636\n",
      "Epoch: 790 Training Loss: 0.034904025211383714 Test Loss: 0.04583286768167492\n",
      "Epoch: 800 Training Loss: 0.03490116491876836 Test Loss: 0.04580529135513585\n",
      "Epoch: 810 Training Loss: 0.03489349375350044 Test Loss: 0.045790289297710826\n",
      "Epoch: 820 Training Loss: 0.034887042049571944 Test Loss: 0.0457963280723142\n",
      "Epoch: 830 Training Loss: 0.034882007934568915 Test Loss: 0.04576665045313357\n",
      "Epoch: 840 Training Loss: 0.03488211397468539 Test Loss: 0.04579796613178379\n",
      "Epoch: 850 Training Loss: 0.034870259805875596 Test Loss: 0.045740642422032164\n",
      "Epoch: 860 Training Loss: 0.03486617447086205 Test Loss: 0.045745428234622174\n",
      "Epoch: 870 Training Loss: 0.034861060825771666 Test Loss: 0.04572178939004492\n",
      "Epoch: 880 Training Loss: 0.034857031301345744 Test Loss: 0.04572549246485947\n",
      "Epoch: 890 Training Loss: 0.0348537901014699 Test Loss: 0.04570536135580737\n",
      "Epoch: 900 Training Loss: 0.03484875459120218 Test Loss: 0.04568694086081806\n",
      "Epoch: 910 Training Loss: 0.03484506690662541 Test Loss: 0.04566822456633338\n",
      "Epoch: 920 Training Loss: 0.034838929137252256 Test Loss: 0.045709923923495224\n",
      "Epoch: 930 Training Loss: 0.03483797477620401 Test Loss: 0.045688654265442115\n",
      "Epoch: 940 Training Loss: 0.03483407222086491 Test Loss: 0.04571801934175973\n",
      "Epoch: 950 Training Loss: 0.034828283267664364 Test Loss: 0.04569274801883544\n",
      "Epoch: 960 Training Loss: 0.034828952994715764 Test Loss: 0.04564741535089098\n",
      "Epoch: 970 Training Loss: 0.034823309149043025 Test Loss: 0.045687035739901474\n",
      "Epoch: 980 Training Loss: 0.03482023259040065 Test Loss: 0.04570129550802682\n",
      "Epoch: 990 Training Loss: 0.034816511419471306 Test Loss: 0.04569495256224425\n",
      "Epoch: 1000 Training Loss: 0.034813200456360945 Test Loss: 0.04568788965165223\n",
      "Epoch: 1010 Training Loss: 0.03481056061556667 Test Loss: 0.045662065797595096\n",
      "Epoch: 1020 Training Loss: 0.034809522538637 Test Loss: 0.04565840179063843\n",
      "Epoch: 1030 Training Loss: 0.03480509117798023 Test Loss: 0.04570099691797018\n",
      "Epoch: 1040 Training Loss: 0.034802260185923375 Test Loss: 0.045652226278532486\n",
      "Epoch: 1050 Training Loss: 0.03480094863711438 Test Loss: 0.04565957661693604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1060 Training Loss: 0.03479608893019765 Test Loss: 0.04567305502790383\n",
      "Epoch: 1070 Training Loss: 0.03479582522517116 Test Loss: 0.04566230299530364\n",
      "Epoch: 1080 Training Loss: 0.03479010324467576 Test Loss: 0.04565334808416583\n",
      "Epoch: 1090 Training Loss: 0.03478691366959347 Test Loss: 0.04566559027648792\n",
      "Epoch: 1100 Training Loss: 0.034785057967555204 Test Loss: 0.045671444874047014\n",
      "Epoch: 1110 Training Loss: 0.03478078427180845 Test Loss: 0.04569929188502995\n",
      "Epoch: 1120 Training Loss: 0.03477596502756775 Test Loss: 0.04563707353079851\n",
      "Epoch: 1130 Training Loss: 0.034769966784663646 Test Loss: 0.045642827668151635\n",
      "Epoch: 1140 Training Loss: 0.03476319696038573 Test Loss: 0.045653063446915575\n",
      "Epoch: 1150 Training Loss: 0.03475804006208995 Test Loss: 0.045649285026946554\n",
      "Epoch: 1160 Training Loss: 0.03475350963764016 Test Loss: 0.045658156221246055\n",
      "Epoch: 1170 Training Loss: 0.034749255475599074 Test Loss: 0.04565039287977351\n",
      "Epoch: 1180 Training Loss: 0.03474601567098792 Test Loss: 0.04563299373021157\n",
      "Epoch: 1190 Training Loss: 0.03474208939614909 Test Loss: 0.045621206399377626\n",
      "Epoch: 1200 Training Loss: 0.03473760641069877 Test Loss: 0.04560722568737998\n",
      "Epoch: 1210 Training Loss: 0.034731433759708365 Test Loss: 0.045644836872271055\n",
      "Epoch: 1220 Training Loss: 0.03472491089728066 Test Loss: 0.04563765954866667\n",
      "Epoch: 1230 Training Loss: 0.034720165602068545 Test Loss: 0.04565946778504623\n",
      "Epoch: 1240 Training Loss: 0.03471493894053824 Test Loss: 0.04564747953380035\n",
      "Epoch: 1250 Training Loss: 0.03471191121616003 Test Loss: 0.04562378487799756\n",
      "Epoch: 1260 Training Loss: 0.03470709336718402 Test Loss: 0.045625115975726674\n",
      "Epoch: 1270 Training Loss: 0.03470445771218382 Test Loss: 0.04560105854695787\n",
      "Epoch: 1280 Training Loss: 0.03469752603720182 Test Loss: 0.04558723689695301\n",
      "Epoch: 1290 Training Loss: 0.03468945164043837 Test Loss: 0.04560768892055196\n",
      "Epoch: 1300 Training Loss: 0.034684290556348515 Test Loss: 0.04564139890077782\n",
      "Epoch: 1310 Training Loss: 0.03467849602208921 Test Loss: 0.04564922921572101\n",
      "Epoch: 1320 Training Loss: 0.03467467020630808 Test Loss: 0.04563925295915583\n",
      "Epoch: 1330 Training Loss: 0.03466767993020908 Test Loss: 0.045675276314680296\n",
      "Epoch: 1340 Training Loss: 0.034664791732299916 Test Loss: 0.04564685444807431\n",
      "Epoch: 1350 Training Loss: 0.03465978412732601 Test Loss: 0.04566591119103477\n",
      "Epoch: 1360 Training Loss: 0.03465870279719093 Test Loss: 0.04570230290064781\n",
      "Epoch: 1370 Training Loss: 0.034651066513540274 Test Loss: 0.045665428423933854\n",
      "Epoch: 1380 Training Loss: 0.03464972147837871 Test Loss: 0.045700271372038165\n",
      "Epoch: 1390 Training Loss: 0.034645507779013644 Test Loss: 0.045693038237208244\n",
      "Epoch: 1400 Training Loss: 0.034642355876077995 Test Loss: 0.04569587344746565\n",
      "Epoch: 1410 Training Loss: 0.03463987090966436 Test Loss: 0.045702626605755935\n",
      "Epoch: 1420 Training Loss: 0.0346320881232212 Test Loss: 0.04572390184493159\n",
      "Epoch: 1430 Training Loss: 0.0346267456547216 Test Loss: 0.04572980388203239\n",
      "Epoch: 1440 Training Loss: 0.03462492064850653 Test Loss: 0.04576301993291223\n",
      "Epoch: 1450 Training Loss: 0.03462129435557613 Test Loss: 0.04573042059607461\n",
      "Epoch: 1460 Training Loss: 0.03461527797423104 Test Loss: 0.04571441672715115\n",
      "Epoch: 1470 Training Loss: 0.03461427617418333 Test Loss: 0.045716199895807136\n",
      "Epoch: 1480 Training Loss: 0.03460961040905857 Test Loss: 0.045701775484566456\n",
      "Epoch: 1490 Training Loss: 0.03460310987086591 Test Loss: 0.04573409018415383\n",
      "Epoch: 1500 Training Loss: 0.03459839806200637 Test Loss: 0.04575659327029136\n",
      "Epoch: 1510 Training Loss: 0.034592851884861955 Test Loss: 0.04577271713334972\n",
      "Epoch: 1520 Training Loss: 0.03458735035618764 Test Loss: 0.04578801778083137\n",
      "Epoch: 1530 Training Loss: 0.03458503561206623 Test Loss: 0.04575957917085772\n",
      "Epoch: 1540 Training Loss: 0.03457946432015739 Test Loss: 0.04579024743929167\n",
      "Epoch: 1550 Training Loss: 0.034577847208381196 Test Loss: 0.04572160800356192\n",
      "Epoch: 1560 Training Loss: 0.034570369984905244 Test Loss: 0.04573342603056991\n",
      "Epoch: 1570 Training Loss: 0.03456885472745145 Test Loss: 0.04575048752221733\n",
      "Epoch: 1580 Training Loss: 0.034565663757104466 Test Loss: 0.0457424200095656\n",
      "Epoch: 1590 Training Loss: 0.034558599531976875 Test Loss: 0.04573137217747005\n",
      "Epoch: 1600 Training Loss: 0.034557152642492914 Test Loss: 0.04572385998651243\n",
      "Epoch: 1610 Training Loss: 0.03455780841689741 Test Loss: 0.04577224831905519\n",
      "Epoch: 1620 Training Loss: 0.03455043583827324 Test Loss: 0.04577009679631064\n",
      "Epoch: 1630 Training Loss: 0.03454524824415427 Test Loss: 0.04576427568548687\n",
      "Epoch: 1640 Training Loss: 0.034545045930774156 Test Loss: 0.04577563326988415\n",
      "Epoch: 1650 Training Loss: 0.03453985973191987 Test Loss: 0.045789262371160905\n",
      "Epoch: 1660 Training Loss: 0.03453790078029452 Test Loss: 0.045784814216485406\n",
      "Epoch: 1670 Training Loss: 0.0345318313788912 Test Loss: 0.045794645363864184\n",
      "Epoch: 1680 Training Loss: 0.034532236005651425 Test Loss: 0.04581664056784931\n",
      "Epoch: 1690 Training Loss: 0.03452655309256735 Test Loss: 0.04576008705301013\n",
      "Epoch: 1700 Training Loss: 0.03452370396206952 Test Loss: 0.04579103995869434\n",
      "Epoch: 1710 Training Loss: 0.034521295735213856 Test Loss: 0.04579685548839555\n",
      "Epoch: 1720 Training Loss: 0.03451573421015785 Test Loss: 0.04574569333794349\n",
      "Epoch: 1730 Training Loss: 0.034511858164847864 Test Loss: 0.0458152229627206\n",
      "Epoch: 1740 Training Loss: 0.03450564784171081 Test Loss: 0.045743153927181444\n",
      "Epoch: 1750 Training Loss: 0.0345021261936322 Test Loss: 0.04577519515176367\n",
      "Epoch: 1760 Training Loss: 0.034498484552790205 Test Loss: 0.04580374817474968\n",
      "Epoch: 1770 Training Loss: 0.034492392827151844 Test Loss: 0.04582781955632487\n",
      "Epoch: 1780 Training Loss: 0.034490839897551404 Test Loss: 0.04579987208513596\n",
      "Epoch: 1790 Training Loss: 0.034492183537448276 Test Loss: 0.04582201518886876\n",
      "Epoch: 1800 Training Loss: 0.034481871136121396 Test Loss: 0.04578595276548641\n",
      "Epoch: 1810 Training Loss: 0.03448455981117984 Test Loss: 0.04580492858216984\n",
      "Epoch: 1820 Training Loss: 0.03447576685310082 Test Loss: 0.045803993744142055\n",
      "Epoch: 1830 Training Loss: 0.034477256995790184 Test Loss: 0.0458003883389722\n",
      "Epoch: 1840 Training Loss: 0.03447039926983678 Test Loss: 0.045836040549846836\n",
      "Epoch: 1850 Training Loss: 0.03446964303637457 Test Loss: 0.04582120034497588\n",
      "Epoch: 1860 Training Loss: 0.03447071459965682 Test Loss: 0.04584166911194249\n",
      "Epoch: 1870 Training Loss: 0.03446612138829596 Test Loss: 0.04586810688948056\n",
      "Epoch: 1880 Training Loss: 0.03446475542416404 Test Loss: 0.04584916455953245\n",
      "Epoch: 1890 Training Loss: 0.034459986409452185 Test Loss: 0.04585271694403804\n",
      "Epoch: 1900 Training Loss: 0.03446218953239836 Test Loss: 0.04586163557787925\n",
      "Epoch: 1910 Training Loss: 0.03446277414830364 Test Loss: 0.04586148767813157\n",
      "Epoch: 1920 Training Loss: 0.03446598604762099 Test Loss: 0.04587333082019106\n",
      "Epoch: 1930 Training Loss: 0.03445775538121221 Test Loss: 0.04586905568031473\n",
      "Epoch: 1940 Training Loss: 0.034458150241119596 Test Loss: 0.04592709656431453\n",
      "Epoch: 1950 Training Loss: 0.03445742051768651 Test Loss: 0.04584145981984672\n",
      "Epoch: 1960 Training Loss: 0.034458952518316585 Test Loss: 0.04587554373528369\n",
      "Epoch: 1970 Training Loss: 0.034451945499041306 Test Loss: 0.04587305734518591\n",
      "Epoch: 1980 Training Loss: 0.03445306729185241 Test Loss: 0.04593715095659547\n",
      "Epoch: 1990 Training Loss: 0.03445198038065857 Test Loss: 0.04585612980047978\n",
      "Epoch: 2000 Training Loss: 0.03445316216985135 Test Loss: 0.0458894937511073\n",
      "Epoch: 2010 Training Loss: 0.03445338541220182 Test Loss: 0.04585908779543337\n",
      "Epoch: 2020 Training Loss: 0.034449459137362984 Test Loss: 0.045849086423816694\n",
      "Epoch: 2030 Training Loss: 0.034450336758853256 Test Loss: 0.04584648841126783\n",
      "Epoch: 2040 Training Loss: 0.03444927496242385 Test Loss: 0.04586607257030964\n",
      "Epoch: 2050 Training Loss: 0.034445626345258405 Test Loss: 0.04586822409305419\n",
      "Epoch: 2060 Training Loss: 0.03444818247017125 Test Loss: 0.04583967386062946\n",
      "Epoch: 2070 Training Loss: 0.034448160145936206 Test Loss: 0.04584872644141196\n",
      "Epoch: 2080 Training Loss: 0.0344446078020344 Test Loss: 0.04584986499041297\n",
      "Epoch: 2090 Training Loss: 0.034444491995065094 Test Loss: 0.04590091551841395\n",
      "Epoch: 2100 Training Loss: 0.03444313300725663 Test Loss: 0.04586473868201925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2110 Training Loss: 0.034437557529553714 Test Loss: 0.045813699316263376\n",
      "Epoch: 2120 Training Loss: 0.03443457724417498 Test Loss: 0.045873674059228126\n",
      "Epoch: 2130 Training Loss: 0.03443274805216585 Test Loss: 0.045869750530072695\n",
      "Epoch: 2140 Training Loss: 0.034430712360982524 Test Loss: 0.04583919109352854\n",
      "Epoch: 2150 Training Loss: 0.03442982357737473 Test Loss: 0.04591335584058671\n",
      "Epoch: 2160 Training Loss: 0.03443050167601427 Test Loss: 0.04585688604258584\n",
      "Epoch: 2170 Training Loss: 0.03442687677834856 Test Loss: 0.045893743775932125\n",
      "Epoch: 2180 Training Loss: 0.034425199670190684 Test Loss: 0.04588032396675115\n",
      "Epoch: 2190 Training Loss: 0.03442803205751223 Test Loss: 0.04584652747912571\n",
      "Epoch: 2200 Training Loss: 0.034419586520341126 Test Loss: 0.04585489079127281\n",
      "Epoch: 2210 Training Loss: 0.034420673431534966 Test Loss: 0.045875259098033444\n",
      "Epoch: 2220 Training Loss: 0.03441618765555527 Test Loss: 0.045848405526865116\n",
      "Epoch: 2230 Training Loss: 0.03441656298175699 Test Loss: 0.045858485034197545\n",
      "Epoch: 2240 Training Loss: 0.034415563972238654 Test Loss: 0.04586139838017071\n",
      "Epoch: 2250 Training Loss: 0.0344102717332679 Test Loss: 0.04584325694130909\n",
      "Epoch: 2260 Training Loss: 0.0344120479052188 Test Loss: 0.045843642038765314\n",
      "Epoch: 2270 Training Loss: 0.034412813905533836 Test Loss: 0.045871226736988215\n",
      "Epoch: 2280 Training Loss: 0.0344079123406764 Test Loss: 0.045896216213223526\n",
      "Epoch: 2290 Training Loss: 0.03440955177668764 Test Loss: 0.04596986191588418\n",
      "Epoch: 2300 Training Loss: 0.03440620593196002 Test Loss: 0.045910741084670185\n",
      "Epoch: 2310 Training Loss: 0.03440397908951411 Test Loss: 0.04590029880437174\n",
      "Epoch: 2320 Training Loss: 0.034403582834342035 Test Loss: 0.045895462761678746\n",
      "Epoch: 2330 Training Loss: 0.0343990007850987 Test Loss: 0.0458742349620448\n",
      "Epoch: 2340 Training Loss: 0.03440454556697842 Test Loss: 0.045898155653311026\n",
      "Epoch: 2350 Training Loss: 0.034399955146146945 Test Loss: 0.04591590083247131\n",
      "Epoch: 2360 Training Loss: 0.0344018108481852 Test Loss: 0.045904825094763\n",
      "Epoch: 2370 Training Loss: 0.034396889749622094 Test Loss: 0.04587876962411988\n",
      "Epoch: 2380 Training Loss: 0.03439400434224231 Test Loss: 0.04591183219412948\n",
      "Epoch: 2390 Training Loss: 0.03439663302091906 Test Loss: 0.04589984952400615\n",
      "Epoch: 2400 Training Loss: 0.03439140496412406 Test Loss: 0.04591203311454142\n",
      "Epoch: 2410 Training Loss: 0.03438866605953677 Test Loss: 0.04589724872089601\n",
      "Epoch: 2420 Training Loss: 0.03439063477801495 Test Loss: 0.0459163752278884\n",
      "Epoch: 2430 Training Loss: 0.034382715255632135 Test Loss: 0.04592700447579239\n",
      "Epoch: 2440 Training Loss: 0.03438484582481441 Test Loss: 0.04592931226996845\n",
      "Epoch: 2450 Training Loss: 0.034382677583485496 Test Loss: 0.04595928568864444\n",
      "Epoch: 2460 Training Loss: 0.034384449569642324 Test Loss: 0.045920223411889345\n",
      "Epoch: 2470 Training Loss: 0.034378034142595784 Test Loss: 0.04595451661942209\n",
      "Epoch: 2480 Training Loss: 0.03437775090386363 Test Loss: 0.045898577028063846\n",
      "Epoch: 2490 Training Loss: 0.03437952986634391 Test Loss: 0.04597597603564203\n",
      "Epoch: 2500 Training Loss: 0.03437291212591726 Test Loss: 0.045938590886214395\n",
      "Epoch: 2510 Training Loss: 0.034369933235803216 Test Loss: 0.045957979705966816\n",
      "Epoch: 2520 Training Loss: 0.03436526886594315 Test Loss: 0.0459798102668366\n",
      "Epoch: 2530 Training Loss: 0.03436160769139549 Test Loss: 0.045949557792032907\n",
      "Epoch: 2540 Training Loss: 0.034361595134013274 Test Loss: 0.04595031961526152\n",
      "Epoch: 2550 Training Loss: 0.03435783350040791 Test Loss: 0.04593499943385093\n",
      "Epoch: 2560 Training Loss: 0.03435492716405777 Test Loss: 0.04594440362535433\n",
      "Epoch: 2570 Training Loss: 0.034350255817874244 Test Loss: 0.045928296505663636\n",
      "Epoch: 2580 Training Loss: 0.03435016233513999 Test Loss: 0.046090807632189555\n",
      "Epoch: 2590 Training Loss: 0.034345906777834216 Test Loss: 0.04594532451057574\n",
      "Epoch: 2600 Training Loss: 0.03434542262098664 Test Loss: 0.04596105211393277\n",
      "Epoch: 2610 Training Loss: 0.034342136772640704 Test Loss: 0.04598516535392711\n",
      "Epoch: 2620 Training Loss: 0.0343371570729606 Test Loss: 0.04600256171292777\n",
      "Epoch: 2630 Training Loss: 0.03433387261987936 Test Loss: 0.04598722199758824\n",
      "Epoch: 2640 Training Loss: 0.03433729520416495 Test Loss: 0.045972300866440254\n",
      "Epoch: 2650 Training Loss: 0.034338453273858 Test Loss: 0.04601105060033233\n",
      "Epoch: 2660 Training Loss: 0.03433784075265891 Test Loss: 0.0459827180316872\n",
      "Epoch: 2670 Training Loss: 0.03433365077279358 Test Loss: 0.04600091249121308\n",
      "Epoch: 2680 Training Loss: 0.03433046259297597 Test Loss: 0.045993740748731256\n",
      "Epoch: 2690 Training Loss: 0.034331165806379946 Test Loss: 0.04598070324644523\n",
      "Epoch: 2700 Training Loss: 0.03432988913918821 Test Loss: 0.046016257787675166\n",
      "Epoch: 2710 Training Loss: 0.03432502245594803 Test Loss: 0.04606540515288525\n",
      "Epoch: 2720 Training Loss: 0.03432425366510361 Test Loss: 0.04597559651930836\n",
      "Epoch: 2730 Training Loss: 0.034324566204394266 Test Loss: 0.045973922182542176\n",
      "Epoch: 2740 Training Loss: 0.03431977347018268 Test Loss: 0.04598150413753172\n",
      "Epoch: 2750 Training Loss: 0.0343203385523823 Test Loss: 0.0459804883732269\n",
      "Epoch: 2760 Training Loss: 0.034320192049589804 Test Loss: 0.045990916700718955\n",
      "Epoch: 2770 Training Loss: 0.03431679178953926 Test Loss: 0.04595196604641494\n",
      "Epoch: 2780 Training Loss: 0.03432035529555858 Test Loss: 0.04596320642723859\n",
      "Epoch: 2790 Training Loss: 0.03431493887803038 Test Loss: 0.045953576200271753\n",
      "Epoch: 2800 Training Loss: 0.03431243577317577 Test Loss: 0.04603544289645437\n",
      "Epoch: 2810 Training Loss: 0.03431142839206929 Test Loss: 0.04595175117319661\n",
      "Epoch: 2820 Training Loss: 0.034311361419364146 Test Loss: 0.045969309384751335\n",
      "Epoch: 2830 Training Loss: 0.034307546765700546 Test Loss: 0.0460088600097299\n",
      "Epoch: 2840 Training Loss: 0.034303673510919945 Test Loss: 0.0459820762025935\n",
      "Epoch: 2850 Training Loss: 0.03431478260838505 Test Loss: 0.0460689380034619\n",
      "Epoch: 2860 Training Loss: 0.03430279449416498 Test Loss: 0.04600740891786588\n",
      "Epoch: 2870 Training Loss: 0.03430093042053858 Test Loss: 0.04598543882893225\n",
      "Epoch: 2880 Training Loss: 0.034303799084742084 Test Loss: 0.04597703644892728\n",
      "Epoch: 2890 Training Loss: 0.034300373709927105 Test Loss: 0.04606132256173704\n",
      "Epoch: 2900 Training Loss: 0.034301342023622255 Test Loss: 0.04596381756015825\n",
      "Epoch: 2910 Training Loss: 0.03430226150305324 Test Loss: 0.04601398348023443\n",
      "Epoch: 2920 Training Loss: 0.034297749217044426 Test Loss: 0.046058897563987344\n",
      "Epoch: 2930 Training Loss: 0.03429752597469396 Test Loss: 0.0460354875454348\n",
      "Epoch: 2940 Training Loss: 0.03429949190264276 Test Loss: 0.04605501868381235\n",
      "Epoch: 2950 Training Loss: 0.034304701820996784 Test Loss: 0.04605080493628412\n",
      "Epoch: 2960 Training Loss: 0.03429680183231963 Test Loss: 0.046004766256336586\n",
      "Epoch: 2970 Training Loss: 0.034295066123044754 Test Loss: 0.046027216321809845\n",
      "Epoch: 2980 Training Loss: 0.03429036966209681 Test Loss: 0.046035788926052715\n",
      "Epoch: 2990 Training Loss: 0.03429188631481529 Test Loss: 0.046006800575507496\n",
      "Epoch: 3000 Training Loss: 0.034295674458449776 Test Loss: 0.04600708242219647\n",
      "Epoch: 3010 Training Loss: 0.03428715915804415 Test Loss: 0.04601226728504909\n",
      "Epoch: 3020 Training Loss: 0.034287305660836646 Test Loss: 0.046063264792385816\n",
      "Epoch: 3030 Training Loss: 0.03428527973650616 Test Loss: 0.046034390854852954\n",
      "Epoch: 3040 Training Loss: 0.03428530624653528 Test Loss: 0.0459957471622894\n",
      "Epoch: 3050 Training Loss: 0.034280025169682045 Test Loss: 0.045982715241125925\n",
      "Epoch: 3060 Training Loss: 0.03428287290491519 Test Loss: 0.045976517404529765\n",
      "Epoch: 3070 Training Loss: 0.03428139113381397 Test Loss: 0.046005929920389085\n",
      "Epoch: 3080 Training Loss: 0.03427959263762802 Test Loss: 0.0459775554933248\n",
      "Epoch: 3090 Training Loss: 0.03427864525290322 Test Loss: 0.04599941675036862\n",
      "Epoch: 3100 Training Loss: 0.03427676443610054 Test Loss: 0.046139648035659175\n",
      "Epoch: 3110 Training Loss: 0.03427534545191038 Test Loss: 0.0460272832952805\n",
      "Epoch: 3120 Training Loss: 0.03427517383435346 Test Loss: 0.04601501040678436\n",
      "Epoch: 3130 Training Loss: 0.03426818774404854 Test Loss: 0.04604047706899803\n",
      "Epoch: 3140 Training Loss: 0.0342684849354276 Test Loss: 0.04611946948706537\n",
      "Epoch: 3150 Training Loss: 0.03427073131157917 Test Loss: 0.045967431337011935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3160 Training Loss: 0.034264015902624186 Test Loss: 0.04596272924126023\n",
      "Epoch: 3170 Training Loss: 0.03425791720066237 Test Loss: 0.046017161929528906\n",
      "Epoch: 3180 Training Loss: 0.03425764233351836 Test Loss: 0.0460494012839618\n",
      "Epoch: 3190 Training Loss: 0.034259784064818145 Test Loss: 0.04607714783473876\n",
      "Epoch: 3200 Training Loss: 0.03425612568079987 Test Loss: 0.04599855725749531\n",
      "Epoch: 3210 Training Loss: 0.03425762419507738 Test Loss: 0.04603218352088286\n",
      "Epoch: 3220 Training Loss: 0.03425118365326641 Test Loss: 0.045995814135760045\n",
      "Epoch: 3230 Training Loss: 0.03425014278580736 Test Loss: 0.0460024975300184\n",
      "Epoch: 3240 Training Loss: 0.03424270742027212 Test Loss: 0.04599288962754178\n",
      "Epoch: 3250 Training Loss: 0.03425232497978317 Test Loss: 0.046000731104730075\n",
      "Epoch: 3260 Training Loss: 0.03424413059025635 Test Loss: 0.0460032370287568\n",
      "Epoch: 3270 Training Loss: 0.034246160700380904 Test Loss: 0.04610633710569592\n",
      "Epoch: 3280 Training Loss: 0.03423977875968693 Test Loss: 0.046001342237649734\n",
      "Epoch: 3290 Training Loss: 0.03424602396444124 Test Loss: 0.046044456409379\n",
      "Epoch: 3300 Training Loss: 0.03423509206559182 Test Loss: 0.04600415512341693\n",
      "Epoch: 3310 Training Loss: 0.0342397257396287 Test Loss: 0.04602522107049681\n",
      "Epoch: 3320 Training Loss: 0.034232912662145384 Test Loss: 0.046157755987785466\n",
      "Epoch: 3330 Training Loss: 0.03423201132115537 Test Loss: 0.046046566073704394\n",
      "Epoch: 3340 Training Loss: 0.03423270755823589 Test Loss: 0.04602268165973476\n",
      "Epoch: 3350 Training Loss: 0.03422895708674805 Test Loss: 0.04601434346263916\n",
      "Epoch: 3360 Training Loss: 0.03422712789473891 Test Loss: 0.045985076055966245\n",
      "Epoch: 3370 Training Loss: 0.0342272018437675 Test Loss: 0.04603734605924527\n",
      "Epoch: 3380 Training Loss: 0.03422794272931812 Test Loss: 0.04610243311046944\n",
      "Epoch: 3390 Training Loss: 0.034228308288667006 Test Loss: 0.046028608811887056\n",
      "Epoch: 3400 Training Loss: 0.03422272443937595 Test Loss: 0.04599848470290211\n",
      "Epoch: 3410 Training Loss: 0.03422168217665221 Test Loss: 0.04603009339048641\n",
      "Epoch: 3420 Training Loss: 0.034225572174609094 Test Loss: 0.04602561453963686\n",
      "Epoch: 3430 Training Loss: 0.03421874653974357 Test Loss: 0.046035724743143346\n",
      "Epoch: 3440 Training Loss: 0.03421775450654868 Test Loss: 0.04605288390443547\n",
      "Epoch: 3450 Training Loss: 0.034213005025542496 Test Loss: 0.04601362070726842\n",
      "Epoch: 3460 Training Loss: 0.0342121301945816 Test Loss: 0.046044406179276014\n",
      "Epoch: 3470 Training Loss: 0.034213376165950146 Test Loss: 0.04600394304075987\n",
      "Epoch: 3480 Training Loss: 0.034209329898347934 Test Loss: 0.04601724843692849\n",
      "Epoch: 3490 Training Loss: 0.03420651146367329 Test Loss: 0.046030431048400926\n",
      "Epoch: 3500 Training Loss: 0.03420889457576452 Test Loss: 0.04600779122476083\n",
      "Epoch: 3510 Training Loss: 0.03420867830973751 Test Loss: 0.04599765869676413\n",
      "Epoch: 3520 Training Loss: 0.03420561988953611 Test Loss: 0.045988932611651025\n",
      "Epoch: 3530 Training Loss: 0.034200751811031244 Test Loss: 0.04602971108359146\n",
      "Epoch: 3540 Training Loss: 0.034202215443691494 Test Loss: 0.04608331776572216\n",
      "Epoch: 3550 Training Loss: 0.034192854612883475 Test Loss: 0.046110866186648454\n",
      "Epoch: 3560 Training Loss: 0.034195963262613725 Test Loss: 0.04600006416058488\n",
      "Epoch: 3570 Training Loss: 0.03419623673449305 Test Loss: 0.04601610430680493\n",
      "Epoch: 3580 Training Loss: 0.03419034034591134 Test Loss: 0.046009694387551724\n",
      "Epoch: 3590 Training Loss: 0.034180196771612 Test Loss: 0.046031463556073404\n",
      "Epoch: 3600 Training Loss: 0.03418041722343309 Test Loss: 0.04601447461901918\n",
      "Epoch: 3610 Training Loss: 0.034178763834774945 Test Loss: 0.046026169861330984\n",
      "Epoch: 3620 Training Loss: 0.034179534020884056 Test Loss: 0.046018253038988204\n",
      "Epoch: 3630 Training Loss: 0.034183916547276656 Test Loss: 0.04610387304008835\n",
      "Epoch: 3640 Training Loss: 0.03417770203834553 Test Loss: 0.04598055813725883\n",
      "Epoch: 3650 Training Loss: 0.03417459896967404 Test Loss: 0.045998041003659076\n",
      "Epoch: 3660 Training Loss: 0.03416741335651839 Test Loss: 0.04613802392899598\n",
      "Epoch: 3670 Training Loss: 0.03417177913973471 Test Loss: 0.04613801834787342\n",
      "Epoch: 3680 Training Loss: 0.034172307945052376 Test Loss: 0.04598960513691878\n",
      "Epoch: 3690 Training Loss: 0.03416614227038542 Test Loss: 0.04598201760080668\n",
      "Epoch: 3700 Training Loss: 0.03417323300554213 Test Loss: 0.04599816657891654\n",
      "Epoch: 3710 Training Loss: 0.03416990808978486 Test Loss: 0.04601791538107369\n",
      "Epoch: 3720 Training Loss: 0.034167424518635914 Test Loss: 0.045980139553067276\n",
      "Epoch: 3730 Training Loss: 0.03416418750455415 Test Loss: 0.04597718434867496\n",
      "Epoch: 3740 Training Loss: 0.034163597307590096 Test Loss: 0.0460953115980906\n",
      "Epoch: 3750 Training Loss: 0.03416614087512073 Test Loss: 0.045995158353859956\n",
      "Epoch: 3760 Training Loss: 0.03416041889462532 Test Loss: 0.04599179014639865\n",
      "Epoch: 3770 Training Loss: 0.03416278805406965 Test Loss: 0.046109364864681444\n",
      "Epoch: 3780 Training Loss: 0.034151917546866604 Test Loss: 0.04597219482511173\n",
      "Epoch: 3790 Training Loss: 0.03415730466383631 Test Loss: 0.04596114978357746\n",
      "Epoch: 3800 Training Loss: 0.03415459645507221 Test Loss: 0.046052303467689855\n",
      "Epoch: 3810 Training Loss: 0.03415331839261579 Test Loss: 0.045967191348742116\n",
      "Epoch: 3820 Training Loss: 0.03414446404289039 Test Loss: 0.04600836328982261\n",
      "Epoch: 3830 Training Loss: 0.034146386717633787 Test Loss: 0.04599358168673847\n",
      "Epoch: 3840 Training Loss: 0.034145549558819537 Test Loss: 0.045956361180426175\n",
      "Epoch: 3850 Training Loss: 0.034145712804788314 Test Loss: 0.04600226312287114\n",
      "Epoch: 3860 Training Loss: 0.034147043887302976 Test Loss: 0.045974993758072534\n",
      "Epoch: 3870 Training Loss: 0.03414608673572535 Test Loss: 0.04601780654918389\n",
      "Epoch: 3880 Training Loss: 0.03413973549085456 Test Loss: 0.04598565370215058\n",
      "Epoch: 3890 Training Loss: 0.03413805419690261 Test Loss: 0.04601851256118696\n",
      "Epoch: 3900 Training Loss: 0.034141874431624974 Test Loss: 0.04600993437582154\n",
      "Epoch: 3910 Training Loss: 0.0341398903652352 Test Loss: 0.046032791863241246\n",
      "Epoch: 3920 Training Loss: 0.03413918715183123 Test Loss: 0.04609105599214321\n",
      "Epoch: 3930 Training Loss: 0.03413221501417321 Test Loss: 0.045995632749277045\n",
      "Epoch: 3940 Training Loss: 0.034141712580920885 Test Loss: 0.046044791276732236\n",
      "Epoch: 3950 Training Loss: 0.03413150063865172 Test Loss: 0.04600051623151175\n",
      "Epoch: 3960 Training Loss: 0.034134377674443356 Test Loss: 0.04600986182122834\n",
      "Epoch: 3970 Training Loss: 0.03413482834493836 Test Loss: 0.046007255436995645\n",
      "Epoch: 3980 Training Loss: 0.034132847069077966 Test Loss: 0.04599570251330897\n",
      "Epoch: 3990 Training Loss: 0.034126116312211394 Test Loss: 0.04598550859296418\n",
      "Epoch: 4000 Training Loss: 0.03412636606459098 Test Loss: 0.046141397717579836\n",
      "Epoch: 4010 Training Loss: 0.03412650140526595 Test Loss: 0.04602173565946187\n",
      "Epoch: 4020 Training Loss: 0.03412391318926522 Test Loss: 0.046083842391242226\n",
      "Epoch: 4030 Training Loss: 0.03412706927799495 Test Loss: 0.045984498409781915\n",
      "Epoch: 4040 Training Loss: 0.034128919398974444 Test Loss: 0.04598947398053876\n",
      "Epoch: 4050 Training Loss: 0.03412308021624504 Test Loss: 0.046028290687901485\n",
      "Epoch: 4060 Training Loss: 0.034122085392520776 Test Loss: 0.046081559712117665\n",
      "Epoch: 4070 Training Loss: 0.03412218445631379 Test Loss: 0.046112825160664894\n",
      "Epoch: 4080 Training Loss: 0.034125785634479766 Test Loss: 0.046088550068116484\n",
      "Epoch: 4090 Training Loss: 0.03412490801298949 Test Loss: 0.04602129475078011\n",
      "Epoch: 4100 Training Loss: 0.03411633411146687 Test Loss: 0.0460203627033136\n",
      "Epoch: 4110 Training Loss: 0.03411396774255192 Test Loss: 0.04602976968537828\n",
      "Epoch: 4120 Training Loss: 0.034112109249984285 Test Loss: 0.04605040588602151\n",
      "Epoch: 4130 Training Loss: 0.034116928494224985 Test Loss: 0.04609445210521729\n",
      "Epoch: 4140 Training Loss: 0.034118428403767186 Test Loss: 0.046199706495462234\n",
      "Epoch: 4150 Training Loss: 0.034110550739325084 Test Loss: 0.046046722345135904\n",
      "Epoch: 4160 Training Loss: 0.03411284036868206 Test Loss: 0.04606788875242176\n",
      "Epoch: 4170 Training Loss: 0.03411361753111462 Test Loss: 0.046026119631228\n",
      "Epoch: 4180 Training Loss: 0.03411239527924582 Test Loss: 0.04610757332434162\n",
      "Epoch: 4190 Training Loss: 0.0341078983411486 Test Loss: 0.04601504668408096\n",
      "Epoch: 4200 Training Loss: 0.03410649749539942 Test Loss: 0.04604143144095476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4210 Training Loss: 0.03410789415535453 Test Loss: 0.04598339613807751\n",
      "Epoch: 4220 Training Loss: 0.03410486224518225 Test Loss: 0.04601808002418903\n",
      "Epoch: 4230 Training Loss: 0.03410171731857005 Test Loss: 0.04611321583924367\n",
      "Epoch: 4240 Training Loss: 0.03410692165586531 Test Loss: 0.046104844155412744\n",
      "Epoch: 4250 Training Loss: 0.03410812297876376 Test Loss: 0.04605031100693809\n",
      "Epoch: 4260 Training Loss: 0.03409751338605782 Test Loss: 0.04601125431130555\n",
      "Epoch: 4270 Training Loss: 0.034104446456304505 Test Loss: 0.046014728560095385\n",
      "Epoch: 4280 Training Loss: 0.03411027866271046 Test Loss: 0.046255852588354976\n",
      "Epoch: 4290 Training Loss: 0.034100164388969616 Test Loss: 0.046008608859214976\n",
      "Epoch: 4300 Training Loss: 0.03409541211743405 Test Loss: 0.046020290148720396\n",
      "Epoch: 4310 Training Loss: 0.034098138464639126 Test Loss: 0.04605160303680934\n",
      "Epoch: 4320 Training Loss: 0.03409745478494082 Test Loss: 0.04600001393048189\n",
      "Epoch: 4330 Training Loss: 0.03409924351427394 Test Loss: 0.045975286767006615\n",
      "Epoch: 4340 Training Loss: 0.034096058124985715 Test Loss: 0.046051770470485955\n",
      "Epoch: 4350 Training Loss: 0.0340968311016242 Test Loss: 0.04596268738284107\n",
      "Epoch: 4360 Training Loss: 0.034095402350581214 Test Loss: 0.045948648069056616\n",
      "Epoch: 4370 Training Loss: 0.03409978208644444 Test Loss: 0.04600602200891122\n",
      "Epoch: 4380 Training Loss: 0.03409027335757924 Test Loss: 0.046035133144152626\n",
      "Epoch: 4390 Training Loss: 0.034092727628169686 Test Loss: 0.04600007253226871\n",
      "Epoch: 4400 Training Loss: 0.03408804651513333 Test Loss: 0.04600373932978666\n",
      "Epoch: 4410 Training Loss: 0.03409341549366206 Test Loss: 0.04598930654686214\n",
      "Epoch: 4420 Training Loss: 0.034090313820255264 Test Loss: 0.045964919831862655\n",
      "Epoch: 4430 Training Loss: 0.03408863671209738 Test Loss: 0.04600042972411216\n",
      "Epoch: 4440 Training Loss: 0.03408833114913017 Test Loss: 0.045992231055080414\n",
      "Epoch: 4450 Training Loss: 0.034086948441821974 Test Loss: 0.045933802283063103\n",
      "Epoch: 4460 Training Loss: 0.034095234918818364 Test Loss: 0.04601019947914285\n",
      "Epoch: 4470 Training Loss: 0.034084551377083834 Test Loss: 0.045964783094360084\n",
      "Epoch: 4480 Training Loss: 0.03408841207448222 Test Loss: 0.04595177907880938\n",
      "Epoch: 4490 Training Loss: 0.03408723307581882 Test Loss: 0.046013149102412616\n",
      "Epoch: 4500 Training Loss: 0.0340888515828597 Test Loss: 0.046059581251500205\n",
      "Epoch: 4510 Training Loss: 0.034081077168004695 Test Loss: 0.04609315170366222\n",
      "Epoch: 4520 Training Loss: 0.03408887111656537 Test Loss: 0.046040485440681865\n",
      "Epoch: 4530 Training Loss: 0.03408370445141675 Test Loss: 0.046008290735229405\n",
      "Epoch: 4540 Training Loss: 0.034086895421763734 Test Loss: 0.0460775803717367\n",
      "Epoch: 4550 Training Loss: 0.034083248199862984 Test Loss: 0.04598693736033799\n",
      "Epoch: 4560 Training Loss: 0.03408098508053512 Test Loss: 0.04602615869908588\n",
      "Epoch: 4570 Training Loss: 0.034087683746313824 Test Loss: 0.045977538749957136\n",
      "Epoch: 4580 Training Loss: 0.03408228128143252 Test Loss: 0.0460033821379432\n",
      "Epoch: 4590 Training Loss: 0.034078445698798566 Test Loss: 0.045973888695806854\n",
      "Epoch: 4600 Training Loss: 0.03408093624627096 Test Loss: 0.04597732387673881\n",
      "Epoch: 4610 Training Loss: 0.034085381559574635 Test Loss: 0.04599209989870039\n",
      "Epoch: 4620 Training Loss: 0.03408480531525749 Test Loss: 0.04598448166641425\n",
      "Epoch: 4630 Training Loss: 0.03408235383519642 Test Loss: 0.04598516535392711\n",
      "Epoch: 4640 Training Loss: 0.03408160876385174 Test Loss: 0.04595202743876303\n",
      "Epoch: 4650 Training Loss: 0.03407498265183694 Test Loss: 0.04620011391740867\n",
      "Epoch: 4660 Training Loss: 0.03407721368007693 Test Loss: 0.04601408673100168\n",
      "Epoch: 4670 Training Loss: 0.03408425976676353 Test Loss: 0.04604226023765402\n",
      "Epoch: 4680 Training Loss: 0.03407763784054281 Test Loss: 0.04603141053540914\n",
      "Epoch: 4690 Training Loss: 0.034076288619587176 Test Loss: 0.04616019214778027\n",
      "Epoch: 4700 Training Loss: 0.0340752226373637 Test Loss: 0.04599851818963743\n",
      "Epoch: 4710 Training Loss: 0.03407975864287225 Test Loss: 0.04602374765414257\n",
      "Epoch: 4720 Training Loss: 0.03407770202271857 Test Loss: 0.04596342967214075\n",
      "Epoch: 4730 Training Loss: 0.03408161713543988 Test Loss: 0.04601857953465761\n",
      "Epoch: 4740 Training Loss: 0.03408525319522311 Test Loss: 0.045971645084540165\n",
      "Epoch: 4750 Training Loss: 0.0340750621819243 Test Loss: 0.04602267328805093\n",
      "Epoch: 4760 Training Loss: 0.03407541378862628 Test Loss: 0.04598703782054396\n",
      "Epoch: 4770 Training Loss: 0.03407907356790925 Test Loss: 0.04613411993376949\n",
      "Epoch: 4780 Training Loss: 0.034074615697223365 Test Loss: 0.04617601183965944\n",
      "Epoch: 4790 Training Loss: 0.03407363343088131 Test Loss: 0.04605524750983706\n",
      "Epoch: 4800 Training Loss: 0.034073115787681164 Test Loss: 0.045990805078267874\n",
      "Epoch: 4810 Training Loss: 0.03407338228323704 Test Loss: 0.046093079149069016\n",
      "Epoch: 4820 Training Loss: 0.03406669896536993 Test Loss: 0.04605379641797304\n",
      "Epoch: 4830 Training Loss: 0.03407714252157771 Test Loss: 0.045986982009318424\n",
      "Epoch: 4840 Training Loss: 0.034078308962858905 Test Loss: 0.046090802051067005\n",
      "Epoch: 4850 Training Loss: 0.03406727660495176 Test Loss: 0.04601156964472985\n",
      "Epoch: 4860 Training Loss: 0.034074746852104265 Test Loss: 0.04610682266335812\n",
      "Epoch: 4870 Training Loss: 0.03406584227285001 Test Loss: 0.046010514812567155\n",
      "Epoch: 4880 Training Loss: 0.03406754170524295 Test Loss: 0.046236508417582985\n",
      "Epoch: 4890 Training Loss: 0.034067166379041225 Test Loss: 0.04615989355772363\n",
      "Epoch: 4900 Training Loss: 0.03406295128441147 Test Loss: 0.04601546526827251\n",
      "Epoch: 4910 Training Loss: 0.03407036013991759 Test Loss: 0.04598343520593538\n",
      "Epoch: 4920 Training Loss: 0.03406370891313837 Test Loss: 0.046033090453297884\n",
      "Epoch: 4930 Training Loss: 0.0340584473699908 Test Loss: 0.046049462676309896\n",
      "Epoch: 4940 Training Loss: 0.0340637912337551 Test Loss: 0.04615179534889785\n",
      "Epoch: 4950 Training Loss: 0.034068547691084736 Test Loss: 0.04618961861644597\n",
      "Epoch: 4960 Training Loss: 0.03405905151960175 Test Loss: 0.04603067382723202\n",
      "Epoch: 4970 Training Loss: 0.0340568888593316 Test Loss: 0.04606369453882247\n",
      "Epoch: 4980 Training Loss: 0.034057159540681545 Test Loss: 0.04602078407806642\n",
      "Epoch: 4990 Training Loss: 0.03405823808028723 Test Loss: 0.04605472567487827\n",
      "Epoch: 5000 Training Loss: 0.03406104395757967 Test Loss: 0.04605890593567118\n",
      "Epoch: 5010 Training Loss: 0.03405226216161818 Test Loss: 0.04613718117949033\n",
      "Epoch: 5020 Training Loss: 0.03405563312111023 Test Loss: 0.04602701261083663\n",
      "Epoch: 5030 Training Loss: 0.034055801948137766 Test Loss: 0.04606913892387385\n",
      "Epoch: 5040 Training Loss: 0.034052263556882865 Test Loss: 0.04606878731315295\n",
      "Epoch: 5050 Training Loss: 0.03405037297322735 Test Loss: 0.046061448136994504\n",
      "Epoch: 5060 Training Loss: 0.03405539313558347 Test Loss: 0.04607314616986758\n",
      "Epoch: 5070 Training Loss: 0.034053286285900944 Test Loss: 0.046129722009196975\n",
      "Epoch: 5080 Training Loss: 0.03405971008453563 Test Loss: 0.04602481643911165\n",
      "Epoch: 5090 Training Loss: 0.034042567862549146 Test Loss: 0.04604005848480649\n",
      "Epoch: 5100 Training Loss: 0.034043165035836646 Test Loss: 0.04611825001178733\n",
      "Epoch: 5110 Training Loss: 0.03404801497590054 Test Loss: 0.04604018126950268\n",
      "Epoch: 5120 Training Loss: 0.03404476400917187 Test Loss: 0.04608431120553676\n",
      "Epoch: 5130 Training Loss: 0.03404213533049512 Test Loss: 0.04603341415840601\n",
      "Epoch: 5140 Training Loss: 0.0340382788188908 Test Loss: 0.04609930210071667\n",
      "Epoch: 5150 Training Loss: 0.03403971315099255 Test Loss: 0.046064562403379604\n",
      "Epoch: 5160 Training Loss: 0.03404187023020394 Test Loss: 0.04606683112969779\n",
      "Epoch: 5170 Training Loss: 0.03403844625065365 Test Loss: 0.0460297306175204\n",
      "Epoch: 5180 Training Loss: 0.03403760211551595 Test Loss: 0.0461179988612724\n",
      "Epoch: 5190 Training Loss: 0.03404710107752832 Test Loss: 0.04604008639041926\n",
      "Epoch: 5200 Training Loss: 0.0340407847142748 Test Loss: 0.04601650056650626\n",
      "Epoch: 5210 Training Loss: 0.0340472350229386 Test Loss: 0.04610139223111313\n",
      "Epoch: 5220 Training Loss: 0.034035143659131434 Test Loss: 0.04608495024406919\n",
      "Epoch: 5230 Training Loss: 0.0340405363571599 Test Loss: 0.0460440322440649\n",
      "Epoch: 5240 Training Loss: 0.0340428511012813 Test Loss: 0.04603450526786531\n",
      "Epoch: 5250 Training Loss: 0.0340379732559236 Test Loss: 0.046069903537663735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5260 Training Loss: 0.03403321679859397 Test Loss: 0.046111993573404356\n",
      "Epoch: 5270 Training Loss: 0.03403922899414498 Test Loss: 0.04603758604751509\n",
      "Epoch: 5280 Training Loss: 0.03404023497998677 Test Loss: 0.04620236590035919\n",
      "Epoch: 5290 Training Loss: 0.034040087081929585 Test Loss: 0.046127096091035345\n",
      "Epoch: 5300 Training Loss: 0.03403224848489882 Test Loss: 0.04608649063389408\n",
      "Epoch: 5310 Training Loss: 0.03403960571561139 Test Loss: 0.046068999395809995\n",
      "Epoch: 5320 Training Loss: 0.034035615258596795 Test Loss: 0.04606865057565038\n",
      "Epoch: 5330 Training Loss: 0.03404530397660706 Test Loss: 0.04618000513284679\n",
      "Epoch: 5340 Training Loss: 0.03403572548450734 Test Loss: 0.04612479666854311\n",
      "Epoch: 5350 Training Loss: 0.034033038204713595 Test Loss: 0.04605551261315838\n",
      "Epoch: 5360 Training Loss: 0.034036028256945156 Test Loss: 0.04604524892878166\n",
      "Epoch: 5370 Training Loss: 0.034032237322781295 Test Loss: 0.046074053102282596\n",
      "Epoch: 5380 Training Loss: 0.03403515342598427 Test Loss: 0.04607255457087686\n",
      "Epoch: 5390 Training Loss: 0.03403408186270203 Test Loss: 0.04613908434228123\n",
      "Epoch: 5400 Training Loss: 0.03403264613533558 Test Loss: 0.04613083544314649\n",
      "Epoch: 5410 Training Loss: 0.03403696727008181 Test Loss: 0.04610795005011402\n",
      "Epoch: 5420 Training Loss: 0.034034726474989 Test Loss: 0.04606587396717979\n",
      "Epoch: 5430 Training Loss: 0.03403453253319703 Test Loss: 0.04610541063935197\n",
      "Epoch: 5440 Training Loss: 0.034028078038739154 Test Loss: 0.046058677109646465\n",
      "Epoch: 5450 Training Loss: 0.03403219406957589 Test Loss: 0.04610520692837875\n",
      "Epoch: 5460 Training Loss: 0.03403379304291111 Test Loss: 0.046099101180304726\n",
      "Epoch: 5470 Training Loss: 0.034029614225163304 Test Loss: 0.046124888757065254\n",
      "Epoch: 5480 Training Loss: 0.03402644139325729 Test Loss: 0.046057304153498194\n",
      "Epoch: 5490 Training Loss: 0.03403419487914195 Test Loss: 0.04602855300066152\n",
      "Epoch: 5500 Training Loss: 0.03402436244886857 Test Loss: 0.04616953773749686\n",
      "Epoch: 5510 Training Loss: 0.03402569632191261 Test Loss: 0.046075900453847955\n",
      "Epoch: 5520 Training Loss: 0.03403228197125138 Test Loss: 0.04612589056856369\n",
      "Epoch: 5530 Training Loss: 0.03403620126976677 Test Loss: 0.04609758590553133\n",
      "Epoch: 5540 Training Loss: 0.034029787237984915 Test Loss: 0.04615228927824387\n",
      "Epoch: 5550 Training Loss: 0.03402803757606313 Test Loss: 0.04603306812880766\n",
      "Epoch: 5560 Training Loss: 0.03402678323310645 Test Loss: 0.046065460964110794\n",
      "Epoch: 5570 Training Loss: 0.03402593351690998 Test Loss: 0.046077597115104355\n",
      "Epoch: 5580 Training Loss: 0.03402886775855393 Test Loss: 0.046123875783321715\n",
      "Epoch: 5590 Training Loss: 0.03402288486356142 Test Loss: 0.0461008983017671\n",
      "Epoch: 5600 Training Loss: 0.034023934102608615 Test Loss: 0.046057292991253086\n",
      "Epoch: 5610 Training Loss: 0.03403212849213544 Test Loss: 0.04606396801382761\n",
      "Epoch: 5620 Training Loss: 0.03402211467745231 Test Loss: 0.04608681992012476\n",
      "Epoch: 5630 Training Loss: 0.03401768610732492 Test Loss: 0.046101877788775315\n",
      "Epoch: 5640 Training Loss: 0.03403034673912578 Test Loss: 0.04609605388739027\n",
      "Epoch: 5650 Training Loss: 0.0340310862294117 Test Loss: 0.04615923498526227\n",
      "Epoch: 5660 Training Loss: 0.034021837019778914 Test Loss: 0.04616397614887185\n",
      "Epoch: 5670 Training Loss: 0.03401910788204446 Test Loss: 0.046264386124739966\n",
      "Epoch: 5680 Training Loss: 0.03402831383847184 Test Loss: 0.04606153185383281\n",
      "Epoch: 5690 Training Loss: 0.03402038594450088 Test Loss: 0.046097064070572534\n",
      "Epoch: 5700 Training Loss: 0.03403060765362288 Test Loss: 0.04609158061766328\n",
      "Epoch: 5710 Training Loss: 0.034020268742266886 Test Loss: 0.046223283947691395\n",
      "Epoch: 5720 Training Loss: 0.034020868706083764 Test Loss: 0.046108044929197435\n",
      "Epoch: 5730 Training Loss: 0.0340208338244665 Test Loss: 0.046046197719615836\n",
      "Epoch: 5740 Training Loss: 0.03401817584523126 Test Loss: 0.04606105466785445\n",
      "Epoch: 5750 Training Loss: 0.03401614713037139 Test Loss: 0.04605734322135607\n",
      "Epoch: 5760 Training Loss: 0.03402026176594343 Test Loss: 0.0460450619611761\n",
      "Epoch: 5770 Training Loss: 0.034017967950792385 Test Loss: 0.04614789972535519\n",
      "Epoch: 5780 Training Loss: 0.03401878976169504 Test Loss: 0.04607246248235472\n",
      "Epoch: 5790 Training Loss: 0.034015625301377174 Test Loss: 0.046146191901853685\n",
      "Epoch: 5800 Training Loss: 0.03401931577648333 Test Loss: 0.04604582378440472\n",
      "Epoch: 5810 Training Loss: 0.03401803771402691 Test Loss: 0.04611371255915097\n",
      "Epoch: 5820 Training Loss: 0.03401815212573152 Test Loss: 0.046077111557442166\n",
      "Epoch: 5830 Training Loss: 0.034022869515649826 Test Loss: 0.0460824526917263\n",
      "Epoch: 5840 Training Loss: 0.03401496394591391 Test Loss: 0.04607397775712812\n",
      "Epoch: 5850 Training Loss: 0.03402176167548563 Test Loss: 0.04609964533975374\n",
      "Epoch: 5860 Training Loss: 0.034025477265356215 Test Loss: 0.04616655462749177\n",
      "Epoch: 5870 Training Loss: 0.03401638013957469 Test Loss: 0.046088817961999075\n",
      "Epoch: 5880 Training Loss: 0.03401551088967256 Test Loss: 0.04605850967596985\n",
      "Epoch: 5890 Training Loss: 0.034013519846959335 Test Loss: 0.04609719801751383\n",
      "Epoch: 5900 Training Loss: 0.03401764983044297 Test Loss: 0.04610439487504715\n",
      "Epoch: 5910 Training Loss: 0.03401550251808442 Test Loss: 0.04606913892387385\n",
      "Epoch: 5920 Training Loss: 0.03401015865432012 Test Loss: 0.04609278893069621\n",
      "Epoch: 5930 Training Loss: 0.034010700017019994 Test Loss: 0.046048452493127626\n",
      "Epoch: 5940 Training Loss: 0.03401486209159151 Test Loss: 0.046081247169254644\n",
      "Epoch: 5950 Training Loss: 0.03401021586017242 Test Loss: 0.046056160023374634\n",
      "Epoch: 5960 Training Loss: 0.03401169484074427 Test Loss: 0.04621643870087898\n",
      "Epoch: 5970 Training Loss: 0.03401764424938421 Test Loss: 0.046078255687565725\n",
      "Epoch: 5980 Training Loss: 0.034012971507935995 Test Loss: 0.046085117677745806\n",
      "Epoch: 5990 Training Loss: 0.03401320172660992 Test Loss: 0.04604106587742748\n",
      "Epoch: 6000 Training Loss: 0.034011763208714095 Test Loss: 0.04607078535502726\n",
      "Epoch: 6010 Training Loss: 0.03401951390406937 Test Loss: 0.04608925608011956\n",
      "Epoch: 6020 Training Loss: 0.03400927963756515 Test Loss: 0.04611562967474825\n",
      "Epoch: 6030 Training Loss: 0.03401083535769497 Test Loss: 0.04610457905209143\n",
      "Epoch: 6040 Training Loss: 0.03401852187087448 Test Loss: 0.04612748956017539\n",
      "Epoch: 6050 Training Loss: 0.03401381424780901 Test Loss: 0.04610985600346619\n",
      "Epoch: 6060 Training Loss: 0.034007799261728616 Test Loss: 0.046187980556976385\n",
      "Epoch: 6070 Training Loss: 0.03400657700985981 Test Loss: 0.04612318372412502\n",
      "Epoch: 6080 Training Loss: 0.034017006613420685 Test Loss: 0.04613379343810008\n",
      "Epoch: 6090 Training Loss: 0.034015695064611697 Test Loss: 0.046151680935885495\n",
      "Epoch: 6100 Training Loss: 0.03400423575570929 Test Loss: 0.046046580026510776\n",
      "Epoch: 6110 Training Loss: 0.034006586776712644 Test Loss: 0.046246875352726945\n",
      "Epoch: 6120 Training Loss: 0.03400978611864777 Test Loss: 0.046105826432982236\n",
      "Epoch: 6130 Training Loss: 0.034009819605000345 Test Loss: 0.04613352833477877\n",
      "Epoch: 6140 Training Loss: 0.03400601611345426 Test Loss: 0.04608353822006304\n",
      "Epoch: 6150 Training Loss: 0.034001397787328975 Test Loss: 0.04628757010782907\n",
      "Epoch: 6160 Training Loss: 0.03401908974360348 Test Loss: 0.046108706292220074\n",
      "Epoch: 6170 Training Loss: 0.03399989787778678 Test Loss: 0.04609043369697844\n",
      "Epoch: 6180 Training Loss: 0.03400996750305753 Test Loss: 0.046105312969707275\n",
      "Epoch: 6190 Training Loss: 0.033998737017564354 Test Loss: 0.04617492910188397\n",
      "Epoch: 6200 Training Loss: 0.03401125812289616 Test Loss: 0.046284600950630375\n",
      "Epoch: 6210 Training Loss: 0.03400760392467196 Test Loss: 0.046219865510127106\n",
      "Epoch: 6220 Training Loss: 0.03400843131663338 Test Loss: 0.0460492561747754\n",
      "Epoch: 6230 Training Loss: 0.034008678278483584 Test Loss: 0.04607002353179865\n",
      "Epoch: 6240 Training Loss: 0.034013998422748146 Test Loss: 0.04616188043735284\n",
      "Epoch: 6250 Training Loss: 0.03401111301536836 Test Loss: 0.04625422848169178\n",
      "Epoch: 6260 Training Loss: 0.03400076712768891 Test Loss: 0.04610695940086069\n",
      "Epoch: 6270 Training Loss: 0.03400117454497851 Test Loss: 0.04608735291732866\n",
      "Epoch: 6280 Training Loss: 0.03399877050391692 Test Loss: 0.046210335743366235\n",
      "Epoch: 6290 Training Loss: 0.034002384239465104 Test Loss: 0.04608579578413611\n",
      "Epoch: 6300 Training Loss: 0.03400987122979389 Test Loss: 0.04608547766015054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6310 Training Loss: 0.03400193915002886 Test Loss: 0.04610215405434174\n",
      "Epoch: 6320 Training Loss: 0.03400511058667018 Test Loss: 0.046060016579059414\n",
      "Epoch: 6330 Training Loss: 0.03400536871063791 Test Loss: 0.046196709432650765\n",
      "Epoch: 6340 Training Loss: 0.03400106431906797 Test Loss: 0.04629875188686591\n",
      "Epoch: 6350 Training Loss: 0.0339983798298036 Test Loss: 0.046152032546606395\n",
      "Epoch: 6360 Training Loss: 0.03400206751438038 Test Loss: 0.04615453288951056\n",
      "Epoch: 6370 Training Loss: 0.034000626205955176 Test Loss: 0.0461287369410662\n",
      "Epoch: 6380 Training Loss: 0.03401193482627102 Test Loss: 0.04613678212922772\n",
      "Epoch: 6390 Training Loss: 0.0340048608342906 Test Loss: 0.04622662424953994\n",
      "Epoch: 6400 Training Loss: 0.03399897281729703 Test Loss: 0.04605919615404398\n",
      "Epoch: 6410 Training Loss: 0.03399710874367063 Test Loss: 0.04605345317893597\n",
      "Epoch: 6420 Training Loss: 0.03399581672856731 Test Loss: 0.046127112834403\n",
      "Epoch: 6430 Training Loss: 0.03399423728893775 Test Loss: 0.046128441141570845\n",
      "Epoch: 6440 Training Loss: 0.03399649482720685 Test Loss: 0.04604159887463138\n",
      "Epoch: 6450 Training Loss: 0.03399492515443013 Test Loss: 0.046133606470494526\n",
      "Epoch: 6460 Training Loss: 0.03399214578716681 Test Loss: 0.0461231502373897\n",
      "Epoch: 6470 Training Loss: 0.033991707674054024 Test Loss: 0.0461488680501183\n",
      "Epoch: 6480 Training Loss: 0.03399692317346681 Test Loss: 0.04627008724142882\n",
      "Epoch: 6490 Training Loss: 0.03399610136256415 Test Loss: 0.04608940118930596\n",
      "Epoch: 6500 Training Loss: 0.033993768480001775 Test Loss: 0.046162466455221\n",
      "Epoch: 6510 Training Loss: 0.03399579859012633 Test Loss: 0.046189300492460394\n",
      "Epoch: 6520 Training Loss: 0.03399441030175936 Test Loss: 0.04620511460321701\n",
      "Epoch: 6530 Training Loss: 0.0340001560017545 Test Loss: 0.046084433990232945\n",
      "Epoch: 6540 Training Loss: 0.03400087874886414 Test Loss: 0.04621160265818598\n",
      "Epoch: 6550 Training Loss: 0.03399383824323629 Test Loss: 0.04614797227994839\n",
      "Epoch: 6560 Training Loss: 0.03399293690224628 Test Loss: 0.04612596312315689\n",
      "Epoch: 6570 Training Loss: 0.03399871608859399 Test Loss: 0.04609685198791549\n",
      "Epoch: 6580 Training Loss: 0.033995133048869 Test Loss: 0.046114267880845085\n",
      "Epoch: 6590 Training Loss: 0.03398778837553864 Test Loss: 0.046044782905048404\n",
      "Epoch: 6600 Training Loss: 0.03399143280691001 Test Loss: 0.04605767808870931\n",
      "Epoch: 6610 Training Loss: 0.03399149838435046 Test Loss: 0.04609976533388865\n",
      "Epoch: 6620 Training Loss: 0.03399370429782601 Test Loss: 0.04610784121822421\n",
      "Epoch: 6630 Training Loss: 0.033995610229393125 Test Loss: 0.046194661160673466\n",
      "Epoch: 6640 Training Loss: 0.03399047705059707 Test Loss: 0.046259884949400205\n",
      "Epoch: 6650 Training Loss: 0.033996059504623435 Test Loss: 0.046041077039672584\n",
      "Epoch: 6660 Training Loss: 0.03399221694566602 Test Loss: 0.04605919057292143\n",
      "Epoch: 6670 Training Loss: 0.0339948414385487 Test Loss: 0.046147140692687855\n",
      "Epoch: 6680 Training Loss: 0.03398792650674299 Test Loss: 0.04606651021515094\n",
      "Epoch: 6690 Training Loss: 0.03398307098562033 Test Loss: 0.046120968018471105\n",
      "Epoch: 6700 Training Loss: 0.03399064727288931 Test Loss: 0.04608555021474374\n",
      "Epoch: 6710 Training Loss: 0.03398515272053844 Test Loss: 0.046137730920061897\n",
      "Epoch: 6720 Training Loss: 0.03398728887077947 Test Loss: 0.04613580822334206\n",
      "Epoch: 6730 Training Loss: 0.03399211648660832 Test Loss: 0.046070383514203374\n",
      "Epoch: 6740 Training Loss: 0.03399870911227054 Test Loss: 0.046128661595911724\n",
      "Epoch: 6750 Training Loss: 0.03398863809173511 Test Loss: 0.046065851642689566\n",
      "Epoch: 6760 Training Loss: 0.03399095283585651 Test Loss: 0.046090735077596354\n",
      "Epoch: 6770 Training Loss: 0.03398983383357479 Test Loss: 0.04604798088827182\n",
      "Epoch: 6780 Training Loss: 0.03399015753498297 Test Loss: 0.046089110970933156\n",
      "Epoch: 6790 Training Loss: 0.03399141048267496 Test Loss: 0.04615304831091121\n",
      "Epoch: 6800 Training Loss: 0.03399727617543348 Test Loss: 0.046041721659327565\n",
      "Epoch: 6810 Training Loss: 0.03399352012288688 Test Loss: 0.04603379367473968\n",
      "Epoch: 6820 Training Loss: 0.033991025389620406 Test Loss: 0.04610360514620577\n",
      "Epoch: 6830 Training Loss: 0.03398533829074227 Test Loss: 0.046185887636018656\n",
      "Epoch: 6840 Training Loss: 0.03398331236641178 Test Loss: 0.046086370639759164\n",
      "Epoch: 6850 Training Loss: 0.033979153082369644 Test Loss: 0.04605671813563003\n",
      "Epoch: 6860 Training Loss: 0.03398937339622696 Test Loss: 0.046064570775063436\n",
      "Epoch: 6870 Training Loss: 0.033986601005287095 Test Loss: 0.04621548153836098\n",
      "Epoch: 6880 Training Loss: 0.0339878232571559 Test Loss: 0.04606738366083063\n",
      "Epoch: 6890 Training Loss: 0.033981597586107255 Test Loss: 0.046095308807529316\n",
      "Epoch: 6900 Training Loss: 0.033979397253690466 Test Loss: 0.046097019421592104\n",
      "Epoch: 6910 Training Loss: 0.03398538572974174 Test Loss: 0.046111226169053186\n",
      "Epoch: 6920 Training Loss: 0.03398129062787536 Test Loss: 0.04609256847635533\n",
      "Epoch: 6930 Training Loss: 0.03397647417416404 Test Loss: 0.04620866140660005\n",
      "Epoch: 6940 Training Loss: 0.03397976002250997 Test Loss: 0.04610174942295658\n",
      "Epoch: 6950 Training Loss: 0.03398785674350847 Test Loss: 0.04614827087000503\n",
      "Epoch: 6960 Training Loss: 0.033984078366726816 Test Loss: 0.04613025779696216\n",
      "Epoch: 6970 Training Loss: 0.03398647961725903 Test Loss: 0.04617829730934528\n",
      "Epoch: 6980 Training Loss: 0.03398628986126113 Test Loss: 0.046129409466333954\n",
      "Epoch: 6990 Training Loss: 0.03398314772517831 Test Loss: 0.04613273302481483\n",
      "Epoch: 7000 Training Loss: 0.03398191849698605 Test Loss: 0.04608535487545435\n",
      "Epoch: 7010 Training Loss: 0.033979536780159504 Test Loss: 0.046066222787339406\n",
      "Epoch: 7020 Training Loss: 0.03398010046709443 Test Loss: 0.04611144383283279\n",
      "Epoch: 7030 Training Loss: 0.03398278356109411 Test Loss: 0.04608104066772015\n",
      "Epoch: 7040 Training Loss: 0.03399283504792388 Test Loss: 0.0461607697939646\n",
      "Epoch: 7050 Training Loss: 0.03398008511918284 Test Loss: 0.04608090672077885\n",
      "Epoch: 7060 Training Loss: 0.03397929400410337 Test Loss: 0.046080181174846836\n",
      "Epoch: 7070 Training Loss: 0.033983672344701905 Test Loss: 0.04618940095266637\n",
      "Epoch: 7080 Training Loss: 0.03397664858225034 Test Loss: 0.04607280014026924\n",
      "Epoch: 7090 Training Loss: 0.03398075205570486 Test Loss: 0.0462751325762176\n",
      "Epoch: 7100 Training Loss: 0.033979893967920255 Test Loss: 0.04609639433586606\n",
      "Epoch: 7110 Training Loss: 0.033974400810834075 Test Loss: 0.046048485979862955\n",
      "Epoch: 7120 Training Loss: 0.03397677973713124 Test Loss: 0.046072015992550404\n",
      "Epoch: 7130 Training Loss: 0.03397959119548243 Test Loss: 0.046029122275162024\n",
      "Epoch: 7140 Training Loss: 0.03397492961615174 Test Loss: 0.046059639853287024\n",
      "Epoch: 7150 Training Loss: 0.033976864848277354 Test Loss: 0.04608113554680356\n",
      "Epoch: 7160 Training Loss: 0.03397573049808404 Test Loss: 0.046088310079846664\n",
      "Epoch: 7170 Training Loss: 0.033975705383319614 Test Loss: 0.04614371667400101\n",
      "Epoch: 7180 Training Loss: 0.033973390639198214 Test Loss: 0.046181542732110406\n",
      "Epoch: 7190 Training Loss: 0.03397189770597946 Test Loss: 0.046037245599039295\n",
      "Epoch: 7200 Training Loss: 0.033980354405268094 Test Loss: 0.04617173949034439\n",
      "Epoch: 7210 Training Loss: 0.03397861730072852 Test Loss: 0.04607685203524341\n",
      "Epoch: 7220 Training Loss: 0.033975991412581154 Test Loss: 0.04604681722421932\n",
      "Epoch: 7230 Training Loss: 0.03397109821931186 Test Loss: 0.04608467955962532\n",
      "Epoch: 7240 Training Loss: 0.033981042270760466 Test Loss: 0.04614842993199782\n",
      "Epoch: 7250 Training Loss: 0.033979542361218265 Test Loss: 0.04604991753779804\n",
      "Epoch: 7260 Training Loss: 0.03397184329065654 Test Loss: 0.046086320409656184\n",
      "Epoch: 7270 Training Loss: 0.03396996107858916 Test Loss: 0.0460796760832557\n",
      "Epoch: 7280 Training Loss: 0.03397699321262888 Test Loss: 0.046063677795454804\n",
      "Epoch: 7290 Training Loss: 0.03397806198538173 Test Loss: 0.046175303037095085\n",
      "Epoch: 7300 Training Loss: 0.033972769746410975 Test Loss: 0.04605576097311203\n",
      "Epoch: 7310 Training Loss: 0.03398546944562316 Test Loss: 0.04607092767365239\n",
      "Epoch: 7320 Training Loss: 0.03396865371557424 Test Loss: 0.046113559078280734\n",
      "Epoch: 7330 Training Loss: 0.03397344924031521 Test Loss: 0.046075093981638914\n",
      "Epoch: 7340 Training Loss: 0.033970491279171525 Test Loss: 0.04604918641074347\n",
      "Epoch: 7350 Training Loss: 0.03396623572186575 Test Loss: 0.0460700291129212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7360 Training Loss: 0.03396756401385103 Test Loss: 0.046230238026393616\n",
      "Epoch: 7370 Training Loss: 0.03396633199512939 Test Loss: 0.046047665554847524\n",
      "Epoch: 7380 Training Loss: 0.03396521996917112 Test Loss: 0.04612569801983558\n",
      "Epoch: 7390 Training Loss: 0.033969564823417085 Test Loss: 0.04619844237120377\n",
      "Epoch: 7400 Training Loss: 0.03396544042099221 Test Loss: 0.046144193859979366\n",
      "Epoch: 7410 Training Loss: 0.033969415530095214 Test Loss: 0.04607492933852357\n",
      "Epoch: 7420 Training Loss: 0.033969885734295886 Test Loss: 0.046012582618473394\n",
      "Epoch: 7430 Training Loss: 0.0339724613929144 Test Loss: 0.046152395319572395\n",
      "Epoch: 7440 Training Loss: 0.033968684411397436 Test Loss: 0.04616592954176573\n",
      "Epoch: 7450 Training Loss: 0.03397900657957715 Test Loss: 0.046105181813327255\n",
      "Epoch: 7460 Training Loss: 0.03397611419587391 Test Loss: 0.04612406554148855\n",
      "Epoch: 7470 Training Loss: 0.03397296368820295 Test Loss: 0.04607596184619605\n",
      "Epoch: 7480 Training Loss: 0.03397839684890743 Test Loss: 0.04608695665762733\n",
      "Epoch: 7490 Training Loss: 0.033967173339737707 Test Loss: 0.0461063036189606\n",
      "Epoch: 7500 Training Loss: 0.03395938776276518 Test Loss: 0.04606704600291611\n",
      "Epoch: 7510 Training Loss: 0.033959108709827095 Test Loss: 0.04604580425047578\n",
      "Epoch: 7520 Training Loss: 0.03396131183277326 Test Loss: 0.04610373909314706\n",
      "Epoch: 7530 Training Loss: 0.033968070494933646 Test Loss: 0.04607014073537228\n",
      "Epoch: 7540 Training Loss: 0.03395946868811722 Test Loss: 0.04605282809320992\n",
      "Epoch: 7550 Training Loss: 0.03396544042099221 Test Loss: 0.04605727345732415\n",
      "Epoch: 7560 Training Loss: 0.03396366145851193 Test Loss: 0.046116182205881096\n",
      "Epoch: 7570 Training Loss: 0.03396796026902311 Test Loss: 0.04601937205406027\n",
      "Epoch: 7580 Training Loss: 0.03395874733627227 Test Loss: 0.04622940922969436\n",
      "Epoch: 7590 Training Loss: 0.03395835387162958 Test Loss: 0.04624552751163016\n",
      "Epoch: 7600 Training Loss: 0.033954733159757944 Test Loss: 0.04610668034473299\n",
      "Epoch: 7610 Training Loss: 0.033961451359242306 Test Loss: 0.04608476327646363\n",
      "Epoch: 7620 Training Loss: 0.03396647291686312 Test Loss: 0.04609007092401243\n",
      "Epoch: 7630 Training Loss: 0.03396088767230738 Test Loss: 0.04611984621283776\n",
      "Epoch: 7640 Training Loss: 0.03396124486006812 Test Loss: 0.04611324932597899\n",
      "Epoch: 7650 Training Loss: 0.03395394204467848 Test Loss: 0.04606108815458977\n",
      "Epoch: 7660 Training Loss: 0.03395645212585654 Test Loss: 0.04624443361160959\n",
      "Epoch: 7670 Training Loss: 0.03395592611106825 Test Loss: 0.04606964680602626\n",
      "Epoch: 7680 Training Loss: 0.03395808319027963 Test Loss: 0.04620102643094624\n",
      "Epoch: 7690 Training Loss: 0.033957950640134044 Test Loss: 0.046075367456644055\n",
      "Epoch: 7700 Training Loss: 0.03396475674129391 Test Loss: 0.046174290063351546\n",
      "Epoch: 7710 Training Loss: 0.033963227531193206 Test Loss: 0.04612023131029398\n",
      "Epoch: 7720 Training Loss: 0.03396439536773909 Test Loss: 0.0460740893795792\n",
      "Epoch: 7730 Training Loss: 0.03395534707622173 Test Loss: 0.04614382271532953\n",
      "Epoch: 7740 Training Loss: 0.033969062528128535 Test Loss: 0.046283149858766345\n",
      "Epoch: 7750 Training Loss: 0.03396601247951528 Test Loss: 0.0462475953175364\n",
      "Epoch: 7760 Training Loss: 0.033960395143871655 Test Loss: 0.04610634268681848\n",
      "Epoch: 7770 Training Loss: 0.03395640887265113 Test Loss: 0.046074560984435006\n",
      "Epoch: 7780 Training Loss: 0.03395718882561308 Test Loss: 0.04606800037487284\n",
      "Epoch: 7790 Training Loss: 0.0339511570963564 Test Loss: 0.04606790549578942\n",
      "Epoch: 7800 Training Loss: 0.0339594714786466 Test Loss: 0.04617132090615284\n",
      "Epoch: 7810 Training Loss: 0.033953501141036305 Test Loss: 0.04616813408517454\n",
      "Epoch: 7820 Training Loss: 0.033951272903325704 Test Loss: 0.04612000248426927\n",
      "Epoch: 7830 Training Loss: 0.033955575899630955 Test Loss: 0.046204578815451834\n",
      "Epoch: 7840 Training Loss: 0.033955253593487465 Test Loss: 0.046137156064438835\n",
      "Epoch: 7850 Training Loss: 0.03395770507354853 Test Loss: 0.046202050566934896\n",
      "Epoch: 7860 Training Loss: 0.03395518383025294 Test Loss: 0.04609362888964058\n",
      "Epoch: 7870 Training Loss: 0.03395713022449608 Test Loss: 0.046099952301494204\n",
      "Epoch: 7880 Training Loss: 0.03396070210210355 Test Loss: 0.046379326553177294\n",
      "Epoch: 7890 Training Loss: 0.03395265840116329 Test Loss: 0.04611097222797698\n",
      "Epoch: 7900 Training Loss: 0.03395303791315908 Test Loss: 0.04616533515221373\n",
      "Epoch: 7910 Training Loss: 0.03395266119169267 Test Loss: 0.0460893481686417\n",
      "Epoch: 7920 Training Loss: 0.03395256212789965 Test Loss: 0.046118716035520584\n",
      "Epoch: 7930 Training Loss: 0.03394988740548812 Test Loss: 0.04612576220274495\n",
      "Epoch: 7940 Training Loss: 0.03394569044929934 Test Loss: 0.04610526273960429\n",
      "Epoch: 7950 Training Loss: 0.03394685549531584 Test Loss: 0.046080895558533744\n",
      "Epoch: 7960 Training Loss: 0.033950219478484436 Test Loss: 0.04607429029999114\n",
      "Epoch: 7970 Training Loss: 0.033946166234558775 Test Loss: 0.04615161396241484\n",
      "Epoch: 7980 Training Loss: 0.033944833756779424 Test Loss: 0.046128187200494636\n",
      "Epoch: 7990 Training Loss: 0.033950368771806315 Test Loss: 0.04609173409853351\n",
      "Epoch: 8000 Training Loss: 0.03394269481600901 Test Loss: 0.046190360905745645\n",
      "Epoch: 8010 Training Loss: 0.03394027542703583 Test Loss: 0.04614858620342933\n",
      "Epoch: 8020 Training Loss: 0.033939376876575195 Test Loss: 0.04611145778563917\n",
      "Epoch: 8030 Training Loss: 0.03394692107275629 Test Loss: 0.04619708894898443\n",
      "Epoch: 8040 Training Loss: 0.033943834747261085 Test Loss: 0.04610335678625212\n",
      "Epoch: 8050 Training Loss: 0.03394862887673736 Test Loss: 0.0461361542529404\n",
      "Epoch: 8060 Training Loss: 0.0339400605562735 Test Loss: 0.046205385287660876\n",
      "Epoch: 8070 Training Loss: 0.033940558665767985 Test Loss: 0.04604112447921429\n",
      "Epoch: 8080 Training Loss: 0.033945975083296186 Test Loss: 0.046098431445598255\n",
      "Epoch: 8090 Training Loss: 0.03393629473687407 Test Loss: 0.04612335394836292\n",
      "Epoch: 8100 Training Loss: 0.03393755466088951 Test Loss: 0.04603679352811243\n",
      "Epoch: 8110 Training Loss: 0.03394438169101973 Test Loss: 0.0460842219075759\n",
      "Epoch: 8120 Training Loss: 0.03394492026319023 Test Loss: 0.04604558658669618\n",
      "Epoch: 8130 Training Loss: 0.03394175301234298 Test Loss: 0.046144891500298615\n",
      "Epoch: 8140 Training Loss: 0.03394187160984167 Test Loss: 0.046316399396381505\n",
      "Epoch: 8150 Training Loss: 0.03393887179075727 Test Loss: 0.04616486354735792\n",
      "Epoch: 8160 Training Loss: 0.033937396995979495 Test Loss: 0.04618071393541114\n",
      "Epoch: 8170 Training Loss: 0.03394025868385954 Test Loss: 0.04611689938012928\n",
      "Epoch: 8180 Training Loss: 0.03393198476424536 Test Loss: 0.046087146415794165\n",
      "Epoch: 8190 Training Loss: 0.03393540316273689 Test Loss: 0.04605662604710789\n",
      "Epoch: 8200 Training Loss: 0.033943463606853436 Test Loss: 0.046079159829419465\n",
      "Epoch: 8210 Training Loss: 0.03393836391440996 Test Loss: 0.046060616549733965\n",
      "Epoch: 8220 Training Loss: 0.033935196663562706 Test Loss: 0.04613497663608152\n",
      "Epoch: 8230 Training Loss: 0.033936972835513605 Test Loss: 0.04607192948515082\n",
      "Epoch: 8240 Training Loss: 0.03394110560952662 Test Loss: 0.046099609062457136\n",
      "Epoch: 8250 Training Loss: 0.03393131503719396 Test Loss: 0.046216474978175576\n",
      "Epoch: 8260 Training Loss: 0.03392964211483015 Test Loss: 0.046151569313434414\n",
      "Epoch: 8270 Training Loss: 0.033933392586317994 Test Loss: 0.04610202010740044\n",
      "Epoch: 8280 Training Loss: 0.03393343863005278 Test Loss: 0.04608322846776129\n",
      "Epoch: 8290 Training Loss: 0.03393034532823412 Test Loss: 0.046109775077189157\n",
      "Epoch: 8300 Training Loss: 0.03393014859591277 Test Loss: 0.046069766800161165\n",
      "Epoch: 8310 Training Loss: 0.03392611907148684 Test Loss: 0.04606956308918794\n",
      "Epoch: 8320 Training Loss: 0.033936410543843366 Test Loss: 0.046115900359192115\n",
      "Epoch: 8330 Training Loss: 0.033933565599139605 Test Loss: 0.0461974321880215\n",
      "Epoch: 8340 Training Loss: 0.03393110016643164 Test Loss: 0.046042910438431554\n",
      "Epoch: 8350 Training Loss: 0.033931024822138355 Test Loss: 0.04603067382723202\n",
      "Epoch: 8360 Training Loss: 0.033938394610233145 Test Loss: 0.04607918773503223\n",
      "Epoch: 8370 Training Loss: 0.03393031742294031 Test Loss: 0.04606318944723133\n",
      "Epoch: 8380 Training Loss: 0.03393276750773669 Test Loss: 0.046088725873476934\n",
      "Epoch: 8390 Training Loss: 0.033925142386203554 Test Loss: 0.04608902446353357\n",
      "Epoch: 8400 Training Loss: 0.033924423824887986 Test Loss: 0.0461897720973162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8410 Training Loss: 0.033923422024840263 Test Loss: 0.04613343345569535\n",
      "Epoch: 8420 Training Loss: 0.03393126062187104 Test Loss: 0.04607919610671606\n",
      "Epoch: 8430 Training Loss: 0.0339259948929294 Test Loss: 0.046109747171576385\n",
      "Epoch: 8440 Training Loss: 0.03392513959567417 Test Loss: 0.046067952935331134\n",
      "Epoch: 8450 Training Loss: 0.03392702320300624 Test Loss: 0.0461039762908556\n",
      "Epoch: 8460 Training Loss: 0.03392654323195273 Test Loss: 0.046141620962482\n",
      "Epoch: 8470 Training Loss: 0.03393110016643164 Test Loss: 0.046102399623734115\n",
      "Epoch: 8480 Training Loss: 0.03393168338707223 Test Loss: 0.04610584875747245\n",
      "Epoch: 8490 Training Loss: 0.033928125462111665 Test Loss: 0.04607138253514053\n",
      "Epoch: 8500 Training Loss: 0.03392144353950925 Test Loss: 0.04612543291651427\n",
      "Epoch: 8510 Training Loss: 0.03392657392777592 Test Loss: 0.046132219561539865\n",
      "Epoch: 8520 Training Loss: 0.03392731620859122 Test Loss: 0.046132239095468805\n",
      "Epoch: 8530 Training Loss: 0.03392778780805658 Test Loss: 0.04607163647621674\n",
      "Epoch: 8540 Training Loss: 0.03392030779405125 Test Loss: 0.0461105703871531\n",
      "Epoch: 8550 Training Loss: 0.033930761117111866 Test Loss: 0.04618164319231637\n",
      "Epoch: 8560 Training Loss: 0.03392351271704514 Test Loss: 0.046101227587997784\n",
      "Epoch: 8570 Training Loss: 0.033926049308252326 Test Loss: 0.046092551732987666\n",
      "Epoch: 8580 Training Loss: 0.03392719761109254 Test Loss: 0.046122502827173444\n",
      "Epoch: 8590 Training Loss: 0.033931651295984355 Test Loss: 0.04619874654238296\n",
      "Epoch: 8600 Training Loss: 0.033919518074236474 Test Loss: 0.0461693870471879\n",
      "Epoch: 8610 Training Loss: 0.03391943575361974 Test Loss: 0.04621817442999326\n",
      "Epoch: 8620 Training Loss: 0.03393016812961844 Test Loss: 0.04614418548829554\n",
      "Epoch: 8630 Training Loss: 0.03391928646029787 Test Loss: 0.046194089095611686\n",
      "Epoch: 8640 Training Loss: 0.03392064405284164 Test Loss: 0.046084062845583106\n",
      "Epoch: 8650 Training Loss: 0.033922459292203874 Test Loss: 0.046095674371056605\n",
      "Epoch: 8660 Training Loss: 0.033913253335776494 Test Loss: 0.04605961473823553\n",
      "Epoch: 8670 Training Loss: 0.033916413610300296 Test Loss: 0.046077806407200134\n",
      "Epoch: 8680 Training Loss: 0.03391853022683566 Test Loss: 0.04616201159373286\n",
      "Epoch: 8690 Training Loss: 0.033918219082809696 Test Loss: 0.04613905922722973\n",
      "Epoch: 8700 Training Loss: 0.03392732876597344 Test Loss: 0.046068993814687445\n",
      "Epoch: 8710 Training Loss: 0.033911704591970133 Test Loss: 0.046122555847837705\n",
      "Epoch: 8720 Training Loss: 0.03391388957647533 Test Loss: 0.04609515532665909\n",
      "Epoch: 8730 Training Loss: 0.033916786145972634 Test Loss: 0.04611274981551041\n",
      "Epoch: 8740 Training Loss: 0.033915686677396584 Test Loss: 0.04625074586121811\n",
      "Epoch: 8750 Training Loss: 0.03391063442395258 Test Loss: 0.04607771989980054\n",
      "Epoch: 8760 Training Loss: 0.03390926008823252 Test Loss: 0.04609266614600002\n",
      "Epoch: 8770 Training Loss: 0.03391460395199682 Test Loss: 0.0460995895285282\n",
      "Epoch: 8780 Training Loss: 0.03391813397166358 Test Loss: 0.04606114396581531\n",
      "Epoch: 8790 Training Loss: 0.0339111995061522 Test Loss: 0.04605394431772072\n",
      "Epoch: 8800 Training Loss: 0.03391667312953271 Test Loss: 0.046062117871700975\n",
      "Epoch: 8810 Training Loss: 0.03392081148460449 Test Loss: 0.0461430748449073\n",
      "Epoch: 8820 Training Loss: 0.033909373104672444 Test Loss: 0.04609504370420801\n",
      "Epoch: 8830 Training Loss: 0.03391261430454828 Test Loss: 0.04611478134412005\n",
      "Epoch: 8840 Training Loss: 0.03390976796457983 Test Loss: 0.04614544124087017\n",
      "Epoch: 8850 Training Loss: 0.033911357171062216 Test Loss: 0.046093494942699285\n",
      "Epoch: 8860 Training Loss: 0.03391736936661323 Test Loss: 0.046124160420571964\n",
      "Epoch: 8870 Training Loss: 0.03390724253549018 Test Loss: 0.04607483725000143\n",
      "Epoch: 8880 Training Loss: 0.03390881918459035 Test Loss: 0.0462239983313783\n",
      "Epoch: 8890 Training Loss: 0.033907266254989916 Test Loss: 0.04616166277357323\n",
      "Epoch: 8900 Training Loss: 0.033911673896146945 Test Loss: 0.046164500774391916\n",
      "Epoch: 8910 Training Loss: 0.03391296033019151 Test Loss: 0.04629723940265379\n",
      "Epoch: 8920 Training Loss: 0.03390925032137969 Test Loss: 0.04609758311497005\n",
      "Epoch: 8930 Training Loss: 0.033910201891898556 Test Loss: 0.04611932437787897\n",
      "Epoch: 8940 Training Loss: 0.03391395933970985 Test Loss: 0.046197002441584846\n",
      "Epoch: 8950 Training Loss: 0.03391032048939724 Test Loss: 0.046189476297820845\n",
      "Epoch: 8960 Training Loss: 0.03391342913912749 Test Loss: 0.046084668397380214\n",
      "Epoch: 8970 Training Loss: 0.03390760530430968 Test Loss: 0.04608775475815255\n",
      "Epoch: 8980 Training Loss: 0.03390313348097689 Test Loss: 0.04612784396145757\n",
      "Epoch: 8990 Training Loss: 0.03390437666181606 Test Loss: 0.046217309355997396\n",
      "Epoch: 9000 Training Loss: 0.03390566728165469 Test Loss: 0.04623685165662005\n",
      "Epoch: 9010 Training Loss: 0.03391115206715273 Test Loss: 0.04606298573625812\n",
      "Epoch: 9020 Training Loss: 0.033902133076193865 Test Loss: 0.046087919401267885\n",
      "Epoch: 9030 Training Loss: 0.033905731463830455 Test Loss: 0.04614099866731723\n",
      "Epoch: 9040 Training Loss: 0.033912918472250794 Test Loss: 0.04620054924496788\n",
      "Epoch: 9050 Training Loss: 0.033918237221250674 Test Loss: 0.04613092474110735\n",
      "Epoch: 9060 Training Loss: 0.03391706240838134 Test Loss: 0.046113430712461996\n",
      "Epoch: 9070 Training Loss: 0.03390970517766877 Test Loss: 0.04612305256774501\n",
      "Epoch: 9080 Training Loss: 0.0339102967698975 Test Loss: 0.04612730817369239\n",
      "Epoch: 9090 Training Loss: 0.03391218735355302 Test Loss: 0.04610620036819335\n",
      "Epoch: 9100 Training Loss: 0.033910729301951534 Test Loss: 0.04610372793090195\n",
      "Epoch: 9110 Training Loss: 0.033902197258369626 Test Loss: 0.04611579431786359\n",
      "Epoch: 9120 Training Loss: 0.033900428062742176 Test Loss: 0.046118679758223986\n",
      "Epoch: 9130 Training Loss: 0.033899956463276815 Test Loss: 0.04623348065859746\n",
      "Epoch: 9140 Training Loss: 0.033918351632955285 Test Loss: 0.046145223577090576\n",
      "Epoch: 9150 Training Loss: 0.03390512312842543 Test Loss: 0.04617008468750714\n",
      "Epoch: 9160 Training Loss: 0.03390559751842017 Test Loss: 0.04619066228636356\n",
      "Epoch: 9170 Training Loss: 0.033899461144311714 Test Loss: 0.04616520678639499\n",
      "Epoch: 9180 Training Loss: 0.033900010878599736 Test Loss: 0.04615040843994319\n",
      "Epoch: 9190 Training Loss: 0.03390512452369012 Test Loss: 0.046279519338545\n",
      "Epoch: 9200 Training Loss: 0.03389670270601876 Test Loss: 0.046208943253289024\n",
      "Epoch: 9210 Training Loss: 0.03389755242221522 Test Loss: 0.04620479647923144\n",
      "Epoch: 9220 Training Loss: 0.03389678223610611 Test Loss: 0.04612250561773472\n",
      "Epoch: 9230 Training Loss: 0.03390357019882499 Test Loss: 0.04633454362580439\n",
      "Epoch: 9240 Training Loss: 0.033894390752426735 Test Loss: 0.04612172705113844\n",
      "Epoch: 9250 Training Loss: 0.03389447446830816 Test Loss: 0.04613504919067472\n",
      "Epoch: 9260 Training Loss: 0.033893649866876124 Test Loss: 0.04627120904706217\n",
      "Epoch: 9270 Training Loss: 0.033901302893703064 Test Loss: 0.04610783563710166\n",
      "Epoch: 9280 Training Loss: 0.033893070832029595 Test Loss: 0.04612927551939266\n",
      "Epoch: 9290 Training Loss: 0.033890810503231124 Test Loss: 0.04618827914703302\n",
      "Epoch: 9300 Training Loss: 0.03389400007831342 Test Loss: 0.046172721767913885\n",
      "Epoch: 9310 Training Loss: 0.03389025239735495 Test Loss: 0.046160390277630935\n",
      "Epoch: 9320 Training Loss: 0.03389450655939604 Test Loss: 0.0461195141360458\n",
      "Epoch: 9330 Training Loss: 0.03389786635677056 Test Loss: 0.046247999948921564\n",
      "Epoch: 9340 Training Loss: 0.03389559486585456 Test Loss: 0.046287773818802294\n",
      "Epoch: 9350 Training Loss: 0.03389079096952546 Test Loss: 0.04613619332079828\n",
      "Epoch: 9360 Training Loss: 0.033896872928310985 Test Loss: 0.04613046150793537\n",
      "Epoch: 9370 Training Loss: 0.03390006110812859 Test Loss: 0.04612642635632887\n",
      "Epoch: 9380 Training Loss: 0.03388997334441687 Test Loss: 0.04615052564351683\n",
      "Epoch: 9390 Training Loss: 0.033889489187569295 Test Loss: 0.04621125104746508\n",
      "Epoch: 9400 Training Loss: 0.03388954360289222 Test Loss: 0.04617042234542166\n",
      "Epoch: 9410 Training Loss: 0.03389894210584688 Test Loss: 0.0461075398376063\n",
      "Epoch: 9420 Training Loss: 0.03389127233584365 Test Loss: 0.04612801976681802\n",
      "Epoch: 9430 Training Loss: 0.03390597284462189 Test Loss: 0.04618892655724928\n",
      "Epoch: 9440 Training Loss: 0.03388799346382117 Test Loss: 0.04628701478613496\n",
      "Epoch: 9450 Training Loss: 0.03388443414359591 Test Loss: 0.046102957735989505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9460 Training Loss: 0.033884815050856394 Test Loss: 0.04614398177732232\n",
      "Epoch: 9470 Training Loss: 0.03388591033363837 Test Loss: 0.04613210514852751\n",
      "Epoch: 9480 Training Loss: 0.033885878242550495 Test Loss: 0.046164916568022185\n",
      "Epoch: 9490 Training Loss: 0.0339041087709955 Test Loss: 0.04619648339718733\n",
      "Epoch: 9500 Training Loss: 0.03388850971175662 Test Loss: 0.046172370157192985\n",
      "Epoch: 9510 Training Loss: 0.033887230254035505 Test Loss: 0.04620750890479266\n",
      "Epoch: 9520 Training Loss: 0.03388704607909637 Test Loss: 0.04619022695880435\n",
      "Epoch: 9530 Training Loss: 0.033893619171052936 Test Loss: 0.04615533378059705\n",
      "Epoch: 9540 Training Loss: 0.03389052586923427 Test Loss: 0.046083328927967265\n",
      "Epoch: 9550 Training Loss: 0.03388336955663712 Test Loss: 0.046221645888221814\n",
      "Epoch: 9560 Training Loss: 0.03388809252761418 Test Loss: 0.046253581071475515\n",
      "Epoch: 9570 Training Loss: 0.033890058455562984 Test Loss: 0.046104849736535294\n",
      "Epoch: 9580 Training Loss: 0.03388365140010458 Test Loss: 0.046309439736556726\n",
      "Epoch: 9590 Training Loss: 0.0338932884933213 Test Loss: 0.04615244554967538\n",
      "Epoch: 9600 Training Loss: 0.0338907407399966 Test Loss: 0.04610034577063426\n",
      "Epoch: 9610 Training Loss: 0.03389404333151882 Test Loss: 0.04613999406525752\n",
      "Epoch: 9620 Training Loss: 0.03388910269925005 Test Loss: 0.04615448824053013\n",
      "Epoch: 9630 Training Loss: 0.03389812448073829 Test Loss: 0.046130620569928164\n",
      "Epoch: 9640 Training Loss: 0.03388692050527423 Test Loss: 0.046145424497502516\n",
      "Epoch: 9650 Training Loss: 0.03388849296858033 Test Loss: 0.04617749083713624\n",
      "Epoch: 9660 Training Loss: 0.03389095002970016 Test Loss: 0.046184422591348244\n",
      "Epoch: 9670 Training Loss: 0.03388543873417301 Test Loss: 0.046115738506638054\n",
      "Epoch: 9680 Training Loss: 0.03388373930178008 Test Loss: 0.04613707792872308\n",
      "Epoch: 9690 Training Loss: 0.03388814554767242 Test Loss: 0.046338740629964965\n",
      "Epoch: 9700 Training Loss: 0.03389360382314134 Test Loss: 0.046126920285674895\n",
      "Epoch: 9710 Training Loss: 0.03389216530524552 Test Loss: 0.04610243590103071\n",
      "Epoch: 9720 Training Loss: 0.03388695259636212 Test Loss: 0.04615286134330565\n",
      "Epoch: 9730 Training Loss: 0.03388489458094375 Test Loss: 0.04614393712834189\n",
      "Epoch: 9740 Training Loss: 0.033881251544837065 Test Loss: 0.04611274981551041\n",
      "Epoch: 9750 Training Loss: 0.03389751893586265 Test Loss: 0.046136031468244215\n",
      "Epoch: 9760 Training Loss: 0.0338912500116086 Test Loss: 0.046153729207862794\n",
      "Epoch: 9770 Training Loss: 0.03388432810347944 Test Loss: 0.046100507623188326\n",
      "Epoch: 9780 Training Loss: 0.03388931059368892 Test Loss: 0.04613475897230192\n",
      "Epoch: 9790 Training Loss: 0.033884223458627655 Test Loss: 0.04616420497489656\n",
      "Epoch: 9800 Training Loss: 0.03389244575344829 Test Loss: 0.04614519009035525\n",
      "Epoch: 9810 Training Loss: 0.03388586568516828 Test Loss: 0.04624795809050241\n",
      "Epoch: 9820 Training Loss: 0.03388688422839228 Test Loss: 0.04613605379273443\n",
      "Epoch: 9830 Training Loss: 0.03388424159706863 Test Loss: 0.04617487329065843\n",
      "Epoch: 9840 Training Loss: 0.033888723187254256 Test Loss: 0.04624842132367439\n",
      "Epoch: 9850 Training Loss: 0.03388313515216913 Test Loss: 0.04610940114197804\n",
      "Epoch: 9860 Training Loss: 0.03388807717970259 Test Loss: 0.04611175637569581\n",
      "Epoch: 9870 Training Loss: 0.03388258262735173 Test Loss: 0.04621376534317564\n",
      "Epoch: 9880 Training Loss: 0.033883728139662554 Test Loss: 0.04618713222634818\n",
      "Epoch: 9890 Training Loss: 0.03388158222256869 Test Loss: 0.046142664632399584\n",
      "Epoch: 9900 Training Loss: 0.03387998324923348 Test Loss: 0.04610272332884224\n",
      "Epoch: 9910 Training Loss: 0.0338862117108115 Test Loss: 0.04614354086864056\n",
      "Epoch: 9920 Training Loss: 0.0338950214120668 Test Loss: 0.046159550318686564\n",
      "Epoch: 9930 Training Loss: 0.033889052469721195 Test Loss: 0.046263657788246676\n",
      "Epoch: 9940 Training Loss: 0.033878454039132776 Test Loss: 0.04620629780119845\n",
      "Epoch: 9950 Training Loss: 0.03388678376933457 Test Loss: 0.04627063698200039\n",
      "Epoch: 9960 Training Loss: 0.033891940667630356 Test Loss: 0.04624122167557979\n",
      "Epoch: 9970 Training Loss: 0.03388661633757172 Test Loss: 0.046163323157533034\n",
      "Epoch: 9980 Training Loss: 0.033881681286361716 Test Loss: 0.04614793042152924\n",
      "Epoch: 9990 Training Loss: 0.03388600102584325 Test Loss: 0.04630274238949199\n",
      "Epoch: 10000 Training Loss: 0.0338964641157567 Test Loss: 0.046369961429531766\n",
      "Epoch: 10010 Training Loss: 0.033890171472002906 Test Loss: 0.046204938797856566\n",
      "Epoch: 10020 Training Loss: 0.03388260355632208 Test Loss: 0.04617183157886653\n",
      "Epoch: 10030 Training Loss: 0.03388756093176714 Test Loss: 0.04633268790255521\n",
      "Epoch: 10040 Training Loss: 0.03388087900916472 Test Loss: 0.04617927400579222\n",
      "Epoch: 10050 Training Loss: 0.03387914329988984 Test Loss: 0.046165195624149884\n",
      "Epoch: 10060 Training Loss: 0.03388006417458552 Test Loss: 0.04614432222579811\n",
      "Epoch: 10070 Training Loss: 0.033894428424573374 Test Loss: 0.04621052271097179\n",
      "Epoch: 10080 Training Loss: 0.03388641262892692 Test Loss: 0.0461472439434551\n",
      "Epoch: 10090 Training Loss: 0.03388069762475497 Test Loss: 0.046199031179633206\n",
      "Epoch: 10100 Training Loss: 0.0338830374836408 Test Loss: 0.046231597029735505\n",
      "Epoch: 10110 Training Loss: 0.03388275703543803 Test Loss: 0.046164383570818285\n",
      "Epoch: 10120 Training Loss: 0.03389107002246354 Test Loss: 0.046202251487346836\n",
      "Epoch: 10130 Training Loss: 0.033875441662666164 Test Loss: 0.046145424497502516\n",
      "Epoch: 10140 Training Loss: 0.033880714367931254 Test Loss: 0.046273756829508045\n",
      "Epoch: 10150 Training Loss: 0.033888318560494034 Test Loss: 0.04617313477098288\n",
      "Epoch: 10160 Training Loss: 0.033884611342211594 Test Loss: 0.04614768206157559\n",
      "Epoch: 10170 Training Loss: 0.03389720360604262 Test Loss: 0.04626836825568221\n",
      "Epoch: 10180 Training Loss: 0.03388773115405937 Test Loss: 0.046316990995372224\n",
      "Epoch: 10190 Training Loss: 0.03388266076217439 Test Loss: 0.046206440119823576\n",
      "Epoch: 10200 Training Loss: 0.03387700994017819 Test Loss: 0.046167277382862505\n",
      "Epoch: 10210 Training Loss: 0.03387754293128993 Test Loss: 0.04617922935681179\n",
      "Epoch: 10220 Training Loss: 0.03387820568201788 Test Loss: 0.04619104180269723\n",
      "Epoch: 10230 Training Loss: 0.03388003068823295 Test Loss: 0.04614710720595253\n",
      "Epoch: 10240 Training Loss: 0.03388990637171173 Test Loss: 0.046131823301838536\n",
      "Epoch: 10250 Training Loss: 0.03388810229446702 Test Loss: 0.04623997987581154\n",
      "Epoch: 10260 Training Loss: 0.03387427382612029 Test Loss: 0.04617976235401569\n",
      "Epoch: 10270 Training Loss: 0.033877157838235375 Test Loss: 0.04621264353754229\n",
      "Epoch: 10280 Training Loss: 0.03387991627652834 Test Loss: 0.04622311930457606\n",
      "Epoch: 10290 Training Loss: 0.03389086073275997 Test Loss: 0.04623580240557991\n",
      "Epoch: 10300 Training Loss: 0.03388435042771448 Test Loss: 0.046176851798603806\n",
      "Epoch: 10310 Training Loss: 0.03388096272504615 Test Loss: 0.046173997054417465\n",
      "Epoch: 10320 Training Loss: 0.03388020649158394 Test Loss: 0.046213366292913026\n",
      "Epoch: 10330 Training Loss: 0.03387468542920396 Test Loss: 0.04619558204589486\n",
      "Epoch: 10340 Training Loss: 0.0338802860216713 Test Loss: 0.0462180962942775\n",
      "Epoch: 10350 Training Loss: 0.033884718777592755 Test Loss: 0.04620309423685248\n",
      "Epoch: 10360 Training Loss: 0.03387995115814559 Test Loss: 0.04623260163179522\n",
      "Epoch: 10370 Training Loss: 0.03387090147136354 Test Loss: 0.0461669871644897\n",
      "Epoch: 10380 Training Loss: 0.03387738108058584 Test Loss: 0.046238372512516\n",
      "Epoch: 10390 Training Loss: 0.03387917260044834 Test Loss: 0.04623727303137287\n",
      "Epoch: 10400 Training Loss: 0.033886764235628905 Test Loss: 0.04625831386340127\n",
      "Epoch: 10410 Training Loss: 0.033874573808028725 Test Loss: 0.04615365944383087\n",
      "Epoch: 10420 Training Loss: 0.03387678809309242 Test Loss: 0.04617456353835669\n",
      "Epoch: 10430 Training Loss: 0.03387846101545623 Test Loss: 0.04633661143171063\n",
      "Epoch: 10440 Training Loss: 0.033881522226187004 Test Loss: 0.046222921174725394\n",
      "Epoch: 10450 Training Loss: 0.033876548107565664 Test Loss: 0.0461695572714258\n",
      "Epoch: 10460 Training Loss: 0.03388252263097004 Test Loss: 0.04618946234501446\n",
      "Epoch: 10470 Training Loss: 0.033874420328912774 Test Loss: 0.04623155517131635\n",
      "Epoch: 10480 Training Loss: 0.03388544571049646 Test Loss: 0.046251596982407585\n",
      "Epoch: 10490 Training Loss: 0.0338929396771487 Test Loss: 0.046200086011795906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10500 Training Loss: 0.033883158871668866 Test Loss: 0.046182008755843655\n",
      "Epoch: 10510 Training Loss: 0.03388684376571626 Test Loss: 0.04620853025022003\n",
      "Epoch: 10520 Training Loss: 0.03387534120360845 Test Loss: 0.04625355316586275\n",
      "Epoch: 10530 Training Loss: 0.03387232324608308 Test Loss: 0.04617773361596733\n",
      "Epoch: 10540 Training Loss: 0.0338785238023673 Test Loss: 0.04618358263240387\n",
      "Epoch: 10550 Training Loss: 0.03388863668084345 Test Loss: 0.04621560711361844\n",
      "Epoch: 10560 Training Loss: 0.03386797699657242 Test Loss: 0.04617513002229592\n",
      "Epoch: 10570 Training Loss: 0.03387428917403188 Test Loss: 0.04619740986353128\n",
      "Epoch: 10580 Training Loss: 0.03387960513250237 Test Loss: 0.046291169931876375\n",
      "Epoch: 10590 Training Loss: 0.033879045631361515 Test Loss: 0.04644607677892254\n",
      "Epoch: 10600 Training Loss: 0.03387202047364526 Test Loss: 0.046213053750050005\n",
      "Epoch: 10610 Training Loss: 0.03387565932395787 Test Loss: 0.04622030083768631\n",
      "Epoch: 10620 Training Loss: 0.033878448458074015 Test Loss: 0.04624853294612547\n",
      "Epoch: 10630 Training Loss: 0.03387477472614414 Test Loss: 0.04624448663227385\n",
      "Epoch: 10640 Training Loss: 0.03387899679709735 Test Loss: 0.04628092857198987\n",
      "Epoch: 10650 Training Loss: 0.03389147604448845 Test Loss: 0.0462540917441892\n",
      "Epoch: 10660 Training Loss: 0.0338792228299772 Test Loss: 0.0462039397769194\n",
      "Epoch: 10670 Training Loss: 0.03387903167871461 Test Loss: 0.04623442386830908\n",
      "Epoch: 10680 Training Loss: 0.033874988201641774 Test Loss: 0.04623337182670766\n",
      "Epoch: 10690 Training Loss: 0.03387256044108045 Test Loss: 0.04623237280577051\n",
      "Epoch: 10700 Training Loss: 0.03387419987709169 Test Loss: 0.04622128590581708\n",
      "Epoch: 10710 Training Loss: 0.033869863394433875 Test Loss: 0.04630799143525398\n",
      "Epoch: 10720 Training Loss: 0.03387867867674793 Test Loss: 0.04632271722711258\n",
      "Epoch: 10730 Training Loss: 0.03388239845241259 Test Loss: 0.046225756384982795\n",
      "Epoch: 10740 Training Loss: 0.03388446762994848 Test Loss: 0.04619288636370131\n",
      "Epoch: 10750 Training Loss: 0.033871914433528785 Test Loss: 0.04625922358637756\n",
      "Epoch: 10760 Training Loss: 0.033874078489063625 Test Loss: 0.04620436115167223\n",
      "Epoch: 10770 Training Loss: 0.033877043426530765 Test Loss: 0.046261274648916134\n",
      "Epoch: 10780 Training Loss: 0.03386952015932003 Test Loss: 0.04627012351872543\n",
      "Epoch: 10790 Training Loss: 0.033875514216430064 Test Loss: 0.04646443867212503\n",
      "Epoch: 10800 Training Loss: 0.033873050178986785 Test Loss: 0.04618320869719276\n",
      "Epoch: 10810 Training Loss: 0.03387851822130854 Test Loss: 0.0461767066894174\n",
      "Epoch: 10820 Training Loss: 0.033872374870876625 Test Loss: 0.04620056040721299\n",
      "Epoch: 10830 Training Loss: 0.0338779112811682 Test Loss: 0.04622055198820124\n",
      "Epoch: 10840 Training Loss: 0.03388195894403511 Test Loss: 0.046155791432646476\n",
      "Epoch: 10850 Training Loss: 0.033878555893455176 Test Loss: 0.04629002301119153\n",
      "Epoch: 10860 Training Loss: 0.03387248509678717 Test Loss: 0.04657733640970762\n",
      "Epoch: 10870 Training Loss: 0.033877623856641975 Test Loss: 0.04630820630847231\n",
      "Epoch: 10880 Training Loss: 0.03386870672000551 Test Loss: 0.04621824698458646\n",
      "Epoch: 10890 Training Loss: 0.03388055670302124 Test Loss: 0.04621391045236204\n",
      "Epoch: 10900 Training Loss: 0.03386445395322912 Test Loss: 0.046210419460204544\n",
      "Epoch: 10910 Training Loss: 0.03387312831380945 Test Loss: 0.046181690631858084\n",
      "Epoch: 10920 Training Loss: 0.03387643230059636 Test Loss: 0.04618684758909793\n",
      "Epoch: 10930 Training Loss: 0.03387439939994242 Test Loss: 0.04621292817479254\n",
      "Epoch: 10940 Training Loss: 0.03386949643982029 Test Loss: 0.046183172419896154\n",
      "Epoch: 10950 Training Loss: 0.03387552816907697 Test Loss: 0.046267221334997374\n",
      "Epoch: 10960 Training Loss: 0.033872010706792424 Test Loss: 0.04630628919287502\n",
      "Epoch: 10970 Training Loss: 0.03387883773692264 Test Loss: 0.04619871584620891\n",
      "Epoch: 10980 Training Loss: 0.033874211039209214 Test Loss: 0.04622212865532273\n",
      "Epoch: 10990 Training Loss: 0.033872584160580185 Test Loss: 0.04617508816387676\n",
      "Epoch: 11000 Training Loss: 0.03386979642172873 Test Loss: 0.04626656555309728\n",
      "Epoch: 11010 Training Loss: 0.033872148837996774 Test Loss: 0.04633259302347179\n",
      "Epoch: 11020 Training Loss: 0.03387601232592455 Test Loss: 0.046328666703755086\n",
      "Epoch: 11030 Training Loss: 0.03387116378112534 Test Loss: 0.04626088676089864\n",
      "Epoch: 11040 Training Loss: 0.03387650904015433 Test Loss: 0.04631405532490884\n",
      "Epoch: 11050 Training Loss: 0.03387490169523097 Test Loss: 0.0462308826460486\n",
      "Epoch: 11060 Training Loss: 0.03388245007720613 Test Loss: 0.04629402188550144\n",
      "Epoch: 11070 Training Loss: 0.03387115680480189 Test Loss: 0.046214373685534015\n",
      "Epoch: 11080 Training Loss: 0.033869116927824496 Test Loss: 0.046250740280095556\n",
      "Epoch: 11090 Training Loss: 0.03387775640678757 Test Loss: 0.04623338298895277\n",
      "Epoch: 11100 Training Loss: 0.033881710586920216 Test Loss: 0.04648897049631092\n",
      "Epoch: 11110 Training Loss: 0.03387490448576035 Test Loss: 0.04635987075995422\n",
      "Epoch: 11120 Training Loss: 0.03386978805014059 Test Loss: 0.04621622382766065\n",
      "Epoch: 11130 Training Loss: 0.03387504540749409 Test Loss: 0.046210042734432154\n",
      "Epoch: 11140 Training Loss: 0.03387575420195682 Test Loss: 0.04622402902755235\n",
      "Epoch: 11150 Training Loss: 0.03387116378112534 Test Loss: 0.04618112135735758\n",
      "Epoch: 11160 Training Loss: 0.03387210279426199 Test Loss: 0.046196248990040066\n",
      "Epoch: 11170 Training Loss: 0.03386743423860785 Test Loss: 0.046220697097387645\n",
      "Epoch: 11180 Training Loss: 0.033884886209355605 Test Loss: 0.04622883716463258\n",
      "Epoch: 11190 Training Loss: 0.03387213209482049 Test Loss: 0.046192964499417066\n",
      "Epoch: 11200 Training Loss: 0.03386631105053207 Test Loss: 0.04621168637502429\n",
      "Epoch: 11210 Training Loss: 0.03386982432702254 Test Loss: 0.04621294770872148\n",
      "Epoch: 11220 Training Loss: 0.03387658577971231 Test Loss: 0.04617903680808368\n",
      "Epoch: 11230 Training Loss: 0.03387346736312922 Test Loss: 0.0462902936956354\n",
      "Epoch: 11240 Training Loss: 0.033866767302085835 Test Loss: 0.04617883309711046\n",
      "Epoch: 11250 Training Loss: 0.033869408538144796 Test Loss: 0.04624522892157353\n",
      "Epoch: 11260 Training Loss: 0.0338706893911306 Test Loss: 0.04621345559087389\n",
      "Epoch: 11270 Training Loss: 0.03386735191799112 Test Loss: 0.04637708573247188\n",
      "Epoch: 11280 Training Loss: 0.03387505936014099 Test Loss: 0.04626900450365336\n",
      "Epoch: 11290 Training Loss: 0.0338683006979806 Test Loss: 0.04631848115509413\n",
      "Epoch: 11300 Training Loss: 0.03387818196251814 Test Loss: 0.046193380293047336\n",
      "Epoch: 11310 Training Loss: 0.033868436038655574 Test Loss: 0.04619211616878886\n",
      "Epoch: 11320 Training Loss: 0.033862457329457134 Test Loss: 0.0462137458092467\n",
      "Epoch: 11330 Training Loss: 0.03386862439938878 Test Loss: 0.04633983732054682\n",
      "Epoch: 11340 Training Loss: 0.03387830195528152 Test Loss: 0.04620302168225928\n",
      "Epoch: 11350 Training Loss: 0.033873390623571246 Test Loss: 0.046207121016775154\n",
      "Epoch: 11360 Training Loss: 0.033867730034722224 Test Loss: 0.04624007754545623\n",
      "Epoch: 11370 Training Loss: 0.033866467320177396 Test Loss: 0.04648013278874674\n",
      "Epoch: 11380 Training Loss: 0.033863241468213144 Test Loss: 0.04624208674957565\n",
      "Epoch: 11390 Training Loss: 0.03386951457826127 Test Loss: 0.046306584992370385\n",
      "Epoch: 11400 Training Loss: 0.03386838022806796 Test Loss: 0.04636061583981518\n",
      "Epoch: 11410 Training Loss: 0.03386633197950242 Test Loss: 0.04649969462329834\n",
      "Epoch: 11420 Training Loss: 0.03387656624600664 Test Loss: 0.04627519396856569\n",
      "Epoch: 11430 Training Loss: 0.03388172314430243 Test Loss: 0.046213821154401175\n",
      "Epoch: 11440 Training Loss: 0.033865462729600296 Test Loss: 0.04623594751476631\n",
      "Epoch: 11450 Training Loss: 0.03386817651942316 Test Loss: 0.046297588222813414\n",
      "Epoch: 11460 Training Loss: 0.033861734582347494 Test Loss: 0.046225775918911735\n",
      "Epoch: 11470 Training Loss: 0.03387290367619429 Test Loss: 0.04625584142610987\n",
      "Epoch: 11480 Training Loss: 0.03387563699972282 Test Loss: 0.04624827342392671\n",
      "Epoch: 11490 Training Loss: 0.03386347866321052 Test Loss: 0.04629246754287016\n",
      "Epoch: 11500 Training Loss: 0.03387073822539476 Test Loss: 0.04629396049315335\n",
      "Epoch: 11510 Training Loss: 0.03387336969460089 Test Loss: 0.046416471714335106\n",
      "Epoch: 11520 Training Loss: 0.03386354424065097 Test Loss: 0.04624528473279907\n",
      "Epoch: 11530 Training Loss: 0.03385987050872109 Test Loss: 0.046256137225605225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11540 Training Loss: 0.033855863308530215 Test Loss: 0.04622169332776352\n",
      "Epoch: 11550 Training Loss: 0.03387289949040022 Test Loss: 0.04627797615815883\n",
      "Epoch: 11560 Training Loss: 0.03387091960980452 Test Loss: 0.04624301600648089\n",
      "Epoch: 11570 Training Loss: 0.0338707772928061 Test Loss: 0.04626239924511076\n",
      "Epoch: 11580 Training Loss: 0.03387355107901065 Test Loss: 0.04623845622935431\n",
      "Epoch: 11590 Training Loss: 0.033876241149333776 Test Loss: 0.04626880637380269\n",
      "Epoch: 11600 Training Loss: 0.03386322472503686 Test Loss: 0.04637913679501046\n",
      "Epoch: 11610 Training Loss: 0.03386401444485164 Test Loss: 0.0462576915682365\n",
      "Epoch: 11620 Training Loss: 0.03386403816435138 Test Loss: 0.04624902687547149\n",
      "Epoch: 11630 Training Loss: 0.033863438200534494 Test Loss: 0.04625759110803053\n",
      "Epoch: 11640 Training Loss: 0.03387278926448968 Test Loss: 0.04622972735367993\n",
      "Epoch: 11650 Training Loss: 0.033860478844126116 Test Loss: 0.046242050472279056\n",
      "Epoch: 11660 Training Loss: 0.033868261630569274 Test Loss: 0.046272450846830425\n",
      "Epoch: 11670 Training Loss: 0.03385922171064005 Test Loss: 0.046357010434645324\n",
      "Epoch: 11680 Training Loss: 0.03386748307287202 Test Loss: 0.04624629491598133\n",
      "Epoch: 11690 Training Loss: 0.033865941305389106 Test Loss: 0.046270050964132226\n",
      "Epoch: 11700 Training Loss: 0.03387146934409254 Test Loss: 0.046322323757972524\n",
      "Epoch: 11710 Training Loss: 0.03386740912384342 Test Loss: 0.046264157298715254\n",
      "Epoch: 11720 Training Loss: 0.03386190619990442 Test Loss: 0.04624065519164057\n",
      "Epoch: 11730 Training Loss: 0.033858331531767566 Test Loss: 0.04637133159511876\n",
      "Epoch: 11740 Training Loss: 0.03387169258644301 Test Loss: 0.046395762959098676\n",
      "Epoch: 11750 Training Loss: 0.03385932775075652 Test Loss: 0.04626457030178424\n",
      "Epoch: 11760 Training Loss: 0.0338765578744185 Test Loss: 0.0462758106826079\n",
      "Epoch: 11770 Training Loss: 0.03385791295236044 Test Loss: 0.04623787021148615\n",
      "Epoch: 11780 Training Loss: 0.033853722972495115 Test Loss: 0.04626438891530124\n",
      "Epoch: 11790 Training Loss: 0.033861823879287684 Test Loss: 0.046327450019038326\n",
      "Epoch: 11800 Training Loss: 0.03386584921791954 Test Loss: 0.046232233277706654\n",
      "Epoch: 11810 Training Loss: 0.03385558983665089 Test Loss: 0.04623884132681053\n",
      "Epoch: 11820 Training Loss: 0.03386268336233698 Test Loss: 0.04634095354505761\n",
      "Epoch: 11830 Training Loss: 0.03386176667343537 Test Loss: 0.04628953187240679\n",
      "Epoch: 11840 Training Loss: 0.03386042582406788 Test Loss: 0.046693945593788574\n",
      "Epoch: 11850 Training Loss: 0.03386535389895444 Test Loss: 0.0462433787794469\n",
      "Epoch: 11860 Training Loss: 0.03387412313753372 Test Loss: 0.04635251204986684\n",
      "Epoch: 11870 Training Loss: 0.033864805559931106 Test Loss: 0.04623732326147586\n",
      "Epoch: 11880 Training Loss: 0.033871377256622974 Test Loss: 0.046291881525002\n",
      "Epoch: 11890 Training Loss: 0.03386449302064045 Test Loss: 0.04631267678763802\n",
      "Epoch: 11900 Training Loss: 0.03386601804494708 Test Loss: 0.046302187067797866\n",
      "Epoch: 11910 Training Loss: 0.03386375213508984 Test Loss: 0.0462639703311097\n",
      "Epoch: 11920 Training Loss: 0.033859295659668644 Test Loss: 0.0462628066670572\n",
      "Epoch: 11930 Training Loss: 0.03386374655403108 Test Loss: 0.04643844459383001\n",
      "Epoch: 11940 Training Loss: 0.03386358609859168 Test Loss: 0.046364980277652366\n",
      "Epoch: 11950 Training Loss: 0.03386649103967713 Test Loss: 0.04667649342356237\n",
      "Epoch: 11960 Training Loss: 0.03386215037122524 Test Loss: 0.04627235038662445\n",
      "Epoch: 11970 Training Loss: 0.03385173611557596 Test Loss: 0.046383872377497484\n",
      "Epoch: 11980 Training Loss: 0.033862928928922495 Test Loss: 0.04641673402709515\n",
      "Epoch: 11990 Training Loss: 0.03385814456629905 Test Loss: 0.04634071355678779\n",
      "Epoch: 12000 Training Loss: 0.033868207215246346 Test Loss: 0.04624046543347373\n",
      "Epoch: 12010 Training Loss: 0.033860209558040866 Test Loss: 0.046407971664685445\n",
      "Epoch: 12020 Training Loss: 0.03385922031537536 Test Loss: 0.046367581080762506\n",
      "Epoch: 12030 Training Loss: 0.033873117151691924 Test Loss: 0.046405973622811125\n",
      "Epoch: 12040 Training Loss: 0.03386082766029872 Test Loss: 0.04634317483183408\n",
      "Epoch: 12050 Training Loss: 0.033861710862847755 Test Loss: 0.046366104873846986\n",
      "Epoch: 12060 Training Loss: 0.03385641583334762 Test Loss: 0.04622425785357706\n",
      "Epoch: 12070 Training Loss: 0.03385701998295857 Test Loss: 0.04628395912153667\n",
      "Epoch: 12080 Training Loss: 0.03385360995605519 Test Loss: 0.046330310344347224\n",
      "Epoch: 12090 Training Loss: 0.033860633718506755 Test Loss: 0.046430028261018656\n",
      "Epoch: 12100 Training Loss: 0.03386791281439667 Test Loss: 0.046736744432093545\n",
      "Epoch: 12110 Training Loss: 0.033862510349515366 Test Loss: 0.04634549378825525\n",
      "Epoch: 12120 Training Loss: 0.03386488788054784 Test Loss: 0.04649404373671246\n",
      "Epoch: 12130 Training Loss: 0.03385499824442215 Test Loss: 0.04651375626157302\n",
      "Epoch: 12140 Training Loss: 0.03385791016183106 Test Loss: 0.04634715138165377\n",
      "Epoch: 12150 Training Loss: 0.03386155738373181 Test Loss: 0.04622201982343293\n",
      "Epoch: 12160 Training Loss: 0.03386622175359188 Test Loss: 0.0464852646309351\n",
      "Epoch: 12170 Training Loss: 0.03385404946443267 Test Loss: 0.046435712634339855\n",
      "Epoch: 12180 Training Loss: 0.033867730034722224 Test Loss: 0.04686085185432574\n",
      "Epoch: 12190 Training Loss: 0.03385821432953357 Test Loss: 0.04645641301789245\n",
      "Epoch: 12200 Training Loss: 0.033864841836813056 Test Loss: 0.04636270597021163\n",
      "Epoch: 12210 Training Loss: 0.03385264861868349 Test Loss: 0.04628549951136156\n",
      "Epoch: 12220 Training Loss: 0.03385084454143878 Test Loss: 0.046337205821262634\n",
      "Epoch: 12230 Training Loss: 0.033858278511709326 Test Loss: 0.04628360192969321\n",
      "Epoch: 12240 Training Loss: 0.033855128004038365 Test Loss: 0.04629283031583617\n",
      "Epoch: 12250 Training Loss: 0.033857328336455154 Test Loss: 0.046364489138867614\n",
      "Epoch: 12260 Training Loss: 0.033861310421881605 Test Loss: 0.046244450354977255\n",
      "Epoch: 12270 Training Loss: 0.03386237779936978 Test Loss: 0.046248678055311875\n",
      "Epoch: 12280 Training Loss: 0.03386689566643735 Test Loss: 0.04626816454470899\n",
      "Epoch: 12290 Training Loss: 0.03385701440189981 Test Loss: 0.046259072896068607\n",
      "Epoch: 12300 Training Loss: 0.03385077477820426 Test Loss: 0.046274596788452416\n",
      "Epoch: 12310 Training Loss: 0.03386192573361008 Test Loss: 0.04629878816416251\n",
      "Epoch: 12320 Training Loss: 0.033859192410081555 Test Loss: 0.046292450799502505\n",
      "Epoch: 12330 Training Loss: 0.03385008133165312 Test Loss: 0.04628904352418332\n",
      "Epoch: 12340 Training Loss: 0.033856867899107315 Test Loss: 0.04625956403485335\n",
      "Epoch: 12350 Training Loss: 0.03385419736248985 Test Loss: 0.046358715467585555\n",
      "Epoch: 12360 Training Loss: 0.03385341461899853 Test Loss: 0.04631592779152569\n",
      "Epoch: 12370 Training Loss: 0.03386276010189495 Test Loss: 0.04632364927457908\n",
      "Epoch: 12380 Training Loss: 0.03385391272849301 Test Loss: 0.04630988064523849\n",
      "Epoch: 12390 Training Loss: 0.033861696910200856 Test Loss: 0.04627682365635144\n",
      "Epoch: 12400 Training Loss: 0.03386054163103719 Test Loss: 0.04651293304599631\n",
      "Epoch: 12410 Training Loss: 0.033859528668871944 Test Loss: 0.04625095794387516\n",
      "Epoch: 12420 Training Loss: 0.033858638489999454 Test Loss: 0.046251845342361236\n",
      "Epoch: 12430 Training Loss: 0.03384176276356885 Test Loss: 0.04624573959428722\n",
      "Epoch: 12440 Training Loss: 0.0338488465224021 Test Loss: 0.046288144963452134\n",
      "Epoch: 12450 Training Loss: 0.033846848503365425 Test Loss: 0.046385167197930004\n",
      "Epoch: 12460 Training Loss: 0.03385031155032704 Test Loss: 0.04633478919519677\n",
      "Epoch: 12470 Training Loss: 0.03384699500615792 Test Loss: 0.04628037604085703\n",
      "Epoch: 12480 Training Loss: 0.033851284049816265 Test Loss: 0.046255852588354976\n",
      "Epoch: 12490 Training Loss: 0.03384682059807161 Test Loss: 0.04625985983434871\n",
      "Epoch: 12500 Training Loss: 0.0338483219028785 Test Loss: 0.046276918535434856\n",
      "Epoch: 12510 Training Loss: 0.03385327927832356 Test Loss: 0.04626216483796349\n",
      "Epoch: 12520 Training Loss: 0.033855295435801215 Test Loss: 0.04637333242755435\n",
      "Epoch: 12530 Training Loss: 0.03385333369364649 Test Loss: 0.04631562082978523\n",
      "Epoch: 12540 Training Loss: 0.03385411504187312 Test Loss: 0.04626911333554316\n",
      "Epoch: 12550 Training Loss: 0.03385528427368369 Test Loss: 0.04628861656830794\n",
      "Epoch: 12560 Training Loss: 0.03385137474202114 Test Loss: 0.046560897213224955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12570 Training Loss: 0.033850583626941676 Test Loss: 0.046328256491247374\n",
      "Epoch: 12580 Training Loss: 0.03386831186009813 Test Loss: 0.046365323516689434\n",
      "Epoch: 12590 Training Loss: 0.03386183085561113 Test Loss: 0.04633065079282302\n",
      "Epoch: 12600 Training Loss: 0.0338426292229416 Test Loss: 0.046246420491238796\n",
      "Epoch: 12610 Training Loss: 0.03385263745656597 Test Loss: 0.046270232350615226\n",
      "Epoch: 12620 Training Loss: 0.03385702416875264 Test Loss: 0.04629773612256109\n",
      "Epoch: 12630 Training Loss: 0.03385169565289994 Test Loss: 0.046328114172622246\n",
      "Epoch: 12640 Training Loss: 0.03385223143454106 Test Loss: 0.04628818961243256\n",
      "Epoch: 12650 Training Loss: 0.03385231375515779 Test Loss: 0.046309802509522734\n",
      "Epoch: 12660 Training Loss: 0.03384399379180883 Test Loss: 0.04632109870157193\n",
      "Epoch: 12670 Training Loss: 0.0338492246391332 Test Loss: 0.04627946631788074\n",
      "Epoch: 12680 Training Loss: 0.03385272117244739 Test Loss: 0.04632628635498583\n",
      "Epoch: 12690 Training Loss: 0.033848104241586796 Test Loss: 0.04631096617357523\n",
      "Epoch: 12700 Training Loss: 0.033840140070733896 Test Loss: 0.046305265056886376\n",
      "Epoch: 12710 Training Loss: 0.0338460364593156 Test Loss: 0.04627127602053282\n",
      "Epoch: 12720 Training Loss: 0.03384267945247045 Test Loss: 0.04622331464386544\n",
      "Epoch: 12730 Training Loss: 0.033849646009069714 Test Loss: 0.04625974542133635\n",
      "Epoch: 12740 Training Loss: 0.03384617598578464 Test Loss: 0.046299616960861774\n",
      "Epoch: 12750 Training Loss: 0.033849520435247575 Test Loss: 0.04627531675326187\n",
      "Epoch: 12760 Training Loss: 0.03385265140921287 Test Loss: 0.046310980126381615\n",
      "Epoch: 12770 Training Loss: 0.03385374250620078 Test Loss: 0.04629833888379692\n",
      "Epoch: 12780 Training Loss: 0.03383555941675525 Test Loss: 0.0463468806972099\n",
      "Epoch: 12790 Training Loss: 0.03384353893551975 Test Loss: 0.046262935032875936\n",
      "Epoch: 12800 Training Loss: 0.033844205872041774 Test Loss: 0.04624558053229443\n",
      "Epoch: 12810 Training Loss: 0.0338511473138766 Test Loss: 0.04622039571676973\n",
      "Epoch: 12820 Training Loss: 0.03384771635800286 Test Loss: 0.046479998841805446\n",
      "Epoch: 12830 Training Loss: 0.03383653191624447 Test Loss: 0.04629547018680419\n",
      "Epoch: 12840 Training Loss: 0.03384233203156254 Test Loss: 0.04641852556743496\n",
      "Epoch: 12850 Training Loss: 0.033847982853558736 Test Loss: 0.04625086585535302\n",
      "Epoch: 12860 Training Loss: 0.03384390589013333 Test Loss: 0.04622610520514242\n",
      "Epoch: 12870 Training Loss: 0.03384119907663392 Test Loss: 0.046268834279415465\n",
      "Epoch: 12880 Training Loss: 0.03383970753867986 Test Loss: 0.04633741790391968\n",
      "Epoch: 12890 Training Loss: 0.03384456724559659 Test Loss: 0.046366110454969536\n",
      "Epoch: 12900 Training Loss: 0.033847463815093896 Test Loss: 0.0463392819988527\n",
      "Epoch: 12910 Training Loss: 0.03384568485261361 Test Loss: 0.046283638206989816\n",
      "Epoch: 12920 Training Loss: 0.03384662944680903 Test Loss: 0.04624990311171246\n",
      "Epoch: 12930 Training Loss: 0.03383610077945513 Test Loss: 0.04628745290425544\n",
      "Epoch: 12940 Training Loss: 0.033847954948264924 Test Loss: 0.04662868831832651\n",
      "Epoch: 12950 Training Loss: 0.03384285386055676 Test Loss: 0.046282374082731344\n",
      "Epoch: 12960 Training Loss: 0.0338334986108075 Test Loss: 0.04642828136965927\n",
      "Epoch: 12970 Training Loss: 0.03384894279566574 Test Loss: 0.04653766579059414\n",
      "Epoch: 12980 Training Loss: 0.03384941718566048 Test Loss: 0.04624733021421509\n",
      "Epoch: 12990 Training Loss: 0.03383391719021463 Test Loss: 0.04628410981184562\n"
     ]
    }
   ],
   "source": [
    "batch_size=int(x_variable.size()[0]/2 )\n",
    "nb_of_epochs=13000\n",
    "my_net.cuda()\n",
    "\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_net.eval()\n",
    "        test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(x_variable.size(0)),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 6.4738e-01 -1.2280e-01  9.5374e-01  ...   9.5954e-01 -6.6858e-01  8.0000e-01\n",
      " 1.4259e+00 -1.8283e-01  7.1479e-01  ...   9.2577e-01 -4.2653e-01  1.0000e+00\n",
      " 6.5825e-01  1.8612e-01  4.9851e-01  ...   1.0180e+00 -6.7245e-01  8.0000e-01\n",
      "                ...                   ⋱                   ...                \n",
      " 1.1867e+00 -1.1245e-01  5.0147e-01  ...   9.3837e-01 -5.3684e-01  9.0000e-01\n",
      " 1.0145e+00  2.0090e-01  5.3360e-01  ...   7.0461e-01 -3.3385e-01  7.0000e-01\n",
      " 5.0250e-01 -2.8801e-02  1.1429e+00  ...   8.3759e-01 -5.5882e-01  4.0000e-01\n",
      "[torch.cuda.FloatTensor of size 174978x25 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(218722, 25)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_variable)\n",
    "\n",
    "polygons_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour1=np.delete(polygons_reshaped[311],24)\n",
    "\n",
    "contour1=polygons[10].reshape(12,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.00646368949978321 -0.1199108451461814 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(contour1,.9)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-248-e05ecdbd7088>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplot_contour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontour1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "plot_contour(contour1)\n",
    "plt.scatter(point_coordinates[:,0],point_coordinates[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (bn_input): BatchNorm1d(25, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc0): Linear(in_features=25, out_features=40, bias=True)\n",
       "  (bn0): BatchNorm1d(40, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (predict): Linear(in_features=40, out_features=2, bias=True)\n",
       "  (fc1): Linear(in_features=40, out_features=40, bias=True)\n",
       "  (bn1): BatchNorm1d(40, eps=1e-05, momentum=0.5, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net.eval()\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=my_net(x_variable).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_inserted_points=predictions.data.numpy()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_inserted_points=predicted_inserted_points.reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e63bef1a58>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "#plt.scatter(polygons_reshaped[3112][0::2].sum()/12,polygons_reshaped[312][1::2].sum()/12,label='Original points')\n",
    "plot_contour(contour1)\n",
    "\n",
    "original_points=point_coordinates.reshape(218722,2)\n",
    "plt.scatter(original_points[:,0],original_points[:,1],label='Original points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6108970818704328, 0, 11)\n",
      "(1.85082522503657, 3, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.029684180794708"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(centers_of_mass[10][0],centers_of_mass[10][1],label='Original points'\n",
    "           )\n",
    "plot_contour(polygons[2])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "points=centers_of_mass\n",
    "hull=ConvexHull(points)\n",
    "hull2=ConvexHull(original_points)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(points[:,0], points[:,1], 'o')\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n",
    "for simplex in hull2.simplices:\n",
    "    plt.plot(original_points[simplex, 0], original_points[simplex, 1], 'k-')\n",
    "\n",
    "    \n",
    "    \n",
    "convex_points=centers_of_mass[hull.vertices]\n",
    "convex_points2=original_points[hull2.vertices]\n",
    "\n",
    "distances=np.empty([convex_points.shape[0],convex_points.shape[0]])    \n",
    "distances2=np.empty([convex_points2.shape[0],convex_points2.shape[0]])    \n",
    "\n",
    "    \n",
    "for i,point_i in enumerate(convex_points):\n",
    "    for j,point_j in enumerate(convex_points):\n",
    "        distances[i][j]=np.linalg.norm(point_i-point_j)\n",
    "for i,point_i in enumerate(convex_points2):\n",
    "    for j,point_j in enumerate(convex_points2):\n",
    "        distances2[i][j]=np.linalg.norm(point_i-point_j)\n",
    "        \n",
    "def max_element(A):\n",
    "    r, (c, l) = max(map(lambda t: (t[0], max(enumerate(t[1]), key=lambda v: v[1])), enumerate(A)), key=lambda v: v[1][1])\n",
    "    return (l, r, c)\n",
    "\n",
    "print(max_element(distances))\n",
    "print(max_element(distances2))\n",
    "\n",
    "maximum_distance_points=convex_points[[0,11]]\n",
    "maximum_distance_points2=convex_points2[[3,11]]\n",
    "\n",
    "plt.plot(maximum_distance_points[:,0],maximum_distance_points[:,1])\n",
    "plt.plot(maximum_distance_points2[:,0],maximum_distance_points2[:,1])\n",
    "1.8508252250365/0.6108970818704328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.908928663899065"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances=[np.linalg.norm(i-j) for i in centers_of_mass for j in predicted_inserted_points]\n",
    "distances=np.array(distances)\n",
    "100*distances.sum()/(x_variable.size(0) *max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 0 is not a Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-ab1a56b168e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tensor_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\python\\Dimensionality_reduction\\Neural_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mACTIVATION\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mpre_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_bn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# input batch normalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mlayer_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     35\u001b[0m         return F.batch_norm(\n\u001b[0;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 0 is not a Variable"
     ]
    }
   ],
   "source": [
    "predictions_test=my_net(x_tensor_test).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable,x_variable_test=x_variable.resize(x_variable.size()[0],1,25),Variable(x_tensor_test.view(x_variable_test.size()[0],1,25),volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using convolution network\n",
    "# O: output dimension\n",
    "# I: Input dimensiion\n",
    "# S: Stride\n",
    "# P: padding\n",
    "# w: kernel size\n",
    "# O=(I-w-2*P)/S+1\n",
    "\n",
    "\n",
    "class Conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Conv_net,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,14,kernel_size=3,stride=2)\n",
    "        self.conv2=nn.Conv1d(14,28,kernel_size=2,stride=1)\n",
    "\n",
    "        self.bn1=nn.BatchNorm1d(num_features=28*11)\n",
    "        self.fc1=nn.Linear(28*11,12)\n",
    "        \n",
    "        self.bn2=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc2=nn.Linear(12,12)\n",
    "        \n",
    "        self.bn3=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc3=nn.Linear(12,12)\n",
    "        \n",
    "        \n",
    "        self.bn4=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc4=nn.Linear(12,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "\n",
    "        x=F.relu(x)\n",
    "        x=self.conv2(x)\n",
    "\n",
    "        #x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        x=F.relu(self.fc1(self.bn1(x.view(-1,28*11))))\n",
    "        x=F.relu(self.fc2(self.bn2(x)))\n",
    "        x=F.relu(self.fc3(self.bn3(x)))\n",
    "\n",
    "        x=self.fc4(self.bn4(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "my_conv_net=Conv_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_conv_net.parameters(), lr=1e-3,weight_decay=0)\n",
    "\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "\n",
    "#loss_func = torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=torch.nn.SmoothL1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'myLossfunction' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-717-a1201bb517b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mloss_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmy_conv_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx_variable\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_variable_test\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_variable_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda activated\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'myLossfunction' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    my_conv_net.cuda()\n",
    "    x_variable , y_variable,x_variable_test,y_variable_test= x_variable.cuda(), y_variable.cuda(),x_variable_test.cuda(),y_variable_test.cuda()\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 3D tensor as input, got 2D tensor instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-815-c13aa2397b48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msum_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_conv_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# input x and predict based on x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m#loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-719-7ecd2ad4f923>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 168\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \"\"\"\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expected 3D tensor as input, got {}D tensor instead.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     f = _ConvNd(_single(stride), _single(padding), _single(dilation), False,\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 3D tensor as input, got 2D tensor instead."
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size=int(x_variable.size()[0]/18  )\n",
    "nb_of_epochs=13000\n",
    "# Train the network #\n",
    "my_conv_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_conv_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        #loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\n",
    "        #loss=torch.sqrt((out[0]-y_variable.narrow(0,b,batch_size)[0]).pow(2)+(out[1]-y_variable.narrow(0,b,batch_size)[1]).pow(2)) \n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_conv_net.eval()\n",
    "        test_loss=loss_func(my_conv_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_conv_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable=x_variable.cpu()\n",
    "my_conv_net=my_conv_net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 extra points\n",
      "13 -0.1925145994756827 -0.0867447465103373 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f958d13940>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contour1=np.delete(polygons_reshaped[15],24)\n",
    "contour1=contour1.reshape(12,2)\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(contour1,.4\n",
    "                                                                              )\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "my_conv_net.eval()\n",
    "predictions=my_conv_net(x_variable).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[15]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(1,2)\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "\n",
    "plot_contour(contour1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09719837, -0.23911646]], dtype=float32)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_inserted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor=torch.FloatTensor([2.2,1.2])\n",
    "a_tensor=torch.FloatTensor([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_variable=Variable(d_tensor)\n",
    "\n",
    "a_variable=Variable(a_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_variable.requires_grad=False\n",
    "a_variable.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4422\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun=torch.sqrt((a_variable[0]-d_variable[0]).pow(2)+(a_variable[1]-d_variable[1]).pow(2))\n",
    "fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.8321\n",
       " 0.5547\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.function import Function\n",
    "from math import pow\n",
    "\n",
    "class myLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target):\n",
    "        self.save_for_backward(output,target) \n",
    "\n",
    "                        \n",
    "       # output=output.view(int(output.size()[0]/2),2)\n",
    "\n",
    "        #target=target.view(int(target.size()[0]/2),2)\n",
    "        distance=torch.nn.PairwiseDistance()\n",
    "        result=distance(output,target)\n",
    "\n",
    "        result=torch.FloatTensor(result)\n",
    "        #self.save_for_backward(result)\n",
    "\n",
    "\n",
    "        return  result \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self,grad_output1):\n",
    "        input1,target=self.saved_variables\n",
    "        \n",
    "        print(input1)\n",
    "        #distance=torch.nn.PairwiseDistance()(input1.view(int(input1.size()[0]/2),2),target.view(int(target.size()[0]/2),2))\n",
    "        distance=torch.nn.PairwiseDistance()(input1,target)\n",
    "\n",
    "        grad_output1=(input1-target)/distance\n",
    "\n",
    "        \n",
    "        return grad_output1,None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2042\n",
       " 0.4508\n",
       " 0.6575\n",
       "   ⋮    \n",
       " 0.3220\n",
       " 0.1622\n",
       " 1.0774\n",
       "[torch.FloatTensor of size 9721x1]"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myloss=myLossfunction().apply(out,y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "myloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.3953e-01 -1.6375e-01\n",
      "-1.9727e-01 -3.8089e-01\n",
      "-4.1456e-01 -9.3787e-02\n",
      "           ⋮            \n",
      "-7.2860e-02 -6.4023e-02\n",
      "-1.8527e-01 -2.7595e-01\n",
      "-2.6488e-01 -8.1565e-01\n",
      "[torch.FloatTensor of size 9721x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myloss.backward(torch.Tensor([1, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.8320  0.5547\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "backward() missing 2 required positional arguments: 'self' and 'grad_output1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-634-fd86ffc8917c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyLossfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: backward() missing 2 required positional arguments: 'self' and 'grad_output1'"
     ]
    }
   ],
   "source": [
    "myLossfunction.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush all variables in GPU\n",
    "del my_conv_net,my_net,x_variable_test,x_variable,y_variable,y_variable_test,loss_func\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2dust=torch.nn.PairwiseDistance(p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4422\n",
       " 2.2472\n",
       "[torch.FloatTensor of size 2x1]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2dust(a_variable.resize(2,2),d_variable.resize(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.09185446,  0.06111541]],\n",
       "\n",
       "       [[ 0.09185446,  0.06111541]],\n",
       "\n",
       "       [[ 0.09185446,  0.06111541]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.29833109, -0.19828089]],\n",
       "\n",
       "       [[ 0.29833109, -0.19828089]],\n",
       "\n",
       "       [[ 0.29833109, -0.19828089]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0   ,.,.) = \n",
       "  5.4394e-01 -1.2854e-01  1.2081e+00  ...   8.7166e-01 -3.9516e-01  4.0000e-01\n",
       "\n",
       "(  1   ,.,.) = \n",
       "  8.0653e-01 -7.0666e-02  7.9998e-01  ...   1.0453e+00 -4.1684e-01  4.0000e-01\n",
       "\n",
       "(  2   ,.,.) = \n",
       "  5.0649e-01  6.8832e-02  9.6722e-01  ...   8.5345e-01 -5.3134e-01  3.0000e-01\n",
       "  ...  \n",
       "\n",
       "(174975,.,.) = \n",
       "  1.1163e+00 -3.1378e-02  7.0993e-01  ...   7.3510e-01 -6.3782e-01  5.0000e-01\n",
       "\n",
       "(174976,.,.) = \n",
       "  7.6960e-01 -4.1094e-02  7.8531e-01  ...   9.1110e-01 -3.1011e-01  9.0000e-01\n",
       "\n",
       "(174977,.,.) = \n",
       "  1.2754e+00 -3.7225e-01  7.9667e-01  ...   5.6758e-01 -2.6036e-01  5.0000e-01\n",
       "[torch.FloatTensor of size 174978x1x25]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Should I order the interior points ?\n",
    "\n",
    "For each interior point calculate distance with points of the polygon\n",
    "\n",
    "find which ti which point it is closer and sort it according to the index.\n",
    "\n",
    "If multiple points are closer to the same point of the contour then take into\n",
    "\n",
    "account the distance to the point.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[4860 x 2]' is invalid for input with 19442 elements at C:\\Anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\TH\\THStorage.c:41",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-768-8090ffc56ab5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput_reshaped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 2: size '[4860 x 2]' is invalid for input with 19442 elements at C:\\Anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\TH\\THStorage.c:41"
     ]
    }
   ],
   "source": [
    " output_reshaped=out.view(int(out.size()[0]/2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " -1.4947e-01 -2.7472e-01\n",
       " -1.9165e-01 -2.2482e-01\n",
       " -1.3642e-01 -3.2461e-01\n",
       "            ⋮            \n",
       " -3.8810e-02 -2.6062e-01\n",
       " -1.4880e-01 -2.9060e-01\n",
       " -1.3283e-01 -3.1349e-01\n",
       " [torch.FloatTensor of size 9721x2], Variable containing:\n",
       " -0.2311  0.0403\n",
       "  0.1704 -0.1201\n",
       "  0.2371 -0.1810\n",
       "        ⋮        \n",
       " -0.0681  0.2579\n",
       " -0.1204 -0.1273\n",
       " -0.0528  0.2407\n",
       " [torch.FloatTensor of size 9721x2])"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out,y_variable.narrow(0,b,batch_size).resize(batch_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (  0   ,.,.) = \n",
       "  -0.2311  0.0403\n",
       " \n",
       " (  1   ,.,.) = \n",
       "   0.1704 -0.1201\n",
       " \n",
       " (  2   ,.,.) = \n",
       "   0.2371 -0.1810\n",
       "   ...  \n",
       " \n",
       " (174975,.,.) = \n",
       "  -0.1804  0.0393\n",
       " \n",
       " (174976,.,.) = \n",
       "   0.2417  0.0188\n",
       " \n",
       " (174977,.,.) = \n",
       "   0.2496 -0.1190\n",
       " [torch.FloatTensor of size 174978x1x2], Variable containing:\n",
       " -1.4947e-01 -2.7472e-01\n",
       " -1.9165e-01 -2.2482e-01\n",
       " -1.3642e-01 -3.2461e-01\n",
       "            ⋮            \n",
       " -3.8810e-02 -2.6062e-01\n",
       " -1.4880e-01 -2.9060e-01\n",
       " -1.3283e-01 -3.1349e-01\n",
       " [torch.FloatTensor of size 9721x2])"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_variable,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0177044392397268e-16"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons[0][:,0].sum()/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-826-db58e474dc7f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-826-db58e474dc7f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for polygon\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_barycenters(polygons):\n",
    "    \n",
    "    for polygon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72589751,  0.84207899,  0.71337615,  0.16721335, -0.53593486,\n",
       "       -1.19396456, -0.99666214, -0.86514855, -0.06646145,  0.05744332,\n",
       "        0.52287723,  0.62928501])"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped[0][0::2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17159901,  0.40510667,  0.63783057,  0.58156785,  0.85478697,\n",
       "        0.50564378,  0.37817729, -0.73118862, -0.53079973, -1.1159677 ,\n",
       "       -0.86178872, -0.29496737])"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped[0][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.72589751,  0.17159901,  0.84207899, ..., -0.86178872,\n",
       "         0.62928501, -0.29496737],\n",
       "       [ 0.72589751,  0.17159901,  0.84207899, ..., -0.86178872,\n",
       "         0.62928501, -0.29496737],\n",
       "       [ 0.72589751,  0.17159901,  0.84207899, ..., -0.86178872,\n",
       "         0.62928501, -0.29496737],\n",
       "       ...,\n",
       "       [ 0.94168403, -0.21109   ,  0.66819129, ..., -0.98049631,\n",
       "         0.94108807, -0.3414368 ],\n",
       "       [ 0.94168403, -0.21109   ,  0.66819129, ..., -0.98049631,\n",
       "         0.94108807, -0.3414368 ],\n",
       "       [ 0.94168403, -0.21109   ,  0.66819129, ..., -0.98049631,\n",
       "         0.94108807, -0.3414368 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "barycenters=[np.array([i[0::2].sum()/12,i[1::2].sum()/12]) for i in polygons_reshaped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "barycenters=np.array(barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218722"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(point_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218722, 24)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=point_coordinates.reshape(len(point_coordinates),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_with_points=np.hstack([polygons_reshaped,point_coordinates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_with_points=polygons_with_points.reshape(len(polygons_with_points),13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.72589751,  0.17159901],\n",
       "        [ 0.84207899,  0.40510667],\n",
       "        [ 0.71337615,  0.63783057],\n",
       "        ...,\n",
       "        [ 0.52287723, -0.86178872],\n",
       "        [ 0.62928501, -0.29496737],\n",
       "        [ 0.09185446,  0.06111541]],\n",
       "\n",
       "       [[ 0.72589751,  0.17159901],\n",
       "        [ 0.84207899,  0.40510667],\n",
       "        [ 0.71337615,  0.63783057],\n",
       "        ...,\n",
       "        [ 0.52287723, -0.86178872],\n",
       "        [ 0.62928501, -0.29496737],\n",
       "        [ 0.09185446,  0.06111541]],\n",
       "\n",
       "       [[ 0.72589751,  0.17159901],\n",
       "        [ 0.84207899,  0.40510667],\n",
       "        [ 0.71337615,  0.63783057],\n",
       "        ...,\n",
       "        [ 0.52287723, -0.86178872],\n",
       "        [ 0.62928501, -0.29496737],\n",
       "        [ 0.09185446,  0.06111541]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.94168403, -0.21109   ],\n",
       "        [ 0.66819129,  0.35508194],\n",
       "        [ 0.41427257,  0.62051448],\n",
       "        ...,\n",
       "        [ 0.46430839, -0.98049631],\n",
       "        [ 0.94108807, -0.3414368 ],\n",
       "        [ 0.29833109, -0.19828089]],\n",
       "\n",
       "       [[ 0.94168403, -0.21109   ],\n",
       "        [ 0.66819129,  0.35508194],\n",
       "        [ 0.41427257,  0.62051448],\n",
       "        ...,\n",
       "        [ 0.46430839, -0.98049631],\n",
       "        [ 0.94108807, -0.3414368 ],\n",
       "        [ 0.29833109, -0.19828089]],\n",
       "\n",
       "       [[ 0.94168403, -0.21109   ],\n",
       "        [ 0.66819129,  0.35508194],\n",
       "        [ 0.41427257,  0.62051448],\n",
       "        ...,\n",
       "        [ 0.46430839, -0.98049631],\n",
       "        [ 0.94108807, -0.3414368 ],\n",
       "        [ 0.29833109, -0.19828089]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_with_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-cd78c0aa7e0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolygons_with_points_unique\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolygons_with_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "polygons_with_points_unique=list(set(polygons_with_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2, 12)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_of_mass=np.array(centers_of_mass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers_of_mass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0  ,.,.) = \n",
       "  1.0804e+00  3.6440e-01  6.1404e-01  ...   4.9729e-01 -1.0542e+00  8.4189e-01\n",
       " -1.1037e+00 -1.0409e-01 -5.6104e-01  ...  -1.1438e+00  7.8898e-01 -5.4535e-01\n",
       "\n",
       "(  1  ,.,.) = \n",
       "  8.2030e-01 -9.6631e-02  1.1965e+00  ...   7.3245e-01 -1.1246e+00  6.5850e-01\n",
       " -9.8128e-01 -1.1748e-01 -4.0551e-01  ...  -7.3543e-01  5.2845e-01 -3.5533e-01\n",
       "\n",
       "(  2  ,.,.) = \n",
       "  8.8570e-01 -9.5701e-02  1.2094e+00  ...   5.8753e-01 -7.4392e-01  6.8171e-01\n",
       " -1.2722e+00 -1.4492e-01 -1.0465e+00  ...  -5.5213e-01  6.8863e-01 -5.0008e-01\n",
       " ...  \n",
       "\n",
       "(47997,.,.) = \n",
       "  1.0355e+00 -1.4388e-01  8.2466e-01  ...   7.1425e-01 -8.3453e-01  3.6346e-01\n",
       " -7.5888e-01  6.7694e-02 -8.8216e-01  ...  -1.0865e+00  3.7439e-01 -3.2188e-01\n",
       "\n",
       "(47998,.,.) = \n",
       "  6.8633e-01  5.9241e-02  9.9623e-01  ...   1.1396e+00 -7.3713e-01  8.0244e-02\n",
       " -8.3097e-01 -2.3101e-01 -8.2566e-01  ...  -7.4034e-01  6.7384e-01 -5.5374e-01\n",
       "\n",
       "(47999,.,.) = \n",
       "  8.7795e-01  6.0293e-02  8.4823e-01  ...   7.8716e-01 -7.3494e-01  3.3091e-01\n",
       " -1.1220e+00  3.2642e-01 -9.8364e-01  ...  -7.1744e-01  1.1813e+00 -3.0074e-01\n",
       "[torch.cuda.FloatTensor of size 48000x2x12 (GPU 0)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour3=apply_procrustes(generate_contour(12))\n",
    "plot_contour(contour3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 -0.1397529684620565 -0.03668817911622362 0\n",
      "14 -0.5556838673885416 0.006489134426013973 0\n",
      "13 -0.1506273310219623 -0.02890481750650344 0\n",
      "14 -0.5584024580285181 0.008434974828444014 0\n",
      "13 -0.1355795824036162 -0.0391512743253334 0\n",
      "14 -0.5546405208739316 0.005873360623736529 0\n",
      "13 -0.1606318248853285 -0.1507826396315696 0\n",
      "13 -0.1029359385443222 -0.01988941712108101 0\n",
      "14 -0.546479609909108 0.01068882492479963 0\n",
      "13 -0.1469844471794247 -0.05733317507392088 0\n",
      "14 0.7059926839926042 0.1405732385547066 0\n",
      "15 -0.5574917370678837 0.001327885436589659 0\n",
      "13 -0.06406265062238639 -0.0485721250445643 0\n",
      "13 -0.1236015407066197 -0.04107046060343363 0\n",
      "14 -0.5516460104496824 0.00539356405421147 0\n",
      "13 -0.14404267783239 -0.03219865149111428 0\n",
      "14 -0.556756294731125 0.00761151633229131 0\n",
      "13 -0.1415910047281703 -0.02934662034552328 0\n",
      "14 -0.5561433764550701 0.008324524118689058 0\n",
      "13 -0.1393203257012531 -0.04295809812348142 0\n",
      "14 -0.5555757066983406 0.004921654674199518 0\n",
      "13 -0.05551841490555372 -0.04630404540602545 0\n",
      "13 -0.1152941486290197 -0.03570669163350242 0\n",
      "14 -0.5495691624302824 0.006734506296694273 0\n",
      "13 -0.1481990857572459 -0.04522022789034128 0\n",
      "14 0.6966692085386884 0.2002444115280739 0\n",
      "15 -0.5577953967123389 0.00435612223248456 0\n",
      "13 -0.07055101030846371 -0.06536714467005897 0\n",
      "14 0.7254559917690627 0.1782028488186872 0\n",
      "13 0.668636742504867 0.1730424300096725 0\n",
      "14 -0.3420547800645272 -0.1011080068174328 0\n",
      "13 -0.2207103626209955 -0.1180203033517528 0\n",
      "14 -0.5759232159282763 -0.01384389663286831 0\n",
      "13 -0.05923516989966464 -0.05308195828137971 0\n",
      "13 -0.1290742129902545 -0.01990870138843091 0\n",
      "14 -0.5530141785205911 0.01068400385796215 0\n",
      "13 -0.06150263613039254 -0.02419357870215745 0\n",
      "13 -0.1118374092753558 -0.02796508085981035 0\n",
      "14 -0.5487049775918664 0.008669908990117291 0\n",
      "13 -0.06851715015816352 -0.03510044619353291 0\n",
      "13 -0.05297605435286387 -0.03865576811700715 0\n",
      "13 -0.3399600547589283 -0.09338459917915261 0\n",
      "13 -0.1467402633727014 -0.02403623576471525 0\n",
      "14 -0.5574306911162028 0.00965212026389106 0\n",
      "13 -0.06130916081936404 -0.03530915509763472 0\n",
      "13 -0.131623347546258 -0.01892871825386482 0\n",
      "14 -0.5536514621595919 0.01092899964160368 0\n",
      "13 -0.1358796847893088 -0.02270373409710344 0\n",
      "14 -0.5547155464703547 0.009985245680794019 0\n",
      "13 -0.1267787163622686 -0.01536690253090524 0\n",
      "14 -0.5524403043635946 0.01181945357234357 0\n",
      "13 -0.131539460607141 -0.0267662067652194 0\n",
      "14 -0.5536304904248127 0.008969627513765028 0\n",
      "13 -0.1365378918460694 -0.04084007827572396 0\n",
      "14 -0.5548800982345449 0.005451159636138889 0\n",
      "13 -0.3301168705830743 -0.08407409346101147 0\n",
      "13 -0.05559670217067636 -0.03394052144826785 0\n",
      "13 -0.04427075026552559 -0.01020100824648889 0\n",
      "13 -0.05258371292902656 -0.07324115685308397 0\n",
      "14 0.7498154331000679 0.1680858372150571 0\n",
      "13 -0.327337823060125 -0.08280235421318367 0\n",
      "13 -0.2280980501718821 -0.12173847705694 0\n",
      "14 -0.577770137815998 -0.01477344005916512 0\n",
      "13 -0.1474131678571638 -0.04244606622435481 0\n",
      "14 -0.5575989172373185 0.005049662648981176 0\n",
      "13 -0.03956674944986851 -0.02710795433156328 0\n",
      "13 -0.0668955770514985 -0.04266069365978505 0\n",
      "13 -0.132949276301365 -0.0122946015815628 0\n",
      "14 -0.5539829443483685 0.01258752880967917 0\n",
      "13 -0.1403216370137341 -0.047837722399509 0\n",
      "14 -0.555826034526461 0.003701748605192623 0\n",
      "13 -0.07562338668806432 -0.05991482889543128 0\n",
      "14 0.7265490209758726 0.1622457298735304 0\n",
      "13 -0.1468261710666018 -0.03980637410123573 0\n",
      "14 -0.5574521680396779 0.005709585679760947 0\n",
      "13 -0.2197673007388687 -0.09477857564662642 0\n",
      "14 -0.5756874504577447 -0.008033464706586727 0\n",
      "13 -0.140856607160155 -0.1568805282005062 0\n",
      "13 -0.05612086084599307 -0.02274351477224481 0\n",
      "13 -0.1306790362512654 -0.02395026286339837 0\n",
      "14 -0.5534153843358438 0.009673613489220287 0\n",
      "13 -0.1466484840583014 -0.03078314879845298 0\n",
      "14 -0.5574077462876028 0.007965392005456633 0\n",
      "13 0.6781525585907586 0.1781686756011178 0\n",
      "14 -0.3398428873609275 -0.1112426127964096 0\n",
      "13 -0.1286765556329334 -0.03834700299175541 0\n",
      "14 -0.5529147641812608 0.006074428457131026 0\n",
      "13 -0.1425208477071837 -0.1480949726841848 0\n",
      "13 -0.2295247198802254 -0.122490114835207 0\n",
      "14 -0.5781268052430838 -0.01496134950373186 0\n",
      "15 0.6657881575066984 0.2005219709940744 0\n",
      "13 -0.1401382249124987 -0.04438696191526676 0\n",
      "14 -0.5557801815011522 0.004564438726253188 0\n",
      "13 -0.3331606528800971 -0.07990415180608197 0\n",
      "13 -0.1526067429149884 -0.04102509893687514 0\n",
      "14 0.6797516074813463 0.2115398151852108 0\n",
      "15 -0.5588973110017745 0.005404904470851093 0\n",
      "13 -0.1366886318782331 -0.01417230109403478 0\n",
      "14 -0.5549177832425858 0.01211810393156118 0\n",
      "13 -0.05981068413271194 -0.04720046188510983 0\n",
      "13 -0.2125650283954261 -0.1049708062633488 0\n",
      "14 -0.573886882371884 -0.01058152236076733 0\n",
      "13 -0.1500018396124621 -0.03656289705671551 0\n",
      "14 -0.5582460851761431 0.006520454940891001 0\n",
      "13 -0.1289985351318677 -0.02400084491377941 0\n",
      "14 -0.5529952590559945 0.009660967976625027 0\n",
      "13 -0.1394876835303045 -0.03746581650992508 0\n",
      "14 -0.5556175461556037 0.006294725077588608 0\n",
      "13 -0.118664770504445 -0.0127507651422367 0\n",
      "14 -0.5504118178991387 0.0124734879195107 0\n",
      "13 -0.1341437017418671 -0.01698123672938327 0\n",
      "14 -0.5542815507084941 0.01141587002272406 0\n",
      "13 -0.1478384116236053 -0.05920448952487276 0\n",
      "14 0.6838935371480942 0.1919000591336481 0\n",
      "15 -0.5577052281789288 0.0008600568238516897 0\n",
      "13 -0.1413124189310576 -0.03188163586906931 0\n",
      "14 -0.5560737300057919 0.007690770237802545 0\n",
      "13 -0.2242051176715144 -0.08375400982115525 0\n",
      "14 -0.5767969046909061 -0.005277323250218933 0\n",
      "13 -0.1515301104572981 -0.05879096046795171 0\n",
      "14 0.689944160006159 0.1719409864438256 0\n",
      "15 -0.558628152887352 0.0009634390880819514 0\n",
      "13 -0.1614356047742784 -0.1424951007962882 0\n",
      "13 -0.1524243529220074 -0.1699834645643355 0\n",
      "14 0.7252994759466482 0.2092065714876347 0\n",
      "13 -0.03151777023174163 -0.05092264937644676 0\n",
      "13 -0.189640727251055 -0.02570138845168707 0\n",
      "13 -0.07358253211827795 -0.06678878014543131 0\n",
      "14 0.7294955491776982 0.1763605516738522 0\n",
      "13 -0.1232421838936155 -0.01819063288795197 0\n",
      "14 -0.5515561712464313 0.01111352098308189 0\n",
      "13 -0.1408574837019793 -0.1242156432237974 0\n",
      "13 -0.1480153563676902 -0.1428787241247025 0\n",
      "13 -0.03666884231416678 -0.03799211108395292 0\n",
      "13 -0.2247757957830015 -0.1238216139338256 0\n",
      "14 -0.5769395742187778 -0.01529422427838652 0\n",
      "13 -0.2167594209378544 -0.1056574500741192 0\n",
      "14 -0.5749354805074911 -0.01075318331345993 0\n",
      "13 -0.1249749877229445 -0.03861279751962351 0\n",
      "14 -0.5519893722037637 0.006007979825164002 0\n",
      "13 -0.06750263578351787 -0.03580310179476102 0\n",
      "13 -0.1345067389077143 -0.1404675268215443 0\n",
      "13 -0.1264632831340571 -0.04099417656393815 0\n",
      "14 -0.5523614460565417 0.005412635064085342 0\n",
      "13 -0.1464945834086179 -0.05695040544565277 0\n",
      "14 0.6905684970705867 0.187069267302955 0\n",
      "15 -0.557369271125182 0.001423577843656685 0\n",
      "13 -0.1487693391512562 -0.1419271050077581 0\n",
      "13 -0.1234861307451766 -0.02284691760734195 0\n",
      "14 -0.5516171579593217 0.009949449803234391 0\n",
      "13 0.6848397681433996 0.1766069557976743 0\n",
      "14 -0.1742528694427543 -0.1793763288269057 0\n",
      "13 -0.1317130355441832 -0.01877169288191783 0\n",
      "14 -0.5536738841590733 0.01096825598459042 0\n",
      "13 -0.2259633919170091 -0.119115970462218 0\n",
      "14 -0.5772364732522796 -0.01411781341048462 0\n",
      "13 -0.04801941787163085 -0.05048921835696252 0\n",
      "13 -0.1305435506635666 -0.02483942972507727 0\n",
      "14 -0.5533815129389191 0.009451321773800561 0\n",
      "13 -0.05310040463287966 -0.04999466596033805 0\n",
      "13 -0.1270756280915445 -0.03395358713439253 0\n",
      "14 -0.5525145322959136 0.007172782421471739 0\n",
      "13 -0.1428323311334145 -0.1547412602474058 0\n",
      "13 -0.1336810424955752 -0.0412095563716969 0\n",
      "14 -0.5541658858969213 0.005358790112145654 0\n",
      "13 -0.2177818512972035 -0.09961526651157587 0\n",
      "14 -0.5751910880973283 -0.00924263742282409 0\n",
      "13 -0.1450776076669155 -0.1407926389674559 0\n",
      "13 -0.1309676650605889 -0.02008444381223736 0\n",
      "14 -0.5534875415381747 0.01064006825201053 0\n",
      "13 -0.2172699141104487 -0.1033083044113232 0\n",
      "14 -0.5750631038006396 -0.01016589689776093 0\n",
      "13 -0.07739642030705787 -0.04701983303141259 0\n",
      "14 0.7058805647781584 0.1897875196112926 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1bd48fd9630>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inserted_points=[]\n",
    "nb_of_sampling=100\n",
    "contour=contour3.copy()\n",
    "plot_contour(contour3)\n",
    "\n",
    "for i in range(0,nb_of_sampling):\n",
    "    contour[0]=np.random.normal(contour3[0],0.08)\n",
    "    contour[11]=np.random.normal(contour3[11],0.08)\n",
    "\n",
    "    nb_of_points,point_coords=get_extrapoints_target_length(contour,0.2)\n",
    "    inserted_points.append(point_coords)\n",
    "    plt.scatter(contour[0][0],contour[0][1])\n",
    "    plt.scatter(contour[11][0],contour[11][1])\n",
    "\n",
    "\n",
    "inserted_points=[i for i in inserted_points if len(i)==1]\n",
    "inserted_points=np.array(inserted_points)\n",
    "inserted_points=inserted_points.reshape(len(inserted_points),2)\n",
    "\n",
    "plt.scatter(inserted_points[:,0],inserted_points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(contour3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
