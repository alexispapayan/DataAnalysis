{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Triangulation import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from  Neural_network import *\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_nb_of_points(point_coordinates):\n",
    "    set_of_numbers=set()\n",
    "    for index,_ in enumerate(point_coordinates):\n",
    "        set_of_numbers.add(len(point_coordinates[index][0]))\n",
    "    return set_of_numbers\n",
    "\n",
    "\n",
    "def get_indices_nb_of_points(set_of_numbers,number_of_points,point_coordinates):\n",
    "    indices=[]\n",
    "    if number_of_points not in set_of_numbers:\n",
    "        return \"No such number of points for sample\"\n",
    "    else:\n",
    "        for index,_ in enumerate(point_coordinates):\n",
    "            if len(point_coordinates[index][0])==number_of_points:\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "    \n",
    "def get_polygons_nb_of_points(indices):\n",
    "    \n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons=load_dataset('12_polygons.pkl')\n",
    "polygons=[i for i in polygons for j in range(10)]\n",
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(2,12))\n",
    "\n",
    "polygons_reshaped=np.array(polygons_reshaped)\n",
    "#polygons_reshaped=polygons_reshaped.reshape(60000,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=load_dataset('12_point_coordinates')\n",
    "number_of_insertion_points=load_dataset('12_nb_of_points.pkl')\n",
    "centers_of_mass=load_dataset('12_centers_of_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_points=get_set_nb_of_points(point_coordinates)        \n",
    "indices=get_indices_nb_of_points(set_of_points,1,point_coordinates)\n",
    "indices=np.asarray(indices)\n",
    "number_of_insertion_points=np.array(number_of_insertion_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped.resize(len(point_coordinates),2*12)\n",
    "\n",
    "polygons_reshaped=np.hstack([polygons_reshaped[indices],number_of_insertion_points[indices,1].reshape(len(indices),1) ])\n",
    "#polygons_reshaped=polygons_reshaped[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=[ point_coordinates[i][0] for i in indices]\n",
    "point_coordinates=np.array(point_coordinates)\n",
    "#barycenters,point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(polygons_reshaped),len(indices)\n",
    "#point_coordinates.shape\n",
    "#centers_of_mass=centers_of_mass.reshape(60000,2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Project set of polygons in 2d and 3d space #\n",
    "# Dimensionality reduction using Isomap, PCA, kernel PCA ... #\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "\n",
    "Polygons_reshaped=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Isomap #\n",
    "#nb_components=8\n",
    "#iso=manifold.Isomap(n_neighbors=8,n_components=nb_components,n_jobs=-1)\n",
    "#iso.fit(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "\n",
    "                        # PCA   #\n",
    "pca=PCA(.999)\n",
    "pca.fit(polygons_reshaped)\n",
    "Polygons_projected=pca.transform(polygons_reshaped)\n",
    "nb_components=int(pca.n_components_)\n",
    "print(nb_components)\n",
    "# Fitting into lesser dimension\n",
    "\n",
    "#Polygons_projected=iso.fit_transform(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "#iso = KernelPCA(n_components=2,kernel=\"rbf\", fit_inverse_transform=True, gamma=1e-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(polygons_reshaped[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(polygons_reshaped[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n",
    "\n",
    "y_tensor=torch.from_numpy(point_coordinates[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(point_coordinates[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "shuffle=torch.randperm(x_variable.shape[0])\n",
    "x_variable = x_variable[shuffle]\n",
    "y_variable=y_variable[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 174978\n"
     ]
    }
   ],
   "source": [
    "my_net=Net(2*12+1,1*2,nb_of_hidden_layers=3, nb_of_hidden_nodes=20,batch_normalization=True)\n",
    "\n",
    "print(\"Training data length:\",x_variable.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=0)\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "#max_distance=0.6108970818704328\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=myOtherLossfunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if  torch.cuda.is_available():\n",
    "#    loss_func\n",
    "#    x_variable , y_variable=x_variable.cuda(), y_variable.cuda()\n",
    "#    x_variable_test,y_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "#    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 0.2615481900537139 Test Loss: 0.24979037303687363\n",
      "Epoch: 10 Training Loss: 0.16236317364718708 Test Loss: 0.15743294124979998\n",
      "Epoch: 20 Training Loss: 0.11650238527196098 Test Loss: 0.11463449362341122\n",
      "Epoch: 30 Training Loss: 0.09511936857776407 Test Loss: 0.09484516135427144\n",
      "Epoch: 40 Training Loss: 0.08506720853277805 Test Loss: 0.08586102187764322\n",
      "Epoch: 50 Training Loss: 0.07837054984233732 Test Loss: 0.0796115989657153\n",
      "Epoch: 60 Training Loss: 0.07266551239488411 Test Loss: 0.07401284564561711\n",
      "Epoch: 70 Training Loss: 0.06768034083548181 Test Loss: 0.06901260074595944\n",
      "Epoch: 80 Training Loss: 0.06362087123876824 Test Loss: 0.06473227042565838\n",
      "Epoch: 90 Training Loss: 0.06042328175609041 Test Loss: 0.06149087723102025\n",
      "Epoch: 100 Training Loss: 0.05788156853223155 Test Loss: 0.05915265477479768\n",
      "Epoch: 110 Training Loss: 0.05580364896542351 Test Loss: 0.057332829305876236\n",
      "Epoch: 120 Training Loss: 0.05405799023244395 Test Loss: 0.05584647590955331\n",
      "Epoch: 130 Training Loss: 0.05260681676265641 Test Loss: 0.054591917695141046\n",
      "Epoch: 140 Training Loss: 0.05135682336923472 Test Loss: 0.05353428939004492\n",
      "Epoch: 150 Training Loss: 0.05031623443989655 Test Loss: 0.052668361482431876\n",
      "Epoch: 160 Training Loss: 0.049475530258067266 Test Loss: 0.05188071439976054\n",
      "Epoch: 170 Training Loss: 0.04876493136927392 Test Loss: 0.05113860369600688\n",
      "Epoch: 180 Training Loss: 0.04813853618132306 Test Loss: 0.05054479458075679\n",
      "Epoch: 190 Training Loss: 0.047570554621676996 Test Loss: 0.05006585054878955\n",
      "Epoch: 200 Training Loss: 0.04708183264244291 Test Loss: 0.04967694676698518\n",
      "Epoch: 210 Training Loss: 0.04665896349402675 Test Loss: 0.04933320542888739\n",
      "Epoch: 220 Training Loss: 0.04627159266986421 Test Loss: 0.04900454428393037\n",
      "Epoch: 230 Training Loss: 0.04591613852549499 Test Loss: 0.04874103716366816\n",
      "Epoch: 240 Training Loss: 0.04558350567921842 Test Loss: 0.0485012665576279\n",
      "Epoch: 250 Training Loss: 0.04528771619235704 Test Loss: 0.04828534966938323\n",
      "Epoch: 260 Training Loss: 0.04502945287256871 Test Loss: 0.04805931420594824\n",
      "Epoch: 270 Training Loss: 0.04479257268179339 Test Loss: 0.04788857092365524\n",
      "Epoch: 280 Training Loss: 0.044569086857637996 Test Loss: 0.04772519472313346\n",
      "Epoch: 290 Training Loss: 0.04436096150245942 Test Loss: 0.047574018856514605\n",
      "Epoch: 300 Training Loss: 0.04416078636548684 Test Loss: 0.04744603255410742\n",
      "Epoch: 310 Training Loss: 0.04397587577304361 Test Loss: 0.047328929440680435\n",
      "Epoch: 320 Training Loss: 0.04379914561748031 Test Loss: 0.047212641171146326\n",
      "Epoch: 330 Training Loss: 0.04362643940528416 Test Loss: 0.04709235123674104\n",
      "Epoch: 340 Training Loss: 0.04345621013672969 Test Loss: 0.04699182963842184\n",
      "Epoch: 350 Training Loss: 0.043291525126830675 Test Loss: 0.04689844071472659\n",
      "Epoch: 360 Training Loss: 0.043128577047063146 Test Loss: 0.046799127429440326\n",
      "Epoch: 370 Training Loss: 0.04297807333306946 Test Loss: 0.046696164090003774\n",
      "Epoch: 380 Training Loss: 0.042838067416198396 Test Loss: 0.046605124818903734\n",
      "Epoch: 390 Training Loss: 0.04270822408410807 Test Loss: 0.04652345067144923\n",
      "Epoch: 400 Training Loss: 0.042586501715740244 Test Loss: 0.046424075993814866\n",
      "Epoch: 410 Training Loss: 0.04246701246850943 Test Loss: 0.04635098840340961\n",
      "Epoch: 420 Training Loss: 0.04234920870102465 Test Loss: 0.046295604133745484\n",
      "Epoch: 430 Training Loss: 0.042232782931829735 Test Loss: 0.04623453549076016\n",
      "Epoch: 440 Training Loss: 0.04211381812971443 Test Loss: 0.0461692531002466\n",
      "Epoch: 450 Training Loss: 0.04199908202605961 Test Loss: 0.04610891837487713\n",
      "Epoch: 460 Training Loss: 0.041892193588656056 Test Loss: 0.046046951171160616\n",
      "Epoch: 470 Training Loss: 0.04179349169241544 Test Loss: 0.04601537876087292\n",
      "Epoch: 480 Training Loss: 0.04169908302562724 Test Loss: 0.04598888238154804\n",
      "Epoch: 490 Training Loss: 0.041605369375062956 Test Loss: 0.04595785413070935\n",
      "Epoch: 500 Training Loss: 0.04150984693823563 Test Loss: 0.04592739794493245\n",
      "Epoch: 510 Training Loss: 0.04141780516358664 Test Loss: 0.045895619033110256\n",
      "Epoch: 520 Training Loss: 0.04132428161783642 Test Loss: 0.04585783762398129\n",
      "Epoch: 530 Training Loss: 0.0412366342293299 Test Loss: 0.04582195937764322\n",
      "Epoch: 540 Training Loss: 0.04115219961218115 Test Loss: 0.045794302124827116\n",
      "Epoch: 550 Training Loss: 0.04107676567314221 Test Loss: 0.04577127441316952\n",
      "Epoch: 560 Training Loss: 0.04100518283905691 Test Loss: 0.04574216327792811\n",
      "Epoch: 570 Training Loss: 0.04093694253594559 Test Loss: 0.0457166045271923\n",
      "Epoch: 580 Training Loss: 0.0408746441419265 Test Loss: 0.045690730443032186\n",
      "Epoch: 590 Training Loss: 0.040815747403222656 Test Loss: 0.04566421452977837\n",
      "Epoch: 600 Training Loss: 0.040756028330391894 Test Loss: 0.04564113379745651\n",
      "Epoch: 610 Training Loss: 0.04069737646064121 Test Loss: 0.045624421125968706\n",
      "Epoch: 620 Training Loss: 0.04064316170701415 Test Loss: 0.045609670219058616\n",
      "Epoch: 630 Training Loss: 0.0405908108526054 Test Loss: 0.04559374169528964\n",
      "Epoch: 640 Training Loss: 0.040535828703398606 Test Loss: 0.04557353803164434\n",
      "Epoch: 650 Training Loss: 0.04048349651065509 Test Loss: 0.04554094985705182\n",
      "Epoch: 660 Training Loss: 0.040431947584627155 Test Loss: 0.04550394422395786\n",
      "Epoch: 670 Training Loss: 0.04037575765942272 Test Loss: 0.04546907616080205\n",
      "Epoch: 680 Training Loss: 0.04031495498915176 Test Loss: 0.04544489873789834\n",
      "Epoch: 690 Training Loss: 0.04025868047602547 Test Loss: 0.045429846450370334\n",
      "Epoch: 700 Training Loss: 0.040204689313565044 Test Loss: 0.04542860744116336\n",
      "Epoch: 710 Training Loss: 0.04015462128659791 Test Loss: 0.04542798514599859\n",
      "Epoch: 720 Training Loss: 0.04010932872744181 Test Loss: 0.04543099616161645\n",
      "Epoch: 730 Training Loss: 0.04006612086814128 Test Loss: 0.045429997140679294\n",
      "Epoch: 740 Training Loss: 0.04002508142457775 Test Loss: 0.04543140637412416\n",
      "Epoch: 750 Training Loss: 0.039983667352444845 Test Loss: 0.045418243296580674\n",
      "Epoch: 760 Training Loss: 0.03994141019162275 Test Loss: 0.04541707126084434\n",
      "Epoch: 770 Training Loss: 0.039899960365832154 Test Loss: 0.04540677408973231\n",
      "Epoch: 780 Training Loss: 0.03985584331717773 Test Loss: 0.04540337797665823\n",
      "Epoch: 790 Training Loss: 0.03980938501437279 Test Loss: 0.04539548347880566\n",
      "Epoch: 800 Training Loss: 0.03976293037413766 Test Loss: 0.04537834664200376\n",
      "Epoch: 810 Training Loss: 0.039718305100319755 Test Loss: 0.04537212648091738\n",
      "Epoch: 820 Training Loss: 0.03967605700871815 Test Loss: 0.04535455710711755\n",
      "Epoch: 830 Training Loss: 0.03963352445752778 Test Loss: 0.04532782073952285\n",
      "Epoch: 840 Training Loss: 0.03959346082720712 Test Loss: 0.04530763102868393\n",
      "Epoch: 850 Training Loss: 0.03955292943439898 Test Loss: 0.04529583532616616\n",
      "Epoch: 860 Training Loss: 0.03951178517157559 Test Loss: 0.045269838457309856\n",
      "Epoch: 870 Training Loss: 0.03947098257419326 Test Loss: 0.04524734653341744\n",
      "Epoch: 880 Training Loss: 0.03943332699632016 Test Loss: 0.045237219586543295\n",
      "Epoch: 890 Training Loss: 0.03939624870921273 Test Loss: 0.04522827304708932\n",
      "Epoch: 900 Training Loss: 0.03936193792961874 Test Loss: 0.04521321517843876\n",
      "Epoch: 910 Training Loss: 0.039329610518782185 Test Loss: 0.04519325429362456\n",
      "Epoch: 920 Training Loss: 0.03929818479775181 Test Loss: 0.04517328224656524\n",
      "Epoch: 930 Training Loss: 0.03926645961803725 Test Loss: 0.04515927083839355\n",
      "Epoch: 940 Training Loss: 0.03923561519915852 Test Loss: 0.04513882718647843\n",
      "Epoch: 950 Training Loss: 0.039204286623432213 Test Loss: 0.04513387952133435\n",
      "Epoch: 960 Training Loss: 0.03917324582104831 Test Loss: 0.04512220381295149\n",
      "Epoch: 970 Training Loss: 0.03914317123946252 Test Loss: 0.04511703011234398\n",
      "Epoch: 980 Training Loss: 0.039115071306229834 Test Loss: 0.04510962954383744\n",
      "Epoch: 990 Training Loss: 0.03908528554443496 Test Loss: 0.04509055047638676\n",
      "Epoch: 1000 Training Loss: 0.03905417899020252 Test Loss: 0.04508117977161868\n",
      "Epoch: 1010 Training Loss: 0.03902306511083045 Test Loss: 0.0450689487415417\n",
      "Epoch: 1020 Training Loss: 0.03899324202570511 Test Loss: 0.04507028542039337\n",
      "Epoch: 1030 Training Loss: 0.03896605843255786 Test Loss: 0.04506493312386413\n",
      "Epoch: 1040 Training Loss: 0.038940524391090865 Test Loss: 0.04506745300069724\n",
      "Epoch: 1050 Training Loss: 0.038914614849014054 Test Loss: 0.04506651537210817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1060 Training Loss: 0.038888406545885414 Test Loss: 0.045064031772571664\n",
      "Epoch: 1070 Training Loss: 0.03886259066095095 Test Loss: 0.045051181237891195\n",
      "Epoch: 1080 Training Loss: 0.03883731212733038 Test Loss: 0.04505067893686134\n",
      "Epoch: 1090 Training Loss: 0.038813678785187905 Test Loss: 0.045050341278946825\n",
      "Epoch: 1100 Training Loss: 0.03879024583793659 Test Loss: 0.04504919435826199\n",
      "Epoch: 1110 Training Loss: 0.038767845735372355 Test Loss: 0.04503999666829308\n",
      "Epoch: 1120 Training Loss: 0.038746411504790065 Test Loss: 0.045034979239117076\n",
      "Epoch: 1130 Training Loss: 0.03872657694517534 Test Loss: 0.04503139894899872\n",
      "Epoch: 1140 Training Loss: 0.03870656396608833 Test Loss: 0.04502000787786611\n",
      "Epoch: 1150 Training Loss: 0.038687157055101215 Test Loss: 0.04499555977051853\n",
      "Epoch: 1160 Training Loss: 0.038669363767728566 Test Loss: 0.04497452451961269\n",
      "Epoch: 1170 Training Loss: 0.038651127483816716 Test Loss: 0.04495062894339795\n",
      "Epoch: 1180 Training Loss: 0.03863329512903274 Test Loss: 0.04493362884409862\n",
      "Epoch: 1190 Training Loss: 0.03861509111061685 Test Loss: 0.04492591573272906\n",
      "Epoch: 1200 Training Loss: 0.03859764158158231 Test Loss: 0.04492633710748188\n",
      "Epoch: 1210 Training Loss: 0.03857945029495672 Test Loss: 0.04491377679117422\n",
      "Epoch: 1220 Training Loss: 0.03856001757157284 Test Loss: 0.04490728873620525\n",
      "Epoch: 1230 Training Loss: 0.03853953595795793 Test Loss: 0.044898040816133356\n",
      "Epoch: 1240 Training Loss: 0.03851837223431749 Test Loss: 0.04489370986503149\n",
      "Epoch: 1250 Training Loss: 0.03849781492759313 Test Loss: 0.044888681273610384\n",
      "Epoch: 1260 Training Loss: 0.03847706769046278 Test Loss: 0.04488672509015522\n",
      "Epoch: 1270 Training Loss: 0.038456999424012414 Test Loss: 0.04487604840270951\n",
      "Epoch: 1280 Training Loss: 0.03843708899688014 Test Loss: 0.044864439667797296\n",
      "Epoch: 1290 Training Loss: 0.038418354952289985 Test Loss: 0.0448700263714738\n",
      "Epoch: 1300 Training Loss: 0.03839854550743969 Test Loss: 0.04486944872528947\n",
      "Epoch: 1310 Training Loss: 0.038381027436027226 Test Loss: 0.04486686745610827\n",
      "Epoch: 1320 Training Loss: 0.038364425007067854 Test Loss: 0.0448607672891568\n",
      "Epoch: 1330 Training Loss: 0.03834743608978924 Test Loss: 0.04486531311347699\n",
      "Epoch: 1340 Training Loss: 0.0383312979351556 Test Loss: 0.04485362624284902\n",
      "Epoch: 1350 Training Loss: 0.03831607298126184 Test Loss: 0.04485507733471305\n",
      "Epoch: 1360 Training Loss: 0.03830153571845237 Test Loss: 0.044854468992354665\n",
      "Epoch: 1370 Training Loss: 0.03828752150549373 Test Loss: 0.044853715540809885\n",
      "Epoch: 1380 Training Loss: 0.03827311958335923 Test Loss: 0.0448433876735238\n",
      "Epoch: 1390 Training Loss: 0.0382563994289416 Test Loss: 0.044842053785233406\n",
      "Epoch: 1400 Training Loss: 0.03823997262892514 Test Loss: 0.044855705211000366\n",
      "Epoch: 1410 Training Loss: 0.03822428531919459 Test Loss: 0.04485976547765837\n",
      "Epoch: 1420 Training Loss: 0.03820866044755894 Test Loss: 0.04485980454551624\n",
      "Epoch: 1430 Training Loss: 0.03819270297970272 Test Loss: 0.04485894784320421\n",
      "Epoch: 1440 Training Loss: 0.03817703032025142 Test Loss: 0.04485761116435254\n",
      "Epoch: 1450 Training Loss: 0.03816277856347923 Test Loss: 0.044859321778415326\n",
      "Epoch: 1460 Training Loss: 0.03814942483394341 Test Loss: 0.044858261365130075\n",
      "Epoch: 1470 Training Loss: 0.038136110695043184 Test Loss: 0.04485457503368319\n",
      "Epoch: 1480 Training Loss: 0.0381229843936519 Test Loss: 0.044856824226072435\n",
      "Epoch: 1490 Training Loss: 0.03811055886395138 Test Loss: 0.04485421226071718\n",
      "Epoch: 1500 Training Loss: 0.0380980398515166 Test Loss: 0.0448492171560314\n",
      "Epoch: 1510 Training Loss: 0.03808611417539142 Test Loss: 0.04484498108401295\n",
      "Epoch: 1520 Training Loss: 0.0380743189565148 Test Loss: 0.04484625637051653\n",
      "Epoch: 1530 Training Loss: 0.03806238142063976 Test Loss: 0.04484430018706137\n",
      "Epoch: 1540 Training Loss: 0.038051075765261386 Test Loss: 0.04483543457388442\n",
      "Epoch: 1550 Training Loss: 0.03804046861426865 Test Loss: 0.0448410240681222\n",
      "Epoch: 1560 Training Loss: 0.03803022022070944 Test Loss: 0.04483698333539314\n",
      "Epoch: 1570 Training Loss: 0.03801918001443842 Test Loss: 0.04482545273619668\n",
      "Epoch: 1580 Training Loss: 0.038008002374595395 Test Loss: 0.04482360259407005\n",
      "Epoch: 1590 Training Loss: 0.03799587386186585 Test Loss: 0.0448160931936737\n",
      "Epoch: 1600 Training Loss: 0.03798419915897686 Test Loss: 0.04482329842289085\n",
      "Epoch: 1610 Training Loss: 0.03797277682458875 Test Loss: 0.04482402675938414\n",
      "Epoch: 1620 Training Loss: 0.03796032513367529 Test Loss: 0.04481909025648517\n",
      "Epoch: 1630 Training Loss: 0.03794797983169447 Test Loss: 0.04483047295593395\n",
      "Epoch: 1640 Training Loss: 0.037935771265653305 Test Loss: 0.04483425974758681\n",
      "Epoch: 1650 Training Loss: 0.037923804952444025 Test Loss: 0.0448445513375763\n",
      "Epoch: 1660 Training Loss: 0.037913014672960675 Test Loss: 0.044842123549265325\n",
      "Epoch: 1670 Training Loss: 0.03790249455160301 Test Loss: 0.04483266912765894\n",
      "Epoch: 1680 Training Loss: 0.03789277077756739 Test Loss: 0.04481981859297846\n",
      "Epoch: 1690 Training Loss: 0.03788377306439506 Test Loss: 0.04481073252546063\n",
      "Epoch: 1700 Training Loss: 0.03787476889812353 Test Loss: 0.04480852240092927\n",
      "Epoch: 1710 Training Loss: 0.037866255516206856 Test Loss: 0.04480748710269551\n",
      "Epoch: 1720 Training Loss: 0.03785788288161583 Test Loss: 0.04480643785165537\n",
      "Epoch: 1730 Training Loss: 0.0378495904747445 Test Loss: 0.044805427668473105\n",
      "Epoch: 1740 Training Loss: 0.037842000060420544 Test Loss: 0.04480200085922498\n",
      "Epoch: 1750 Training Loss: 0.03783436900901247 Test Loss: 0.044800750687772894\n",
      "Epoch: 1760 Training Loss: 0.03782676394440926 Test Loss: 0.044801654829626635\n",
      "Epoch: 1770 Training Loss: 0.037819254804253515 Test Loss: 0.04479929122422504\n",
      "Epoch: 1780 Training Loss: 0.03781196123249244 Test Loss: 0.044795775117016046\n",
      "Epoch: 1790 Training Loss: 0.037804703239980975 Test Loss: 0.04478387337316975\n",
      "Epoch: 1800 Training Loss: 0.03779773162554721 Test Loss: 0.044767411852196876\n",
      "Epoch: 1810 Training Loss: 0.03779026992439094 Test Loss: 0.044754460857310434\n",
      "Epoch: 1820 Training Loss: 0.03778297024834685 Test Loss: 0.044746005456641195\n",
      "Epoch: 1830 Training Loss: 0.03777614722960262 Test Loss: 0.04473616035645603\n",
      "Epoch: 1840 Training Loss: 0.03776905562240548 Test Loss: 0.04472485021160045\n",
      "Epoch: 1850 Training Loss: 0.03776170780972956 Test Loss: 0.04472638781086406\n",
      "Epoch: 1860 Training Loss: 0.03775438633267468 Test Loss: 0.04472356376285176\n",
      "Epoch: 1870 Training Loss: 0.03774681946344237 Test Loss: 0.04472505950369622\n",
      "Epoch: 1880 Training Loss: 0.03773922870030223 Test Loss: 0.044727208235879494\n",
      "Epoch: 1890 Training Loss: 0.037731914722795065 Test Loss: 0.044736899855194426\n",
      "Epoch: 1900 Training Loss: 0.037724577897828585 Test Loss: 0.04474645473700679\n",
      "Epoch: 1910 Training Loss: 0.03771739280789719 Test Loss: 0.04475830066962755\n",
      "Epoch: 1920 Training Loss: 0.03771004621607788 Test Loss: 0.04476064195053893\n",
      "Epoch: 1930 Training Loss: 0.03770212861218401 Test Loss: 0.04476401015800024\n",
      "Epoch: 1940 Training Loss: 0.037693928816006514 Test Loss: 0.04477090563491565\n",
      "Epoch: 1950 Training Loss: 0.03768622259471325 Test Loss: 0.044767261161887915\n",
      "Epoch: 1960 Training Loss: 0.0376791756359862 Test Loss: 0.04477141351706806\n",
      "Epoch: 1970 Training Loss: 0.037672207160897996 Test Loss: 0.04477635839165085\n",
      "Epoch: 1980 Training Loss: 0.03766545460302063 Test Loss: 0.04478554212881338\n",
      "Epoch: 1990 Training Loss: 0.0376589791795924 Test Loss: 0.04479146090928184\n",
      "Epoch: 2000 Training Loss: 0.03765254369561593 Test Loss: 0.04479426821392648\n",
      "Epoch: 2010 Training Loss: 0.037646244773171036 Test Loss: 0.04479590906395734\n",
      "Epoch: 2020 Training Loss: 0.03764056447620826 Test Loss: 0.04480176924263899\n",
      "Epoch: 2030 Training Loss: 0.03763523421627469 Test Loss: 0.04479392776545069\n",
      "Epoch: 2040 Training Loss: 0.037630263585815074 Test Loss: 0.04481030836014654\n",
      "Epoch: 2050 Training Loss: 0.03762543038892747 Test Loss: 0.04480979489687157\n",
      "Epoch: 2060 Training Loss: 0.03762087118714344 Test Loss: 0.04481398911047087\n",
      "Epoch: 2070 Training Loss: 0.037615858001110766 Test Loss: 0.04480928701471916\n",
      "Epoch: 2080 Training Loss: 0.037610986783260345 Test Loss: 0.044812217104059984\n",
      "Epoch: 2090 Training Loss: 0.037606036907363 Test Loss: 0.04480530209321564\n",
      "Epoch: 2100 Training Loss: 0.037600730192521074 Test Loss: 0.04480071720103757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2110 Training Loss: 0.0375951873291303 Test Loss: 0.04479989677602214\n",
      "Epoch: 2120 Training Loss: 0.037589138333473084 Test Loss: 0.044805871367716146\n",
      "Epoch: 2130 Training Loss: 0.03758348506976368 Test Loss: 0.044816009476835394\n",
      "Epoch: 2140 Training Loss: 0.03757739334412531 Test Loss: 0.04481394446149043\n",
      "Epoch: 2150 Training Loss: 0.0375712526098147 Test Loss: 0.04481123761705177\n",
      "Epoch: 2160 Training Loss: 0.03756530163150198 Test Loss: 0.04481233151707234\n",
      "Epoch: 2170 Training Loss: 0.03755925228702858 Test Loss: 0.04480565649449782\n",
      "Epoch: 2180 Training Loss: 0.0375535020524232 Test Loss: 0.04479997491173789\n",
      "Epoch: 2190 Training Loss: 0.03754802685936991 Test Loss: 0.04479085535748474\n",
      "Epoch: 2200 Training Loss: 0.03754213971441678 Test Loss: 0.04478761830640345\n",
      "Epoch: 2210 Training Loss: 0.037536175306681414 Test Loss: 0.04478853919162485\n",
      "Epoch: 2220 Training Loss: 0.03752975883318635 Test Loss: 0.0447933082608472\n",
      "Epoch: 2230 Training Loss: 0.03752262501923233 Test Loss: 0.044789440542917315\n",
      "Epoch: 2240 Training Loss: 0.03751548510099528 Test Loss: 0.04478515703135716\n",
      "Epoch: 2250 Training Loss: 0.037508514881826216 Test Loss: 0.0447881568847299\n",
      "Epoch: 2260 Training Loss: 0.0375010753304969 Test Loss: 0.04479116231922521\n",
      "Epoch: 2270 Training Loss: 0.03749353601774223 Test Loss: 0.044806443432777926\n",
      "Epoch: 2280 Training Loss: 0.03748620407620217 Test Loss: 0.044812685918354515\n",
      "Epoch: 2290 Training Loss: 0.037478740630965034 Test Loss: 0.04482779680766934\n",
      "Epoch: 2300 Training Loss: 0.037470993598179576 Test Loss: 0.04483808281653627\n",
      "Epoch: 2310 Training Loss: 0.03746349945711925 Test Loss: 0.04484459319599545\n",
      "Epoch: 2320 Training Loss: 0.03745569870664321 Test Loss: 0.04485302627217447\n",
      "Epoch: 2330 Training Loss: 0.03744790214196124 Test Loss: 0.044865846110680896\n",
      "Epoch: 2340 Training Loss: 0.03744066682250099 Test Loss: 0.04486794740332246\n",
      "Epoch: 2350 Training Loss: 0.03743316727478999 Test Loss: 0.04488431404521191\n",
      "Epoch: 2360 Training Loss: 0.037426221821569176 Test Loss: 0.04488990074888842\n",
      "Epoch: 2370 Training Loss: 0.03742019427810657 Test Loss: 0.04488885428840955\n",
      "Epoch: 2380 Training Loss: 0.037414042381678426 Test Loss: 0.0448909332565609\n",
      "Epoch: 2390 Training Loss: 0.03740829860017224 Test Loss: 0.04489259922164325\n",
      "Epoch: 2400 Training Loss: 0.037402868404405216 Test Loss: 0.04488608884218407\n",
      "Epoch: 2410 Training Loss: 0.03739690417107794 Test Loss: 0.0448880003766588\n",
      "Epoch: 2420 Training Loss: 0.037391024525672514 Test Loss: 0.0448914299764682\n",
      "Epoch: 2430 Training Loss: 0.03738495181051556 Test Loss: 0.044890944418806006\n",
      "Epoch: 2440 Training Loss: 0.037378867758832986 Test Loss: 0.04489859055670492\n",
      "Epoch: 2450 Training Loss: 0.03737326577110096 Test Loss: 0.04490671667114347\n",
      "Epoch: 2460 Training Loss: 0.037367970043968475 Test Loss: 0.04490141460471722\n",
      "Epoch: 2470 Training Loss: 0.03736234625522566 Test Loss: 0.04490502000988707\n",
      "Epoch: 2480 Training Loss: 0.037356483701812694 Test Loss: 0.04490634831705491\n",
      "Epoch: 2490 Training Loss: 0.03735142272796437 Test Loss: 0.04490853611709606\n",
      "Epoch: 2500 Training Loss: 0.03734658185712097 Test Loss: 0.04490026768403238\n",
      "Epoch: 2510 Training Loss: 0.03734163145799936 Test Loss: 0.044899229595237344\n",
      "Epoch: 2520 Training Loss: 0.03733684290958185 Test Loss: 0.04490158203839384\n",
      "Epoch: 2530 Training Loss: 0.03733130562724984 Test Loss: 0.04490590740837315\n",
      "Epoch: 2540 Training Loss: 0.03732687025520708 Test Loss: 0.04490706828186437\n",
      "Epoch: 2550 Training Loss: 0.0373218152112337 Test Loss: 0.04490448701268317\n",
      "Epoch: 2560 Training Loss: 0.037316551575189094 Test Loss: 0.04491862120555105\n",
      "Epoch: 2570 Training Loss: 0.03731190726225895 Test Loss: 0.044921403395144194\n",
      "Epoch: 2580 Training Loss: 0.03730768135432785 Test Loss: 0.044927107302394326\n",
      "Epoch: 2590 Training Loss: 0.03730300512471791 Test Loss: 0.04492717985698753\n",
      "Epoch: 2600 Training Loss: 0.037298556148844426 Test Loss: 0.044921916858419154\n",
      "Epoch: 2610 Training Loss: 0.03729448598733439 Test Loss: 0.04492889884273415\n",
      "Epoch: 2620 Training Loss: 0.03729105398501213 Test Loss: 0.04492999274275472\n",
      "Epoch: 2630 Training Loss: 0.037287694710861866 Test Loss: 0.0449244199918846\n",
      "Epoch: 2640 Training Loss: 0.037283941100028466 Test Loss: 0.04492671383325428\n",
      "Epoch: 2650 Training Loss: 0.037280285332131485 Test Loss: 0.044932674472141894\n",
      "Epoch: 2660 Training Loss: 0.03727675130107874 Test Loss: 0.044937292851055285\n",
      "Epoch: 2670 Training Loss: 0.037273810955151775 Test Loss: 0.044944922245586535\n",
      "Epoch: 2680 Training Loss: 0.03727052022337942 Test Loss: 0.04494082291107066\n",
      "Epoch: 2690 Training Loss: 0.03726671690624143 Test Loss: 0.04494527664686871\n",
      "Epoch: 2700 Training Loss: 0.03726420542979868 Test Loss: 0.044945072935895496\n",
      "Epoch: 2710 Training Loss: 0.037261006262271634 Test Loss: 0.04495253489675013\n",
      "Epoch: 2720 Training Loss: 0.03725778354965294 Test Loss: 0.04495322416538554\n",
      "Epoch: 2730 Training Loss: 0.037253894772552656 Test Loss: 0.04495848995451519\n",
      "Epoch: 2740 Training Loss: 0.03725085954862674 Test Loss: 0.04496110750099299\n",
      "Epoch: 2750 Training Loss: 0.037247399641010676 Test Loss: 0.04496474360233689\n",
      "Epoch: 2760 Training Loss: 0.037244132279921884 Test Loss: 0.04497024379861381\n",
      "Epoch: 2770 Training Loss: 0.03724113751867199 Test Loss: 0.0449776136709463\n",
      "Epoch: 2780 Training Loss: 0.037238260657288434 Test Loss: 0.04497968984853637\n",
      "Epoch: 2790 Training Loss: 0.0372359119035902 Test Loss: 0.044984285902959546\n",
      "Epoch: 2800 Training Loss: 0.037233363975857414 Test Loss: 0.0449858067588555\n",
      "Epoch: 2810 Training Loss: 0.03723107312564383 Test Loss: 0.04498109071029741\n",
      "Epoch: 2820 Training Loss: 0.03722904162025458 Test Loss: 0.04497817457376297\n",
      "Epoch: 2830 Training Loss: 0.03722670559834665 Test Loss: 0.04497605374719247\n",
      "Epoch: 2840 Training Loss: 0.03722491372966798 Test Loss: 0.04497908429673927\n",
      "Epoch: 2850 Training Loss: 0.03722298878761946 Test Loss: 0.04498424962566295\n",
      "Epoch: 2860 Training Loss: 0.03722108180960383 Test Loss: 0.04499295617684711\n",
      "Epoch: 2870 Training Loss: 0.037219752122353865 Test Loss: 0.044992945014602\n",
      "Epoch: 2880 Training Loss: 0.03721831953433297 Test Loss: 0.044988508022171615\n",
      "Epoch: 2890 Training Loss: 0.03721687421452179 Test Loss: 0.044985340735122246\n",
      "Epoch: 2900 Training Loss: 0.03721575730513711 Test Loss: 0.044991583220698836\n",
      "Epoch: 2910 Training Loss: 0.03721446354595292 Test Loss: 0.04499738758815495\n",
      "Epoch: 2920 Training Loss: 0.03721367958160499 Test Loss: 0.04499714759988513\n",
      "Epoch: 2930 Training Loss: 0.0372122114143345 Test Loss: 0.04500240501733095\n",
      "Epoch: 2940 Training Loss: 0.03721072685270389 Test Loss: 0.04500659085924641\n",
      "Epoch: 2950 Training Loss: 0.03720920060754066 Test Loss: 0.04501205477822673\n",
      "Epoch: 2960 Training Loss: 0.037207667909278236 Test Loss: 0.045015877847176186\n",
      "Epoch: 2970 Training Loss: 0.03720660314791136 Test Loss: 0.045009696753947684\n",
      "Epoch: 2980 Training Loss: 0.037204990745153495 Test Loss: 0.04501444907980237\n",
      "Epoch: 2990 Training Loss: 0.0372034144448695 Test Loss: 0.04501565739283531\n",
      "Epoch: 3000 Training Loss: 0.03720199650712786 Test Loss: 0.045029945066573415\n",
      "Epoch: 3010 Training Loss: 0.03720056444233123 Test Loss: 0.04501526392369525\n",
      "Epoch: 3020 Training Loss: 0.037199148771894716 Test Loss: 0.04502336492308231\n",
      "Epoch: 3030 Training Loss: 0.037197874895232366 Test Loss: 0.04502414628023986\n",
      "Epoch: 3040 Training Loss: 0.03719721127246398 Test Loss: 0.04502563643996177\n",
      "Epoch: 3050 Training Loss: 0.03719560898537513 Test Loss: 0.045026172227726945\n",
      "Epoch: 3060 Training Loss: 0.037194470972612005 Test Loss: 0.04503237564544566\n",
      "Epoch: 3070 Training Loss: 0.03719274973920829 Test Loss: 0.04503247331509035\n",
      "Epoch: 3080 Training Loss: 0.03719123099359277 Test Loss: 0.04503710006568758\n",
      "Epoch: 3090 Training Loss: 0.03718989101626571 Test Loss: 0.04503424811206251\n",
      "Epoch: 3100 Training Loss: 0.037188652893261044 Test Loss: 0.045039011600162306\n",
      "Epoch: 3110 Training Loss: 0.0371874909865901 Test Loss: 0.04503390208246417\n",
      "Epoch: 3120 Training Loss: 0.03718643250391433 Test Loss: 0.045040351069575255\n",
      "Epoch: 3130 Training Loss: 0.03718535745247036 Test Loss: 0.0450381883845856\n",
      "Epoch: 3140 Training Loss: 0.03718447581959411 Test Loss: 0.045050472435326845\n",
      "Epoch: 3150 Training Loss: 0.03718314438826328 Test Loss: 0.045051220305749075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3160 Training Loss: 0.037181785923679066 Test Loss: 0.045048072552628646\n",
      "Epoch: 3170 Training Loss: 0.03718050664036604 Test Loss: 0.04504664657581611\n",
      "Epoch: 3180 Training Loss: 0.03717921968309722 Test Loss: 0.04504545221558957\n",
      "Epoch: 3190 Training Loss: 0.0371779606311222 Test Loss: 0.04504090918183065\n",
      "Epoch: 3200 Training Loss: 0.03717717387624489 Test Loss: 0.04505180911417852\n",
      "Epoch: 3210 Training Loss: 0.03717522172653491 Test Loss: 0.04505262395807139\n",
      "Epoch: 3220 Training Loss: 0.037173545664825544 Test Loss: 0.04506476569018751\n",
      "Epoch: 3230 Training Loss: 0.03717224004589148 Test Loss: 0.045059954762546005\n",
      "Epoch: 3240 Training Loss: 0.037170917858189226 Test Loss: 0.04506487452207732\n",
      "Epoch: 3250 Training Loss: 0.03716991727899811 Test Loss: 0.0450686334081174\n",
      "Epoch: 3260 Training Loss: 0.0371690070431957 Test Loss: 0.045065639135867204\n",
      "Epoch: 3270 Training Loss: 0.037167576548071846 Test Loss: 0.045073756878621925\n",
      "Epoch: 3280 Training Loss: 0.03716729034440223 Test Loss: 0.04507552888503281\n",
      "Epoch: 3290 Training Loss: 0.03716607175510323 Test Loss: 0.045076014442694996\n",
      "Epoch: 3300 Training Loss: 0.03716512367274609 Test Loss: 0.04508232390174224\n",
      "Epoch: 3310 Training Loss: 0.037164167393208894 Test Loss: 0.04508359639768454\n",
      "Epoch: 3320 Training Loss: 0.037163315060891135 Test Loss: 0.0450942786662528\n",
      "Epoch: 3330 Training Loss: 0.037162402383375516 Test Loss: 0.04509146578048561\n",
      "Epoch: 3340 Training Loss: 0.03716179317593006 Test Loss: 0.04509111416976471\n",
      "Epoch: 3350 Training Loss: 0.03716084526798101 Test Loss: 0.0451035991409179\n",
      "Epoch: 3360 Training Loss: 0.03716025036199863 Test Loss: 0.04509347498460503\n",
      "Epoch: 3370 Training Loss: 0.03715970865048258 Test Loss: 0.045106283660866345\n",
      "Epoch: 3380 Training Loss: 0.03715852145463912 Test Loss: 0.04510386424423921\n",
      "Epoch: 3390 Training Loss: 0.03715733164267437 Test Loss: 0.04510788544303933\n",
      "Epoch: 3400 Training Loss: 0.03715676743251518 Test Loss: 0.04510638133051104\n",
      "Epoch: 3410 Training Loss: 0.037155741564151544 Test Loss: 0.045106922699398776\n",
      "Epoch: 3420 Training Loss: 0.037155336065350895 Test Loss: 0.04510689758434728\n",
      "Epoch: 3430 Training Loss: 0.037154255432848164 Test Loss: 0.04511792867307517\n",
      "Epoch: 3440 Training Loss: 0.03715320357767968 Test Loss: 0.04511689616540268\n",
      "Epoch: 3450 Training Loss: 0.03715185627521299 Test Loss: 0.0451259236311337\n",
      "Epoch: 3460 Training Loss: 0.037150653382641764 Test Loss: 0.04512669103548487\n",
      "Epoch: 3470 Training Loss: 0.0371492778260651 Test Loss: 0.045122856804290304\n",
      "Epoch: 3480 Training Loss: 0.037147891805003254 Test Loss: 0.045130343880196426\n",
      "Epoch: 3490 Training Loss: 0.03714642468418128 Test Loss: 0.045135475722384785\n",
      "Epoch: 3500 Training Loss: 0.03714465548855383 Test Loss: 0.04514152565923327\n",
      "Epoch: 3510 Training Loss: 0.03714362107419397 Test Loss: 0.045141165676828535\n",
      "Epoch: 3520 Training Loss: 0.03714180740450451 Test Loss: 0.04514033688012928\n",
      "Epoch: 3530 Training Loss: 0.03714054312028691 Test Loss: 0.045147249100412346\n",
      "Epoch: 3540 Training Loss: 0.03713750964044185 Test Loss: 0.04515985406570044\n",
      "Epoch: 3550 Training Loss: 0.037135925840610144 Test Loss: 0.04517110002764665\n",
      "Epoch: 3560 Training Loss: 0.03713369446355399 Test Loss: 0.0451675057847219\n",
      "Epoch: 3570 Training Loss: 0.03713083382212246 Test Loss: 0.045170460989114224\n",
      "Epoch: 3580 Training Loss: 0.03712903759324163 Test Loss: 0.045177225309649606\n",
      "Epoch: 3590 Training Loss: 0.037126979577823266 Test Loss: 0.045182401800818395\n",
      "Epoch: 3600 Training Loss: 0.03712549030717433 Test Loss: 0.04519130369129195\n",
      "Epoch: 3610 Training Loss: 0.03712375616757223 Test Loss: 0.045192659904072556\n",
      "Epoch: 3620 Training Loss: 0.03712190936034637 Test Loss: 0.04519980932206417\n",
      "Epoch: 3630 Training Loss: 0.03711886698568891 Test Loss: 0.04520605459820204\n",
      "Epoch: 3640 Training Loss: 0.037115441785282026 Test Loss: 0.04519756571079748\n",
      "Epoch: 3650 Training Loss: 0.03711146580413859 Test Loss: 0.04519266269463384\n",
      "Epoch: 3660 Training Loss: 0.03710781230354673 Test Loss: 0.04520198875042149\n",
      "Epoch: 3670 Training Loss: 0.037104472911918304 Test Loss: 0.04520216176522066\n",
      "Epoch: 3680 Training Loss: 0.03710209625292626 Test Loss: 0.04520935862275398\n",
      "Epoch: 3690 Training Loss: 0.037099837668208645 Test Loss: 0.045216530365235805\n",
      "Epoch: 3700 Training Loss: 0.0370976966345412 Test Loss: 0.04522801910601311\n",
      "Epoch: 3710 Training Loss: 0.03709561821337674 Test Loss: 0.04523023202110575\n",
      "Epoch: 3720 Training Loss: 0.037093089121717264 Test Loss: 0.04523372022270197\n",
      "Epoch: 3730 Training Loss: 0.03709152014657289 Test Loss: 0.04523232494206348\n",
      "Epoch: 3740 Training Loss: 0.037089202960738275 Test Loss: 0.04524134682667194\n",
      "Epoch: 3750 Training Loss: 0.037087315690836405 Test Loss: 0.04524232910424144\n",
      "Epoch: 3760 Training Loss: 0.03708523622322342 Test Loss: 0.04524245188893763\n",
      "Epoch: 3770 Training Loss: 0.0370836831192149 Test Loss: 0.04524644518212498\n",
      "Epoch: 3780 Training Loss: 0.03708146831092694 Test Loss: 0.045245019205312445\n",
      "Epoch: 3790 Training Loss: 0.03707984090907366 Test Loss: 0.0452505389355183\n",
      "Epoch: 3800 Training Loss: 0.037078018518979884 Test Loss: 0.045259393386450145\n",
      "Epoch: 3810 Training Loss: 0.037076122005449436 Test Loss: 0.04526390572403501\n",
      "Epoch: 3820 Training Loss: 0.037074244327992306 Test Loss: 0.045274495904081134\n",
      "Epoch: 3830 Training Loss: 0.037072493968438175 Test Loss: 0.045278586866913176\n",
      "Epoch: 3840 Training Loss: 0.03707081180244579 Test Loss: 0.04528150579400889\n",
      "Epoch: 3850 Training Loss: 0.03706909109226633 Test Loss: 0.04528921890537845\n",
      "Epoch: 3860 Training Loss: 0.03706789308312152 Test Loss: 0.045292492233756344\n",
      "Epoch: 3870 Training Loss: 0.037066645193264025 Test Loss: 0.04528626370098614\n",
      "Epoch: 3880 Training Loss: 0.03706522934841942 Test Loss: 0.0452946911960426\n",
      "Epoch: 3890 Training Loss: 0.03706392250862876 Test Loss: 0.04530203037220105\n",
      "Epoch: 3900 Training Loss: 0.037062867165298545 Test Loss: 0.045294989786099235\n",
      "Epoch: 3910 Training Loss: 0.03706142533364909 Test Loss: 0.04529883517953891\n",
      "Epoch: 3920 Training Loss: 0.037060039661403414 Test Loss: 0.04529010909442581\n",
      "Epoch: 3930 Training Loss: 0.03705879613174808 Test Loss: 0.04529224387380269\n",
      "Epoch: 3940 Training Loss: 0.03705732988296654 Test Loss: 0.045287100869369226\n",
      "Epoch: 3950 Training Loss: 0.037055774860469064 Test Loss: 0.04529168297098602\n",
      "Epoch: 3960 Training Loss: 0.03705434715587459 Test Loss: 0.045300967168354515\n",
      "Epoch: 3970 Training Loss: 0.03705234495104385 Test Loss: 0.045307393830975394\n",
      "Epoch: 3980 Training Loss: 0.0370503821624406 Test Loss: 0.04531304192699999\n",
      "Epoch: 3990 Training Loss: 0.03704858575915169 Test Loss: 0.04530984952489913\n",
      "Epoch: 4000 Training Loss: 0.03704695783407414 Test Loss: 0.04532486832569181\n",
      "Epoch: 4010 Training Loss: 0.03704543978609096 Test Loss: 0.0453157431903161\n",
      "Epoch: 4020 Training Loss: 0.0370438146515428 Test Loss: 0.045325750143055336\n",
      "Epoch: 4030 Training Loss: 0.037042271314387115 Test Loss: 0.04532650080403884\n",
      "Epoch: 4040 Training Loss: 0.03704030486321405 Test Loss: 0.04533766025858546\n",
      "Epoch: 4050 Training Loss: 0.037038049417841994 Test Loss: 0.04532879743596979\n",
      "Epoch: 4060 Training Loss: 0.03703655857752028 Test Loss: 0.045339024843049906\n",
      "Epoch: 4070 Training Loss: 0.03703472729261411 Test Loss: 0.04534183493825582\n",
      "Epoch: 4080 Training Loss: 0.03703280705958392 Test Loss: 0.045341748430856234\n",
      "Epoch: 4090 Training Loss: 0.03703074154461784 Test Loss: 0.045350736828729366\n",
      "Epoch: 4100 Training Loss: 0.03702893886263782 Test Loss: 0.04535048846877572\n",
      "Epoch: 4110 Training Loss: 0.0370269833991742 Test Loss: 0.045360590300598365\n",
      "Epoch: 4120 Training Loss: 0.03702423333246938 Test Loss: 0.04535054148943998\n",
      "Epoch: 4130 Training Loss: 0.037021923471774396 Test Loss: 0.045348922963899335\n",
      "Epoch: 4140 Training Loss: 0.03701993853334419 Test Loss: 0.04534668772431648\n",
      "Epoch: 4150 Training Loss: 0.03701824293792916 Test Loss: 0.045353722729295734\n",
      "Epoch: 4160 Training Loss: 0.03701599342243204 Test Loss: 0.04535811786330697\n",
      "Epoch: 4170 Training Loss: 0.037014301838402985 Test Loss: 0.045360902843461386\n",
      "Epoch: 4180 Training Loss: 0.03701283960100743 Test Loss: 0.04535807042376526\n",
      "Epoch: 4190 Training Loss: 0.03701100343267484 Test Loss: 0.04535926199343053\n",
      "Epoch: 4200 Training Loss: 0.03700955253180489 Test Loss: 0.04535716628191152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4210 Training Loss: 0.03700846998081321 Test Loss: 0.045364851487668306\n",
      "Epoch: 4220 Training Loss: 0.03700691792325321 Test Loss: 0.04536958148903278\n",
      "Epoch: 4230 Training Loss: 0.037005285637973506 Test Loss: 0.04535775229977968\n",
      "Epoch: 4240 Training Loss: 0.03700403042297639 Test Loss: 0.04536655652060854\n",
      "Epoch: 4250 Training Loss: 0.03700279561372537 Test Loss: 0.04536803272752406\n",
      "Epoch: 4260 Training Loss: 0.037001495401441986 Test Loss: 0.04537019541251371\n",
      "Epoch: 4270 Training Loss: 0.036999609875620974 Test Loss: 0.045367974125737245\n",
      "Epoch: 4280 Training Loss: 0.03699880184295713 Test Loss: 0.04537864244149912\n",
      "Epoch: 4290 Training Loss: 0.036997039798061224 Test Loss: 0.04538559652020134\n",
      "Epoch: 4300 Training Loss: 0.03699562639492983 Test Loss: 0.045384357510994365\n",
      "Epoch: 4310 Training Loss: 0.036995026431112944 Test Loss: 0.04538600394214778\n",
      "Epoch: 4320 Training Loss: 0.036993764065384294 Test Loss: 0.0453863164850108\n",
      "Epoch: 4330 Training Loss: 0.03699237455616072 Test Loss: 0.045389743294258926\n",
      "Epoch: 4340 Training Loss: 0.03699145507672973 Test Loss: 0.04539806474798687\n",
      "Epoch: 4350 Training Loss: 0.03698994557474279 Test Loss: 0.04540173154550481\n",
      "Epoch: 4360 Training Loss: 0.03698915097150159 Test Loss: 0.04539617274744108\n",
      "Epoch: 4370 Training Loss: 0.03698787325786135 Test Loss: 0.04540448024836263\n",
      "Epoch: 4380 Training Loss: 0.036986538861593046 Test Loss: 0.04540690245555105\n",
      "Epoch: 4390 Training Loss: 0.036985233765883245 Test Loss: 0.04540519184148826\n",
      "Epoch: 4400 Training Loss: 0.03698407935876001 Test Loss: 0.0454081805326159\n",
      "Epoch: 4410 Training Loss: 0.03698333917084175 Test Loss: 0.04540374354018551\n",
      "Epoch: 4420 Training Loss: 0.03698204384198478 Test Loss: 0.04540309892053053\n",
      "Epoch: 4430 Training Loss: 0.03698077851131865 Test Loss: 0.045407376850968136\n",
      "Epoch: 4440 Training Loss: 0.03697978787338846 Test Loss: 0.04540000976919692\n",
      "Epoch: 4450 Training Loss: 0.036979613639710245 Test Loss: 0.04539933445336789\n",
      "Epoch: 4460 Training Loss: 0.036978444233491586 Test Loss: 0.04539645180356878\n",
      "Epoch: 4470 Training Loss: 0.03697794437991624 Test Loss: 0.04539726664746165\n",
      "Epoch: 4480 Training Loss: 0.036976466096976746 Test Loss: 0.04539550859385716\n",
      "Epoch: 4490 Training Loss: 0.0369759484537766 Test Loss: 0.04540178735673035\n",
      "Epoch: 4500 Training Loss: 0.036975152106454544 Test Loss: 0.04539647691862027\n",
      "Epoch: 4510 Training Loss: 0.03697418396716748 Test Loss: 0.045401522253409037\n",
      "Epoch: 4520 Training Loss: 0.03697310159058389 Test Loss: 0.045401078554166\n",
      "Epoch: 4530 Training Loss: 0.03697156522975165 Test Loss: 0.045399744665875603\n",
      "Epoch: 4540 Training Loss: 0.036969766035933355 Test Loss: 0.04538774804294589\n",
      "Epoch: 4550 Training Loss: 0.03696849791473785 Test Loss: 0.045394431437204244\n",
      "Epoch: 4560 Training Loss: 0.03696664866579879 Test Loss: 0.04540177898504652\n",
      "Epoch: 4570 Training Loss: 0.03696529456141674 Test Loss: 0.04539207620348648\n",
      "Epoch: 4580 Training Loss: 0.03696422648629622 Test Loss: 0.045398804246725266\n",
      "Epoch: 4590 Training Loss: 0.036962678265714116 Test Loss: 0.04540310450165309\n",
      "Epoch: 4600 Training Loss: 0.03696089878000958 Test Loss: 0.04540466721596819\n",
      "Epoch: 4610 Training Loss: 0.03695971646759253 Test Loss: 0.045417127072069886\n",
      "Epoch: 4620 Training Loss: 0.03695822179029292 Test Loss: 0.04541732520192055\n",
      "Epoch: 4630 Training Loss: 0.03695683332751787 Test Loss: 0.04540781217852734\n",
      "Epoch: 4640 Training Loss: 0.03695548794354014 Test Loss: 0.04541405187354266\n",
      "Epoch: 4650 Training Loss: 0.0369541144798605 Test Loss: 0.04541505926616365\n",
      "Epoch: 4660 Training Loss: 0.036952926237568524 Test Loss: 0.04541606107766208\n",
      "Epoch: 4670 Training Loss: 0.036951426851250586 Test Loss: 0.045416370829963824\n",
      "Epoch: 4680 Training Loss: 0.03695006821225829 Test Loss: 0.04542661777097288\n",
      "Epoch: 4690 Training Loss: 0.0369485758022638 Test Loss: 0.04543435599739393\n",
      "Epoch: 4700 Training Loss: 0.03694757661833738 Test Loss: 0.0454274270337432\n",
      "Epoch: 4710 Training Loss: 0.03694647889384219 Test Loss: 0.04544338067256366\n",
      "Epoch: 4720 Training Loss: 0.03694556011204355 Test Loss: 0.045436052658650326\n",
      "Epoch: 4730 Training Loss: 0.036944379718115455 Test Loss: 0.04543751770332074\n",
      "Epoch: 4740 Training Loss: 0.03694380905485708 Test Loss: 0.045440676618686275\n",
      "Epoch: 4750 Training Loss: 0.036942265717701386 Test Loss: 0.04544843437903627\n",
      "Epoch: 4760 Training Loss: 0.03694152239043757 Test Loss: 0.04544990779539051\n",
      "Epoch: 4770 Training Loss: 0.03694076842428048 Test Loss: 0.04544538987668309\n",
      "Epoch: 4780 Training Loss: 0.03693973209143167 Test Loss: 0.04544842042622988\n",
      "Epoch: 4790 Training Loss: 0.036938982136660574 Test Loss: 0.045459058045817714\n",
      "Epoch: 4800 Training Loss: 0.03693790394587105 Test Loss: 0.045460104506296575\n",
      "Epoch: 4810 Training Loss: 0.0369364352553763 Test Loss: 0.04546650884442724\n",
      "Epoch: 4820 Training Loss: 0.03693511463734682 Test Loss: 0.045470744916445684\n",
      "Epoch: 4830 Training Loss: 0.03693431567390347 Test Loss: 0.04548033607555465\n",
      "Epoch: 4840 Training Loss: 0.03693305766837697 Test Loss: 0.04547887661200679\n",
      "Epoch: 4850 Training Loss: 0.03693244200783232 Test Loss: 0.04547887661200679\n",
      "Epoch: 4860 Training Loss: 0.03693156002613989 Test Loss: 0.04547837152041566\n",
      "Epoch: 4870 Training Loss: 0.03693040718868944 Test Loss: 0.04548667623077594\n",
      "Epoch: 4880 Training Loss: 0.03692948038411883 Test Loss: 0.0454881105792723\n",
      "Epoch: 4890 Training Loss: 0.03692862037784527 Test Loss: 0.0454882919657553\n",
      "Epoch: 4900 Training Loss: 0.036927701421638544 Test Loss: 0.0454892742433248\n",
      "Epoch: 4910 Training Loss: 0.03692640591837349 Test Loss: 0.04548605393561117\n",
      "Epoch: 4920 Training Loss: 0.03692558306102232 Test Loss: 0.045484371227161154\n",
      "Epoch: 4930 Training Loss: 0.03692438191253195 Test Loss: 0.04549172993724854\n",
      "Epoch: 4940 Training Loss: 0.03692328785060658 Test Loss: 0.045493314976053856\n",
      "Epoch: 4950 Training Loss: 0.036922252040982026 Test Loss: 0.045499989998628386\n",
      "Epoch: 4960 Training Loss: 0.0369211089703844 Test Loss: 0.04550593668470962\n",
      "Epoch: 4970 Training Loss: 0.03692003374453235 Test Loss: 0.045508372844704416\n",
      "Epoch: 4980 Training Loss: 0.0369189253811439 Test Loss: 0.045519864376043\n",
      "Epoch: 4990 Training Loss: 0.036917971368911826 Test Loss: 0.045518667225255174\n",
      "Epoch: 5000 Training Loss: 0.036916718072403654 Test Loss: 0.0455241423064806\n",
      "Epoch: 5010 Training Loss: 0.03691560918579095 Test Loss: 0.04552679333969373\n",
      "Epoch: 5020 Training Loss: 0.03691491608805598 Test Loss: 0.045529472278519625\n",
      "Epoch: 5030 Training Loss: 0.036914068464756554 Test Loss: 0.04553292420281924\n",
      "Epoch: 5040 Training Loss: 0.03691323671259297 Test Loss: 0.04554087172133607\n",
      "Epoch: 5050 Training Loss: 0.03691214910376679 Test Loss: 0.04553741979703645\n",
      "Epoch: 5060 Training Loss: 0.036911040042746 Test Loss: 0.04554136565068209\n",
      "Epoch: 5070 Training Loss: 0.0369102403816703 Test Loss: 0.045541725633086824\n",
      "Epoch: 5080 Training Loss: 0.03690909155560583 Test Loss: 0.045543494848936425\n",
      "Epoch: 5090 Training Loss: 0.036907995575191505 Test Loss: 0.04554288650657805\n",
      "Epoch: 5100 Training Loss: 0.03690702272688611 Test Loss: 0.04555178281592904\n",
      "Epoch: 5110 Training Loss: 0.036906076388609835 Test Loss: 0.045553752952190586\n",
      "Epoch: 5120 Training Loss: 0.03690496192093837 Test Loss: 0.04555765973797835\n",
      "Epoch: 5130 Training Loss: 0.03690425155680285 Test Loss: 0.04555451756598048\n",
      "Epoch: 5140 Training Loss: 0.036902937043056395 Test Loss: 0.04555788298288051\n",
      "Epoch: 5150 Training Loss: 0.03690157264859725 Test Loss: 0.045566603486871055\n",
      "Epoch: 5160 Training Loss: 0.03690089350350919 Test Loss: 0.04556065121966727\n",
      "Epoch: 5170 Training Loss: 0.036899264183166955 Test Loss: 0.0455672648498937\n",
      "Epoch: 5180 Training Loss: 0.036898451092668615 Test Loss: 0.04556793737516145\n",
      "Epoch: 5190 Training Loss: 0.03689747074481551 Test Loss: 0.045571099081088265\n",
      "Epoch: 5200 Training Loss: 0.03689608786309922 Test Loss: 0.04557699553706651\n",
      "Epoch: 5210 Training Loss: 0.03689456179234407 Test Loss: 0.045568498277978126\n",
      "Epoch: 5220 Training Loss: 0.036893630278755134 Test Loss: 0.045559188965558134\n",
      "Epoch: 5230 Training Loss: 0.0368926070265128 Test Loss: 0.04555962150255607\n",
      "Epoch: 5240 Training Loss: 0.03689125832878142 Test Loss: 0.04556495705571764\n",
      "Epoch: 5250 Training Loss: 0.036890201590186515 Test Loss: 0.04555839086503292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5260 Training Loss: 0.03688924007840673 Test Loss: 0.045575552816886314\n",
      "Epoch: 5270 Training Loss: 0.036888509482933214 Test Loss: 0.04556878849635093\n",
      "Epoch: 5280 Training Loss: 0.03688750995019061 Test Loss: 0.04557082560608312\n",
      "Epoch: 5290 Training Loss: 0.03688698410981041 Test Loss: 0.045576230923276624\n",
      "Epoch: 5300 Training Loss: 0.03688572697632435 Test Loss: 0.045572653423719536\n",
      "Epoch: 5310 Training Loss: 0.03688516032445195 Test Loss: 0.045576875542931605\n",
      "Epoch: 5320 Training Loss: 0.036884032427357834 Test Loss: 0.0455677252925044\n",
      "Epoch: 5330 Training Loss: 0.03688366948413024 Test Loss: 0.04557201717574839\n",
      "Epoch: 5340 Training Loss: 0.03688290644875267 Test Loss: 0.045562702282205846\n",
      "Epoch: 5350 Training Loss: 0.03688175308807795 Test Loss: 0.04556262414649009\n",
      "Epoch: 5360 Training Loss: 0.036880964937935946 Test Loss: 0.045568470372365354\n",
      "Epoch: 5370 Training Loss: 0.03688042723780588 Test Loss: 0.04557270365382252\n",
      "Epoch: 5380 Training Loss: 0.03687964152937709 Test Loss: 0.04557993399809117\n",
      "Epoch: 5390 Training Loss: 0.03687833329432173 Test Loss: 0.04557725505926527\n",
      "Epoch: 5400 Training Loss: 0.03687787721717605 Test Loss: 0.04558317383973373\n",
      "Epoch: 5410 Training Loss: 0.0368765855508889 Test Loss: 0.04558404728541343\n",
      "Epoch: 5420 Training Loss: 0.036875504046345736 Test Loss: 0.04557476308804493\n",
      "Epoch: 5430 Training Loss: 0.03687484007476118 Test Loss: 0.04559190829653067\n",
      "Epoch: 5440 Training Loss: 0.03687394239634099 Test Loss: 0.0455914673878489\n",
      "Epoch: 5450 Training Loss: 0.03687274194548297 Test Loss: 0.045602384063564434\n",
      "Epoch: 5460 Training Loss: 0.036871571492815794 Test Loss: 0.045602085473507796\n",
      "Epoch: 5470 Training Loss: 0.03687065515273036 Test Loss: 0.045611598496901\n",
      "Epoch: 5480 Training Loss: 0.036869861944753855 Test Loss: 0.045605076955196715\n",
      "Epoch: 5490 Training Loss: 0.03686826035529735 Test Loss: 0.0456093325611441\n",
      "Epoch: 5500 Training Loss: 0.03686669975174111 Test Loss: 0.045614098839805176\n",
      "Epoch: 5510 Training Loss: 0.03686506136217839 Test Loss: 0.04561834886463001\n",
      "Epoch: 5520 Training Loss: 0.036863499886581726 Test Loss: 0.04562022691236941\n",
      "Epoch: 5530 Training Loss: 0.036862115609600746 Test Loss: 0.04560869631317295\n",
      "Epoch: 5540 Training Loss: 0.036860801619078544 Test Loss: 0.04561270355916668\n",
      "Epoch: 5550 Training Loss: 0.03685956000791216 Test Loss: 0.045609753935896925\n",
      "Epoch: 5560 Training Loss: 0.036857831798184984 Test Loss: 0.045619216729187145\n",
      "Epoch: 5570 Training Loss: 0.03685658495477601 Test Loss: 0.04560998276192164\n",
      "Epoch: 5580 Training Loss: 0.03685481384065961 Test Loss: 0.0456092600065509\n",
      "Epoch: 5590 Training Loss: 0.03685386104928414 Test Loss: 0.045611528732869076\n",
      "Epoch: 5600 Training Loss: 0.036852170860519784 Test Loss: 0.04561203382446021\n",
      "Epoch: 5610 Training Loss: 0.03685146991442094 Test Loss: 0.04561389791939323\n",
      "Epoch: 5620 Training Loss: 0.03685068908941856 Test Loss: 0.04560921814813174\n",
      "Epoch: 5630 Training Loss: 0.03684974222791803 Test Loss: 0.04561743076996988\n",
      "Epoch: 5640 Training Loss: 0.03684873066101747 Test Loss: 0.04562342210503155\n",
      "Epoch: 5650 Training Loss: 0.03684755410406728 Test Loss: 0.04561107108081965\n",
      "Epoch: 5660 Training Loss: 0.03684661352125785 Test Loss: 0.045614319294146055\n",
      "Epoch: 5670 Training Loss: 0.03684577304868996 Test Loss: 0.04562415881320867\n",
      "Epoch: 5680 Training Loss: 0.03684493955244552 Test Loss: 0.04561951810980506\n",
      "Epoch: 5690 Training Loss: 0.03684402652611373 Test Loss: 0.04562043899502646\n",
      "Epoch: 5700 Training Loss: 0.03684331424348927 Test Loss: 0.04563063570593253\n",
      "Epoch: 5710 Training Loss: 0.03684203025115791 Test Loss: 0.045632407712343406\n",
      "Epoch: 5720 Training Loss: 0.03684211501348786 Test Loss: 0.04563046548169463\n",
      "Epoch: 5730 Training Loss: 0.036840407907139126 Test Loss: 0.04563100126945981\n",
      "Epoch: 5740 Training Loss: 0.03683970469373516 Test Loss: 0.04563013619546395\n",
      "Epoch: 5750 Training Loss: 0.036839108043671916 Test Loss: 0.04563633961318266\n",
      "Epoch: 5760 Training Loss: 0.03683876864553597 Test Loss: 0.04563824277597356\n",
      "Epoch: 5770 Training Loss: 0.03683762435408174 Test Loss: 0.04564387133806922\n",
      "Epoch: 5780 Training Loss: 0.03683670714195588 Test Loss: 0.045633987170026175\n",
      "Epoch: 5790 Training Loss: 0.03683619839356814 Test Loss: 0.04563918040456263\n",
      "Epoch: 5800 Training Loss: 0.03683536245561049 Test Loss: 0.04563573406138556\n",
      "Epoch: 5810 Training Loss: 0.036834489543138546 Test Loss: 0.045624490890000625\n",
      "Epoch: 5820 Training Loss: 0.03683370121858846 Test Loss: 0.04562846464925904\n",
      "Epoch: 5830 Training Loss: 0.036833407864187304 Test Loss: 0.0456299966674001\n",
      "Epoch: 5840 Training Loss: 0.036832714766452335 Test Loss: 0.045617662386555874\n",
      "Epoch: 5850 Training Loss: 0.036831734244191144 Test Loss: 0.045627144713775034\n",
      "Epoch: 5860 Training Loss: 0.03683128706185786 Test Loss: 0.04562371511396563\n",
      "Epoch: 5870 Training Loss: 0.03683071552655905 Test Loss: 0.045623712323404356\n",
      "Epoch: 5880 Training Loss: 0.03683008190198152 Test Loss: 0.04562996876178733\n",
      "Epoch: 5890 Training Loss: 0.03682958309485469 Test Loss: 0.04562782002960406\n",
      "Epoch: 5900 Training Loss: 0.03682861059536547 Test Loss: 0.04562022970293069\n",
      "Epoch: 5910 Training Loss: 0.0368281709125799 Test Loss: 0.04561765122431076\n",
      "Epoch: 5920 Training Loss: 0.036827228411281526 Test Loss: 0.045622428665216944\n",
      "Epoch: 5930 Training Loss: 0.03682609004970223 Test Loss: 0.04562424811116953\n",
      "Epoch: 5940 Training Loss: 0.0368256358910455 Test Loss: 0.045615951772493085\n",
      "Epoch: 5950 Training Loss: 0.036824668623798865 Test Loss: 0.04562166684198833\n",
      "Epoch: 5960 Training Loss: 0.03682356723673387 Test Loss: 0.045612061730072984\n",
      "Epoch: 5970 Training Loss: 0.03682327981220764 Test Loss: 0.04562646660738473\n",
      "Epoch: 5980 Training Loss: 0.03682258619124842 Test Loss: 0.04561222079206577\n",
      "Epoch: 5990 Training Loss: 0.03682165607292417 Test Loss: 0.04562238122567523\n",
      "Epoch: 6000 Training Loss: 0.03682110023435312 Test Loss: 0.045615164834212976\n",
      "Epoch: 6010 Training Loss: 0.036820046111879515 Test Loss: 0.0456078424014222\n",
      "Epoch: 6020 Training Loss: 0.03681901047666305 Test Loss: 0.045627867469145766\n",
      "Epoch: 6030 Training Loss: 0.036818303600689266 Test Loss: 0.045616875448275765\n",
      "Epoch: 6040 Training Loss: 0.03681686543160961 Test Loss: 0.04561823166105637\n",
      "Epoch: 6050 Training Loss: 0.03681625151514583 Test Loss: 0.045624412754284874\n",
      "Epoch: 6060 Training Loss: 0.03681514315175738 Test Loss: 0.04562040271772986\n",
      "Epoch: 6070 Training Loss: 0.03681349394889331 Test Loss: 0.0456245913502066\n",
      "Epoch: 6080 Training Loss: 0.03681220838688917 Test Loss: 0.045627965138790465\n",
      "Epoch: 6090 Training Loss: 0.03681105485180637 Test Loss: 0.04562378766855883\n",
      "Epoch: 6100 Training Loss: 0.03681005688873655 Test Loss: 0.045620575732529034\n",
      "Epoch: 6110 Training Loss: 0.03680875301388335 Test Loss: 0.04561422999618519\n",
      "Epoch: 6120 Training Loss: 0.03680765371971539 Test Loss: 0.04561924463479992\n",
      "Epoch: 6130 Training Loss: 0.03680729775281125 Test Loss: 0.0456119138303253\n",
      "Epoch: 6140 Training Loss: 0.036805842666147226 Test Loss: 0.04561629780209143\n",
      "Epoch: 6150 Training Loss: 0.03680498928738095 Test Loss: 0.04561462625588652\n",
      "Epoch: 6160 Training Loss: 0.036804388974747894 Test Loss: 0.0456197915848102\n",
      "Epoch: 6170 Training Loss: 0.036803491819551955 Test Loss: 0.045618948835304554\n",
      "Epoch: 6180 Training Loss: 0.0368022714861721 Test Loss: 0.04561966042843019\n",
      "Epoch: 6190 Training Loss: 0.03680124509458421 Test Loss: 0.0456266396221839\n",
      "Epoch: 6200 Training Loss: 0.03680065385117165 Test Loss: 0.045618795354434326\n",
      "Epoch: 6210 Training Loss: 0.036799602344819334 Test Loss: 0.045621136635345706\n",
      "Epoch: 6220 Training Loss: 0.03679906098211945 Test Loss: 0.045621440806524895\n",
      "Epoch: 6230 Training Loss: 0.0367985027018352 Test Loss: 0.04562342210503155\n",
      "Epoch: 6240 Training Loss: 0.036797680193300195 Test Loss: 0.045630507340113785\n",
      "Epoch: 6250 Training Loss: 0.03679727591535614 Test Loss: 0.04562575780482037\n",
      "Epoch: 6260 Training Loss: 0.036796714844542514 Test Loss: 0.04562416997545378\n",
      "Epoch: 6270 Training Loss: 0.03679698918846226 Test Loss: 0.045627306566329094\n",
      "Epoch: 6280 Training Loss: 0.0367961190665197 Test Loss: 0.04562237843511396\n",
      "Epoch: 6290 Training Loss: 0.0367953003949626 Test Loss: 0.04562036923099454\n",
      "Epoch: 6300 Training Loss: 0.03679457974075 Test Loss: 0.04562336350324473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6310 Training Loss: 0.03679406558571158 Test Loss: 0.045627568879089135\n",
      "Epoch: 6320 Training Loss: 0.0367936604357271 Test Loss: 0.04562305654150426\n",
      "Epoch: 6330 Training Loss: 0.03679333847839979 Test Loss: 0.04561962136057231\n",
      "Epoch: 6340 Training Loss: 0.03679248178587987 Test Loss: 0.04561588200846116\n",
      "Epoch: 6350 Training Loss: 0.036792021174123946 Test Loss: 0.04561189708695764\n",
      "Epoch: 6360 Training Loss: 0.036791727645314694 Test Loss: 0.04560939116293092\n",
      "Epoch: 6370 Training Loss: 0.036791079196049827 Test Loss: 0.04561051575912554\n",
      "Epoch: 6380 Training Loss: 0.036790010074480795 Test Loss: 0.04560722568737998\n",
      "Epoch: 6390 Training Loss: 0.03678969282617181 Test Loss: 0.04560457186360558\n",
      "Epoch: 6400 Training Loss: 0.03678935220717926 Test Loss: 0.045599197242586126\n",
      "Epoch: 6410 Training Loss: 0.036788823227453506 Test Loss: 0.04560758566978471\n",
      "Epoch: 6420 Training Loss: 0.036788876945144086 Test Loss: 0.045607730778971116\n",
      "Epoch: 6430 Training Loss: 0.036788584986007615 Test Loss: 0.04559597972543377\n",
      "Epoch: 6440 Training Loss: 0.03678770875978203 Test Loss: 0.04559579833895077\n",
      "Epoch: 6450 Training Loss: 0.036787189895725284 Test Loss: 0.04560285008729768\n",
      "Epoch: 6460 Training Loss: 0.03678721413844928 Test Loss: 0.04559980279438323\n",
      "Epoch: 6470 Training Loss: 0.03678655975930947 Test Loss: 0.045595243017256655\n",
      "Epoch: 6480 Training Loss: 0.036786140482270004 Test Loss: 0.045604993238358406\n",
      "Epoch: 6490 Training Loss: 0.03678652697058925 Test Loss: 0.04560506021182905\n",
      "Epoch: 6500 Training Loss: 0.03678538564407249 Test Loss: 0.045585414660439146\n",
      "Epoch: 6510 Training Loss: 0.03678534692547733 Test Loss: 0.04559307475114444\n",
      "Epoch: 6520 Training Loss: 0.03678544686131078 Test Loss: 0.04559632575503212\n",
      "Epoch: 6530 Training Loss: 0.03678488910425078 Test Loss: 0.04559188318147917\n",
      "Epoch: 6540 Training Loss: 0.03678496427413598 Test Loss: 0.045601641774264755\n",
      "Epoch: 6550 Training Loss: 0.036784044794705 Test Loss: 0.04559832100634516\n",
      "Epoch: 6560 Training Loss: 0.036784008343414956 Test Loss: 0.04559292406083549\n",
      "Epoch: 6570 Training Loss: 0.03678373556916798 Test Loss: 0.045593747276412196\n",
      "Epoch: 6580 Training Loss: 0.036783291526180255 Test Loss: 0.045593909128966256\n",
      "Epoch: 6590 Training Loss: 0.03678364696986014 Test Loss: 0.04560123156175704\n",
      "Epoch: 6600 Training Loss: 0.036782632263614036 Test Loss: 0.045599105154063985\n",
      "Epoch: 6610 Training Loss: 0.03678165802004395 Test Loss: 0.04560507974575799\n",
      "Epoch: 6620 Training Loss: 0.03678175324685907 Test Loss: 0.04560264637632447\n",
      "Epoch: 6630 Training Loss: 0.03678128949575759 Test Loss: 0.04559549974889413\n",
      "Epoch: 6640 Training Loss: 0.03678082609347229 Test Loss: 0.04559836565532559\n",
      "Epoch: 6650 Training Loss: 0.03678041658328565 Test Loss: 0.04560130132578896\n",
      "Epoch: 6660 Training Loss: 0.036780441872458165 Test Loss: 0.04560567971643254\n",
      "Epoch: 6670 Training Loss: 0.036779984748863966 Test Loss: 0.04560821354607203\n",
      "Epoch: 6680 Training Loss: 0.036779914287997095 Test Loss: 0.04560874933383721\n",
      "Epoch: 6690 Training Loss: 0.03677925554865513 Test Loss: 0.04561316400177739\n",
      "Epoch: 6700 Training Loss: 0.03677966418680134 Test Loss: 0.045598507973950715\n",
      "Epoch: 6710 Training Loss: 0.03677833380191903 Test Loss: 0.04560777821851283\n",
      "Epoch: 6720 Training Loss: 0.03677848919952392 Test Loss: 0.04560692151620079\n",
      "Epoch: 6730 Training Loss: 0.03677758524241262 Test Loss: 0.04560357842379098\n",
      "Epoch: 6740 Training Loss: 0.03677684801943182 Test Loss: 0.04561042088004212\n",
      "Epoch: 6750 Training Loss: 0.036776371885356214 Test Loss: 0.0456096116172718\n",
      "Epoch: 6760 Training Loss: 0.036776789767130995 Test Loss: 0.04561315283953228\n",
      "Epoch: 6770 Training Loss: 0.03677568122933446 Test Loss: 0.045612778904321166\n",
      "Epoch: 6780 Training Loss: 0.03677519271228472 Test Loss: 0.04561510344186488\n",
      "Epoch: 6790 Training Loss: 0.036775124344314895 Test Loss: 0.04562330490145791\n",
      "Epoch: 6800 Training Loss: 0.03677452246200906 Test Loss: 0.045624041609635034\n",
      "Epoch: 6810 Training Loss: 0.03677470942747758 Test Loss: 0.045624906683630895\n",
      "Epoch: 6820 Training Loss: 0.036774178180446704 Test Loss: 0.04563338719935162\n",
      "Epoch: 6830 Training Loss: 0.036773618853713934 Test Loss: 0.04564255419314649\n",
      "Epoch: 6840 Training Loss: 0.036773174287501945 Test Loss: 0.04563051571179762\n",
      "Epoch: 6850 Training Loss: 0.03677260728681338 Test Loss: 0.045644063886797336\n",
      "Epoch: 6860 Training Loss: 0.036771586650692335 Test Loss: 0.04562754655459891\n",
      "Epoch: 6870 Training Loss: 0.03677144921712033 Test Loss: 0.04563497223815695\n",
      "Epoch: 6880 Training Loss: 0.03677051229688071 Test Loss: 0.045630794767925316\n",
      "Epoch: 6890 Training Loss: 0.03677024091789843 Test Loss: 0.04563607730042263\n",
      "Epoch: 6900 Training Loss: 0.036769543111145137 Test Loss: 0.04563690330656062\n",
      "Epoch: 6910 Training Loss: 0.03676899826028353 Test Loss: 0.045632603051632796\n",
      "Epoch: 6920 Training Loss: 0.03676883222378537 Test Loss: 0.04563682796140613\n",
      "Epoch: 6930 Training Loss: 0.036768528404899026 Test Loss: 0.04563750885835772\n",
      "Epoch: 6940 Training Loss: 0.03676767432850041 Test Loss: 0.04564062033418154\n",
      "Epoch: 6950 Training Loss: 0.0367673776603456 Test Loss: 0.045643885290875603\n",
      "Epoch: 6960 Training Loss: 0.036767625668644326 Test Loss: 0.04564148261761613\n",
      "Epoch: 6970 Training Loss: 0.03676714866252829 Test Loss: 0.045639437136200105\n",
      "Epoch: 6980 Training Loss: 0.03676673461773141 Test Loss: 0.045645208016920895\n",
      "Epoch: 6990 Training Loss: 0.036766582882696325 Test Loss: 0.04563872554307448\n",
      "Epoch: 7000 Training Loss: 0.03676587216974464 Test Loss: 0.04564644702612787\n",
      "Epoch: 7010 Training Loss: 0.03676556573473701 Test Loss: 0.04564118681812077\n",
      "Epoch: 7020 Training Loss: 0.036765458996988194 Test Loss: 0.04564135425179739\n",
      "Epoch: 7030 Training Loss: 0.0367650208838754 Test Loss: 0.04564051987397558\n",
      "Epoch: 7040 Training Loss: 0.036764867404759455 Test Loss: 0.04564992406547898\n",
      "Epoch: 7050 Training Loss: 0.03676404977965087 Test Loss: 0.04565170444357369\n",
      "Epoch: 7060 Training Loss: 0.03676349481312026 Test Loss: 0.04565494707577754\n",
      "Epoch: 7070 Training Loss: 0.03676305582796703 Test Loss: 0.04565300763569004\n",
      "Epoch: 7080 Training Loss: 0.036762805203547015 Test Loss: 0.045652393712209104\n",
      "Epoch: 7090 Training Loss: 0.03676195862669611 Test Loss: 0.04565996729551482\n",
      "Epoch: 7100 Training Loss: 0.03676200484483898 Test Loss: 0.045654402916328524\n",
      "Epoch: 7110 Training Loss: 0.03676128296976977 Test Loss: 0.04566445730860947\n",
      "Epoch: 7120 Training Loss: 0.036760601906192764 Test Loss: 0.04566446847085458\n",
      "Epoch: 7130 Training Loss: 0.036760173385524716 Test Loss: 0.04566387687186386\n",
      "Epoch: 7140 Training Loss: 0.03675986642729282 Test Loss: 0.045687842212110516\n",
      "Epoch: 7150 Training Loss: 0.036758967179199846 Test Loss: 0.04566882174644665\n",
      "Epoch: 7160 Training Loss: 0.03675850778830053 Test Loss: 0.045676903211904776\n",
      "Epoch: 7170 Training Loss: 0.03675844029237113 Test Loss: 0.04567271736998931\n",
      "Epoch: 7180 Training Loss: 0.036757496221399975 Test Loss: 0.04567670787261539\n",
      "Epoch: 7190 Training Loss: 0.036756333442688596 Test Loss: 0.04567040678525198\n",
      "Epoch: 7200 Training Loss: 0.03675561645104581 Test Loss: 0.04567067467913457\n",
      "Epoch: 7210 Training Loss: 0.0367542452546713 Test Loss: 0.045675985117244654\n",
      "Epoch: 7220 Training Loss: 0.03675288870857604 Test Loss: 0.045664856358872075\n",
      "Epoch: 7230 Training Loss: 0.03675167622356007 Test Loss: 0.04567746411472145\n",
      "Epoch: 7240 Training Loss: 0.036750800694966834 Test Loss: 0.04566830549261042\n",
      "Epoch: 7250 Training Loss: 0.03674896749157171 Test Loss: 0.045670990012558865\n",
      "Epoch: 7260 Training Loss: 0.03674779529482367 Test Loss: 0.045667744589793745\n",
      "Epoch: 7270 Training Loss: 0.03674750856792979 Test Loss: 0.04568076255815083\n",
      "Epoch: 7280 Training Loss: 0.0367456673417627 Test Loss: 0.04567144208348573\n",
      "Epoch: 7290 Training Loss: 0.03674464182221524 Test Loss: 0.04567821477570495\n",
      "Epoch: 7300 Training Loss: 0.03674317348053666 Test Loss: 0.045675918143774\n",
      "Epoch: 7310 Training Loss: 0.03674127225798788 Test Loss: 0.04567408474501503\n",
      "Epoch: 7320 Training Loss: 0.03673982536850392 Test Loss: 0.04568411681280576\n",
      "Epoch: 7330 Training Loss: 0.03673862334797312 Test Loss: 0.045695471606641766\n",
      "Epoch: 7340 Training Loss: 0.0367367697388319 Test Loss: 0.045673415010308555\n",
      "Epoch: 7350 Training Loss: 0.03673548940907036 Test Loss: 0.045682593166348526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7360 Training Loss: 0.03673384038061436 Test Loss: 0.04567841569611689\n"
     ]
    }
   ],
   "source": [
    "batch_size=int(x_variable.size()[0]/18)\n",
    "nb_of_epochs=13000\n",
    "#my_net.cuda()\n",
    "\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss1=0\n",
    "    sum_loss2=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss2= loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "        \n",
    "        #loss1=my_torch_loss_function(out, y_variable.narrow(0,b,batch_size).resize(batch_size,1*2)).sum()\n",
    "        #sum_loss1+=loss1.data[0]\n",
    "        sum_loss2+=loss2.data[0]\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss2.sum().backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss1.data[0],loss2.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0: \n",
    "        my_net.eval()\n",
    "        test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss2/(x_variable.size(0)),\"Test Loss:\",test_loss/x_variable_test.size(0))\n",
    "        #print(\"Epoch:\",t,\"Training Loss:\",sum_loss1/(x_variable.size(0)),sum_loss2/(x_variable.size(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.00646368949978321 -0.1199108451461814 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x241a9b2c9e8>"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_index=7\n",
    "sample_contour=np.delete(polygons_reshaped[sample_index],24)\n",
    "sample_contour=sample_contour.reshape(12,2)\n",
    "\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(sample_contour,.8)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "\n",
    "my_net.eval()\n",
    "predictions=my_net(x_variable).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(1,2)\n",
    "\n",
    "\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "#plt.scatter(polygons_reshaped[3112][0::2].sum()/12,polygons_reshaped[312][1::2].sum()/12,label='Original points')\n",
    "\n",
    "plot_contour(sample_contour)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "#original_points=point_coordinates.reshape(218722,2)\n",
    "#plt.scatter(original_points[:,0],original_points[:,1],label='Original points')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (bn_input): BatchNorm1d(25, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc0): Linear(in_features=25, out_features=20, bias=True)\n",
       "  (bn0): BatchNorm1d(20, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (predict): Linear(in_features=20, out_features=2, bias=True)\n",
       "  (fc1): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.5, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2427de9c550>"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plt.scatter(centers_of_mass[10][0],centers_of_mass[10][1],label='Original points'\n",
    "           )\n",
    "plot_contour(polygons[2])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "points=centers_of_mass\n",
    "hull=ConvexHull(points)\n",
    "hull2=ConvexHull(original_points)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(points[:,0], points[:,1], 'o')\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n",
    "for simplex in hull2.simplices:\n",
    "    plt.plot(original_points[simplex, 0], original_points[simplex, 1], 'k-')\n",
    "\n",
    "    \n",
    "    \n",
    "convex_points=centers_of_mass[hull.vertices]\n",
    "convex_points2=original_points[hull2.vertices]\n",
    "\n",
    "distances=np.empty([convex_points.shape[0],convex_points.shape[0]])    \n",
    "distances2=np.empty([convex_points2.shape[0],convex_points2.shape[0]])    \n",
    "\n",
    "    \n",
    "for i,point_i in enumerate(convex_points):\n",
    "    for j,point_j in enumerate(convex_points):\n",
    "        distances[i][j]=np.linalg.norm(point_i-point_j)\n",
    "for i,point_i in enumerate(convex_points2):\n",
    "    for j,point_j in enumerate(convex_points2):\n",
    "        distances2[i][j]=np.linalg.norm(point_i-point_j)\n",
    "        \n",
    "def max_element(A):\n",
    "    r, (c, l) = max(map(lambda t: (t[0], max(enumerate(t[1]), key=lambda v: v[1])), enumerate(A)), key=lambda v: v[1][1])\n",
    "    return (l, r, c)\n",
    "\n",
    "print(max_element(distances))\n",
    "print(max_element(distances2))\n",
    "\n",
    "maximum_distance_points=convex_points[[0,11]]\n",
    "maximum_distance_points2=convex_points2[[3,11]]\n",
    "\n",
    "plt.plot(maximum_distance_points[:,0],maximum_distance_points[:,1])\n",
    "plt.plot(maximum_distance_points2[:,0],maximum_distance_points2[:,1])\n",
    "1.8508252250365/0.6108970818704328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distances=[np.linalg.norm(i-j) for i in centers_of_mass for j in predicted_inserted_points]\n",
    "distances=np.array(distances)\n",
    "100*distances.sum()/(x_variable.size(0) *max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable,x_variable_test=x_variable.resize(x_variable.size()[0],1,25),Variable(x_tensor_test.view(x_variable_test.size()[0],1,25),volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using convolution network\n",
    "# O: output dimension\n",
    "# I: Input dimensiion\n",
    "# S: Stride\n",
    "# P: padding\n",
    "# w: kernel size\n",
    "# O=(I-w-2*P)/S+1\n",
    "\n",
    "\n",
    "nb_of_points_output=1\n",
    "\n",
    "class Conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Conv_net,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,14,kernel_size=14,stride=1)\n",
    "        self.conv2=nn.Conv1d(14,28,kernel_size=2,stride=1)\n",
    "\n",
    "        self.bn1=nn.BatchNorm1d(num_features=28*9)\n",
    "        self.fc1=nn.Linear(28*9,12)\n",
    "        self.dp_1=nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.bn2=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc2=nn.Linear(12,12)\n",
    "        self.dp_2=nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.bn3=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc3=nn.Linear(12,12)\n",
    "        self.dp_3=nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.bn4=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc4=nn.Linear(12,nb_of_points_output*2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(F.max_pool1d(self.conv1(x),kernel_size=2,stride=1))\n",
    "        \n",
    "        #x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        #x=self.conv2(x)\n",
    "\n",
    "        x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        x=F.relu(self.fc1(self.bn1(x.view(-1,28*9))))\n",
    "        x=F.relu(self.fc2( self.dp_1(self.bn2(x))))\n",
    "        x=F.relu(self.fc3(self.dp_2(self.bn3(x))))\n",
    "\n",
    "        x=self.fc4(self.bn4(self.dp_3(x)))\n",
    "\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (77) : an illegal memory access was encountered at torch/csrc/cuda/Module.cpp:321",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-394-926c26ed70c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmy_conv_net\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mConv_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_conv_net\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    351\u001b[0m     `nvidia-smi`.\"\"\"\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_initialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (77) : an illegal memory access was encountered at torch/csrc/cuda/Module.cpp:321"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "my_conv_net=Conv_net()\n",
    "print(my_conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_conv_net.parameters(), lr=1e-4,weight_decay=0.5)\n",
    "\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=torch.nn.SmoothL1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    my_conv_net.cuda()\n",
    "    x_variable , y_variable,x_variable_test,y_variable_test= x_variable.cuda(), y_variable.cuda(),x_variable_test.cuda(),y_variable_test.cuda()\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "reduce failed to synchronize: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-393-c8bd23ac7281>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_conv_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# input x and predict based on x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# must be (1. nn output, 2. target), the target label is NOT one-hotted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m#loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#loss=torch.sqrt((out[0]-y_variable.narrow(0,b,batch_size)[0]).pow(2)+(out[1]-y_variable.narrow(0,b,batch_size)[1]).pow(2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \"\"\"\n\u001b[0;32m   1281\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[1;32m-> 1282\u001b[1;33m                            input, target, size_average, reduce)\n\u001b[0m\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[1;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: reduce failed to synchronize: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size=int(x_variable.size()[0]/2 )\n",
    "nb_of_epochs=13000\n",
    "# Train the network #\n",
    "my_conv_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_conv_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\n",
    "        #loss=torch.sqrt((out[0]-y_variable.narrow(0,b,batch_size)[0]).pow(2)+(out[1]-y_variable.narrow(0,b,batch_size)[1]).pow(2)) \n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_conv_net.eval()\n",
    "        test_loss=loss_func(my_conv_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_conv_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable=x_variable.cpu()\n",
    "my_conv_net=my_conv_net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour1=np.delete(polygons_reshaped[0],24)\n",
    "contour1=contour1.reshape(12,2)\n",
    "plot_contour(contour1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 -0.3033108530139085 0.1448011230082973 0\n",
      "14 0.4449916807636812 0.4447385301615993 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x241a0474630>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contour1=np.delete(polygons_reshaped[11],24)\n",
    "contour1=contour1.reshape(12,2)\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(contour1,.8\n",
    "                                                                              )\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "my_conv_net.eval()\n",
    "predictions=my_conv_net(x_variable).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[11]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(2,2)\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "\n",
    "plot_contour(contour1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72589751,  0.17159901,  0.84207899,  0.40510667,  0.71337615,\n",
       "        0.63783057,  0.16721335,  0.58156785, -0.53593486,  0.85478697,\n",
       "       -1.19396456,  0.50564378, -0.99666214,  0.37817729, -0.86514855,\n",
       "       -0.73118862, -0.06646145, -0.53079973,  0.05744332, -1.1159677 ,\n",
       "        0.52287723, -0.86178872,  0.62928501, -0.29496737,  0.5       ])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contour=polygons_reshaped[1]\n",
    "contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor=torch.FloatTensor([[2.2,1.2],[3,3]])\n",
    "a_tensor=torch.FloatTensor([[1,2],[3,2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_variable=Variable(d_tensor)\n",
    "\n",
    "a_variable=Variable(a_tensor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_variable.requires_grad=False\n",
    "a_variable.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.4422\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fun=torch.sqrt((a_variable[0]-d_variable[0]).pow(2)+(a_variable[1]-d_variable[1]).pow(2))\n",
    "#print(fun)\n",
    "\n",
    "def my_torch_loss_function(a,b)->Variable:\n",
    "    torch_sum=0\n",
    "    for i in range(0,a.size()[1],2):\n",
    "        euclidean_distance=torch.sqrt((a[:,i]-b[:,i]).pow(2)+(a[:,i+1]-b[:,i+1]).pow(2))\n",
    "        torch_sum+=euclidean_distance\n",
    "    return  torch_sum   \n",
    "    \n",
    "        \n",
    "    \n",
    "loss_result=my_torch_loss_function(a_variable,d_variable) \n",
    "\n",
    "loss_result.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_result.sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.4000  1.6000\n",
       " 0.0000 -2.0000\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[0 x 0 x 87489]' is invalid for input with 349956 elements at C:\\Anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\TH\\THStorage.c:41",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-254-5a51bc598015>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0manother_loss_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0manother_loss_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manother_loss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0manother_loss_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 2: size '[0 x 0 x 87489]' is invalid for input with 349956 elements at C:\\Anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\TH\\THStorage.c:41"
     ]
    }
   ],
   "source": [
    "another_loss_func=torch.nn.MSELoss(size_average=False)\n",
    "another_loss_result=another_loss_func(out,y_variable.view(0,b,batch_size))\n",
    "\n",
    "another_loss_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.function import Function\n",
    "from math import pow\n",
    "\n",
    "class myLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target):\n",
    "        self.save_for_backward(output,target) \n",
    "\n",
    "                        \n",
    "       # output=output.view(int(output.size()[0]/2),2)\n",
    "\n",
    "        #target=target.view(int(target.size()[0]/2),2)\n",
    "        distance=torch.nn.PairwiseDistance()\n",
    "        result=distance(output,target)\n",
    "\n",
    "        result=torch.FloatTensor(result)\n",
    "        #self.save_for_backward(result)\n",
    "\n",
    "\n",
    "        return  result \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self,grad_output1):\n",
    "        input1,target=self.saved_variables\n",
    "        \n",
    "        print(input1)\n",
    "        #distance=torch.nn.PairwiseDistance()(input1.view(int(input1.size()[0]/2),2),target.view(int(target.size()[0]/2),2))\n",
    "        distance=torch.nn.PairwiseDistance()(input1,target)\n",
    "\n",
    "        grad_output1=(input1-target)/distance\n",
    "\n",
    "        \n",
    "        return grad_output1,None\n",
    "    \n",
    "    \n",
    "class myOtherLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target)->Variable:\n",
    "        \n",
    "        torch_sum=0\n",
    "        for i in range(0,output.size()[1],2):\n",
    "            euclidean_distance=(output[:,i]-target[:,i]).pow(2)+(output[:,i+1]-target[:,i+1]).pow(2)\n",
    "            torch_sum+=euclidean_distance\n",
    "        return  torch_sum\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-3.7984e-01  2.5756e-01\n",
      " 3.1686e-02  1.0344e-01\n",
      "-1.7014e-01 -1.0829e-02\n",
      "                       \n",
      "-1.2210e-01 -1.6554e-01\n",
      "-2.6152e-02 -1.0011e-01\n",
      "-1.9689e-01 -1.8628e-01\n",
      "[torch.FloatTensor of size 87489x2]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-92a9dc829b64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmyloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyLossfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmyloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "myloss=myLossfunction().apply(out,y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "myloss.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_conv_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-bbbcb1049779>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Flush all variables in GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mmy_conv_net\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmy_net\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_variable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_variable_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_conv_net' is not defined"
     ]
    }
   ],
   "source": [
    "# Flush all variables in GPU\n",
    "del my_conv_net,my_net,x_variable_test,x_variable,y_variable,y_variable_test,loss_func\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShould I order the interior points ?\\n\\nFor each interior point calculate distance with points of the polygon\\n\\nfind which ti which point it is closer and sort it according to the index.\\n\\nIf multiple points are closer to the same point of the contour then take into\\n\\naccount the distance to the point.\\n\\n'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Should I order the interior points ?\n",
    "\n",
    "For each interior point calculate distance with points of the polygon\n",
    "\n",
    "find which ti which point it is closer and sort it according to the index.\n",
    "\n",
    "If multiple points are closer to the same point of the contour then take into\n",
    "\n",
    "account the distance to the point.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Checking jumps to insertion points ##\n",
    "\n",
    "contour3=apply_procrustes(generate_contour(12))\n",
    "plot_contour(contour3)\n",
    "\n",
    "\n",
    "inserted_points=[]\n",
    "nb_of_sampling=100\n",
    "contour=contour3.copy()\n",
    "plot_contour(contour3)\n",
    "\n",
    "for i in range(0,nb_of_sampling):\n",
    "    contour[0]=np.random.normal(contour3[0],0.08)\n",
    "    contour[11]=np.random.normal(contour3[11],0.08)\n",
    "\n",
    "    nb_of_points,point_coords=get_extrapoints_target_length(contour,0.2)\n",
    "    inserted_points.append(point_coords)\n",
    "    plt.scatter(contour[0][0],contour[0][1])\n",
    "    plt.scatter(contour[11][0],contour[11][1])\n",
    "\n",
    "\n",
    "inserted_points=[i for i in inserted_points if len(i)==1]\n",
    "inserted_points=np.array(inserted_points)\n",
    "inserted_points=inserted_points.reshape(len(inserted_points),2)\n",
    "\n",
    "plt.scatter(inserted_points[:,0],inserted_points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_er(n,r)->'joules':\n",
    "    return n*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'return': 'joules'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_er.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-769f05b78af1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{:,},{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_er\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "print(\"{:,},{}\".format(get_er(4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0   ,.,.) = \n",
       " -0.0732 -0.0308\n",
       "\n",
       "(  1   ,.,.) = \n",
       " -0.1070 -0.2541\n",
       "\n",
       "(  2   ,.,.) = \n",
       " -0.2270  0.0384\n",
       "  ...  \n",
       "\n",
       "(174975,.,.) = \n",
       " -0.0972 -0.0611\n",
       "\n",
       "(174976,.,.) = \n",
       "  0.1325  0.0653\n",
       "\n",
       "(174977,.,.) = \n",
       " -0.2360  0.0878\n",
       "[torch.FloatTensor of size 174978x1x2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 14048.2607\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ewew=my_torch_loss_function(out,y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "ewew.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 14047.7451\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(out,y_variable.narrow(0,b,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MSELoss' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-52a0569421b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    396\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 398\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MSELoss' object has no attribute 'backward'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
