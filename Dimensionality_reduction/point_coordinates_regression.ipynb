{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Triangulation import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from  Neural_network import *\n",
    "\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_nb_of_points(point_coordinates):\n",
    "    set_of_numbers=set()\n",
    "    for index,_ in enumerate(point_coordinates):\n",
    "        set_of_numbers.add(len(point_coordinates[index][0]))\n",
    "    return set_of_numbers\n",
    "\n",
    "\n",
    "def get_indices_nb_of_points(set_of_numbers,number_of_points,point_coordinates):\n",
    "    indices=[]\n",
    "    if number_of_points not in set_of_numbers:\n",
    "        return \"No such number of points for sample\"\n",
    "    else:\n",
    "        for index,_ in enumerate(point_coordinates):\n",
    "            if len(point_coordinates[index][0])==number_of_points:\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "    \n",
    "def get_polygons_nb_of_points(indices):\n",
    "    \n",
    "    pass\n",
    "\n",
    "def get_edge_lengths(polygon):\n",
    "    polygon_edge_lengths=np.empty([polygon.shape[0]])\n",
    "    for index,_ in enumerate(polygon):\n",
    "        polygon_edge_lengths[index]=np.linalg.norm(polygon[(index+1)%(polygon.shape[0])]-polygon[index])\n",
    "    return polygon_edge_lengths\n",
    "    \n",
    "\n",
    "def extract_lengths_angles(polygon_set):\n",
    "    lengths=[]\n",
    "    angles=[]\n",
    "    target_edge_length=[]\n",
    "    for polygons in polygons_reshaped:\n",
    "        polygon=np.delete(polygons,16).reshape(8,2)\n",
    "        lengths.append(get_edge_lengths(polygon))\n",
    "        angles.append(np.array(np.multiply(pi/180,get_polygon_angles(polygon))))\n",
    "        target_edge_length.append([polygons[16]])\n",
    "    data=np.hstack([lengths,angles,target_edge_length])\n",
    "    return data,lengths,angles\n",
    "    \n",
    "def extract_lengths_angles_in_triangle_form(polygon_set,nb_of_points):\n",
    "    data,lengths,angles=extract_lengths_angles(polygon_set)\n",
    "    data_reformed=np.empty([data.shape[0],3*nb_of_points+1])\n",
    "    for polygon_index,polygon_lengths in enumerate(lengths.copy()):\n",
    "        data_reformed[polygon_index][0:3*nb_of_points:3]=polygon_lengths[0:nb_of_points]\n",
    "        data_reformed[polygon_index][1:3*(nb_of_points-1):3]=polygon_lengths[1:nb_of_points]\n",
    "        data_reformed[polygon_index][3*nb_of_points-2]=polygon_lengths[0]\n",
    "        data_reformed[polygon_index][2:3*nb_of_points:3]=angles[polygon_index]\n",
    "        #including target edge length\n",
    "        data_reformed[polygon_index][-1]=polygon_set[polygon_index][2*nb_of_points]\n",
    "        \n",
    "    return data_reformed\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.function import Function\n",
    "from math import pow\n",
    "\n",
    "class myLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target):\n",
    "        self.save_for_backward(output,target) \n",
    "\n",
    "                        \n",
    "       # output=output.view(int(output.size()[0]/2),2)\n",
    "\n",
    "        #target=target.view(int(target.size()[0]/2),2)\n",
    "        distance=torch.nn.PairwiseDistance()\n",
    "        result=distance(output,target)\n",
    "\n",
    "        result=torch.FloatTensor(result)\n",
    "        #self.save_for_backward(result)\n",
    "\n",
    "\n",
    "        return  result \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self,grad_output1):\n",
    "        input1,target=self.saved_variables\n",
    "        \n",
    "        print(input1)\n",
    "        #distance=torch.nn.PairwiseDistance()(input1.view(int(input1.size()[0]/2),2),target.view(int(target.size()[0]/2),2))\n",
    "        distance=torch.nn.PairwiseDistance()(input1,target)\n",
    "\n",
    "        grad_output1=(input1-target)/distance\n",
    "\n",
    "        \n",
    "        return grad_output1,None\n",
    "    \n",
    "    \n",
    "class myOtherLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target)->Variable:\n",
    "        \n",
    "        torch_sum=0\n",
    "        for i in range(0,output.size()[1],2):\n",
    "            euclidean_distance=(output[:,i]-target[:,i]).pow(2)+(output[:,i+1]-target[:,i+1]).pow(2)\n",
    "            torch_sum+=euclidean_distance\n",
    "        return  torch_sum\n",
    "\n",
    "def my_torch_loss_function(a,b)->Variable:\n",
    "    torch_sum=0\n",
    "    for i in range(0,a.size()[1],2):\n",
    "        euclidean_distance=torch.sqrt((a[:,i]-b[:,i]).pow(2)+(a[:,i+1]-b[:,i+1]).pow(2))\n",
    "        torch_sum+=euclidean_distance\n",
    "    return  torch_sum   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using convolution network\n",
    "# O: output dimension\n",
    "# I: Input dimensiion\n",
    "# S: Stride\n",
    "# P: padding\n",
    "# w: kernel size\n",
    "# O=(I-w-2*P)/S+1\n",
    "\n",
    "# Included batch normalization with dropout layers\n",
    "\n",
    "nb_of_points_output=1\n",
    "\n",
    "class Conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Conv_net,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,14,kernel_size=14,stride=1)\n",
    "        self.conv2=nn.Conv1d(14,28,kernel_size=2,stride=1)\n",
    "\n",
    "        self.bn1=nn.BatchNorm1d(num_features=28*9)\n",
    "        self.fc1=nn.Linear(28*9,12)\n",
    "        self.dp_1=nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.bn2=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc2=nn.Linear(12,12)\n",
    "        self.dp_2=nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.bn3=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc3=nn.Linear(12,12)\n",
    "        self.dp_3=nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.bn4=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc4=nn.Linear(12,nb_of_points_output*2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(F.max_pool1d(self.conv1(x),kernel_size=2,stride=1))\n",
    "        \n",
    "        #x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        #x=self.conv2(x)\n",
    "\n",
    "        x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        x=F.relu(self.fc1(self.bn1(x.view(-1,28*9))))\n",
    "        x=F.relu(self.fc2( self.dp_1(self.bn2(x))))\n",
    "        x=F.relu(self.fc3(self.dp_2(self.bn3(x))))\n",
    "\n",
    "        x=self.fc4(self.bn4(self.dp_3(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another convolution network involving the pairing of lengths and angles through convolution and then \n",
    "# fully connectiong in a  linear fashion\n",
    "# Input : a matrix X where lengths take X/2 and angles X/2 and the target edge length\n",
    "\n",
    "class alt_conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,nb_of_filters,nb_of_hidden_nodes,out_dimension):\n",
    "        super(alt_conv_net,self).__init__()\n",
    "        \n",
    "        self.nb_of_filters=nb_of_filters\n",
    "        \n",
    "        self.conv1=nn.Sequential(nn.Conv1d(in_channels=1,out_channels=nb_of_filters,stride=1,kernel_size=2),\n",
    "                                nn.MaxPool1d(stride=1,kernel_size=2),nn.Tanh(inplace=True))\n",
    "        self.conv2=nn.Sequential(nn.Conv1d(in_channels=1,out_channels=nb_of_filters,stride=1,kernel_size=2),\n",
    "                                nn.MaxPool1d(stride=1,kernel_size=2),nn.Tanh(inplace=True))\n",
    "        # The linear connections are fixed for 12 polygon example\n",
    "   \n",
    "        self.fc=nn.Sequential(  nn.BatchNorm1d(num_features=nb_of_filters*10+nb_of_filters*10+1),\n",
    "\n",
    "                                nn.Linear(nb_of_filters*10+nb_of_filters*10+1,nb_of_hidden_nodes),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                               nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                               nn.Linear(nb_of_hidden_nodes,out_dimension) )                                                                        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        lenghts=x.narrow(1,0,1).narrow(2,0,12)\n",
    "        angles=x.narrow(1,0,1).narrow(2,12,12)\n",
    "        target_edge_length=x.narrow(1,0,1).narrow(2,24,1).resize(x.size()[0],1)\n",
    "\n",
    "       \n",
    "        conv_result1=self.conv1(lenghts)\n",
    "        conv_result2=self.conv2(angles)        \n",
    "        \n",
    "        # reshape the convolution results\n",
    "        \n",
    "        conv_result1=conv_result1.view(-1,self.nb_of_filters*10)\n",
    "        conv_result2=conv_result2.view(-1,self.nb_of_filters*10)\n",
    "        \n",
    "        concat_tensor=torch.cat([conv_result1,conv_result2,target_edge_length],1)\n",
    "        output=self.fc(concat_tensor)\n",
    "        return output\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another convolutional network. The input of the network is : Length[index], Length[index+1], angle[index] e.g the triangles\n",
    "# formed by connecting the neighbouring edges of the contour.\n",
    "# The input is convoluted by a kernel of size 3 with a stride of 3.\n",
    "\n",
    "\n",
    "class triangle_convoluting_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,nb_of_filters,nb_of_hidden_nodes,out_dimension):\n",
    "        super(triangle_convoluting_net,self).__init__()\n",
    "        self.nb_of_filters=nb_of_filters\n",
    "        \n",
    "        self.conv=nn.Sequential(nn.Conv1d(in_channels=1,out_channels=nb_of_filters,stride=3,kernel_size=3),\n",
    "                                #nn.Conv1d(in_channels=nb_of_filters,out_channels=nb_of_filters,stride=1,kernel_size=2),\n",
    "                                #nn.MaxPool1d(stride=1,kernel_size=2),nn.ReLU(inplace=True),\n",
    "                                nn.Conv1d(in_channels=nb_of_filters,out_channels=nb_of_filters,kernel_size=2,stride=1),\n",
    "                                nn.MaxPool1d(stride=1,kernel_size=2),nn.ReLU(inplace=True))\n",
    "       \n",
    "        # The linear connections are fixed for 12 polygon example\n",
    "   \n",
    "        self.fc=nn.Sequential(  nn.BatchNorm1d(num_features= nb_of_filters*6+1),\n",
    "                              \n",
    "                                # 1rst layer\n",
    "                                nn.Linear(nb_of_filters*6+1,nb_of_hidden_nodes),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                               nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                             \n",
    "                              # 2nd layer\n",
    "                              nn.Linear(nb_of_hidden_nodes,nb_of_hidden_nodes),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                               nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                                \n",
    "                              # 3rd layer\n",
    "                              nn.Linear(nb_of_hidden_nodes,nb_of_hidden_nodes),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                              nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                              \n",
    "                            \n",
    "                              nn.Linear(nb_of_hidden_nodes,out_dimension)\n",
    "                             \n",
    "                             \n",
    "                             \n",
    "                             )      \n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        triangles=x.narrow(1,0,1).narrow(2,0,3*8)\n",
    "        target_edge_length=x.narrow(1,0,1).narrow(2,3*8,1).resize(x.size()[0],1)\n",
    "        \n",
    "        conv_result=self.conv(triangles)\n",
    "        \n",
    "        \n",
    "        \n",
    "        conv_result=conv_result.view(-1,self.nb_of_filters*6)\n",
    "        #print(conv_result,target_edge_length)\n",
    "\n",
    "        concat_tensor=torch.cat([conv_result,target_edge_length],1)\n",
    "        output=self.fc(concat_tensor)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network(filename,net):\n",
    "    path=os.path.join('networks',filename)\n",
    "\n",
    "    with open(path,'wb') as output:\n",
    "        pickle.dump(net,output)\n",
    "        \n",
    "def load_network(filename):\n",
    "    path=os.path.join('networks',filename)\n",
    "        \n",
    "    with open(path,'rb') as input:\n",
    "        net=pickle.load(input)\n",
    "        \n",
    "    net.eval()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons=load_dataset('8_polygons.pkl')\n",
    "polygons=[i for i in polygons for j in range(10)]\n",
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(2,8))\n",
    "\n",
    "polygons_reshaped=np.array(polygons_reshaped)\n",
    "#polygons_reshaped=polygons_reshaped.reshape(60000,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=load_dataset('8_point_coordinates')\n",
    "number_of_insertion_points=load_dataset('8_nb_of_points.pkl')\n",
    "#centers_of_mass=load_dataset('12_centers_of_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_points=1\n",
    "set_of_points=get_set_nb_of_points(point_coordinates)        \n",
    "indices=get_indices_nb_of_points(set_of_points,nb_of_points,point_coordinates)\n",
    "indices=np.asarray(indices)\n",
    "number_of_insertion_points=np.array(number_of_insertion_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot resize an array that references or is referenced\nby another array in this way.  Use the resize function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-253-2d64b5dd3011>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolygons_reshaped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint_coordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpolygons_reshaped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpolygons_reshaped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumber_of_insertion_points\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#polygons_reshaped=polygons_reshaped[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot resize an array that references or is referenced\nby another array in this way.  Use the resize function"
     ]
    }
   ],
   "source": [
    "polygons_reshaped.resize(len(point_coordinates),2*8)\n",
    "\n",
    "polygons_reshaped=np.hstack([polygons_reshaped[indices],number_of_insertion_points[indices,1].reshape(len(indices),1) ])\n",
    "#polygons_reshaped=polygons_reshaped[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=[ point_coordinates[i][0]for i in indices]\n",
    "point_coordinates=np.array(point_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(polygons_reshaped),len(indices)\n",
    "#point_coordinates.shape\n",
    "#centers_of_mass=centers_of_mass.reshape(60000,2)\n",
    "point_coordinates=point_coordinates.reshape(polygons_reshaped.shape[0],1,2*nb_of_points)\n",
    "#point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting angles and length of edges\n",
    "\n",
    "#data,_,_=extract_lengths_angles(polygons_reshaped)\n",
    "data_triangle=extract_lengths_angles_in_triangle_form(polygons_reshaped,8)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Project set of polygons in 2d and 3d space #\n",
    "# Dimensionality reduction using Isomap, PCA, kernel PCA ... #\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "\n",
    "Polygons_reshaped=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Isomap #\n",
    "#nb_components=8\n",
    "#iso=manifold.Isomap(n_neighbors=8,n_components=nb_components,n_jobs=-1)\n",
    "#iso.fit(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "\n",
    "                        # PCA   #\n",
    "pca=PCA(.999)\n",
    "pca.fit(polygons_reshaped)\n",
    "Polygons_projected=pca.transform(polygons_reshaped)\n",
    "nb_components=int(pca.n_components_)\n",
    "print(nb_components)\n",
    "# Fitting into lesser dimension\n",
    "\n",
    "#Polygons_projected=iso.fit_transform(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "#iso = KernelPCA(n_components=2,kernel=\"rbf\", fit_inverse_transform=True, gamma=1e-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([76672, 1, 2]) torch.Size([76672, 25])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_tensor=torch.from_numpy(data_triangle[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "#mixed_tensor=torch.cat((lengths_tensor[:nb_of_training_data],angles_tensor[:nb_of_training_data]),1)\n",
    "#double_mixed_tensor=angles_tensor[:nb_of_training_data]\n",
    "\n",
    "x_tensor_test=torch.from_numpy(data_triangle[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "#mixed_tensor_test=torch.cat((lengths_tensor[nb_of_training_data:],angles_tensor[nb_of_training_data:]),1)\n",
    "#double_mixed_tensor_test=angles_tensor[nb_of_training_data:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n",
    "#x_variable,x_variable_test=Variable(mixed_tensor),Variable(mixed_tensor_test)\n",
    "#x_variable,x_variable_test=Variable(double_mixed_tensor),Variable(double_mixed_tensor_test)\n",
    "\n",
    "y_tensor=torch.from_numpy(point_coordinates[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(point_coordinates[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "shuffle=torch.randperm(x_variable.shape[0])\n",
    "x_variable = x_variable[shuffle]\n",
    "y_variable=y_variable[shuffle]\n",
    "print(y_variable.size(),x_variable.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 17 2\n"
     ]
    }
   ],
   "source": [
    "my_net=Net(x_variable.size()[1],y_variable.size()[2],nb_of_hidden_layers=3, nb_of_hidden_nodes=20,batch_normalization=True)\n",
    "\n",
    "print(\"Training data length:\",x_variable_test.size()[1],y_variable.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=0.2)\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "#max_distance=0.6108970818704328\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=myOtherLossfunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "        \n",
    "    x_variable , y_variable=x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test,y_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "    #x_variable_test,y_variable_test= Variable(double_mixed_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 0.3912437260648444 Test Loss: 0.36413092764470734\n",
      "Epoch: 10 Training Loss: 0.14577855331471845 Test Loss: 0.1441662618035267\n",
      "Epoch: 20 Training Loss: 0.08916065529710264 Test Loss: 0.09028867489109453\n",
      "Epoch: 30 Training Loss: 0.0730989829128692 Test Loss: 0.07417344489758321\n",
      "Epoch: 40 Training Loss: 0.0658567385601878 Test Loss: 0.06713007447715594\n",
      "Epoch: 50 Training Loss: 0.05990956561991289 Test Loss: 0.06096041620474228\n",
      "Epoch: 60 Training Loss: 0.0554591233224821 Test Loss: 0.0566039746114129\n",
      "Epoch: 70 Training Loss: 0.052859940990581736 Test Loss: 0.05433836922621687\n",
      "Epoch: 80 Training Loss: 0.05126572988666159 Test Loss: 0.05281165286973243\n",
      "Epoch: 90 Training Loss: 0.050030097440009524 Test Loss: 0.05167440181980547\n",
      "Epoch: 100 Training Loss: 0.04893287583464175 Test Loss: 0.05088513522394909\n",
      "Epoch: 110 Training Loss: 0.047986153967193454 Test Loss: 0.05023567186970145\n",
      "Epoch: 120 Training Loss: 0.047063930205789355 Test Loss: 0.0497162616710631\n",
      "Epoch: 130 Training Loss: 0.046180023673380755 Test Loss: 0.04925808723462444\n",
      "Epoch: 140 Training Loss: 0.04539047736357369 Test Loss: 0.0487288155022368\n",
      "Epoch: 150 Training Loss: 0.04470875963742824 Test Loss: 0.04826731037018892\n",
      "Epoch: 160 Training Loss: 0.044113988072326867 Test Loss: 0.04793790584812578\n",
      "Epoch: 170 Training Loss: 0.043567174165595156 Test Loss: 0.0476172993298564\n",
      "Epoch: 180 Training Loss: 0.043104841832525546 Test Loss: 0.04735065899627634\n",
      "Epoch: 190 Training Loss: 0.04271152729980138 Test Loss: 0.047126502544931656\n",
      "Epoch: 200 Training Loss: 0.042344437218667666 Test Loss: 0.04687485671003593\n",
      "Epoch: 210 Training Loss: 0.04200929821234116 Test Loss: 0.046638784902123656\n",
      "Epoch: 220 Training Loss: 0.04171092850537053 Test Loss: 0.04650915843218515\n",
      "Epoch: 230 Training Loss: 0.04143846552439644 Test Loss: 0.04642640370160391\n",
      "Epoch: 240 Training Loss: 0.04121306304740587 Test Loss: 0.04635826454735757\n",
      "Epoch: 250 Training Loss: 0.04101350450754564 Test Loss: 0.04629827381573456\n",
      "Epoch: 260 Training Loss: 0.040831459384529735 Test Loss: 0.046220056600682125\n",
      "Epoch: 270 Training Loss: 0.040670156677895676 Test Loss: 0.046147895774777625\n",
      "Epoch: 280 Training Loss: 0.0405287920732132 Test Loss: 0.04605360779419964\n",
      "Epoch: 290 Training Loss: 0.04039876170468848 Test Loss: 0.04594818777552431\n",
      "Epoch: 300 Training Loss: 0.04028196098012399 Test Loss: 0.04585060094155135\n",
      "Epoch: 310 Training Loss: 0.040169422833469755 Test Loss: 0.045788922572573755\n",
      "Epoch: 320 Training Loss: 0.04006283430503883 Test Loss: 0.045748715408656354\n",
      "Epoch: 330 Training Loss: 0.039957450506882194 Test Loss: 0.045714676081636714\n",
      "Epoch: 340 Training Loss: 0.03984900428377129 Test Loss: 0.04566327001097206\n",
      "Epoch: 350 Training Loss: 0.0397477436543307 Test Loss: 0.04565574887996922\n",
      "Epoch: 360 Training Loss: 0.03965758003655181 Test Loss: 0.04564832327560909\n",
      "Epoch: 370 Training Loss: 0.039571178078850446 Test Loss: 0.04563140869140625\n",
      "Epoch: 380 Training Loss: 0.03948876634862069 Test Loss: 0.045590768473375225\n",
      "Epoch: 390 Training Loss: 0.039411922727084915 Test Loss: 0.04554668929620657\n",
      "Epoch: 400 Training Loss: 0.03934087598065104 Test Loss: 0.045491025921498396\n",
      "Epoch: 410 Training Loss: 0.03927279722710484 Test Loss: 0.045435375283675916\n",
      "Epoch: 420 Training Loss: 0.03920677398799457 Test Loss: 0.045391811950377906\n",
      "Epoch: 430 Training Loss: 0.039140965385309645 Test Loss: 0.04535439416442769\n",
      "Epoch: 440 Training Loss: 0.039069253932653564 Test Loss: 0.04533667715642607\n",
      "Epoch: 450 Training Loss: 0.03900072938810804 Test Loss: 0.04531578866388643\n",
      "Epoch: 460 Training Loss: 0.03893467112256211 Test Loss: 0.045280571174940006\n",
      "Epoch: 470 Training Loss: 0.03887252795676357 Test Loss: 0.04526907931982177\n",
      "Epoch: 480 Training Loss: 0.038812319603507624 Test Loss: 0.04524102314485731\n",
      "Epoch: 490 Training Loss: 0.03875970452377116 Test Loss: 0.045222115238043224\n",
      "Epoch: 500 Training Loss: 0.03871184706886941 Test Loss: 0.0452129064696858\n",
      "Epoch: 510 Training Loss: 0.0386656816296267 Test Loss: 0.045180841359948876\n",
      "Epoch: 520 Training Loss: 0.03862122153997023 Test Loss: 0.04516074255432231\n",
      "Epoch: 530 Training Loss: 0.03857709231082108 Test Loss: 0.045164105092145766\n",
      "Epoch: 540 Training Loss: 0.03853307711660165 Test Loss: 0.04515220247246387\n",
      "Epoch: 550 Training Loss: 0.038494412309936374 Test Loss: 0.0451399400357611\n",
      "Epoch: 560 Training Loss: 0.038459577922629995 Test Loss: 0.04513639281309507\n",
      "Epoch: 570 Training Loss: 0.0384268523854684 Test Loss: 0.045143789759462385\n",
      "Epoch: 580 Training Loss: 0.03839576413516011 Test Loss: 0.04515991465675214\n",
      "Epoch: 590 Training Loss: 0.03836802280008893 Test Loss: 0.0451638949335318\n",
      "Epoch: 600 Training Loss: 0.03834340040990228 Test Loss: 0.045173017727910776\n",
      "Epoch: 610 Training Loss: 0.0383183138199362 Test Loss: 0.04518968394284256\n",
      "Epoch: 620 Training Loss: 0.03829468998168665 Test Loss: 0.045207219450223025\n",
      "Epoch: 630 Training Loss: 0.038272656164503656 Test Loss: 0.04522169810503671\n",
      "Epoch: 640 Training Loss: 0.03825152318346282 Test Loss: 0.045229187393825324\n",
      "Epoch: 650 Training Loss: 0.03823142587044004 Test Loss: 0.0452393164021742\n",
      "Epoch: 660 Training Loss: 0.0382125437359181 Test Loss: 0.04524213762235562\n",
      "Epoch: 670 Training Loss: 0.03819256145289426 Test Loss: 0.04525665130360497\n",
      "Epoch: 680 Training Loss: 0.03817119950643167 Test Loss: 0.04527240683120957\n",
      "Epoch: 690 Training Loss: 0.03814893682333384 Test Loss: 0.045298590683976875\n",
      "Epoch: 700 Training Loss: 0.03812717555560333 Test Loss: 0.04531827235659693\n",
      "Epoch: 710 Training Loss: 0.038106418312690495 Test Loss: 0.045330805452120725\n",
      "Epoch: 720 Training Loss: 0.03808669454864349 Test Loss: 0.04534248199208153\n",
      "Epoch: 730 Training Loss: 0.03806577093016126 Test Loss: 0.045344236498086006\n",
      "Epoch: 740 Training Loss: 0.03804489875675641 Test Loss: 0.04534876446095054\n",
      "Epoch: 750 Training Loss: 0.038022371086731975 Test Loss: 0.045339734008992855\n",
      "Epoch: 760 Training Loss: 0.03799826543597825 Test Loss: 0.0453363046025195\n",
      "Epoch: 770 Training Loss: 0.037975045595821835 Test Loss: 0.04532858286556696\n",
      "Epoch: 780 Training Loss: 0.037953893111424775 Test Loss: 0.0453288567086094\n",
      "Epoch: 790 Training Loss: 0.03793425203762787 Test Loss: 0.0453273123612189\n",
      "Epoch: 800 Training Loss: 0.03791514780366162 Test Loss: 0.0453304933984212\n",
      "Epoch: 810 Training Loss: 0.03789613093677069 Test Loss: 0.04534081664427692\n",
      "Epoch: 820 Training Loss: 0.03787995917172185 Test Loss: 0.04533260772144655\n",
      "Epoch: 830 Training Loss: 0.037862327839177916 Test Loss: 0.0453424246760959\n",
      "Epoch: 840 Training Loss: 0.037843767510033606 Test Loss: 0.04534463452576397\n",
      "Epoch: 850 Training Loss: 0.03782482965163675 Test Loss: 0.045351057100375625\n",
      "Epoch: 860 Training Loss: 0.037805045884917295 Test Loss: 0.045342332333674615\n",
      "Epoch: 870 Training Loss: 0.037786906768563194 Test Loss: 0.04533223198331855\n",
      "Epoch: 880 Training Loss: 0.03776838256043066 Test Loss: 0.045331868982076244\n",
      "Epoch: 890 Training Loss: 0.0377508271516663 Test Loss: 0.04533313630220289\n",
      "Epoch: 900 Training Loss: 0.03773271340957667 Test Loss: 0.04534037722172045\n",
      "Epoch: 910 Training Loss: 0.03771661309049603 Test Loss: 0.045346809348996375\n",
      "Epoch: 920 Training Loss: 0.03770155261872408 Test Loss: 0.04536341187949969\n",
      "Epoch: 930 Training Loss: 0.0376867324561627 Test Loss: 0.04537777908656354\n",
      "Epoch: 940 Training Loss: 0.03767159407047278 Test Loss: 0.045385707797908625\n",
      "Epoch: 950 Training Loss: 0.03765770569070552 Test Loss: 0.045386777696306994\n",
      "Epoch: 960 Training Loss: 0.03764285945733123 Test Loss: 0.04538111615061561\n",
      "Epoch: 970 Training Loss: 0.03762889346216676 Test Loss: 0.04537967051408924\n",
      "Epoch: 980 Training Loss: 0.037615444007421374 Test Loss: 0.045370987142266735\n",
      "Epoch: 990 Training Loss: 0.03760372189329144 Test Loss: 0.045363685722542124\n",
      "Epoch: 1000 Training Loss: 0.03759270677582449 Test Loss: 0.04535783630778674\n",
      "Epoch: 1010 Training Loss: 0.037583408550746454 Test Loss: 0.045359285128534536\n",
      "Epoch: 1020 Training Loss: 0.03757398494694985 Test Loss: 0.04535597672247529\n",
      "Epoch: 1030 Training Loss: 0.03756424172692784 Test Loss: 0.04535285936970145\n",
      "Epoch: 1040 Training Loss: 0.037554475321793596 Test Loss: 0.04534368244355828\n",
      "Epoch: 1050 Training Loss: 0.037545717220274555 Test Loss: 0.045336753577740245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1060 Training Loss: 0.037537887418807446 Test Loss: 0.04533652749801916\n",
      "Epoch: 1070 Training Loss: 0.03752943231585826 Test Loss: 0.04532991387012208\n",
      "Epoch: 1080 Training Loss: 0.03752135991652143 Test Loss: 0.045321641262863234\n",
      "Epoch: 1090 Training Loss: 0.03751339856690676 Test Loss: 0.045304159887246974\n",
      "Epoch: 1100 Training Loss: 0.03750600788947537 Test Loss: 0.0452897226273118\n",
      "Epoch: 1110 Training Loss: 0.037497997781470144 Test Loss: 0.04527174451315343\n",
      "Epoch: 1120 Training Loss: 0.03748998269811894 Test Loss: 0.045260615659277505\n",
      "Epoch: 1130 Training Loss: 0.03748232106334578 Test Loss: 0.04524270759799046\n",
      "Epoch: 1140 Training Loss: 0.03747535368834991 Test Loss: 0.04523798539761908\n",
      "Epoch: 1150 Training Loss: 0.03746794211446542 Test Loss: 0.0452296682112603\n",
      "Epoch: 1160 Training Loss: 0.03746090100484221 Test Loss: 0.04522411492909732\n",
      "Epoch: 1170 Training Loss: 0.03745386875133483 Test Loss: 0.04520877653449923\n",
      "Epoch: 1180 Training Loss: 0.03744679211774135 Test Loss: 0.045202022800859505\n",
      "Epoch: 1190 Training Loss: 0.037439760859303166 Test Loss: 0.0451879580948309\n",
      "Epoch: 1200 Training Loss: 0.03743265676179991 Test Loss: 0.04518536932281341\n",
      "Epoch: 1210 Training Loss: 0.0374260123862647 Test Loss: 0.04517237451518318\n",
      "Epoch: 1220 Training Loss: 0.03741971230467094 Test Loss: 0.045156794119756885\n",
      "Epoch: 1230 Training Loss: 0.037412398944116 Test Loss: 0.04514578308207364\n",
      "Epoch: 1240 Training Loss: 0.03740602711803328 Test Loss: 0.0451419397268152\n",
      "Epoch: 1250 Training Loss: 0.0373985635020299 Test Loss: 0.04513123437438863\n",
      "Epoch: 1260 Training Loss: 0.03739239705822902 Test Loss: 0.045123117793979546\n",
      "Epoch: 1270 Training Loss: 0.037385330972369006 Test Loss: 0.045119755256156094\n",
      "Epoch: 1280 Training Loss: 0.03737793502107088 Test Loss: 0.045102678276660646\n",
      "Epoch: 1290 Training Loss: 0.03737082704279777 Test Loss: 0.04510203187971163\n",
      "Epoch: 1300 Training Loss: 0.03736363995652366 Test Loss: 0.045097414758647224\n",
      "Epoch: 1310 Training Loss: 0.037355274111281256 Test Loss: 0.04508883009768885\n",
      "Epoch: 1320 Training Loss: 0.03734694030726692 Test Loss: 0.04507003682284602\n",
      "Epoch: 1330 Training Loss: 0.03733880899983376 Test Loss: 0.045055363930525484\n",
      "Epoch: 1340 Training Loss: 0.03733022573197226 Test Loss: 0.045039210375242916\n",
      "Epoch: 1350 Training Loss: 0.037321042536893154 Test Loss: 0.045022652423839536\n",
      "Epoch: 1360 Training Loss: 0.03731186998905443 Test Loss: 0.045018178592739204\n",
      "Epoch: 1370 Training Loss: 0.037303943267847736 Test Loss: 0.045010931304778797\n",
      "Epoch: 1380 Training Loss: 0.03729508864461679 Test Loss: 0.044999738766474395\n",
      "Epoch: 1390 Training Loss: 0.0372860617749082 Test Loss: 0.04499124644793732\n",
      "Epoch: 1400 Training Loss: 0.037277374820836595 Test Loss: 0.04498853985972715\n",
      "Epoch: 1410 Training Loss: 0.037268957331502976 Test Loss: 0.04497727726855143\n",
      "Epoch: 1420 Training Loss: 0.03726017853254468 Test Loss: 0.04496619299377543\n",
      "Epoch: 1430 Training Loss: 0.037251066285899165 Test Loss: 0.04495783759675957\n",
      "Epoch: 1440 Training Loss: 0.037239738716705016 Test Loss: 0.044963690195736386\n",
      "Epoch: 1450 Training Loss: 0.03722775858113284 Test Loss: 0.04496375388016486\n",
      "Epoch: 1460 Training Loss: 0.03721476029076043 Test Loss: 0.04497549092033271\n",
      "Epoch: 1470 Training Loss: 0.03720032900124042 Test Loss: 0.044978522299128104\n",
      "Epoch: 1480 Training Loss: 0.03718779600522355 Test Loss: 0.04498695730167956\n",
      "Epoch: 1490 Training Loss: 0.037174531732855336 Test Loss: 0.04498061114838207\n",
      "Epoch: 1500 Training Loss: 0.037161826689374663 Test Loss: 0.044975156577083224\n",
      "Epoch: 1510 Training Loss: 0.03714981719926124 Test Loss: 0.04498865130747698\n",
      "Epoch: 1520 Training Loss: 0.03713742281638322 Test Loss: 0.04500044884785189\n",
      "Epoch: 1530 Training Loss: 0.03712639426548214 Test Loss: 0.04500710068640605\n",
      "Epoch: 1540 Training Loss: 0.03711466916614462 Test Loss: 0.04499299140127752\n",
      "Epoch: 1550 Training Loss: 0.0371035827022164 Test Loss: 0.04499530633025257\n",
      "Epoch: 1560 Training Loss: 0.03709211044996131 Test Loss: 0.04499408677344728\n",
      "Epoch: 1570 Training Loss: 0.03707922808515011 Test Loss: 0.04499373650909067\n",
      "Epoch: 1580 Training Loss: 0.03706509969469303 Test Loss: 0.044987103775865046\n",
      "Epoch: 1590 Training Loss: 0.03705093578026569 Test Loss: 0.044978917142584646\n",
      "Epoch: 1600 Training Loss: 0.03703759588263867 Test Loss: 0.04497363133502126\n",
      "Epoch: 1610 Training Loss: 0.037023539037656705 Test Loss: 0.044969434531184786\n",
      "Epoch: 1620 Training Loss: 0.037011284063972895 Test Loss: 0.04496820542171523\n",
      "Epoch: 1630 Training Loss: 0.03699841990892796 Test Loss: 0.04496655599501774\n",
      "Epoch: 1640 Training Loss: 0.03698478387870852 Test Loss: 0.04496687760138154\n",
      "Epoch: 1650 Training Loss: 0.036971941615185876 Test Loss: 0.04496635220484662\n",
      "Epoch: 1660 Training Loss: 0.0369602611944552 Test Loss: 0.04496273811353069\n",
      "Epoch: 1670 Training Loss: 0.03694916109807901 Test Loss: 0.0449586463890012\n",
      "Epoch: 1680 Training Loss: 0.036937065235959465 Test Loss: 0.044950848230734496\n",
      "Epoch: 1690 Training Loss: 0.036923334674166516 Test Loss: 0.04494135288244894\n",
      "Epoch: 1700 Training Loss: 0.03691027966485 Test Loss: 0.04493734076345504\n",
      "Epoch: 1710 Training Loss: 0.036896400638733166 Test Loss: 0.04493809542393246\n",
      "Epoch: 1720 Training Loss: 0.03688542940381771 Test Loss: 0.0449396970873086\n",
      "Epoch: 1730 Training Loss: 0.036874970231509964 Test Loss: 0.04494442565612283\n",
      "Epoch: 1740 Training Loss: 0.03686316164586699 Test Loss: 0.0449426361236827\n",
      "Epoch: 1750 Training Loss: 0.03684871493277446 Test Loss: 0.04493360248750359\n",
      "Epoch: 1760 Training Loss: 0.03683485740015224 Test Loss: 0.04492055036388773\n",
      "Epoch: 1770 Training Loss: 0.03682245804192825 Test Loss: 0.04491971291365329\n",
      "Epoch: 1780 Training Loss: 0.036809350890985915 Test Loss: 0.044913965393983464\n",
      "Epoch: 1790 Training Loss: 0.03679543345519816 Test Loss: 0.04490739316096489\n",
      "Epoch: 1800 Training Loss: 0.03678071061040404 Test Loss: 0.044916531876450984\n",
      "Epoch: 1810 Training Loss: 0.036768003974812655 Test Loss: 0.04491889775296882\n",
      "Epoch: 1820 Training Loss: 0.03675416962730268 Test Loss: 0.04491679935105058\n",
      "Epoch: 1830 Training Loss: 0.03674217605829637 Test Loss: 0.04492260100248461\n",
      "Epoch: 1840 Training Loss: 0.03672894661334998 Test Loss: 0.04492498280010956\n",
      "Epoch: 1850 Training Loss: 0.036717681932528945 Test Loss: 0.04493124616365003\n",
      "Epoch: 1860 Training Loss: 0.03670624649783407 Test Loss: 0.04495018909689978\n",
      "Epoch: 1870 Training Loss: 0.036696352126801356 Test Loss: 0.0449508354938488\n",
      "Epoch: 1880 Training Loss: 0.03668653964996338 Test Loss: 0.044949730569014766\n",
      "Epoch: 1890 Training Loss: 0.0366760430630539 Test Loss: 0.04496193887395333\n",
      "Epoch: 1900 Training Loss: 0.0366666429428903 Test Loss: 0.04496023213127021\n",
      "Epoch: 1910 Training Loss: 0.03665760880917659 Test Loss: 0.04497582844780363\n",
      "Epoch: 1920 Training Loss: 0.036648169184765954 Test Loss: 0.045000856428194125\n",
      "Epoch: 1930 Training Loss: 0.03663854577107501 Test Loss: 0.04501171462324904\n",
      "Epoch: 1940 Training Loss: 0.036630278935217496 Test Loss: 0.04501416328952388\n",
      "Epoch: 1950 Training Loss: 0.0366202181886154 Test Loss: 0.04503874866313648\n",
      "Epoch: 1960 Training Loss: 0.036610778962232436 Test Loss: 0.04504929480449186\n",
      "Epoch: 1970 Training Loss: 0.03660221659082403 Test Loss: 0.045061687794273005\n",
      "Epoch: 1980 Training Loss: 0.03659259874952058 Test Loss: 0.04506211766416521\n",
      "Epoch: 1990 Training Loss: 0.03658253700784928 Test Loss: 0.04505321139684305\n",
      "Epoch: 2000 Training Loss: 0.03657260263503494 Test Loss: 0.04505691146213742\n",
      "Epoch: 2010 Training Loss: 0.03656245690952358 Test Loss: 0.04507301088565578\n",
      "Epoch: 2020 Training Loss: 0.03655351710836955 Test Loss: 0.04508606619349306\n",
      "Epoch: 2030 Training Loss: 0.036544793137723894 Test Loss: 0.04510753421433183\n",
      "Epoch: 2040 Training Loss: 0.03653572288299841 Test Loss: 0.045124248192584974\n",
      "Epoch: 2050 Training Loss: 0.03652415162135842 Test Loss: 0.04512485001043406\n",
      "Epoch: 2060 Training Loss: 0.03651317003772334 Test Loss: 0.04513611260160978\n",
      "Epoch: 2070 Training Loss: 0.03650371280058796 Test Loss: 0.045135826021681646\n",
      "Epoch: 2080 Training Loss: 0.0364946243360962 Test Loss: 0.04513753594858619\n",
      "Epoch: 2090 Training Loss: 0.03648635869432172 Test Loss: 0.045147292403028484\n",
      "Epoch: 2100 Training Loss: 0.03647792986119928 Test Loss: 0.04514056095893872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2110 Training Loss: 0.0364710104485386 Test Loss: 0.04513177887625209\n",
      "Epoch: 2120 Training Loss: 0.03646510073259 Test Loss: 0.045130530661453984\n",
      "Epoch: 2130 Training Loss: 0.036459235695248254 Test Loss: 0.04512614917277494\n",
      "Epoch: 2140 Training Loss: 0.03645301959749454 Test Loss: 0.04511194436100369\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-274-a9393b444b63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# clear gradients for next train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mloss2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# backpropagation, compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#print(t,loss1.data[0],loss2.data[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvolatile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=int(x_variable.size()[0]/32 )\n",
    "nb_of_epochs=13000\n",
    "my_net.cuda()\n",
    "#my_net.cpu()\n",
    "\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss1=0\n",
    "    sum_loss2=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss2= loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "        \n",
    "        #loss1=my_torch_loss_function(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*2)).sum()\n",
    "        #sum_loss1+=loss1.data[0]\n",
    "        sum_loss2+=loss2.data[0]\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss2.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss1.data[0],loss2.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0: \n",
    "        my_net.eval()\n",
    "        test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "        #my_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss2/(x_variable.size(0)),\"Test Loss:\",test_loss/x_variable_test.size(0))\n",
    "       # print(\"Epoch:\",t,\"Training Loss:\",sum_loss1/(x_variable.size(0)),sum_loss2/(x_variable.size(0)))\n",
    "        my_net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.2569781749782649 0.05046506055488852 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 1 elements not 17",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-286-c3b5d1833b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmy_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mpredicted_inserted_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpredicted_inserted_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredicted_inserted_points\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_of_points\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\python\\Dimensionality_reduction\\Neural_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mACTIVATION\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mpre_activation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_bn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# input batch normalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mlayer_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_hidden_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     35\u001b[0m         return F.batch_norm(\n\u001b[0;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1011\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 1 elements not 17"
     ]
    }
   ],
   "source": [
    "# Visualization of error\n",
    "\n",
    "sample_index=264\n",
    "sample_contour=np.delete(polygons_reshaped[sample_index],16)\n",
    "#sample_contour=polygons_reshaped[sample_index]\n",
    "sample_contour=sample_contour.reshape(8,2)\n",
    "_,inserted_point_coordinates=get_extrapoints_target_length(sample_contour,.8)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "\n",
    "my_net.eval()\n",
    "predictions=my_net(x_variable_test).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(nb_of_points,2)\n",
    "\n",
    "\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "#plt.scatter(polygons_reshaped[3112][0::2].sum()/12,polygons_reshaped[312][1::2].sum()/12,label='Original points')\n",
    "\n",
    "plot_contour(sample_contour)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "#original_points=point_coordinates.reshape(218722,2)\n",
    "#plt.scatter(original_points[:,0],original_points[:,1],label='Original points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_network('8_point_regression_FNN.pkl',my_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNN_net=load_network('8_point_regression_FNN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 24 is out of bounds for axis 0 with size 17",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-285-6583f842db11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2641\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msample_contour\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolygons_reshaped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#sample_contour=polygons_reshaped[sample_index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample_contour\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_contour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minserted_point_coordinates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_extrapoints_target_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_contour\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(arr, obj, axis)\u001b[0m\n\u001b[0;32m   4856\u001b[0m             raise IndexError(\n\u001b[0;32m   4857\u001b[0m                 \u001b[1;34m\"index %i is out of bounds for axis %i with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4858\u001b[1;33m                 \"size %i\" % (obj, axis, N))\n\u001b[0m\u001b[0;32m   4859\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4860\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 24 is out of bounds for axis 0 with size 17"
     ]
    }
   ],
   "source": [
    "sample_index=2641\n",
    "sample_contour=np.delete(polygons_reshaped[sample_index],24)\n",
    "#sample_contour=polygons_reshaped[sample_index]\n",
    "sample_contour=sample_contour.reshape(12,2)\n",
    "_,inserted_point_coordinates=get_extrapoints_target_length(sample_contour,8)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "\n",
    "predictions=FNN_net(x_variable_test).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(nb_of_points,2)\n",
    "\n",
    "\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "#plt.scatter(polygons_reshaped[3112][0::2].sum()/12,polygons_reshaped[312][1::2].sum()/12,label='Original points')\n",
    "\n",
    "plot_contour(sample_contour)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plotting the convex hull of inserted points \n",
    "\n",
    "plt.scatter(centers_of_mass[10][0],centers_of_mass[10][1],label='Original points'\n",
    "           )\n",
    "plot_contour(polygons[2])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "points=centers_of_mass\n",
    "hull=ConvexHull(points)\n",
    "hull2=ConvexHull(original_points)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(points[:,0], points[:,1], 'o')\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n",
    "for simplex in hull2.simplices:\n",
    "    plt.plot(original_points[simplex, 0], original_points[simplex, 1], 'k-')\n",
    "\n",
    "    \n",
    "    \n",
    "convex_points=centers_of_mass[hull.vertices]\n",
    "convex_points2=original_points[hull2.vertices]\n",
    "\n",
    "distances=np.empty([convex_points.shape[0],convex_points.shape[0]])    \n",
    "distances2=np.empty([convex_points2.shape[0],convex_points2.shape[0]])    \n",
    "\n",
    "    \n",
    "for i,point_i in enumerate(convex_points):\n",
    "    for j,point_j in enumerate(convex_points):\n",
    "        distances[i][j]=np.linalg.norm(point_i-point_j)\n",
    "for i,point_i in enumerate(convex_points2):\n",
    "    for j,point_j in enumerate(convex_points2):\n",
    "        distances2[i][j]=np.linalg.norm(point_i-point_j)\n",
    "        \n",
    "def max_element(A):\n",
    "    r, (c, l) = max(map(lambda t: (t[0], max(enumerate(t[1]), key=lambda v: v[1])), enumerate(A)), key=lambda v: v[1][1])\n",
    "    return (l, r, c)\n",
    "\n",
    "print(max_element(distances))\n",
    "print(max_element(distances2))\n",
    "\n",
    "maximum_distance_points=convex_points[[0,11]]\n",
    "maximum_distance_points2=convex_points2[[3,11]]\n",
    "\n",
    "plt.plot(maximum_distance_points[:,0],maximum_distance_points[:,1])\n",
    "plt.plot(maximum_distance_points2[:,0],maximum_distance_points2[:,1])\n",
    "1.8508252250365/0.6108970818704328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distances=[np.linalg.norm(i-j) for i in centers_of_mass for j in predicted_inserted_points]\n",
    "distances=np.array(distances)\n",
    "100*distances.sum()/(x_variable.size(0) *max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([76672, 25])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable,x_variable_test=x_variable.resize(x_variable.size()[0],1,x_variable.size()[1]),Variable(x_tensor_test.view(x_variable_test.size()[0],1,x_variable_test.size()[1]),volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([76672, 1, 2])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_variable.size()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triangle_convoluting_net(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv1d(1, 3, kernel_size=(3,), stride=(3,))\n",
      "    (1): Conv1d(3, 3, kernel_size=(2,), stride=(1,))\n",
      "    (2): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU(inplace)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): BatchNorm1d(19, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): Linear(in_features=19, out_features=20, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): Linear(in_features=20, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "my_conv_net=triangle_convoluting_net(nb_of_filters=3,out_dimension=2*nb_of_points,nb_of_hidden_nodes=20)\n",
    "print(my_conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76672, 76672)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(my_conv_net.parameters(), lr=1e-4,weight_decay=0)\n",
    "\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=torch.nn.SmoothL1Loss()\n",
    "x_variable.size()[0],y_variable.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    my_conv_net.cuda()\n",
    "    x_variable , y_variable,x_variable_test,y_variable_test= x_variable.cuda(), y_variable.cuda(),x_variable_test.cuda(),y_variable_test.cuda()\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 0.342637263474759 Test Loss: 0.10341109337910188\n",
      "Epoch: 10 Training Loss: 0.10393777753355507 Test Loss: 0.10251571578852123\n",
      "Epoch: 20 Training Loss: 0.08156946007914853 Test Loss: 0.08168588934438256\n",
      "Epoch: 30 Training Loss: 0.07429209734641253 Test Loss: 0.07521713715363823\n",
      "Epoch: 40 Training Loss: 0.07000375689568623 Test Loss: 0.07117099793805105\n",
      "Epoch: 50 Training Loss: 0.0670653603510785 Test Loss: 0.06835124886692664\n",
      "Epoch: 60 Training Loss: 0.06479949923309937 Test Loss: 0.06601516392473784\n",
      "Epoch: 70 Training Loss: 0.06286543120127887 Test Loss: 0.064062408294423\n",
      "Epoch: 80 Training Loss: 0.06079361116348801 Test Loss: 0.06210059673837907\n",
      "Epoch: 90 Training Loss: 0.058530415437854394 Test Loss: 0.059982236120259026\n",
      "Epoch: 100 Training Loss: 0.0572348019117505 Test Loss: 0.05901767176658363\n",
      "Epoch: 110 Training Loss: 0.05625218769942779 Test Loss: 0.05832276638839798\n",
      "Epoch: 120 Training Loss: 0.05538744828935856 Test Loss: 0.057691723755086605\n",
      "Epoch: 130 Training Loss: 0.054562874648328215 Test Loss: 0.057104655219636896\n",
      "Epoch: 140 Training Loss: 0.05386111752218715 Test Loss: 0.05672320459840294\n",
      "Epoch: 150 Training Loss: 0.05326602614184652 Test Loss: 0.05638081800559526\n",
      "Epoch: 160 Training Loss: 0.05274577670184917 Test Loss: 0.05602621037096332\n",
      "Epoch: 170 Training Loss: 0.052267595702698316 Test Loss: 0.05575893956154137\n",
      "Epoch: 180 Training Loss: 0.05183063783311287 Test Loss: 0.05549454728828647\n",
      "Epoch: 190 Training Loss: 0.05145787247433288 Test Loss: 0.055284350463663395\n",
      "Epoch: 200 Training Loss: 0.05114442478237247 Test Loss: 0.055108352177131156\n",
      "Epoch: 210 Training Loss: 0.05086234983100318 Test Loss: 0.05496584553153368\n",
      "Epoch: 220 Training Loss: 0.0505814220949087 Test Loss: 0.05489557613315487\n",
      "Epoch: 230 Training Loss: 0.050314590011494784 Test Loss: 0.05476009388001813\n",
      "Epoch: 240 Training Loss: 0.05004886494256021 Test Loss: 0.05463714472240518\n",
      "Epoch: 250 Training Loss: 0.04977959076032814 Test Loss: 0.054528492718984765\n",
      "Epoch: 260 Training Loss: 0.04953545073236965 Test Loss: 0.05450455374232119\n",
      "Epoch: 270 Training Loss: 0.04932398837874449 Test Loss: 0.05452444238933379\n",
      "Epoch: 280 Training Loss: 0.04914599338636574 Test Loss: 0.054526849660730126\n",
      "Epoch: 290 Training Loss: 0.048994984571046146 Test Loss: 0.054571275918033964\n",
      "Epoch: 300 Training Loss: 0.04885744162911366 Test Loss: 0.0546200900324597\n",
      "Epoch: 310 Training Loss: 0.04873764087043342 Test Loss: 0.05466475829059174\n",
      "Epoch: 320 Training Loss: 0.04862799389732501 Test Loss: 0.05470459290060257\n",
      "Epoch: 330 Training Loss: 0.04852446679877916 Test Loss: 0.054778046520405105\n",
      "Epoch: 340 Training Loss: 0.04843292580622066 Test Loss: 0.054834604661333344\n",
      "Epoch: 350 Training Loss: 0.04834056617023551 Test Loss: 0.05489379933760043\n",
      "Epoch: 360 Training Loss: 0.048255007533676834 Test Loss: 0.054949408580544395\n",
      "Epoch: 370 Training Loss: 0.04817044963820749 Test Loss: 0.054971003970240115\n",
      "Epoch: 380 Training Loss: 0.04808669028576706 Test Loss: 0.05501708602268429\n",
      "Epoch: 390 Training Loss: 0.048007487952211665 Test Loss: 0.05507875165477619\n",
      "Epoch: 400 Training Loss: 0.04793696391562587 Test Loss: 0.055118911055372234\n",
      "Epoch: 410 Training Loss: 0.04786868644676145 Test Loss: 0.05516997959856597\n",
      "Epoch: 420 Training Loss: 0.047800850987633405 Test Loss: 0.05517825857426766\n",
      "Epoch: 430 Training Loss: 0.04773782370285518 Test Loss: 0.05516910712189587\n",
      "Epoch: 440 Training Loss: 0.047678511409409256 Test Loss: 0.055186098127412875\n",
      "Epoch: 450 Training Loss: 0.0476279680637366 Test Loss: 0.055188900242265755\n",
      "Epoch: 460 Training Loss: 0.04758272634722752 Test Loss: 0.05520128686360405\n",
      "Epoch: 470 Training Loss: 0.047539043705133045 Test Loss: 0.05521239979637286\n",
      "Epoch: 480 Training Loss: 0.04749821432444806 Test Loss: 0.05521443769808405\n",
      "Epoch: 490 Training Loss: 0.047451573341637104 Test Loss: 0.055210521105732865\n",
      "Epoch: 500 Training Loss: 0.04740448109494625 Test Loss: 0.05522559520995279\n",
      "Epoch: 510 Training Loss: 0.04736100353661284 Test Loss: 0.0552246972595113\n",
      "Epoch: 520 Training Loss: 0.047315684702241165 Test Loss: 0.055243789851167964\n",
      "Epoch: 530 Training Loss: 0.04727312817995457 Test Loss: 0.055271842841911\n",
      "Epoch: 540 Training Loss: 0.04723069693687961 Test Loss: 0.05529364839022068\n",
      "Epoch: 550 Training Loss: 0.047182102036197514 Test Loss: 0.055312454401949214\n",
      "Epoch: 560 Training Loss: 0.04713556294847211 Test Loss: 0.05534700320439665\n",
      "Epoch: 570 Training Loss: 0.04708907172357499 Test Loss: 0.05535816071626539\n",
      "Epoch: 580 Training Loss: 0.047045906915489544 Test Loss: 0.055385010071310255\n",
      "Epoch: 590 Training Loss: 0.04699797015357296 Test Loss: 0.055428761273672265\n",
      "Epoch: 600 Training Loss: 0.046952598082999356 Test Loss: 0.055453330726177745\n",
      "Epoch: 610 Training Loss: 0.04691424463348516 Test Loss: 0.05549063069593528\n",
      "Epoch: 620 Training Loss: 0.046878503937156056 Test Loss: 0.05551148734626069\n",
      "Epoch: 630 Training Loss: 0.04684155552534508 Test Loss: 0.05552798798167845\n",
      "Epoch: 640 Training Loss: 0.04680563710965776 Test Loss: 0.055546934099149625\n",
      "Epoch: 650 Training Loss: 0.046766870706427675 Test Loss: 0.05557045912702812\n",
      "Epoch: 660 Training Loss: 0.04673060540961901 Test Loss: 0.05560900094314091\n",
      "Epoch: 670 Training Loss: 0.04669379113313551 Test Loss: 0.05564167105494835\n",
      "Epoch: 680 Training Loss: 0.04665885455222281 Test Loss: 0.05564373443043093\n",
      "Epoch: 690 Training Loss: 0.04662409579017524 Test Loss: 0.0556552995226419\n",
      "Epoch: 700 Training Loss: 0.046591177010575996 Test Loss: 0.05565457352015729\n",
      "Epoch: 710 Training Loss: 0.04655977541297823 Test Loss: 0.055670278100219114\n",
      "Epoch: 720 Training Loss: 0.04653029002012912 Test Loss: 0.05568171582357314\n",
      "Epoch: 730 Training Loss: 0.046502922234033704 Test Loss: 0.05568320603919945\n",
      "Epoch: 740 Training Loss: 0.04647502397655048 Test Loss: 0.05567966518497626\n",
      "Epoch: 750 Training Loss: 0.04644740543301794 Test Loss: 0.055678856392734635\n",
      "Epoch: 760 Training Loss: 0.04642187832591928 Test Loss: 0.05571453240956607\n",
      "Epoch: 770 Training Loss: 0.04639392573765801 Test Loss: 0.055728504773173385\n",
      "Epoch: 780 Training Loss: 0.04637001959827786 Test Loss: 0.05574455961759182\n",
      "Epoch: 790 Training Loss: 0.04634720753747752 Test Loss: 0.05576490042404659\n",
      "Epoch: 800 Training Loss: 0.04632378868348212 Test Loss: 0.0557541250187487\n",
      "Epoch: 810 Training Loss: 0.046300667753203684 Test Loss: 0.05575614381513134\n",
      "Epoch: 820 Training Loss: 0.046277314971802826 Test Loss: 0.055773211241962516\n",
      "Epoch: 830 Training Loss: 0.04625776833007252 Test Loss: 0.055762556837078724\n",
      "Epoch: 840 Training Loss: 0.04623760733262127 Test Loss: 0.05577607067280102\n",
      "Epoch: 850 Training Loss: 0.0462201162053269 Test Loss: 0.05578389748906054\n",
      "Epoch: 860 Training Loss: 0.046202011816887704 Test Loss: 0.05579443726197308\n",
      "Epoch: 870 Training Loss: 0.046183685030483444 Test Loss: 0.0557863748133282\n",
      "Epoch: 880 Training Loss: 0.04616737196560893 Test Loss: 0.055785693389943526\n",
      "Epoch: 890 Training Loss: 0.046151375034217644 Test Loss: 0.05579786985266786\n",
      "Epoch: 900 Training Loss: 0.04613754765219203 Test Loss: 0.05580183739256182\n",
      "Epoch: 910 Training Loss: 0.0461225250925564 Test Loss: 0.05581712802383856\n",
      "Epoch: 920 Training Loss: 0.046109629393817984 Test Loss: 0.0558075116751389\n",
      "Epoch: 930 Training Loss: 0.046095370350775614 Test Loss: 0.05579903527770894\n",
      "Epoch: 940 Training Loss: 0.046080115541989895 Test Loss: 0.05579132309342068\n",
      "Epoch: 950 Training Loss: 0.04606672429879241 Test Loss: 0.05577535740720211\n",
      "Epoch: 960 Training Loss: 0.046051743830583726 Test Loss: 0.05577658014822882\n",
      "Epoch: 970 Training Loss: 0.04603641419259455 Test Loss: 0.05577884094543967\n",
      "Epoch: 980 Training Loss: 0.04602297031023665 Test Loss: 0.05580583677467002\n",
      "Epoch: 990 Training Loss: 0.04600792635661334 Test Loss: 0.05580676656732575\n",
      "Epoch: 1000 Training Loss: 0.04599512051262322 Test Loss: 0.0558215222494033\n",
      "Epoch: 1010 Training Loss: 0.045981191036498205 Test Loss: 0.0558360422990955\n",
      "Epoch: 1020 Training Loss: 0.04596887964239105 Test Loss: 0.05584119436935909\n",
      "Epoch: 1030 Training Loss: 0.04595503594123063 Test Loss: 0.055845397541638406\n",
      "Epoch: 1040 Training Loss: 0.045941271348071216 Test Loss: 0.05585371472799718\n",
      "Epoch: 1050 Training Loss: 0.045928337836703394 Test Loss: 0.055864592028380636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1060 Training Loss: 0.04591508530615168 Test Loss: 0.05588955632434266\n",
      "Epoch: 1070 Training Loss: 0.04590160769094808 Test Loss: 0.05588830810954455\n",
      "Epoch: 1080 Training Loss: 0.04588915469849448 Test Loss: 0.055907044068401766\n",
      "Epoch: 1090 Training Loss: 0.045874359014635296 Test Loss: 0.0559322121545349\n",
      "Epoch: 1100 Training Loss: 0.0458596276122461 Test Loss: 0.055958367349309396\n",
      "Epoch: 1110 Training Loss: 0.0458460172548119 Test Loss: 0.055986407603166734\n",
      "Epoch: 1120 Training Loss: 0.045832733280073625 Test Loss: 0.05601036568515886\n",
      "Epoch: 1130 Training Loss: 0.04581974543792776 Test Loss: 0.05603387160770881\n",
      "Epoch: 1140 Training Loss: 0.04580683272350611 Test Loss: 0.056046003491333214\n",
      "Epoch: 1150 Training Loss: 0.04579445077899302 Test Loss: 0.05605982301231219\n",
      "Epoch: 1160 Training Loss: 0.04578086818398936 Test Loss: 0.05608215714137821\n",
      "Epoch: 1170 Training Loss: 0.04576584433076378 Test Loss: 0.056096607138199084\n",
      "Epoch: 1180 Training Loss: 0.04575339850280838 Test Loss: 0.05611837447585168\n",
      "Epoch: 1190 Training Loss: 0.045741623251983436 Test Loss: 0.056150019268360676\n",
      "Epoch: 1200 Training Loss: 0.04573047121299527 Test Loss: 0.05616920738666006\n",
      "Epoch: 1210 Training Loss: 0.045719164142226536 Test Loss: 0.05616562832177979\n",
      "Epoch: 1220 Training Loss: 0.045707347198002325 Test Loss: 0.05618625570816269\n",
      "Epoch: 1230 Training Loss: 0.04569677916512466 Test Loss: 0.05620612524984675\n",
      "Epoch: 1240 Training Loss: 0.04568558334309191 Test Loss: 0.05623323571104836\n",
      "Epoch: 1250 Training Loss: 0.045672669036559546 Test Loss: 0.05626812204096671\n",
      "Epoch: 1260 Training Loss: 0.04566160943949959 Test Loss: 0.05627550943466976\n",
      "Epoch: 1270 Training Loss: 0.04564946959532163 Test Loss: 0.056276897755210505\n",
      "Epoch: 1280 Training Loss: 0.045638682746728 Test Loss: 0.0563136946179831\n",
      "Epoch: 1290 Training Loss: 0.04562676818621576 Test Loss: 0.05629710482436548\n",
      "Epoch: 1300 Training Loss: 0.04561577983611016 Test Loss: 0.05630696317389334\n",
      "Epoch: 1310 Training Loss: 0.04560296911628099 Test Loss: 0.05632441907573821\n",
      "Epoch: 1320 Training Loss: 0.045592709057319145 Test Loss: 0.05633639174829142\n",
      "Epoch: 1330 Training Loss: 0.04558158816399678 Test Loss: 0.05635166964268246\n",
      "Epoch: 1340 Training Loss: 0.04557114859455217 Test Loss: 0.0563592162474567\n",
      "Epoch: 1350 Training Loss: 0.045560627727954335 Test Loss: 0.05638232095810726\n",
      "Epoch: 1360 Training Loss: 0.04555165608458606 Test Loss: 0.056391918201478375\n",
      "Epoch: 1370 Training Loss: 0.04554188251495361 Test Loss: 0.05639929285829573\n",
      "Epoch: 1380 Training Loss: 0.04553316570880616 Test Loss: 0.056404948035544265\n",
      "Epoch: 1390 Training Loss: 0.04552451328363562 Test Loss: 0.05641195969111931\n",
      "Epoch: 1400 Training Loss: 0.04551754451554287 Test Loss: 0.056411450215691515\n",
      "Epoch: 1410 Training Loss: 0.04551028936653583 Test Loss: 0.056405884196642844\n",
      "Epoch: 1420 Training Loss: 0.04550393395909483 Test Loss: 0.05640467419250183\n",
      "Epoch: 1430 Training Loss: 0.04549374932637796 Test Loss: 0.056409437787751725\n",
      "Epoch: 1440 Training Loss: 0.0454856987190565 Test Loss: 0.056415163017871583\n",
      "Epoch: 1450 Training Loss: 0.045478682784683915 Test Loss: 0.056427645165852594\n",
      "Epoch: 1460 Training Loss: 0.04546740148620733 Test Loss: 0.05643258070905937\n",
      "Epoch: 1470 Training Loss: 0.045459330380460454 Test Loss: 0.05644053489417584\n",
      "Epoch: 1480 Training Loss: 0.04545269973688014 Test Loss: 0.05644773760303631\n",
      "Epoch: 1490 Training Loss: 0.045445905404417265 Test Loss: 0.05643385439762886\n",
      "Epoch: 1500 Training Loss: 0.04543991956567525 Test Loss: 0.05643767546333733\n",
      "Epoch: 1510 Training Loss: 0.04543461694144248 Test Loss: 0.056462085704771625\n",
      "Epoch: 1520 Training Loss: 0.04542769354850502 Test Loss: 0.0564736507969826\n",
      "Epoch: 1530 Training Loss: 0.04542288129437149 Test Loss: 0.05646498971471006\n",
      "Epoch: 1540 Training Loss: 0.0454169267008022 Test Loss: 0.05646625703483671\n",
      "Epoch: 1550 Training Loss: 0.045410297151798 Test Loss: 0.05645903522064769\n",
      "Epoch: 1560 Training Loss: 0.04540537514153228 Test Loss: 0.05645734121485027\n",
      "Epoch: 1570 Training Loss: 0.04539739180088839 Test Loss: 0.05646342307776959\n",
      "Epoch: 1580 Training Loss: 0.045389284375115906 Test Loss: 0.05647551038229406\n",
      "Epoch: 1590 Training Loss: 0.04538382771417175 Test Loss: 0.05649175627999791\n",
      "Epoch: 1600 Training Loss: 0.04537807024380998 Test Loss: 0.05648948911434422\n",
      "Epoch: 1610 Training Loss: 0.04537075539065124 Test Loss: 0.05649752608921771\n",
      "Epoch: 1620 Training Loss: 0.045364200670850494 Test Loss: 0.056508390652715464\n",
      "Epoch: 1630 Training Loss: 0.04535702821209355 Test Loss: 0.0565345522159328\n",
      "Epoch: 1640 Training Loss: 0.04534994849378557 Test Loss: 0.056534278372890365\n",
      "Epoch: 1650 Training Loss: 0.0453422015815824 Test Loss: 0.05654284392852019\n",
      "Epoch: 1660 Training Loss: 0.0453365302842129 Test Loss: 0.05655053063903707\n",
      "Epoch: 1670 Training Loss: 0.0453297771476147 Test Loss: 0.05657399835092994\n",
      "Epoch: 1680 Training Loss: 0.045321951923465886 Test Loss: 0.056576004410426886\n",
      "Epoch: 1690 Training Loss: 0.04531494275556383 Test Loss: 0.05661014563253208\n",
      "Epoch: 1700 Training Loss: 0.045307071260697454 Test Loss: 0.05660295566055731\n",
      "Epoch: 1710 Training Loss: 0.045299254395129884 Test Loss: 0.05662382504776842\n",
      "Epoch: 1720 Training Loss: 0.045290200658553034 Test Loss: 0.0566418732147981\n",
      "Epoch: 1730 Training Loss: 0.04528215204136996 Test Loss: 0.056641268212727594\n",
      "Epoch: 1740 Training Loss: 0.04527577583698279 Test Loss: 0.05664980192614318\n",
      "Epoch: 1750 Training Loss: 0.045267700551945296 Test Loss: 0.05666088301669775\n",
      "Epoch: 1760 Training Loss: 0.04526046530432216 Test Loss: 0.05666706677470263\n",
      "Epoch: 1770 Training Loss: 0.04525455907111574 Test Loss: 0.056654266204579244\n",
      "Epoch: 1780 Training Loss: 0.04524748711434748 Test Loss: 0.0566551323128065\n",
      "Epoch: 1790 Training Loss: 0.04523985791883007 Test Loss: 0.056653572044308875\n",
      "Epoch: 1800 Training Loss: 0.04523244485234179 Test Loss: 0.05665546347183457\n",
      "Epoch: 1810 Training Loss: 0.04522418378788561 Test Loss: 0.0566616281245109\n",
      "Epoch: 1820 Training Loss: 0.04521754697487629 Test Loss: 0.05666479324060609\n",
      "Epoch: 1830 Training Loss: 0.045209558658886435 Test Loss: 0.05665622768497626\n",
      "Epoch: 1840 Training Loss: 0.04520330076822653 Test Loss: 0.05665903616827199\n",
      "Epoch: 1850 Training Loss: 0.045196438074509966 Test Loss: 0.05667037836498331\n",
      "Epoch: 1860 Training Loss: 0.04519061124782531 Test Loss: 0.05666233502166697\n",
      "Epoch: 1870 Training Loss: 0.04518186180340826 Test Loss: 0.05666573577014751\n",
      "Epoch: 1880 Training Loss: 0.04517501065249435 Test Loss: 0.05668075255838181\n",
      "Epoch: 1890 Training Loss: 0.04516848558575562 Test Loss: 0.05669161712187956\n",
      "Epoch: 1900 Training Loss: 0.0451603713935126 Test Loss: 0.056696718244600375\n",
      "Epoch: 1910 Training Loss: 0.045154449338705986 Test Loss: 0.05669867335655454\n",
      "Epoch: 1920 Training Loss: 0.04514858340580197 Test Loss: 0.056687808793056786\n",
      "Epoch: 1930 Training Loss: 0.04514180489493929 Test Loss: 0.05669925288485366\n",
      "Epoch: 1940 Training Loss: 0.045135042504197566 Test Loss: 0.05668373298963442\n",
      "Epoch: 1950 Training Loss: 0.04512698542892634 Test Loss: 0.056689687483696787\n",
      "Epoch: 1960 Training Loss: 0.0451218284828237 Test Loss: 0.056687388475828854\n",
      "Epoch: 1970 Training Loss: 0.04511574084850305 Test Loss: 0.05670066031072295\n",
      "Epoch: 1980 Training Loss: 0.04510970873904348 Test Loss: 0.05669344486497679\n",
      "Epoch: 1990 Training Loss: 0.04510403455597332 Test Loss: 0.056684115096205266\n",
      "Epoch: 2000 Training Loss: 0.04509891064616794 Test Loss: 0.05668393041136269\n",
      "Epoch: 2010 Training Loss: 0.045093236960632374 Test Loss: 0.056681306612909536\n",
      "Epoch: 2020 Training Loss: 0.045087920206417026 Test Loss: 0.05668092450633869\n",
      "Epoch: 2030 Training Loss: 0.045082484342419044 Test Loss: 0.05667572785697517\n",
      "Epoch: 2040 Training Loss: 0.045076009924702336 Test Loss: 0.05669349581251956\n",
      "Epoch: 2050 Training Loss: 0.045070652870184594 Test Loss: 0.056702118684135015\n",
      "Epoch: 2060 Training Loss: 0.045066624730576654 Test Loss: 0.056692808020692036\n",
      "Epoch: 2070 Training Loss: 0.045060439778488746 Test Loss: 0.05670670396298518\n",
      "Epoch: 2080 Training Loss: 0.045055285320059084 Test Loss: 0.05671751757894016\n",
      "Epoch: 2090 Training Loss: 0.04505082581795516 Test Loss: 0.05671428240997366\n",
      "Epoch: 2100 Training Loss: 0.045045583594422504 Test Loss: 0.05672715940141121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2110 Training Loss: 0.04504131763327699 Test Loss: 0.05672410254884443\n",
      "Epoch: 2120 Training Loss: 0.045036451446393094 Test Loss: 0.05673495437545649\n",
      "Epoch: 2130 Training Loss: 0.04503242828213114 Test Loss: 0.056713632828803213\n",
      "Epoch: 2140 Training Loss: 0.045028705131231445 Test Loss: 0.05671690620842681\n",
      "Epoch: 2150 Training Loss: 0.045023550971322546 Test Loss: 0.05673179562780415\n",
      "Epoch: 2160 Training Loss: 0.04501946820241581 Test Loss: 0.05673757180546679\n",
      "Epoch: 2170 Training Loss: 0.045014985216678884 Test Loss: 0.05674377466880021\n",
      "Epoch: 2180 Training Loss: 0.04501157053722961 Test Loss: 0.05673077667694856\n",
      "Epoch: 2190 Training Loss: 0.045006669821444655 Test Loss: 0.056732120418389376\n",
      "Epoch: 2200 Training Loss: 0.045002396397280174 Test Loss: 0.056739889918663265\n",
      "Epoch: 2210 Training Loss: 0.04499756264965204 Test Loss: 0.05674002365596306\n",
      "Epoch: 2220 Training Loss: 0.04499362734999999 Test Loss: 0.05673346415983019\n",
      "Epoch: 2230 Training Loss: 0.044989590055755466 Test Loss: 0.056747321891466244\n",
      "Epoch: 2240 Training Loss: 0.04498673092343771 Test Loss: 0.056741227291661235\n",
      "Epoch: 2250 Training Loss: 0.04498272845661501 Test Loss: 0.05674296587655859\n",
      "Epoch: 2260 Training Loss: 0.04497915506362915 Test Loss: 0.056735075375870594\n",
      "Epoch: 2270 Training Loss: 0.044975403453750486 Test Loss: 0.05674271113884469\n",
      "Epoch: 2280 Training Loss: 0.04497207733545956 Test Loss: 0.05674355177330055\n",
      "Epoch: 2290 Training Loss: 0.044969550357239076 Test Loss: 0.05673196120731819\n",
      "Epoch: 2300 Training Loss: 0.044965361513956165 Test Loss: 0.056742902192130115\n",
      "Epoch: 2310 Training Loss: 0.044962715624966884 Test Loss: 0.05675241027730136\n",
      "Epoch: 2320 Training Loss: 0.044958906301074915 Test Loss: 0.0567628290497998\n",
      "Epoch: 2330 Training Loss: 0.04495738414372745 Test Loss: 0.05675557539339655\n",
      "Epoch: 2340 Training Loss: 0.044954111958186896 Test Loss: 0.05676117962310231\n",
      "Epoch: 2350 Training Loss: 0.0449516993134169 Test Loss: 0.056766688326165354\n",
      "Epoch: 2360 Training Loss: 0.044947834464663855 Test Loss: 0.056755002233540276\n",
      "Epoch: 2370 Training Loss: 0.044945228079921615 Test Loss: 0.05675950472263343\n",
      "Epoch: 2380 Training Loss: 0.044942645676346975 Test Loss: 0.0567584411926779\n",
      "Epoch: 2390 Training Loss: 0.04494034507636833 Test Loss: 0.05678020216188765\n",
      "Epoch: 2400 Training Loss: 0.04493667725131587 Test Loss: 0.056776763202750026\n",
      "Epoch: 2410 Training Loss: 0.04493496712539749 Test Loss: 0.05676712774872183\n",
      "Epoch: 2420 Training Loss: 0.044932260736201164 Test Loss: 0.056756097605710036\n",
      "Epoch: 2430 Training Loss: 0.04492905034445761 Test Loss: 0.056755715499139194\n",
      "Epoch: 2440 Training Loss: 0.04492569337902164 Test Loss: 0.05676231320592915\n",
      "Epoch: 2450 Training Loss: 0.04492215361937458 Test Loss: 0.05677866099871857\n",
      "Epoch: 2460 Training Loss: 0.044917943183090135 Test Loss: 0.05679093935652846\n",
      "Epoch: 2470 Training Loss: 0.04491197207137221 Test Loss: 0.05678869129620331\n",
      "Epoch: 2480 Training Loss: 0.04491045369528769 Test Loss: 0.056798148433831776\n",
      "Epoch: 2490 Training Loss: 0.04490699224758626 Test Loss: 0.05677703067734962\n",
      "Epoch: 2500 Training Loss: 0.044904189038157266 Test Loss: 0.05679251236191178\n",
      "Epoch: 2510 Training Loss: 0.04490281753428591 Test Loss: 0.05677422856249674\n",
      "Epoch: 2520 Training Loss: 0.04489974764631268 Test Loss: 0.05677522840802379\n",
      "Epoch: 2530 Training Loss: 0.04489407415979096 Test Loss: 0.056777864943362635\n",
      "Epoch: 2540 Training Loss: 0.04488890039701891 Test Loss: 0.056790124195843984\n",
      "Epoch: 2550 Training Loss: 0.044883464632527854 Test Loss: 0.05681364922372248\n",
      "Epoch: 2560 Training Loss: 0.04488105487345853 Test Loss: 0.056786239445707036\n",
      "Epoch: 2570 Training Loss: 0.04487844420991875 Test Loss: 0.05678594012889321\n",
      "Epoch: 2580 Training Loss: 0.04487496266181958 Test Loss: 0.05678490207270907\n",
      "Epoch: 2590 Training Loss: 0.04487134041093626 Test Loss: 0.05679416815705212\n",
      "Epoch: 2600 Training Loss: 0.044867914487205084 Test Loss: 0.05680461877176479\n",
      "Epoch: 2610 Training Loss: 0.04486362325130201 Test Loss: 0.05680327503032398\n",
      "Epoch: 2620 Training Loss: 0.04485797752721083 Test Loss: 0.056821991883852646\n",
      "Epoch: 2630 Training Loss: 0.044854480157511464 Test Loss: 0.056833327712121115\n",
      "Epoch: 2640 Training Loss: 0.044851183294254866 Test Loss: 0.05684629386175853\n",
      "Epoch: 2650 Training Loss: 0.04484831729595968 Test Loss: 0.056834238399448296\n",
      "Epoch: 2660 Training Loss: 0.044843687139488816 Test Loss: 0.056840581368524365\n",
      "Epoch: 2670 Training Loss: 0.044841475399189284 Test Loss: 0.05682483857780546\n",
      "Epoch: 2680 Training Loss: 0.044839784378599444 Test Loss: 0.0568161074426616\n",
      "Epoch: 2690 Training Loss: 0.044834680370177966 Test Loss: 0.05683000975339759\n",
      "Epoch: 2700 Training Loss: 0.044828031616338304 Test Loss: 0.05683856257214172\n",
      "Epoch: 2710 Training Loss: 0.0448247688839551 Test Loss: 0.056844478855547005\n",
      "Epoch: 2720 Training Loss: 0.04481940217726617 Test Loss: 0.05685270688370592\n",
      "Epoch: 2730 Training Loss: 0.044815927694158285 Test Loss: 0.056848573764297916\n",
      "Epoch: 2740 Training Loss: 0.04481169118706094 Test Loss: 0.056865125347258454\n",
      "Epoch: 2750 Training Loss: 0.04480822088324367 Test Loss: 0.05685821558676896\n",
      "Epoch: 2760 Training Loss: 0.04480406786245177 Test Loss: 0.05687954987030794\n",
      "Epoch: 2770 Training Loss: 0.04479931182972776 Test Loss: 0.056890083274777625\n",
      "Epoch: 2780 Training Loss: 0.04479725442466035 Test Loss: 0.05689099396210481\n",
      "Epoch: 2790 Training Loss: 0.04479319205467212 Test Loss: 0.05689630524343959\n",
      "Epoch: 2800 Training Loss: 0.04478750503520934 Test Loss: 0.05690863454879226\n",
      "Epoch: 2810 Training Loss: 0.04478310782443701 Test Loss: 0.05693664932887821\n",
      "Epoch: 2820 Training Loss: 0.04477936477215342 Test Loss: 0.05693540111408011\n",
      "Epoch: 2830 Training Loss: 0.044775402804646945 Test Loss: 0.05694109450198573\n",
      "Epoch: 2840 Training Loss: 0.044770235509824674 Test Loss: 0.056932656315212854\n",
      "Epoch: 2850 Training Loss: 0.04476461048317274 Test Loss: 0.05695921272188674\n",
      "Epoch: 2860 Training Loss: 0.04476068563174724 Test Loss: 0.05696477237249257\n",
      "Epoch: 2870 Training Loss: 0.04475654564636179 Test Loss: 0.05697595535813269\n",
      "Epoch: 2880 Training Loss: 0.044752275107897976 Test Loss: 0.05698059795296849\n",
      "Epoch: 2890 Training Loss: 0.044747172393066456 Test Loss: 0.05699930843805431\n",
      "Epoch: 2900 Training Loss: 0.044741324371407945 Test Loss: 0.057005040036617015\n",
      "Epoch: 2910 Training Loss: 0.044737230159205464 Test Loss: 0.05699809843391329\n",
      "Epoch: 2920 Training Loss: 0.04472962633795253 Test Loss: 0.0570238396799027\n",
      "Epoch: 2930 Training Loss: 0.04472433515701549 Test Loss: 0.05702513884224358\n",
      "Epoch: 2940 Training Loss: 0.04471778660664375 Test Loss: 0.05704196426824656\n",
      "Epoch: 2950 Training Loss: 0.04471256975737557 Test Loss: 0.057051173036603973\n",
      "Epoch: 2960 Training Loss: 0.04470778078786121 Test Loss: 0.057083410094297786\n",
      "Epoch: 2970 Training Loss: 0.044701644693670765 Test Loss: 0.05709654182344924\n",
      "Epoch: 2980 Training Loss: 0.0446960731022346 Test Loss: 0.057116347680704824\n",
      "Epoch: 2990 Training Loss: 0.04469342442705158 Test Loss: 0.0571101448173714\n",
      "Epoch: 3000 Training Loss: 0.04468789512605619 Test Loss: 0.05711250750966781\n",
      "Epoch: 3010 Training Loss: 0.04468170271094932 Test Loss: 0.05712983604265573\n",
      "Epoch: 3020 Training Loss: 0.04467948380615158 Test Loss: 0.05712834582702942\n",
      "Epoch: 3030 Training Loss: 0.044674618216309206 Test Loss: 0.057134574164134234\n",
      "Epoch: 3040 Training Loss: 0.04466972964036843 Test Loss: 0.05715589571078751\n",
      "Epoch: 3050 Training Loss: 0.04466913140476845 Test Loss: 0.0571283967745722\n",
      "Epoch: 3060 Training Loss: 0.04466578399199675 Test Loss: 0.05712944756764203\n",
      "Epoch: 3070 Training Loss: 0.04466392629731677 Test Loss: 0.05711924532220041\n",
      "Epoch: 3080 Training Loss: 0.044659865917466915 Test Loss: 0.057115991047905365\n",
      "Epoch: 3090 Training Loss: 0.04465207591876761 Test Loss: 0.057154704811975036\n",
      "Epoch: 3100 Training Loss: 0.0446477799065324 Test Loss: 0.05714364282674901\n",
      "Epoch: 3110 Training Loss: 0.04464690951950761 Test Loss: 0.057140535026639454\n",
      "Epoch: 3120 Training Loss: 0.044643805898688674 Test Loss: 0.05713407742559213\n",
      "Epoch: 3130 Training Loss: 0.04464192790459711 Test Loss: 0.057134230268220475\n",
      "Epoch: 3140 Training Loss: 0.044638854732895535 Test Loss: 0.05713354247639295\n",
      "Epoch: 3150 Training Loss: 0.04463749298070229 Test Loss: 0.05711892053161519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3160 Training Loss: 0.04463453862026259 Test Loss: 0.05711834100331607\n",
      "Epoch: 3170 Training Loss: 0.04462819594970729 Test Loss: 0.05713424300510617\n",
      "Epoch: 3180 Training Loss: 0.04462599137390595 Test Loss: 0.057129199198370985\n",
      "Epoch: 3190 Training Loss: 0.04462079810777769 Test Loss: 0.057134032846492204\n",
      "Epoch: 3200 Training Loss: 0.044617606423335 Test Loss: 0.05714413956529111\n",
      "Epoch: 3210 Training Loss: 0.044613125527243384 Test Loss: 0.057142579296793485\n",
      "Epoch: 3220 Training Loss: 0.04460910305952986 Test Loss: 0.057145922729288395\n",
      "Epoch: 3230 Training Loss: 0.044606631009128934 Test Loss: 0.057139949129897484\n",
      "Epoch: 3240 Training Loss: 0.0446025671465369 Test Loss: 0.05714383388003443\n",
      "Epoch: 3250 Training Loss: 0.044601255048296484 Test Loss: 0.05714026755203986\n",
      "Epoch: 3260 Training Loss: 0.044597637872266054 Test Loss: 0.057126218767118375\n",
      "Epoch: 3270 Training Loss: 0.04459518164346533 Test Loss: 0.0571303964656263\n",
      "Epoch: 3280 Training Loss: 0.044589703688040404 Test Loss: 0.05715452649557531\n",
      "Epoch: 3290 Training Loss: 0.044586659074824724 Test Loss: 0.05715630329112975\n",
      "Epoch: 3300 Training Loss: 0.04458382362515381 Test Loss: 0.05714793515722819\n",
      "Epoch: 3310 Training Loss: 0.044580418299354975 Test Loss: 0.05714733652360053\n",
      "Epoch: 3320 Training Loss: 0.04457876956920592 Test Loss: 0.05714654683668745\n",
      "Epoch: 3330 Training Loss: 0.04457530871854601 Test Loss: 0.05715014500689626\n",
      "Epoch: 3340 Training Loss: 0.044574761529995725 Test Loss: 0.05715732861042819\n",
      "Epoch: 3350 Training Loss: 0.04457168397998969 Test Loss: 0.05715172438072243\n",
      "Epoch: 3360 Training Loss: 0.04456871618611586 Test Loss: 0.057155265234945615\n",
      "Epoch: 3370 Training Loss: 0.04456698814895197 Test Loss: 0.05716302836677666\n",
      "Epoch: 3380 Training Loss: 0.04456460615231318 Test Loss: 0.057160111619952526\n",
      "Epoch: 3390 Training Loss: 0.04456283035182794 Test Loss: 0.057155583657087984\n",
      "Epoch: 3400 Training Loss: 0.044559965249095015 Test Loss: 0.05714756578754304\n",
      "Epoch: 3410 Training Loss: 0.0445561418947274 Test Loss: 0.057155437182902495\n",
      "Epoch: 3420 Training Loss: 0.04455541012084146 Test Loss: 0.05714547056984623\n",
      "Epoch: 3430 Training Loss: 0.04455098176440333 Test Loss: 0.057144279671033756\n",
      "Epoch: 3440 Training Loss: 0.04454848075748883 Test Loss: 0.05714452167186196\n",
      "Epoch: 3450 Training Loss: 0.04454564202408958 Test Loss: 0.05714486556777572\n",
      "Epoch: 3460 Training Loss: 0.04454237899318561 Test Loss: 0.057142553823022094\n",
      "Epoch: 3470 Training Loss: 0.04453925974978031 Test Loss: 0.05715350117627687\n",
      "Epoch: 3480 Training Loss: 0.04453634549062917 Test Loss: 0.0571574114001852\n",
      "Epoch: 3490 Training Loss: 0.044533934238956054 Test Loss: 0.05716683032715659\n",
      "Epoch: 3500 Training Loss: 0.04453196619110235 Test Loss: 0.057147451155571787\n",
      "Epoch: 3510 Training Loss: 0.04452943015775219 Test Loss: 0.05714998579582507\n",
      "Epoch: 3520 Training Loss: 0.04452801825407152 Test Loss: 0.05714556609648894\n",
      "Epoch: 3530 Training Loss: 0.044526608539543086 Test Loss: 0.05714822810559918\n",
      "Epoch: 3540 Training Loss: 0.044525055236529824 Test Loss: 0.057139955498340333\n",
      "Epoch: 3550 Training Loss: 0.04452344541358629 Test Loss: 0.05712103485464055\n",
      "Epoch: 3560 Training Loss: 0.04452087713999422 Test Loss: 0.05711312524862401\n",
      "Epoch: 3570 Training Loss: 0.04451894869589448 Test Loss: 0.0571108962936274\n",
      "Epoch: 3580 Training Loss: 0.04451772436275705 Test Loss: 0.05710918955094428\n",
      "Epoch: 3590 Training Loss: 0.04451597552864699 Test Loss: 0.0571075146504754\n",
      "Epoch: 3600 Training Loss: 0.04451338436846343 Test Loss: 0.05710008267767242\n",
      "Epoch: 3610 Training Loss: 0.044511385672478525 Test Loss: 0.057109074918973025\n",
      "Epoch: 3620 Training Loss: 0.04451048871710623 Test Loss: 0.05710153468264164\n",
      "Epoch: 3630 Training Loss: 0.04450914447813082 Test Loss: 0.057103483426152964\n",
      "Epoch: 3640 Training Loss: 0.04450692189157706 Test Loss: 0.057096822034934526\n",
      "Epoch: 3650 Training Loss: 0.04450559456877796 Test Loss: 0.05709683477182022\n",
      "Epoch: 3660 Training Loss: 0.04450536112554483 Test Loss: 0.05708500220500965\n",
      "Epoch: 3670 Training Loss: 0.04450330312343591 Test Loss: 0.05708780431986253\n",
      "Epoch: 3680 Training Loss: 0.0445019344057583 Test Loss: 0.05708715473869209\n",
      "Epoch: 3690 Training Loss: 0.04449985789536236 Test Loss: 0.057091045857271885\n",
      "Epoch: 3700 Training Loss: 0.04449905875529192 Test Loss: 0.057082486670084906\n",
      "Epoch: 3710 Training Loss: 0.044497349922963496 Test Loss: 0.05707547501450986\n",
      "Epoch: 3720 Training Loss: 0.04449455815683024 Test Loss: 0.057078557340848025\n",
      "Epoch: 3730 Training Loss: 0.04449261190099191 Test Loss: 0.057089141692860494\n",
      "Epoch: 3740 Training Loss: 0.044492478959747464 Test Loss: 0.05708951106254565\n",
      "Epoch: 3750 Training Loss: 0.044490337272319255 Test Loss: 0.0570849958365668\n",
      "Epoch: 3760 Training Loss: 0.04448956251144409 Test Loss: 0.05707697796702186\n",
      "Epoch: 3770 Training Loss: 0.0444876547647836 Test Loss: 0.05707554506738118\n",
      "Epoch: 3780 Training Loss: 0.04448653939172302 Test Loss: 0.05708255672295623\n",
      "Epoch: 3790 Training Loss: 0.044486329332615974 Test Loss: 0.057083422831183485\n",
      "Epoch: 3800 Training Loss: 0.04448552442114421 Test Loss: 0.05708375399021155\n",
      "Epoch: 3810 Training Loss: 0.044482933858002165 Test Loss: 0.05708774063543406\n",
      "Epoch: 3820 Training Loss: 0.044483397560246996 Test Loss: 0.05708316809346958\n",
      "Epoch: 3830 Training Loss: 0.04448141120312011 Test Loss: 0.05707071778770281\n",
      "Epoch: 3840 Training Loss: 0.0444802660774906 Test Loss: 0.05707805423386308\n",
      "Epoch: 3850 Training Loss: 0.04447865416490176 Test Loss: 0.05707194689717237\n",
      "Epoch: 3860 Training Loss: 0.04447771601366479 Test Loss: 0.057075436803852776\n",
      "Epoch: 3870 Training Loss: 0.04447623982851414 Test Loss: 0.05707290853204234\n",
      "Epoch: 3880 Training Loss: 0.04447429844851486 Test Loss: 0.05707090247254539\n",
      "Epoch: 3890 Training Loss: 0.04447215138771299 Test Loss: 0.05707904771094728\n",
      "Epoch: 3900 Training Loss: 0.044470705850693536 Test Loss: 0.057067590882264714\n",
      "Epoch: 3910 Training Loss: 0.044470505642771524 Test Loss: 0.057059948750847765\n",
      "Epoch: 3920 Training Loss: 0.04446870307492493 Test Loss: 0.05705794905979367\n",
      "Epoch: 3930 Training Loss: 0.04446755098381106 Test Loss: 0.05707139921108749\n",
      "Epoch: 3940 Training Loss: 0.04446419561048581 Test Loss: 0.05705890432622079\n",
      "Epoch: 3950 Training Loss: 0.04446273246074161 Test Loss: 0.05705745232125157\n",
      "Epoch: 3960 Training Loss: 0.044462652059150655 Test Loss: 0.057069081097891015\n",
      "Epoch: 3970 Training Loss: 0.04445996079500609 Test Loss: 0.057067055933065526\n",
      "Epoch: 3980 Training Loss: 0.04446006020241866 Test Loss: 0.057078557340848025\n",
      "Epoch: 3990 Training Loss: 0.04445841336290108 Test Loss: 0.05706360423704221\n",
      "Epoch: 4000 Training Loss: 0.04445704195853665 Test Loss: 0.05706141349270268\n",
      "Epoch: 4010 Training Loss: 0.04445773134247488 Test Loss: 0.057057707058965464\n",
      "Epoch: 4020 Training Loss: 0.04445575085625625 Test Loss: 0.057052128303031094\n",
      "Epoch: 4030 Training Loss: 0.044455273024028845 Test Loss: 0.05703765920088168\n",
      "Epoch: 4040 Training Loss: 0.04445377733552197 Test Loss: 0.05704632028315421\n",
      "Epoch: 4050 Training Loss: 0.044452892320980014 Test Loss: 0.057054032467442485\n",
      "Epoch: 4060 Training Loss: 0.044451520220067146 Test Loss: 0.05703702235659693\n",
      "Epoch: 4070 Training Loss: 0.04445072506027349 Test Loss: 0.057029813279293615\n",
      "Epoch: 4080 Training Loss: 0.04444865382374428 Test Loss: 0.057025183421343516\n",
      "Epoch: 4090 Training Loss: 0.044446108536250604 Test Loss: 0.057018859557595995\n",
      "Epoch: 4100 Training Loss: 0.04444194427117681 Test Loss: 0.05702139419784928\n",
      "Epoch: 4110 Training Loss: 0.04444224179686609 Test Loss: 0.05702378873235992\n",
      "Epoch: 4120 Training Loss: 0.04443893279376532 Test Loss: 0.05702790911488222\n",
      "Epoch: 4130 Training Loss: 0.04443806758110034 Test Loss: 0.057018260923968334\n",
      "Epoch: 4140 Training Loss: 0.04443543293639296 Test Loss: 0.05702153430359192\n",
      "Epoch: 4150 Training Loss: 0.044435376515969606 Test Loss: 0.05701507033410176\n",
      "Epoch: 4160 Training Loss: 0.04443252415012239 Test Loss: 0.057015114913201694\n",
      "Epoch: 4170 Training Loss: 0.04443108975787791 Test Loss: 0.05700352434721932\n",
      "Epoch: 4180 Training Loss: 0.04442892359174751 Test Loss: 0.057011000899122234\n",
      "Epoch: 4190 Training Loss: 0.0444257701179619 Test Loss: 0.057004689772260406\n",
      "Epoch: 4200 Training Loss: 0.04442355011858805 Test Loss: 0.056998391382284276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4210 Training Loss: 0.044421115084761174 Test Loss: 0.05699379336654841\n",
      "Epoch: 4220 Training Loss: 0.04441812709098268 Test Loss: 0.05699072377709594\n",
      "Epoch: 4230 Training Loss: 0.044416151381096176 Test Loss: 0.056986845395401844\n",
      "Epoch: 4240 Training Loss: 0.04441403397335632 Test Loss: 0.05698538065354693\n",
      "Epoch: 4250 Training Loss: 0.04441221210116735 Test Loss: 0.05698191622063791\n",
      "Epoch: 4260 Training Loss: 0.044409753981735155 Test Loss: 0.0569743059314352\n",
      "Epoch: 4270 Training Loss: 0.04440809828610173 Test Loss: 0.05697692336144551\n",
      "Epoch: 4280 Training Loss: 0.044404765799367966 Test Loss: 0.05695720029394694\n",
      "Epoch: 4290 Training Loss: 0.04440226618555034 Test Loss: 0.0569543217577799\n",
      "Epoch: 4300 Training Loss: 0.04440037416098313 Test Loss: 0.056962244100682125\n",
      "Epoch: 4310 Training Loss: 0.044399543676233055 Test Loss: 0.056955378919292574\n",
      "Epoch: 4320 Training Loss: 0.04439740726267157 Test Loss: 0.05694371830043888\n",
      "Epoch: 4330 Training Loss: 0.04439540199922998 Test Loss: 0.05693612074812187\n",
      "Epoch: 4340 Training Loss: 0.044395129151256735 Test Loss: 0.05693059293973028\n",
      "Epoch: 4350 Training Loss: 0.044392126629467996 Test Loss: 0.056921893646800656\n",
      "Epoch: 4360 Training Loss: 0.044390807963770894 Test Loss: 0.05693280915784119\n",
      "Epoch: 4370 Training Loss: 0.04438867493344468 Test Loss: 0.05693403826731075\n",
      "Epoch: 4380 Training Loss: 0.04438642398741886 Test Loss: 0.05691267214155754\n",
      "Epoch: 4390 Training Loss: 0.04438382815041009 Test Loss: 0.05691620662733788\n",
      "Epoch: 4400 Training Loss: 0.04438358694563724 Test Loss: 0.056890229748963114\n",
      "Epoch: 4410 Training Loss: 0.04438030590398085 Test Loss: 0.056891955596974776\n",
      "Epoch: 4420 Training Loss: 0.04438147272211881 Test Loss: 0.05689374512941491\n",
      "Epoch: 4430 Training Loss: 0.04437861747057092 Test Loss: 0.05687549317221411\n",
      "Epoch: 4440 Training Loss: 0.044374572812814345 Test Loss: 0.05686653277312774\n",
      "Epoch: 4450 Training Loss: 0.04437249003348247 Test Loss: 0.056863036498004484\n",
      "Epoch: 4460 Training Loss: 0.04436921516125509 Test Loss: 0.05686522724234401\n",
      "Epoch: 4470 Training Loss: 0.04436915078027818 Test Loss: 0.05685574463094415\n",
      "Epoch: 4480 Training Loss: 0.04436634388909316 Test Loss: 0.05683791935941413\n",
      "Epoch: 4490 Training Loss: 0.04436512691946778 Test Loss: 0.05681729834147407\n",
      "Epoch: 4500 Training Loss: 0.044362732484464054 Test Loss: 0.05681634307504695\n",
      "Epoch: 4510 Training Loss: 0.0443606722932029 Test Loss: 0.056827341375844505\n",
      "Epoch: 4520 Training Loss: 0.0443590054527945 Test Loss: 0.056813439065108516\n",
      "Epoch: 4530 Training Loss: 0.044357682209779106 Test Loss: 0.05679616784810622\n",
      "Epoch: 4540 Training Loss: 0.0443539661238707 Test Loss: 0.05680938236701469\n",
      "Epoch: 4550 Training Loss: 0.044354369027387715 Test Loss: 0.0568058542496772\n",
      "Epoch: 4560 Training Loss: 0.04435220514991645 Test Loss: 0.056790703724143105\n",
      "Epoch: 4570 Training Loss: 0.04434996992598392 Test Loss: 0.056785296916165615\n",
      "Epoch: 4580 Training Loss: 0.044347242640334496 Test Loss: 0.05678360927881104\n",
      "Epoch: 4590 Training Loss: 0.044344452864339634 Test Loss: 0.05680419208609401\n",
      "Epoch: 4600 Training Loss: 0.04434028879827968 Test Loss: 0.05679290720536832\n",
      "Epoch: 4610 Training Loss: 0.044338462846307006 Test Loss: 0.05679736511536154\n",
      "Epoch: 4620 Training Loss: 0.044336851033225086 Test Loss: 0.05678267948615531\n",
      "Epoch: 4630 Training Loss: 0.04433476576622022 Test Loss: 0.056784398965724125\n",
      "Epoch: 4640 Training Loss: 0.04433211430484345 Test Loss: 0.056779565317602906\n",
      "Epoch: 4650 Training Loss: 0.04433046995299885 Test Loss: 0.05678654513096371\n",
      "Epoch: 4660 Training Loss: 0.044328656339884205 Test Loss: 0.05678075621641538\n",
      "Epoch: 4670 Training Loss: 0.04432680332202943 Test Loss: 0.05678098548035789\n",
      "Epoch: 4680 Training Loss: 0.04432407275265167 Test Loss: 0.056768184910234505\n",
      "Epoch: 4690 Training Loss: 0.04432157204425793 Test Loss: 0.05675479207492631\n",
      "Epoch: 4700 Training Loss: 0.044319993466487115 Test Loss: 0.05676683480035084\n",
      "Epoch: 4710 Training Loss: 0.04431972360372145 Test Loss: 0.05676049183127478\n",
      "Epoch: 4720 Training Loss: 0.044316694414078296 Test Loss: 0.05675964482837607\n",
      "Epoch: 4730 Training Loss: 0.04431408066582401 Test Loss: 0.05676956049388956\n",
      "Epoch: 4740 Training Loss: 0.04431129735777892 Test Loss: 0.0567628290497998\n",
      "Epoch: 4750 Training Loss: 0.044308355037676474 Test Loss: 0.05675676629220902\n",
      "Epoch: 4760 Training Loss: 0.04430760346191355 Test Loss: 0.056762803576028406\n",
      "Epoch: 4770 Training Loss: 0.044305557997676485 Test Loss: 0.056765038899467865\n",
      "Epoch: 4780 Training Loss: 0.04430229506627944 Test Loss: 0.05675354386012821\n",
      "Epoch: 4790 Training Loss: 0.04430318485715353 Test Loss: 0.056736158011154655\n",
      "Epoch: 4800 Training Loss: 0.04430101341715639 Test Loss: 0.056739717970706385\n",
      "Epoch: 4810 Training Loss: 0.04430045558136572 Test Loss: 0.05671970832327968\n",
      "Epoch: 4820 Training Loss: 0.044298021443101125 Test Loss: 0.056730738466291473\n",
      "Epoch: 4830 Training Loss: 0.04429678954743782 Test Loss: 0.056718511056024365\n",
      "Epoch: 4840 Training Loss: 0.044291274077904244 Test Loss: 0.05673995997153459\n",
      "Epoch: 4850 Training Loss: 0.04428833971835536 Test Loss: 0.0567474365234375\n",
      "Epoch: 4860 Training Loss: 0.044289398969513345 Test Loss: 0.056736552854611204\n",
      "Epoch: 4870 Training Loss: 0.044288653463672514 Test Loss: 0.05672140869751995\n",
      "Epoch: 4880 Training Loss: 0.044285740896139 Test Loss: 0.05671914790030911\n",
      "Epoch: 4890 Training Loss: 0.04428458472524143 Test Loss: 0.05673025446463507\n",
      "Epoch: 4900 Training Loss: 0.04428104327397673 Test Loss: 0.056716829787112635\n",
      "Epoch: 4910 Training Loss: 0.04427784134032133 Test Loss: 0.05672990420027846\n",
      "Epoch: 4920 Training Loss: 0.04427508460062374 Test Loss: 0.05673713238291032\n",
      "Epoch: 4930 Training Loss: 0.04427395430152524 Test Loss: 0.05673555300908415\n",
      "Epoch: 4940 Training Loss: 0.044272153027268606 Test Loss: 0.0567406987109049\n",
      "Epoch: 4950 Training Loss: 0.0442695784847406 Test Loss: 0.05673046462324904\n",
      "Epoch: 4960 Training Loss: 0.04426711091215105 Test Loss: 0.0567223002795186\n",
      "Epoch: 4970 Training Loss: 0.04426626390925234 Test Loss: 0.05671279219434735\n",
      "Epoch: 4980 Training Loss: 0.04426438292995319 Test Loss: 0.056713499091503417\n",
      "Epoch: 4990 Training Loss: 0.04426182560212226 Test Loss: 0.05672150422416267\n",
      "Epoch: 5000 Training Loss: 0.04426028971281991 Test Loss: 0.05672343386234545\n",
      "Epoch: 5010 Training Loss: 0.04425801806935484 Test Loss: 0.056719982166322125\n",
      "Epoch: 5020 Training Loss: 0.04425605081755649 Test Loss: 0.05671861295110992\n",
      "Epoch: 5030 Training Loss: 0.04425424038866326 Test Loss: 0.05671902053145216\n",
      "Epoch: 5040 Training Loss: 0.04425256021432765 Test Loss: 0.0567091621819243\n",
      "Epoch: 5050 Training Loss: 0.044250928101834154 Test Loss: 0.056713085142718334\n",
      "Epoch: 5060 Training Loss: 0.044249433010368794 Test Loss: 0.056717587631811485\n",
      "Epoch: 5070 Training Loss: 0.04424686642839435 Test Loss: 0.056715874520685515\n",
      "Epoch: 5080 Training Loss: 0.04424514386411103 Test Loss: 0.05671146755823508\n",
      "Epoch: 5090 Training Loss: 0.044244145212667015 Test Loss: 0.05669724682535671\n",
      "Epoch: 5100 Training Loss: 0.044242202539077784 Test Loss: 0.05669407534081868\n",
      "Epoch: 5110 Training Loss: 0.044240346536015426 Test Loss: 0.05670945513029528\n",
      "Epoch: 5120 Training Loss: 0.04423933693881027 Test Loss: 0.05669278254692065\n",
      "Epoch: 5130 Training Loss: 0.044235747525210374 Test Loss: 0.0566949223437174\n",
      "Epoch: 5140 Training Loss: 0.04423263673989323 Test Loss: 0.05668416604374804\n",
      "Epoch: 5150 Training Loss: 0.04423130125752674 Test Loss: 0.05668069524239618\n",
      "Epoch: 5160 Training Loss: 0.044230139016707075 Test Loss: 0.05667560048811822\n",
      "Epoch: 5170 Training Loss: 0.04422763333296736 Test Loss: 0.05667637743814561\n",
      "Epoch: 5180 Training Loss: 0.04422562637790814 Test Loss: 0.05666348770982236\n",
      "Epoch: 5190 Training Loss: 0.04422488933015546 Test Loss: 0.05664503196245044\n",
      "Epoch: 5200 Training Loss: 0.04422229040843219 Test Loss: 0.0566437519054381\n",
      "Epoch: 5210 Training Loss: 0.044221517538188496 Test Loss: 0.056637937517118375\n",
      "Epoch: 5220 Training Loss: 0.04421929704128004 Test Loss: 0.056632677183326374\n",
      "Epoch: 5230 Training Loss: 0.04421679175556801 Test Loss: 0.056641784056598234\n",
      "Epoch: 5240 Training Loss: 0.04421480420435808 Test Loss: 0.05663084307178631\n",
      "Epoch: 5250 Training Loss: 0.04421278152720558 Test Loss: 0.05662801548316204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5260 Training Loss: 0.044210266091787756 Test Loss: 0.05660991636858958\n",
      "Epoch: 5270 Training Loss: 0.0442092161942802 Test Loss: 0.056619328927118115\n",
      "Epoch: 5280 Training Loss: 0.044206013763090204 Test Loss: 0.056612973221156354\n",
      "Epoch: 5290 Training Loss: 0.04420478674326596 Test Loss: 0.05661717639343567\n",
      "Epoch: 5300 Training Loss: 0.04420344290231822 Test Loss: 0.05663309750055431\n",
      "Epoch: 5310 Training Loss: 0.04420112090835189 Test Loss: 0.0566188703992331\n",
      "Epoch: 5320 Training Loss: 0.04419800355557806 Test Loss: 0.05662895801270346\n",
      "Epoch: 5330 Training Loss: 0.04419661304588509 Test Loss: 0.05661841823979093\n",
      "Epoch: 5340 Training Loss: 0.04419523925335459 Test Loss: 0.05662402246949669\n",
      "Epoch: 5350 Training Loss: 0.044192743918334505 Test Loss: 0.05661753302623513\n",
      "Epoch: 5360 Training Loss: 0.04419132972599469 Test Loss: 0.0566196218754891\n",
      "Epoch: 5370 Training Loss: 0.04418916763964798 Test Loss: 0.05661536138722415\n",
      "Epoch: 5380 Training Loss: 0.04418799604517789 Test Loss: 0.05660995457924666\n",
      "Epoch: 5390 Training Loss: 0.044185811171746615 Test Loss: 0.056619348032446656\n",
      "Epoch: 5400 Training Loss: 0.044183657941515735 Test Loss: 0.0566167369708792\n",
      "Epoch: 5410 Training Loss: 0.04418055810195974 Test Loss: 0.05661741202582103\n",
      "Epoch: 5420 Training Loss: 0.04417802853655935 Test Loss: 0.056627155743377636\n",
      "Epoch: 5430 Training Loss: 0.04417479117844061 Test Loss: 0.05662122672308666\n",
      "Epoch: 5440 Training Loss: 0.04417248261790841 Test Loss: 0.05659896264689195\n",
      "Epoch: 5450 Training Loss: 0.044170375160860496 Test Loss: 0.05657304945294567\n",
      "Epoch: 5460 Training Loss: 0.04416875419314198 Test Loss: 0.056583022434444775\n",
      "Epoch: 5470 Training Loss: 0.04416470087828342 Test Loss: 0.05656998623193604\n",
      "Epoch: 5480 Training Loss: 0.04416282646644096 Test Loss: 0.05658029674090607\n",
      "Epoch: 5490 Training Loss: 0.04416088379285173 Test Loss: 0.056574679774314615\n",
      "Epoch: 5500 Training Loss: 0.0441587359359945 Test Loss: 0.05657532935548505\n",
      "Epoch: 5510 Training Loss: 0.04415709188267066 Test Loss: 0.05656566842768546\n",
      "Epoch: 5520 Training Loss: 0.044153931542907615 Test Loss: 0.056571845817247494\n",
      "Epoch: 5530 Training Loss: 0.044152210570735005 Test Loss: 0.05655452365270242\n",
      "Epoch: 5540 Training Loss: 0.0441500913718706 Test Loss: 0.05657924594783624\n",
      "Epoch: 5550 Training Loss: 0.044147507674706 Test Loss: 0.056574323141515157\n",
      "Epoch: 5560 Training Loss: 0.04414492218641686 Test Loss: 0.056556039342100115\n",
      "Epoch: 5570 Training Loss: 0.04414508189502264 Test Loss: 0.05654198418873578\n",
      "Epoch: 5580 Training Loss: 0.04414233112574022 Test Loss: 0.05656290452348967\n",
      "Epoch: 5590 Training Loss: 0.04413995360691281 Test Loss: 0.05657648841208329\n",
      "Epoch: 5600 Training Loss: 0.044137966354223644 Test Loss: 0.056567795487596516\n",
      "Epoch: 5610 Training Loss: 0.04413606457797832 Test Loss: 0.05656601869204207\n",
      "Epoch: 5620 Training Loss: 0.0441346329719276 Test Loss: 0.05657406203535841\n",
      "Epoch: 5630 Training Loss: 0.04413247814958601 Test Loss: 0.056575571356313255\n",
      "Epoch: 5640 Training Loss: 0.044131136895818185 Test Loss: 0.05656791011956777\n",
      "Epoch: 5650 Training Loss: 0.04412967623374697 Test Loss: 0.05655472744287354\n",
      "Epoch: 5660 Training Loss: 0.044127217517273255 Test Loss: 0.05656638169328438\n",
      "Epoch: 5670 Training Loss: 0.04412601796135879 Test Loss: 0.05655688634499883\n",
      "Epoch: 5680 Training Loss: 0.04412417170997454 Test Loss: 0.05656746432856845\n",
      "Epoch: 5690 Training Loss: 0.044122983199328135 Test Loss: 0.056585448811169656\n",
      "Epoch: 5700 Training Loss: 0.04412115067989878 Test Loss: 0.05655612850029998\n",
      "Epoch: 5710 Training Loss: 0.04411868071914316 Test Loss: 0.056567209590854546\n",
      "Epoch: 5720 Training Loss: 0.04411799879822389 Test Loss: 0.05656566842768546\n",
      "Epoch: 5730 Training Loss: 0.04411549092533194 Test Loss: 0.0565624651009332\n",
      "Epoch: 5740 Training Loss: 0.04411595024927232 Test Loss: 0.05655810271758269\n",
      "Epoch: 5750 Training Loss: 0.04411378468018343 Test Loss: 0.05656734969659719\n",
      "Epoch: 5760 Training Loss: 0.04411278125240727 Test Loss: 0.05656540095308587\n",
      "Epoch: 5770 Training Loss: 0.04411198430148906 Test Loss: 0.05654732094384195\n",
      "Epoch: 5780 Training Loss: 0.04411051647491965 Test Loss: 0.05655590560480032\n",
      "Epoch: 5790 Training Loss: 0.0441092690561769 Test Loss: 0.05655202722310622\n",
      "Epoch: 5800 Training Loss: 0.044107676845958114 Test Loss: 0.05654030291982406\n",
      "Epoch: 5810 Training Loss: 0.04410614841967473 Test Loss: 0.05655757413682635\n",
      "Epoch: 5820 Training Loss: 0.044104100268750834 Test Loss: 0.05655389954530337\n",
      "Epoch: 5830 Training Loss: 0.04410272050580517 Test Loss: 0.056551358536607234\n",
      "Epoch: 5840 Training Loss: 0.04410117705397693 Test Loss: 0.05655951651189482\n",
      "Epoch: 5850 Training Loss: 0.044099713307191216 Test Loss: 0.056550868166507985\n",
      "Epoch: 5860 Training Loss: 0.04409806537309752 Test Loss: 0.056552855120676385\n",
      "Epoch: 5870 Training Loss: 0.04409672252721898 Test Loss: 0.05655703281918432\n",
      "Epoch: 5880 Training Loss: 0.04409597771792659 Test Loss: 0.05656147799229184\n",
      "Epoch: 5890 Training Loss: 0.04409394101029843 Test Loss: 0.05656543916374296\n",
      "Epoch: 5900 Training Loss: 0.04409217326987367 Test Loss: 0.05655628134292832\n",
      "Epoch: 5910 Training Loss: 0.044090890028639904 Test Loss: 0.05656585948097089\n",
      "Epoch: 5920 Training Loss: 0.04408982978241272 Test Loss: 0.056550307743537406\n",
      "Epoch: 5930 Training Loss: 0.044087519331249055 Test Loss: 0.05656619063999896\n",
      "Epoch: 5940 Training Loss: 0.044087011447931965 Test Loss: 0.056573909192730074\n",
      "Epoch: 5950 Training Loss: 0.044086137677671916 Test Loss: 0.0565757114620559\n",
      "Epoch: 5960 Training Loss: 0.04408530430722117 Test Loss: 0.056565146215371974\n",
      "Epoch: 5970 Training Loss: 0.0440834400450845 Test Loss: 0.05657794678549536\n",
      "Epoch: 5980 Training Loss: 0.04408140522808782 Test Loss: 0.056576908729311225\n",
      "Epoch: 5990 Training Loss: 0.04407839026793415 Test Loss: 0.05657248266153224\n",
      "Epoch: 6000 Training Loss: 0.0440770386654467 Test Loss: 0.05657067402376356\n",
      "Epoch: 6010 Training Loss: 0.04407619783197699 Test Loss: 0.05657035560162119\n",
      "Epoch: 6020 Training Loss: 0.044075232316337164 Test Loss: 0.05657103065656302\n",
      "Epoch: 6030 Training Loss: 0.04407436909381058 Test Loss: 0.05656585311252804\n",
      "Epoch: 6040 Training Loss: 0.04407305759261168 Test Loss: 0.056589161613349724\n",
      "Epoch: 6050 Training Loss: 0.04407147344245337 Test Loss: 0.056588607558821995\n",
      "Epoch: 6060 Training Loss: 0.0440701150734954 Test Loss: 0.056579309632264714\n",
      "Epoch: 6070 Training Loss: 0.044069597637514044 Test Loss: 0.05658649323579664\n",
      "Epoch: 6080 Training Loss: 0.04406821180464628 Test Loss: 0.05658630218251122\n",
      "Epoch: 6090 Training Loss: 0.044067603120819754 Test Loss: 0.05658293964468776\n",
      "Epoch: 6100 Training Loss: 0.044066113303221126 Test Loss: 0.05659595037342511\n",
      "Epoch: 6110 Training Loss: 0.044065997477166836 Test Loss: 0.056597727168979546\n",
      "Epoch: 6120 Training Loss: 0.044064509152172006 Test Loss: 0.056596612691481245\n",
      "Epoch: 6130 Training Loss: 0.04406476647706581 Test Loss: 0.05659744058905141\n",
      "Epoch: 6140 Training Loss: 0.04406286539736893 Test Loss: 0.056606668462737375\n",
      "Epoch: 6150 Training Loss: 0.04406167718524328 Test Loss: 0.0566041083487127\n",
      "Epoch: 6160 Training Loss: 0.04406152613373949 Test Loss: 0.056614820069582114\n",
      "Epoch: 6170 Training Loss: 0.044061135370066645 Test Loss: 0.05661899776809005\n",
      "Epoch: 6180 Training Loss: 0.04406005353083794 Test Loss: 0.05661214532358619\n",
      "Epoch: 6190 Training Loss: 0.04405860859086 Test Loss: 0.05661882582013317\n",
      "Epoch: 6200 Training Loss: 0.04405812160399601 Test Loss: 0.05662429631253913\n",
      "Epoch: 6210 Training Loss: 0.04405763969198491 Test Loss: 0.05663239697184109\n",
      "Epoch: 6220 Training Loss: 0.044056941252917 Test Loss: 0.05663803941220393\n",
      "Epoch: 6230 Training Loss: 0.044055620995109186 Test Loss: 0.05664099436968515\n",
      "Epoch: 6240 Training Loss: 0.04405522217137587 Test Loss: 0.05663424382026685\n",
      "Epoch: 6250 Training Loss: 0.0440554745209237 Test Loss: 0.056638829099117015\n",
      "Epoch: 6260 Training Loss: 0.04405446243604555 Test Loss: 0.05663597603672136\n",
      "Epoch: 6270 Training Loss: 0.04405391833220977 Test Loss: 0.05663312934276855\n",
      "Epoch: 6280 Training Loss: 0.04405385902608575 Test Loss: 0.05664082242172827\n",
      "Epoch: 6290 Training Loss: 0.04405307690169855 Test Loss: 0.05663240970872679\n",
      "Epoch: 6300 Training Loss: 0.04405315511413727 Test Loss: 0.056633307659168276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6310 Training Loss: 0.04405215148734728 Test Loss: 0.05663153723205668\n",
      "Epoch: 6320 Training Loss: 0.04405160161211017 Test Loss: 0.056632900078826036\n",
      "Epoch: 6330 Training Loss: 0.04405099461990127 Test Loss: 0.05663784199047566\n",
      "Epoch: 6340 Training Loss: 0.044049571272924866 Test Loss: 0.05664377101076664\n",
      "Epoch: 6350 Training Loss: 0.044049287777711235 Test Loss: 0.05664415311733749\n",
      "Epoch: 6360 Training Loss: 0.04404911383961596 Test Loss: 0.05665290972625274\n",
      "Epoch: 6370 Training Loss: 0.04404795786773223 Test Loss: 0.05663968247045858\n",
      "Epoch: 6380 Training Loss: 0.04404697961520671 Test Loss: 0.05665605573701938\n",
      "Epoch: 6390 Training Loss: 0.044046714727787026 Test Loss: 0.056650973719627115\n",
      "Epoch: 6400 Training Loss: 0.044046242766467875 Test Loss: 0.056657291214931786\n",
      "Epoch: 6410 Training Loss: 0.04404636476195117 Test Loss: 0.05666723235421666\n",
      "Epoch: 6420 Training Loss: 0.04404431631250652 Test Loss: 0.056658405692430094\n",
      "Epoch: 6430 Training Loss: 0.044044581498446964 Test Loss: 0.0566792750796412\n",
      "Epoch: 6440 Training Loss: 0.044043401744409474 Test Loss: 0.05666406723812148\n",
      "Epoch: 6450 Training Loss: 0.04404293734561621 Test Loss: 0.056674938170062084\n",
      "Epoch: 6460 Training Loss: 0.04404243841792188 Test Loss: 0.05666165359828229\n",
      "Epoch: 6470 Training Loss: 0.04404201113520958 Test Loss: 0.05667733907301557\n",
      "Epoch: 6480 Training Loss: 0.044041473698337406 Test Loss: 0.056673460691321474\n",
      "Epoch: 6490 Training Loss: 0.04404117050075372 Test Loss: 0.05668419788596228\n",
      "Epoch: 6500 Training Loss: 0.04404062420776571 Test Loss: 0.056672505424894354\n",
      "Epoch: 6510 Training Loss: 0.04404084849636224 Test Loss: 0.05667538396106141\n",
      "Epoch: 6520 Training Loss: 0.044039414999680046 Test Loss: 0.056669550467413135\n",
      "Epoch: 6530 Training Loss: 0.044038325995953136 Test Loss: 0.05667826249722845\n",
      "Epoch: 6540 Training Loss: 0.04403816081446678 Test Loss: 0.05666563387506195\n",
      "Epoch: 6550 Training Loss: 0.04403755640943779 Test Loss: 0.05665332367503782\n",
      "Epoch: 6560 Training Loss: 0.044037830650507905 Test Loss: 0.056659118958029005\n",
      "Epoch: 6570 Training Loss: 0.04403739709885968 Test Loss: 0.05664605728174888\n",
      "Epoch: 6580 Training Loss: 0.04403667000179896 Test Loss: 0.05665285877870996\n",
      "Epoch: 6590 Training Loss: 0.04403563682145388 Test Loss: 0.056647267285889896\n",
      "Epoch: 6600 Training Loss: 0.04403547472468203 Test Loss: 0.0566436181681383\n",
      "Epoch: 6610 Training Loss: 0.04403433676355072 Test Loss: 0.05665122208889816\n",
      "Epoch: 6620 Training Loss: 0.04403416262644161 Test Loss: 0.056640911579928135\n",
      "Epoch: 6630 Training Loss: 0.044033518916179425 Test Loss: 0.0566486365011021\n",
      "Epoch: 6640 Training Loss: 0.04403338896014256 Test Loss: 0.056646866073990505\n",
      "Epoch: 6650 Training Loss: 0.04403265688773587 Test Loss: 0.05664216616316908\n",
      "Epoch: 6660 Training Loss: 0.04403072197568635 Test Loss: 0.05662628326670753\n",
      "Epoch: 6670 Training Loss: 0.04403044594349169 Test Loss: 0.056631632758699396\n",
      "Epoch: 6680 Training Loss: 0.04402811011806354 Test Loss: 0.05662318820348367\n",
      "Epoch: 6690 Training Loss: 0.04402628496214623 Test Loss: 0.05661859018774781\n",
      "Epoch: 6700 Training Loss: 0.044025464527595023 Test Loss: 0.05662071724765886\n",
      "Epoch: 6710 Training Loss: 0.04402469026425446 Test Loss: 0.05661441248923988\n",
      "Epoch: 6720 Training Loss: 0.04402397819273858 Test Loss: 0.056617049024578724\n",
      "Epoch: 6730 Training Loss: 0.04402279624954886 Test Loss: 0.05662036698330225\n",
      "Epoch: 6740 Training Loss: 0.04402253514339211 Test Loss: 0.05661041310713168\n",
      "Epoch: 6750 Training Loss: 0.044022149554079085 Test Loss: 0.05661604281060883\n",
      "Epoch: 6760 Training Loss: 0.04402026867428686 Test Loss: 0.05661188421742944\n",
      "Epoch: 6770 Training Loss: 0.04401880512651497 Test Loss: 0.05659893717312057\n",
      "Epoch: 6780 Training Loss: 0.04401791105684334 Test Loss: 0.05659887348869209\n",
      "Epoch: 6790 Training Loss: 0.044016240136651244 Test Loss: 0.056594944159455214\n",
      "Epoch: 6800 Training Loss: 0.04401520904595147 Test Loss: 0.056589728404763145\n",
      "Epoch: 6810 Training Loss: 0.04401392192394785 Test Loss: 0.05659168351671732\n",
      "Epoch: 6820 Training Loss: 0.04401313353062472 Test Loss: 0.05660026817767568\n",
      "Epoch: 6830 Training Loss: 0.044013237813876345 Test Loss: 0.056598841646477854\n",
      "Epoch: 6840 Training Loss: 0.04401133683368638 Test Loss: 0.056601490918702396\n",
      "Epoch: 6850 Training Loss: 0.044010111107452086 Test Loss: 0.056598497750564095\n",
      "Epoch: 6860 Training Loss: 0.044009692481841785 Test Loss: 0.05659856143499257\n",
      "Epoch: 6870 Training Loss: 0.04400873472774168 Test Loss: 0.05660994821080381\n",
      "Epoch: 6880 Training Loss: 0.044009175642901945 Test Loss: 0.056594765843055485\n",
      "Epoch: 6890 Training Loss: 0.04400767269038995 Test Loss: 0.05659317373234362\n",
      "Epoch: 6900 Training Loss: 0.044007532584647305 Test Loss: 0.05659333294341481\n",
      "Epoch: 6910 Training Loss: 0.044005847932500314 Test Loss: 0.056585104915255896\n",
      "Epoch: 6920 Training Loss: 0.04400534800973679 Test Loss: 0.05658426428080003\n",
      "Epoch: 6930 Training Loss: 0.044005802258824264 Test Loss: 0.05659424363074199\n",
      "Epoch: 6940 Training Loss: 0.04400439582802417 Test Loss: 0.056586760710396235\n",
      "Epoch: 6950 Training Loss: 0.0440029394447505 Test Loss: 0.05658701544811013\n",
      "Epoch: 6960 Training Loss: 0.0440037062450721 Test Loss: 0.056586404077596776\n",
      "Epoch: 6970 Training Loss: 0.04400267077606787 Test Loss: 0.05657901668389373\n",
      "Epoch: 6980 Training Loss: 0.04400168744868946 Test Loss: 0.056569279334779975\n",
      "Epoch: 6990 Training Loss: 0.044000693971605255 Test Loss: 0.056559032510238416\n",
      "Epoch: 7000 Training Loss: 0.044000309576375254 Test Loss: 0.05655249848787693\n",
      "Epoch: 7010 Training Loss: 0.043999923589034554 Test Loss: 0.05654704710079951\n",
      "Epoch: 7020 Training Loss: 0.04399892155435527 Test Loss: 0.056549543530395714\n",
      "Epoch: 7030 Training Loss: 0.04399780797241924 Test Loss: 0.05656299368168954\n",
      "Epoch: 7040 Training Loss: 0.04399734536475053 Test Loss: 0.05653616980041606\n",
      "Epoch: 7050 Training Loss: 0.043997553334212264 Test Loss: 0.05654613004502947\n",
      "Epoch: 7060 Training Loss: 0.04399625845066892 Test Loss: 0.056562535153804515\n",
      "Epoch: 7070 Training Loss: 0.04399535323622231 Test Loss: 0.056543404351490766\n",
      "Epoch: 7080 Training Loss: 0.0439939845185447 Test Loss: 0.056546588572914495\n",
      "Epoch: 7090 Training Loss: 0.04399324349051525 Test Loss: 0.05654811699919788\n",
      "Epoch: 7100 Training Loss: 0.04399283362151386 Test Loss: 0.0565438246687187\n",
      "Epoch: 7110 Training Loss: 0.04399127464660619 Test Loss: 0.056537271541028666\n",
      "Epoch: 7120 Training Loss: 0.043990560186924245 Test Loss: 0.056532590735535786\n",
      "Epoch: 7130 Training Loss: 0.04399025360610529 Test Loss: 0.0565433151932909\n",
      "Epoch: 7140 Training Loss: 0.04398987577833198 Test Loss: 0.05653302378964942\n",
      "Epoch: 7150 Training Loss: 0.043987050776887616 Test Loss: 0.056524241706962776\n",
      "Epoch: 7160 Training Loss: 0.04398602685068605 Test Loss: 0.05653154631090881\n",
      "Epoch: 7170 Training Loss: 0.04398542254516398 Test Loss: 0.056546238308557886\n",
      "Epoch: 7180 Training Loss: 0.04398452509225709 Test Loss: 0.05651915332112766\n",
      "Epoch: 7190 Training Loss: 0.04398329906750203 Test Loss: 0.05651910237358488\n",
      "Epoch: 7200 Training Loss: 0.0439836103251462 Test Loss: 0.056535392850388666\n",
      "Epoch: 7210 Training Loss: 0.04398250679340904 Test Loss: 0.056529444724769146\n",
      "Epoch: 7220 Training Loss: 0.043981327934933825 Test Loss: 0.056537252435700125\n",
      "Epoch: 7230 Training Loss: 0.043979819012006656 Test Loss: 0.056533526896634365\n",
      "Epoch: 7240 Training Loss: 0.0439798570236499 Test Loss: 0.05652224838435152\n",
      "Epoch: 7250 Training Loss: 0.04397904365408998 Test Loss: 0.056524248075405625\n",
      "Epoch: 7260 Training Loss: 0.04397929421251326 Test Loss: 0.05651552330870461\n",
      "Epoch: 7270 Training Loss: 0.04397802649435893 Test Loss: 0.056511364715525224\n",
      "Epoch: 7280 Training Loss: 0.04397693092317533 Test Loss: 0.05651728099893051\n",
      "Epoch: 7290 Training Loss: 0.04397602899245706 Test Loss: 0.0565097917101419\n",
      "Epoch: 7300 Training Loss: 0.04397413438070995 Test Loss: 0.05649400434032306\n",
      "Epoch: 7310 Training Loss: 0.043974693410583646 Test Loss: 0.05650162736641147\n",
      "Epoch: 7320 Training Loss: 0.04397473480546216 Test Loss: 0.05648562346953581\n",
      "Epoch: 7330 Training Loss: 0.04397490327067686 Test Loss: 0.05647200137028511\n",
      "Epoch: 7340 Training Loss: 0.043974715103092095 Test Loss: 0.05647840802378965\n",
      "Epoch: 7350 Training Loss: 0.043973100304802594 Test Loss: 0.05647654843847819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7360 Training Loss: 0.043972061751083855 Test Loss: 0.05646918651854654\n",
      "Epoch: 7370 Training Loss: 0.04397060536781018 Test Loss: 0.056460207014131625\n",
      "Epoch: 7380 Training Loss: 0.04396953308124574 Test Loss: 0.0564651425573384\n",
      "Epoch: 7390 Training Loss: 0.043969434071860845 Test Loss: 0.05647197589651372\n",
      "Epoch: 7400 Training Loss: 0.04396810863969322 Test Loss: 0.056465581979894874\n",
      "Epoch: 7410 Training Loss: 0.04396687773909911 Test Loss: 0.056468982728375416\n",
      "Epoch: 7420 Training Loss: 0.043964788491817466 Test Loss: 0.056471574684614334\n",
      "Epoch: 7430 Training Loss: 0.04396526642355179 Test Loss: 0.05647221789734192\n",
      "Epoch: 7440 Training Loss: 0.04396458559720862 Test Loss: 0.056473587112554124\n",
      "Epoch: 7450 Training Loss: 0.04396377182962102 Test Loss: 0.05646484960896742\n",
      "Epoch: 7460 Training Loss: 0.043963091301798624 Test Loss: 0.056461149543673046\n",
      "Epoch: 7470 Training Loss: 0.04396077826345504 Test Loss: 0.056471339052228973\n",
      "Epoch: 7480 Training Loss: 0.04396032789513742 Test Loss: 0.05646270981217068\n",
      "Epoch: 7490 Training Loss: 0.04396008639184382 Test Loss: 0.056452946989285525\n",
      "Epoch: 7500 Training Loss: 0.043959635227470845 Test Loss: 0.05644798597230736\n",
      "Epoch: 7510 Training Loss: 0.04395852443172856 Test Loss: 0.056443700010271027\n",
      "Epoch: 7520 Training Loss: 0.04395718437204377 Test Loss: 0.05644698612678031\n",
      "Epoch: 7530 Training Loss: 0.04395607725805751 Test Loss: 0.05644820886780702\n",
      "Epoch: 7540 Training Loss: 0.043954842576200456 Test Loss: 0.05646932662428918\n",
      "Epoch: 7550 Training Loss: 0.04395382621252477 Test Loss: 0.056448329868221125\n",
      "Epoch: 7560 Training Loss: 0.04395293005320783 Test Loss: 0.05645340551717055\n",
      "Epoch: 7570 Training Loss: 0.043951579246775736 Test Loss: 0.05646453118682505\n",
      "Epoch: 7580 Training Loss: 0.04395039232824004 Test Loss: 0.056473822744939485\n",
      "Epoch: 7590 Training Loss: 0.0439484787106713 Test Loss: 0.0564679064615342\n",
      "Epoch: 7600 Training Loss: 0.04394763558854245 Test Loss: 0.056454277993840646\n",
      "Epoch: 7610 Training Loss: 0.043947320748649175 Test Loss: 0.05646537182128091\n",
      "Epoch: 7620 Training Loss: 0.0439453369787022 Test Loss: 0.056461690861315084\n",
      "Epoch: 7630 Training Loss: 0.04394395363350743 Test Loss: 0.05646307281341298\n",
      "Epoch: 7640 Training Loss: 0.04394285119634638 Test Loss: 0.056470447470230334\n",
      "Epoch: 7650 Training Loss: 0.043942026383490713 Test Loss: 0.05646010511904607\n",
      "Epoch: 7660 Training Loss: 0.04394069567745636 Test Loss: 0.056485725364621375\n",
      "Epoch: 7670 Training Loss: 0.0439392542202206 Test Loss: 0.05648747031796158\n",
      "Epoch: 7680 Training Loss: 0.0439379866015732 Test Loss: 0.056486241208492015\n",
      "Epoch: 7690 Training Loss: 0.04393713731001534 Test Loss: 0.056476038963050396\n",
      "Epoch: 7700 Training Loss: 0.04393600890154831 Test Loss: 0.05646995710013108\n",
      "Epoch: 7710 Training Loss: 0.0439353242939422 Test Loss: 0.05647579696222219\n",
      "Epoch: 7720 Training Loss: 0.04393337445585477 Test Loss: 0.05646138517605841\n",
      "Epoch: 7730 Training Loss: 0.043933118325044 Test Loss: 0.05646825672589081\n",
      "Epoch: 7740 Training Loss: 0.043931911903152085 Test Loss: 0.056463709657697723\n",
      "Epoch: 7750 Training Loss: 0.04393046945084714 Test Loss: 0.05645474289016851\n",
      "Epoch: 7760 Training Loss: 0.0439304723365478 Test Loss: 0.05647070220794423\n",
      "Epoch: 7770 Training Loss: 0.043928787385880054 Test Loss: 0.05647105884074369\n",
      "Epoch: 7780 Training Loss: 0.04392769549645247 Test Loss: 0.05647724896719141\n",
      "Epoch: 7790 Training Loss: 0.0439254034540689 Test Loss: 0.0564867761576912\n",
      "Epoch: 7800 Training Loss: 0.04392444370983041 Test Loss: 0.05647462516873826\n",
      "Epoch: 7810 Training Loss: 0.04392372327973329 Test Loss: 0.056490374327900016\n",
      "Epoch: 7820 Training Loss: 0.04392215166744684 Test Loss: 0.0564837829895529\n",
      "Epoch: 7830 Training Loss: 0.04392156596971872 Test Loss: 0.056482579353854735\n",
      "Epoch: 7840 Training Loss: 0.04392093201113464 Test Loss: 0.05647836981313256\n",
      "Epoch: 7850 Training Loss: 0.043918948141680736 Test Loss: 0.05646695119510708\n",
      "Epoch: 7860 Training Loss: 0.04391820761118588 Test Loss: 0.05646857514803318\n",
      "Epoch: 7870 Training Loss: 0.043917305978988366 Test Loss: 0.05646323202448417\n",
      "Epoch: 7880 Training Loss: 0.04391580690724623 Test Loss: 0.0564690336759182\n",
      "Epoch: 7890 Training Loss: 0.04391555575178143 Test Loss: 0.056477720231962125\n",
      "Epoch: 7900 Training Loss: 0.0439128467754052 Test Loss: 0.056462461442899625\n",
      "Epoch: 7910 Training Loss: 0.04391062458687911 Test Loss: 0.0564630218658702\n",
      "Epoch: 7920 Training Loss: 0.043909813505978335 Test Loss: 0.05647039652268755\n",
      "Epoch: 7930 Training Loss: 0.0439082449784064 Test Loss: 0.05646982973127413\n",
      "Epoch: 7940 Training Loss: 0.043906999848322796 Test Loss: 0.05646434650198247\n",
      "Epoch: 7950 Training Loss: 0.04390506115501035 Test Loss: 0.05645774879519251\n",
      "Epoch: 7960 Training Loss: 0.04390492552707908 Test Loss: 0.05648643226177744\n",
      "Epoch: 7970 Training Loss: 0.04390388249554897 Test Loss: 0.05647572054090803\n",
      "Epoch: 7980 Training Loss: 0.0439026512964341 Test Loss: 0.05646977878373135\n",
      "Epoch: 7990 Training Loss: 0.04390186638585315 Test Loss: 0.05647980908121609\n",
      "Epoch: 8000 Training Loss: 0.04390093480207287 Test Loss: 0.05647726170407711\n",
      "Epoch: 8010 Training Loss: 0.04389935274155988 Test Loss: 0.05648264303828321\n",
      "Epoch: 8020 Training Loss: 0.043898067510187724 Test Loss: 0.056478465339775276\n",
      "Epoch: 8030 Training Loss: 0.04389667341824565 Test Loss: 0.05648745121263304\n",
      "Epoch: 8040 Training Loss: 0.04389696497351976 Test Loss: 0.05649659629656198\n",
      "Epoch: 8050 Training Loss: 0.04389665570601398 Test Loss: 0.05648992216845785\n",
      "Epoch: 8060 Training Loss: 0.04389538321152752 Test Loss: 0.056498366723673574\n",
      "Epoch: 8070 Training Loss: 0.04389407499405697 Test Loss: 0.05649086469799927\n",
      "Epoch: 8080 Training Loss: 0.04389354074140622 Test Loss: 0.056494259078036964\n",
      "Epoch: 8090 Training Loss: 0.04389252925356959 Test Loss: 0.05649857051384469\n",
      "Epoch: 8100 Training Loss: 0.04389267343909593 Test Loss: 0.05649190912262625\n",
      "Epoch: 8110 Training Loss: 0.04389193828197473 Test Loss: 0.05649976778110001\n",
      "Epoch: 8120 Training Loss: 0.04389091057451022 Test Loss: 0.056494838606336084\n",
      "Epoch: 8130 Training Loss: 0.04388870012780064 Test Loss: 0.056497653458074656\n",
      "Epoch: 8140 Training Loss: 0.04388725857105796 Test Loss: 0.05649543723996374\n",
      "Epoch: 8150 Training Loss: 0.04388613842166525 Test Loss: 0.05649016416928605\n",
      "Epoch: 8160 Training Loss: 0.0438858155217115 Test Loss: 0.0565060470657476\n",
      "Epoch: 8170 Training Loss: 0.04388457417289084 Test Loss: 0.0565040346378078\n",
      "Epoch: 8180 Training Loss: 0.04388382737346006 Test Loss: 0.056497806300702995\n",
      "Epoch: 8190 Training Loss: 0.04388251318557434 Test Loss: 0.05649518887069269\n",
      "Epoch: 8200 Training Loss: 0.04388178360084062 Test Loss: 0.05649752608921771\n",
      "Epoch: 8210 Training Loss: 0.04388171076177556 Test Loss: 0.05649813109128821\n",
      "Epoch: 8220 Training Loss: 0.04387987018228572 Test Loss: 0.05651148571593933\n",
      "Epoch: 8230 Training Loss: 0.04387969365701055 Test Loss: 0.05649813745973106\n",
      "Epoch: 8240 Training Loss: 0.04387943175479844 Test Loss: 0.05650157641886869\n",
      "Epoch: 8250 Training Loss: 0.04387797457546941 Test Loss: 0.056501907577896755\n",
      "Epoch: 8260 Training Loss: 0.0438763725140656 Test Loss: 0.056514765464005766\n",
      "Epoch: 8270 Training Loss: 0.04387547426510335 Test Loss: 0.05649722040396103\n",
      "Epoch: 8280 Training Loss: 0.04387402594189015 Test Loss: 0.056489616483201166\n",
      "Epoch: 8290 Training Loss: 0.043873139733265155 Test Loss: 0.05649287712593907\n",
      "Epoch: 8300 Training Loss: 0.04387146513131704 Test Loss: 0.05649064817094246\n",
      "Epoch: 8310 Training Loss: 0.043870344782910485 Test Loss: 0.056497105771989777\n",
      "Epoch: 8320 Training Loss: 0.043869392899718626 Test Loss: 0.05648729200156185\n",
      "Epoch: 8330 Training Loss: 0.043869066318008854 Test Loss: 0.056476994229477516\n",
      "Epoch: 8340 Training Loss: 0.04386822926580209 Test Loss: 0.0564850439412367\n",
      "Epoch: 8350 Training Loss: 0.04386533888831162 Test Loss: 0.056480528715257855\n",
      "Epoch: 8360 Training Loss: 0.043865370432005106 Test Loss: 0.056495806609648896\n",
      "Epoch: 8370 Training Loss: 0.04386387852476117 Test Loss: 0.05650444221815004\n",
      "Epoch: 8380 Training Loss: 0.04386329422012991 Test Loss: 0.05651610920544658\n",
      "Epoch: 8390 Training Loss: 0.04386239139384937 Test Loss: 0.056510097395398584\n",
      "Epoch: 8400 Training Loss: 0.043861101286638164 Test Loss: 0.056507518176045365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8410 Training Loss: 0.04386105222972685 Test Loss: 0.05651890495185661\n",
      "Epoch: 8420 Training Loss: 0.043858906960049536 Test Loss: 0.05652186627778068\n",
      "Epoch: 8430 Training Loss: 0.04385595050996453 Test Loss: 0.056516191995203593\n",
      "Epoch: 8440 Training Loss: 0.0438554587467684 Test Loss: 0.056515141202133766\n",
      "Epoch: 8450 Training Loss: 0.04385355866214071 Test Loss: 0.056506906805532005\n",
      "Epoch: 8460 Training Loss: 0.043852497918378926 Test Loss: 0.05651151118971071\n",
      "Epoch: 8470 Training Loss: 0.04385202814621201 Test Loss: 0.056511364715525224\n",
      "Epoch: 8480 Training Loss: 0.04385032558281951 Test Loss: 0.056498545040073296\n",
      "Epoch: 8490 Training Loss: 0.043849744064382 Test Loss: 0.056511944243824344\n",
      "Epoch: 8500 Training Loss: 0.043848626999703794 Test Loss: 0.05650668391003234\n",
      "Epoch: 8510 Training Loss: 0.04384570816323435 Test Loss: 0.05651274666762312\n",
      "Epoch: 8520 Training Loss: 0.04384518286620635 Test Loss: 0.0565091739711857\n",
      "Epoch: 8530 Training Loss: 0.043842552002761916 Test Loss: 0.05651242824548075\n",
      "Epoch: 8540 Training Loss: 0.043843240590644796 Test Loss: 0.05651863110881417\n",
      "Epoch: 8550 Training Loss: 0.043841346376925355 Test Loss: 0.05651955453302705\n",
      "Epoch: 8560 Training Loss: 0.043839441316951694 Test Loss: 0.05652061806298257\n",
      "Epoch: 8570 Training Loss: 0.04383883730995038 Test Loss: 0.05653013251659667\n",
      "Epoch: 8580 Training Loss: 0.043837262712456346 Test Loss: 0.056525922975874505\n",
      "Epoch: 8590 Training Loss: 0.043836969863592284 Test Loss: 0.0565303108329964\n",
      "Epoch: 8600 Training Loss: 0.043835058634189614 Test Loss: 0.056530737518667176\n",
      "Epoch: 8610 Training Loss: 0.04383369160812964 Test Loss: 0.056532947368335244\n",
      "Epoch: 8620 Training Loss: 0.04383228259014964 Test Loss: 0.05653587685204507\n",
      "Epoch: 8630 Training Loss: 0.04383047753463006 Test Loss: 0.056536513696329824\n",
      "Epoch: 8640 Training Loss: 0.0438291008563989 Test Loss: 0.05654071686860914\n",
      "Epoch: 8650 Training Loss: 0.043828714769551275 Test Loss: 0.056550632534122625\n",
      "Epoch: 8660 Training Loss: 0.04382791503243932 Test Loss: 0.05654884937012534\n",
      "Epoch: 8670 Training Loss: 0.043827331225342665 Test Loss: 0.056547333680727646\n",
      "Epoch: 8680 Training Loss: 0.04382556119625875 Test Loss: 0.05654104165919436\n",
      "Epoch: 8690 Training Loss: 0.043825113813148714 Test Loss: 0.0565463210983149\n",
      "Epoch: 8700 Training Loss: 0.04382453289175273 Test Loss: 0.05656063735783598\n",
      "Epoch: 8710 Training Loss: 0.043823663101769446 Test Loss: 0.05653850065049822\n",
      "Epoch: 8720 Training Loss: 0.04382259310386416 Test Loss: 0.056547887735255375\n",
      "Epoch: 8730 Training Loss: 0.0438217591363719 Test Loss: 0.056551606905878286\n",
      "Epoch: 8740 Training Loss: 0.04382086466867259 Test Loss: 0.05654593899174405\n",
      "Epoch: 8750 Training Loss: 0.04382087760457212 Test Loss: 0.056553810387103505\n",
      "Epoch: 8760 Training Loss: 0.04381942490305447 Test Loss: 0.056553982335060385\n",
      "Epoch: 8770 Training Loss: 0.04381914041277164 Test Loss: 0.05654694520571395\n",
      "Epoch: 8780 Training Loss: 0.043818832637869655 Test Loss: 0.05655673987081333\n",
      "Epoch: 8790 Training Loss: 0.04381822305848085 Test Loss: 0.05654223255800683\n",
      "Epoch: 8800 Training Loss: 0.043817776371919255 Test Loss: 0.05656094304309266\n",
      "Epoch: 8810 Training Loss: 0.04381682558330351 Test Loss: 0.056544123985532525\n",
      "Epoch: 8820 Training Loss: 0.04381597290851039 Test Loss: 0.056545633306487375\n",
      "Epoch: 8830 Training Loss: 0.043815914895976325 Test Loss: 0.05655943372213781\n",
      "Epoch: 8840 Training Loss: 0.043815362334052387 Test Loss: 0.0565463210983149\n",
      "Epoch: 8850 Training Loss: 0.04381519745108679 Test Loss: 0.05654529577901646\n",
      "Epoch: 8860 Training Loss: 0.04381534969667361 Test Loss: 0.056558497561039235\n",
      "Epoch: 8870 Training Loss: 0.04381476290436937 Test Loss: 0.056559070720895506\n",
      "Epoch: 8880 Training Loss: 0.04381416446975555 Test Loss: 0.05655859945612479\n",
      "Epoch: 8890 Training Loss: 0.04381311676140023 Test Loss: 0.05656249694314743\n",
      "Epoch: 8900 Training Loss: 0.043812170649609704 Test Loss: 0.056557153819598416\n",
      "Epoch: 8910 Training Loss: 0.043811576692807254 Test Loss: 0.05656380884237401\n",
      "Epoch: 8920 Training Loss: 0.04381290202546796 Test Loss: 0.05656303189234662\n",
      "Epoch: 8930 Training Loss: 0.04381214457879679 Test Loss: 0.05654997021606649\n",
      "Epoch: 8940 Training Loss: 0.043811912031125944 Test Loss: 0.056545066515073954\n",
      "Epoch: 8950 Training Loss: 0.04381207611803618 Test Loss: 0.05654691973194256\n",
      "Epoch: 8960 Training Loss: 0.043810444503077285 Test Loss: 0.056556962766312995\n",
      "Epoch: 8970 Training Loss: 0.04380787573195061 Test Loss: 0.056558064506925604\n",
      "Epoch: 8980 Training Loss: 0.0438062379475627 Test Loss: 0.05655766966346906\n",
      "Epoch: 8990 Training Loss: 0.04380569414224768 Test Loss: 0.0565649487936437\n",
      "Epoch: 9000 Training Loss: 0.04380347255076311 Test Loss: 0.056566279798198824\n",
      "Epoch: 9010 Training Loss: 0.04380351832394608 Test Loss: 0.056563108313660786\n",
      "Epoch: 9020 Training Loss: 0.043802616691748564 Test Loss: 0.05656431831780181\n",
      "Epoch: 9030 Training Loss: 0.043801930392524835 Test Loss: 0.056568878122880584\n",
      "Epoch: 9040 Training Loss: 0.043801283697055056 Test Loss: 0.05657015181145007\n",
      "Epoch: 9050 Training Loss: 0.043800256984659745 Test Loss: 0.05657231708201821\n",
      "Epoch: 9060 Training Loss: 0.04380013061087199 Test Loss: 0.056571094340991494\n",
      "Epoch: 9070 Training Loss: 0.04380019529036966 Test Loss: 0.05656392347434526\n",
      "Epoch: 9080 Training Loss: 0.04380095303556159 Test Loss: 0.05656435652845889\n",
      "Epoch: 9090 Training Loss: 0.04379911822747309 Test Loss: 0.05654929516112466\n",
      "Epoch: 9100 Training Loss: 0.04379806066793273 Test Loss: 0.05656284720750404\n",
      "Epoch: 9110 Training Loss: 0.04379764353492622 Test Loss: 0.05656379610548831\n",
      "Epoch: 9120 Training Loss: 0.04379750402622509 Test Loss: 0.05656763627652533\n",
      "Epoch: 9130 Training Loss: 0.04379658935862113 Test Loss: 0.05656005146109401\n",
      "Epoch: 9140 Training Loss: 0.043796924995460576 Test Loss: 0.05655240932967707\n",
      "Epoch: 9150 Training Loss: 0.04379564304781677 Test Loss: 0.05655162601120683\n",
      "Epoch: 9160 Training Loss: 0.0437962931265218 Test Loss: 0.05655801355938283\n",
      "Epoch: 9170 Training Loss: 0.04379623362138395 Test Loss: 0.05655842113972506\n",
      "Epoch: 9180 Training Loss: 0.043794980431239874 Test Loss: 0.05654814247296927\n",
      "Epoch: 9190 Training Loss: 0.043794934956577665 Test Loss: 0.05655875866719598\n",
      "Epoch: 9200 Training Loss: 0.04379386416261702 Test Loss: 0.05655277869936222\n",
      "Epoch: 9210 Training Loss: 0.04379500600451818 Test Loss: 0.05655462554778798\n",
      "Epoch: 9220 Training Loss: 0.04379421442697363 Test Loss: 0.05655597565767164\n",
      "Epoch: 9230 Training Loss: 0.04379252380441147 Test Loss: 0.05655097643003639\n",
      "Epoch: 9240 Training Loss: 0.04379213214517635 Test Loss: 0.0565408187636947\n",
      "Epoch: 9250 Training Loss: 0.0437911119007308 Test Loss: 0.05653801664884182\n",
      "Epoch: 9260 Training Loss: 0.04379184407264442 Test Loss: 0.056543226035091036\n",
      "Epoch: 9270 Training Loss: 0.04379166237300943 Test Loss: 0.05654069139483775\n",
      "Epoch: 9280 Training Loss: 0.04379081656419375 Test Loss: 0.05654416856463246\n",
      "Epoch: 9290 Training Loss: 0.04379077895057818 Test Loss: 0.056543060455577006\n",
      "Epoch: 9300 Training Loss: 0.04378889986191051 Test Loss: 0.05653996539235314\n",
      "Epoch: 9310 Training Loss: 0.0437888268238316 Test Loss: 0.05654553777984466\n",
      "Epoch: 9320 Training Loss: 0.04378845347386967 Test Loss: 0.056545276673687916\n",
      "Epoch: 9330 Training Loss: 0.043788187591380784 Test Loss: 0.056540927027223103\n",
      "Epoch: 9340 Training Loss: 0.04378666384192261 Test Loss: 0.056543589036333344\n",
      "Epoch: 9350 Training Loss: 0.043786885841859996 Test Loss: 0.056542837560077344\n",
      "Epoch: 9360 Training Loss: 0.04378669588315069 Test Loss: 0.05654009912965294\n",
      "Epoch: 9370 Training Loss: 0.043785908186375996 Test Loss: 0.05654369729986175\n",
      "Epoch: 9380 Training Loss: 0.043786176059003264 Test Loss: 0.05654792594591246\n",
      "Epoch: 9390 Training Loss: 0.043785875249585646 Test Loss: 0.05655211638130608\n",
      "Epoch: 9400 Training Loss: 0.043785444882158846 Test Loss: 0.056554186125231505\n",
      "Epoch: 9410 Training Loss: 0.04378473052198381 Test Loss: 0.0565593254586094\n",
      "Epoch: 9420 Training Loss: 0.04378365266303188 Test Loss: 0.056542831191634495\n",
      "Epoch: 9430 Training Loss: 0.04378382311838497 Test Loss: 0.056547276364742015\n",
      "Epoch: 9440 Training Loss: 0.04378359674014313 Test Loss: 0.0565564532908852\n",
      "Epoch: 9450 Training Loss: 0.04378429677132175 Test Loss: 0.05656373242105984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9460 Training Loss: 0.04378266376326598 Test Loss: 0.0565583319815252\n",
      "Epoch: 9470 Training Loss: 0.04378277272334282 Test Loss: 0.0565584529819393\n",
      "Epoch: 9480 Training Loss: 0.04378246922723837 Test Loss: 0.056565553795714214\n",
      "Epoch: 9490 Training Loss: 0.04378226115826971 Test Loss: 0.05657773662688139\n",
      "Epoch: 9500 Training Loss: 0.043781082498808334 Test Loss: 0.05656675106296953\n",
      "Epoch: 9510 Training Loss: 0.043780528643294445 Test Loss: 0.05656771906628234\n",
      "Epoch: 9520 Training Loss: 0.043780063149925066 Test Loss: 0.05655597565767164\n",
      "Epoch: 9530 Training Loss: 0.043779001212080254 Test Loss: 0.0565659550076136\n",
      "Epoch: 9540 Training Loss: 0.043779089275704 Test Loss: 0.05656641990394146\n",
      "Epoch: 9550 Training Loss: 0.04377855402798406 Test Loss: 0.05657127902583407\n",
      "Epoch: 9560 Training Loss: 0.04377907862846362 Test Loss: 0.0565583319815252\n",
      "Epoch: 9570 Training Loss: 0.04377740034475947 Test Loss: 0.05657441229971502\n",
      "Epoch: 9580 Training Loss: 0.043776572248175466 Test Loss: 0.05657104339344872\n",
      "Epoch: 9590 Training Loss: 0.04377665593349476 Test Loss: 0.05657891478880817\n",
      "Epoch: 9600 Training Loss: 0.043776976246268605 Test Loss: 0.056564942425200854\n",
      "Epoch: 9610 Training Loss: 0.04377695913107845 Test Loss: 0.056577857627295496\n",
      "Epoch: 9620 Training Loss: 0.04377377879042458 Test Loss: 0.05656993528439326\n",
      "Epoch: 9630 Training Loss: 0.04377431871496974 Test Loss: 0.05657436772061509\n",
      "Epoch: 9640 Training Loss: 0.043773720180849005 Test Loss: 0.05657527203949943\n",
      "Epoch: 9650 Training Loss: 0.04377330991381994 Test Loss: 0.056579131315864985\n",
      "Epoch: 9660 Training Loss: 0.04377334673138015 Test Loss: 0.05659131414703216\n",
      "Epoch: 9670 Training Loss: 0.04377194527592603 Test Loss: 0.0565870918694243\n",
      "Epoch: 9680 Training Loss: 0.043772611872779706 Test Loss: 0.056583570120529655\n",
      "Epoch: 9690 Training Loss: 0.043772827802795004 Test Loss: 0.056587110974752844\n",
      "Epoch: 9700 Training Loss: 0.043772302704780844 Test Loss: 0.05659337115407189\n",
      "Epoch: 9710 Training Loss: 0.04377120982028407 Test Loss: 0.05658974751009169\n",
      "Epoch: 9720 Training Loss: 0.043770925628522 Test Loss: 0.05658537238985549\n",
      "Epoch: 9730 Training Loss: 0.04376975731778025 Test Loss: 0.05658886229653589\n",
      "Epoch: 9740 Training Loss: 0.0437700194190062 Test Loss: 0.056589919458048574\n",
      "Epoch: 9750 Training Loss: 0.04376953273066296 Test Loss: 0.05659018056420532\n",
      "Epoch: 9760 Training Loss: 0.04376954457198638 Test Loss: 0.05658253206434553\n",
      "Epoch: 9770 Training Loss: 0.043769381281131496 Test Loss: 0.056598835278035005\n",
      "Epoch: 9780 Training Loss: 0.04376890255334182 Test Loss: 0.05661027936983188\n",
      "Epoch: 9790 Training Loss: 0.0437677292672541 Test Loss: 0.05659842132924992\n",
      "Epoch: 9800 Training Loss: 0.043768410292611096 Test Loss: 0.05659518616028342\n",
      "Epoch: 9810 Training Loss: 0.0437675802058887 Test Loss: 0.05659038435437643\n",
      "Epoch: 9820 Training Loss: 0.04376695868566956 Test Loss: 0.05659731958863731\n",
      "Epoch: 9830 Training Loss: 0.04376667130968607 Test Loss: 0.05660688498979419\n",
      "Epoch: 9840 Training Loss: 0.04376598650306612 Test Loss: 0.05660715246439378\n",
      "Epoch: 9850 Training Loss: 0.04376714774881659 Test Loss: 0.05659396978769955\n",
      "Epoch: 9860 Training Loss: 0.04376518985066868 Test Loss: 0.056598669698520974\n",
      "Epoch: 9870 Training Loss: 0.04376503511740887 Test Loss: 0.05659479768526972\n",
      "Epoch: 9880 Training Loss: 0.04376441916957721 Test Loss: 0.05660844525829181\n",
      "Epoch: 9890 Training Loss: 0.04376409716518574 Test Loss: 0.056606292724609375\n",
      "Epoch: 9900 Training Loss: 0.04376355186726693 Test Loss: 0.05660418477002687\n",
      "Epoch: 9910 Training Loss: 0.043763753866313496 Test Loss: 0.05661664781267933\n",
      "Epoch: 9920 Training Loss: 0.04376261779581366 Test Loss: 0.05661193516497222\n",
      "Epoch: 9930 Training Loss: 0.043763605501496536 Test Loss: 0.056619723770574656\n",
      "Epoch: 9940 Training Loss: 0.04376311383780732 Test Loss: 0.05661283311541371\n",
      "Epoch: 9950 Training Loss: 0.04376168332633271 Test Loss: 0.0566163612327512\n",
      "Epoch: 9960 Training Loss: 0.043762061154106024 Test Loss: 0.056621920883357026\n",
      "Epoch: 9970 Training Loss: 0.04376198722046484 Test Loss: 0.05662444915516747\n",
      "Epoch: 9980 Training Loss: 0.04376148590460444 Test Loss: 0.0566261176871935\n",
      "Epoch: 9990 Training Loss: 0.04376240226382604 Test Loss: 0.0566291108553318\n",
      "Epoch: 10000 Training Loss: 0.043760511632355704 Test Loss: 0.05662051982593059\n",
      "Epoch: 10010 Training Loss: 0.0437603793876597 Test Loss: 0.056626901005663734\n",
      "Epoch: 10020 Training Loss: 0.04375992742723137 Test Loss: 0.05662717484870618\n",
      "Epoch: 10030 Training Loss: 0.043759548703895784 Test Loss: 0.056628996223360546\n",
      "Epoch: 10040 Training Loss: 0.043760544171118375 Test Loss: 0.05662885611761791\n",
      "Epoch: 10050 Training Loss: 0.043759380537201846 Test Loss: 0.05662015045624544\n",
      "Epoch: 10060 Training Loss: 0.04375886787755263 Test Loss: 0.05662460199779581\n",
      "Epoch: 10070 Training Loss: 0.043758730458496806 Test Loss: 0.05663192570707038\n",
      "Epoch: 10080 Training Loss: 0.04375763030999491 Test Loss: 0.05663333950138251\n",
      "Epoch: 10090 Training Loss: 0.04375745368521281 Test Loss: 0.056629015328689095\n",
      "Epoch: 10100 Training Loss: 0.04375724870095866 Test Loss: 0.0566188576623474\n",
      "Epoch: 10110 Training Loss: 0.04375660638379334 Test Loss: 0.05662403520638238\n",
      "Epoch: 10120 Training Loss: 0.043755439963683064 Test Loss: 0.056635912352292886\n",
      "Epoch: 10130 Training Loss: 0.0437552813496534 Test Loss: 0.05662905990778902\n",
      "Epoch: 10140 Training Loss: 0.043754668287522405 Test Loss: 0.0566196218754891\n",
      "Epoch: 10150 Training Loss: 0.04375296134582545 Test Loss: 0.056630626544729494\n",
      "Epoch: 10160 Training Loss: 0.043753332108607476 Test Loss: 0.05663249886692665\n",
      "Epoch: 10170 Training Loss: 0.043753408430414725 Test Loss: 0.05663002154265899\n",
      "Epoch: 10180 Training Loss: 0.043751514216695285 Test Loss: 0.05663878452001708\n",
      "Epoch: 10190 Training Loss: 0.04375098314826596 Test Loss: 0.05664621649282006\n",
      "Epoch: 10200 Training Loss: 0.043749961510723544 Test Loss: 0.05664114721231349\n",
      "Epoch: 10210 Training Loss: 0.04374978120418542 Test Loss: 0.0566551450496922\n",
      "Epoch: 10220 Training Loss: 0.04375095458978007 Test Loss: 0.05664151658199865\n",
      "Epoch: 10230 Training Loss: 0.043750263713238036 Test Loss: 0.056664200975421276\n",
      "Epoch: 10240 Training Loss: 0.04374789763770637 Test Loss: 0.056656164000547786\n",
      "Epoch: 10250 Training Loss: 0.04374795744136498 Test Loss: 0.05666689482674575\n",
      "Epoch: 10260 Training Loss: 0.04374776071618515 Test Loss: 0.05665532973453477\n",
      "Epoch: 10270 Training Loss: 0.043747663696938645 Test Loss: 0.05665231109262508\n",
      "Epoch: 10280 Training Loss: 0.04374762528726772 Test Loss: 0.056658781430558094\n",
      "Epoch: 10290 Training Loss: 0.04374736776336008 Test Loss: 0.056665105294305615\n",
      "Epoch: 10300 Training Loss: 0.043746960680552435 Test Loss: 0.05665884511498657\n",
      "Epoch: 10310 Training Loss: 0.04374716964508337 Test Loss: 0.05665861585104406\n",
      "Epoch: 10320 Training Loss: 0.043746129698267765 Test Loss: 0.05666109317531172\n",
      "Epoch: 10330 Training Loss: 0.04374544737932081 Test Loss: 0.05665887058875796\n",
      "Epoch: 10340 Training Loss: 0.04374506099395242 Test Loss: 0.056657998112087854\n",
      "Epoch: 10350 Training Loss: 0.043743948108564835 Test Loss: 0.056654113361950906\n",
      "Epoch: 10360 Training Loss: 0.043745148062506976 Test Loss: 0.05665598568414806\n",
      "Epoch: 10370 Training Loss: 0.04374345972860397 Test Loss: 0.05665868590391538\n",
      "Epoch: 10380 Training Loss: 0.04374431280142477 Test Loss: 0.05665659705466142\n",
      "Epoch: 10390 Training Loss: 0.04374359674963211 Test Loss: 0.056657832532573824\n",
      "Epoch: 10400 Training Loss: 0.04374344629516984 Test Loss: 0.056657679689945485\n",
      "Epoch: 10410 Training Loss: 0.04374390323094414 Test Loss: 0.056660252540855854\n",
      "Epoch: 10420 Training Loss: 0.04374291094794297 Test Loss: 0.05666390802705029\n",
      "Epoch: 10430 Training Loss: 0.04374326161032726 Test Loss: 0.05666700309027416\n",
      "Epoch: 10440 Training Loss: 0.04374188373801306 Test Loss: 0.056656539738675786\n",
      "Epoch: 10450 Training Loss: 0.04374241978178836 Test Loss: 0.0566571447407463\n",
      "Epoch: 10460 Training Loss: 0.04374226863077765 Test Loss: 0.056656138526776395\n",
      "Epoch: 10470 Training Loss: 0.043740943696144625 Test Loss: 0.05666598413941856\n",
      "Epoch: 10480 Training Loss: 0.04374141625450529 Test Loss: 0.05667723080948717\n",
      "Epoch: 10490 Training Loss: 0.04374110061855666 Test Loss: 0.05666820035752947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10500 Training Loss: 0.04374048427269733 Test Loss: 0.05666424555452121\n",
      "Epoch: 10510 Training Loss: 0.04374041103560459 Test Loss: 0.05667692512423049\n",
      "Epoch: 10520 Training Loss: 0.043740736124710564 Test Loss: 0.05668119198093828\n",
      "Epoch: 10530 Training Loss: 0.043739650703232755 Test Loss: 0.0566851404155037\n",
      "Epoch: 10540 Training Loss: 0.043739257750407684 Test Loss: 0.056691521595236855\n",
      "Epoch: 10550 Training Loss: 0.04373825710882528 Test Loss: 0.05668642684095889\n",
      "Epoch: 10560 Training Loss: 0.043739824740834946 Test Loss: 0.05668361835766316\n",
      "Epoch: 10570 Training Loss: 0.04373910331566863 Test Loss: 0.05668561168027442\n",
      "Epoch: 10580 Training Loss: 0.043739323225960705 Test Loss: 0.0566919037018077\n",
      "Epoch: 10590 Training Loss: 0.04373871523868261 Test Loss: 0.056683911306034146\n",
      "Epoch: 10600 Training Loss: 0.04373710949552278 Test Loss: 0.056682019878508454\n",
      "Epoch: 10610 Training Loss: 0.04373806098068696 Test Loss: 0.056682382879750755\n",
      "Epoch: 10620 Training Loss: 0.04373708033999537 Test Loss: 0.05668489204623265\n",
      "Epoch: 10630 Training Loss: 0.04373718342916396 Test Loss: 0.05668392404291984\n",
      "Epoch: 10640 Training Loss: 0.04373783480145895 Test Loss: 0.05667730723080133\n",
      "Epoch: 10650 Training Loss: 0.043735849936935856 Test Loss: 0.056673397006893\n",
      "Epoch: 10660 Training Loss: 0.0437369656085172 Test Loss: 0.0566746452216911\n",
      "Epoch: 10670 Training Loss: 0.04373510263997048 Test Loss: 0.056672556372437136\n",
      "Epoch: 10680 Training Loss: 0.04373443474952685 Test Loss: 0.0566834081990492\n",
      "Epoch: 10690 Training Loss: 0.04373580446227365 Test Loss: 0.056677612916058016\n",
      "Epoch: 10700 Training Loss: 0.04373331569471025 Test Loss: 0.056681274770695296\n",
      "Epoch: 10710 Training Loss: 0.04373184348983637 Test Loss: 0.056684064148662484\n",
      "Epoch: 10720 Training Loss: 0.043731044548779775 Test Loss: 0.05668263761746466\n",
      "Epoch: 10730 Training Loss: 0.043730448203811265 Test Loss: 0.056695794820387495\n",
      "Epoch: 10740 Training Loss: 0.04372944925384649 Test Loss: 0.05668728021230045\n",
      "Epoch: 10750 Training Loss: 0.0437291373991608 Test Loss: 0.05669092933005204\n",
      "Epoch: 10760 Training Loss: 0.04372852801878584 Test Loss: 0.0566927634415921\n",
      "Epoch: 10770 Training Loss: 0.04372916227589067 Test Loss: 0.05669231128214994\n",
      "Epoch: 10780 Training Loss: 0.04372885380444025 Test Loss: 0.056700246361937864\n",
      "Epoch: 10790 Training Loss: 0.043727943614647664 Test Loss: 0.056692088386650276\n",
      "Epoch: 10800 Training Loss: 0.04372629090422183 Test Loss: 0.056691349647279975\n",
      "Epoch: 10810 Training Loss: 0.043727032927320476 Test Loss: 0.05669345760186248\n",
      "Epoch: 10820 Training Loss: 0.0437281357625092 Test Loss: 0.05669305638996309\n",
      "Epoch: 10830 Training Loss: 0.04372755096034335 Test Loss: 0.056694699448217735\n",
      "Epoch: 10840 Training Loss: 0.043725606694643405 Test Loss: 0.05670915581348145\n",
      "Epoch: 10850 Training Loss: 0.043724088219051965 Test Loss: 0.056695762978173254\n",
      "Epoch: 10860 Training Loss: 0.04372564908459111 Test Loss: 0.05669529171340255\n",
      "Epoch: 10870 Training Loss: 0.04372565396043016 Test Loss: 0.056709206761024235\n",
      "Epoch: 10880 Training Loss: 0.043724160859103196 Test Loss: 0.05670410563830342\n",
      "Epoch: 10890 Training Loss: 0.04372460306785342 Test Loss: 0.056702851055062474\n",
      "Epoch: 10900 Training Loss: 0.04372440266091756 Test Loss: 0.056714575358344635\n",
      "Epoch: 10910 Training Loss: 0.043723723526192027 Test Loss: 0.0567187976359525\n",
      "Epoch: 10920 Training Loss: 0.04372239361621302 Test Loss: 0.056727465086667886\n",
      "Epoch: 10930 Training Loss: 0.043722040764676506 Test Loss: 0.05671444798948769\n",
      "Epoch: 10940 Training Loss: 0.043721560444776125 Test Loss: 0.05671988663967941\n",
      "Epoch: 10950 Training Loss: 0.04372068030607322 Test Loss: 0.0567171609461407\n",
      "Epoch: 10960 Training Loss: 0.043719569112303264 Test Loss: 0.05671025755409406\n",
      "Epoch: 10970 Training Loss: 0.04371862140840203 Test Loss: 0.056708283336811355\n",
      "Epoch: 10980 Training Loss: 0.04371901306763714 Test Loss: 0.05671396398783128\n",
      "Epoch: 10990 Training Loss: 0.04371738752260033 Test Loss: 0.05672290528158911\n",
      "Epoch: 11000 Training Loss: 0.0437164771337939 Test Loss: 0.05671699536662667\n",
      "Epoch: 11010 Training Loss: 0.043715928552146746 Test Loss: 0.05672494318330029\n",
      "Epoch: 11020 Training Loss: 0.043715959399291786 Test Loss: 0.056738240491965776\n",
      "Epoch: 11030 Training Loss: 0.043715482064598986 Test Loss: 0.056729636725678864\n",
      "Epoch: 11040 Training Loss: 0.04371457625311086 Test Loss: 0.05672421718081568\n",
      "Epoch: 11050 Training Loss: 0.04371398488348832 Test Loss: 0.056723682231616494\n",
      "Epoch: 11060 Training Loss: 0.04371368695977136 Test Loss: 0.056729815042078593\n",
      "Epoch: 11070 Training Loss: 0.04371266940201264 Test Loss: 0.0567345531635571\n",
      "Epoch: 11080 Training Loss: 0.04371351889258434 Test Loss: 0.05673883912559344\n",
      "Epoch: 11090 Training Loss: 0.04371195444479609 Test Loss: 0.056729872358064225\n",
      "Epoch: 11100 Training Loss: 0.04371087210803279 Test Loss: 0.05673296742128808\n",
      "Epoch: 11110 Training Loss: 0.04370996589851698 Test Loss: 0.0567331712114592\n",
      "Epoch: 11120 Training Loss: 0.04371052890866746 Test Loss: 0.05672340202013121\n",
      "Epoch: 11130 Training Loss: 0.04370872843046618 Test Loss: 0.05672586023907033\n",
      "Epoch: 11140 Training Loss: 0.04370985474928791 Test Loss: 0.05673010162200673\n",
      "Epoch: 11150 Training Loss: 0.04370943562614301 Test Loss: 0.05673370616065839\n",
      "Epoch: 11160 Training Loss: 0.043708525734871176 Test Loss: 0.056736387275097167\n",
      "Epoch: 11170 Training Loss: 0.043709499111557644 Test Loss: 0.05674216345275981\n",
      "Epoch: 11180 Training Loss: 0.04370826821096353 Test Loss: 0.05674420772291384\n",
      "Epoch: 11190 Training Loss: 0.0437088270418234 Test Loss: 0.056747493839423124\n",
      "Epoch: 11200 Training Loss: 0.04370867827897876 Test Loss: 0.05675175432768807\n",
      "Epoch: 11210 Training Loss: 0.04370801058754897 Test Loss: 0.05675248669861553\n",
      "Epoch: 11220 Training Loss: 0.043707757441945785 Test Loss: 0.05675821192873539\n",
      "Epoch: 11230 Training Loss: 0.04370634514023744 Test Loss: 0.0567614470977019\n",
      "Epoch: 11240 Training Loss: 0.043706728639905164 Test Loss: 0.0567734452440265\n",
      "Epoch: 11250 Training Loss: 0.04370628464003039 Test Loss: 0.056771706659129145\n",
      "Epoch: 11260 Training Loss: 0.04370518110829323 Test Loss: 0.05678446901859545\n",
      "Epoch: 11270 Training Loss: 0.0437065378851405 Test Loss: 0.05678089632215803\n",
      "Epoch: 11280 Training Loss: 0.043706112095031995 Test Loss: 0.05679044261798636\n",
      "Epoch: 11290 Training Loss: 0.0437050538389432 Test Loss: 0.05679151888482758\n",
      "Epoch: 11300 Training Loss: 0.04370413101177184 Test Loss: 0.056805574038191906\n",
      "Epoch: 11310 Training Loss: 0.04370319524870094 Test Loss: 0.056797759958818084\n",
      "Epoch: 11320 Training Loss: 0.04370290956433508 Test Loss: 0.056816948077117464\n",
      "Epoch: 11330 Training Loss: 0.04370290379293375 Test Loss: 0.05681715186728858\n",
      "Epoch: 11340 Training Loss: 0.043701501441917354 Test Loss: 0.05681392306676492\n",
      "Epoch: 11350 Training Loss: 0.04370093683965616 Test Loss: 0.05681524770287719\n",
      "Epoch: 11360 Training Loss: 0.04370050836286083 Test Loss: 0.056819508191142135\n",
      "Epoch: 11370 Training Loss: 0.04369902809792647 Test Loss: 0.05681424148890729\n",
      "Epoch: 11380 Training Loss: 0.043701110976765274 Test Loss: 0.05682837943202864\n",
      "Epoch: 11390 Training Loss: 0.04369951339317284 Test Loss: 0.056827723482415354\n",
      "Epoch: 11400 Training Loss: 0.04369875415537711 Test Loss: 0.056830875861624844\n",
      "Epoch: 11410 Training Loss: 0.043697871728015064 Test Loss: 0.05682554547496153\n",
      "Epoch: 11420 Training Loss: 0.04369809641463928 Test Loss: 0.05682353941546458\n",
      "Epoch: 11430 Training Loss: 0.04369734961520849 Test Loss: 0.056821023880539834\n",
      "Epoch: 11440 Training Loss: 0.04369845533609788 Test Loss: 0.05682953212018403\n",
      "Epoch: 11450 Training Loss: 0.04369778018164913 Test Loss: 0.056832939237107416\n",
      "Epoch: 11460 Training Loss: 0.04369595223953807 Test Loss: 0.05681978840262743\n",
      "Epoch: 11470 Training Loss: 0.043698104872727433 Test Loss: 0.05681066879246987\n",
      "Epoch: 11480 Training Loss: 0.04369572506524087 Test Loss: 0.05681270032573821\n",
      "Epoch: 11490 Training Loss: 0.043696690680387625 Test Loss: 0.05682080735348302\n",
      "Epoch: 11500 Training Loss: 0.0436972357792926 Test Loss: 0.05681625391684709\n",
      "Epoch: 11510 Training Loss: 0.04369681128277404 Test Loss: 0.056820864669468646\n",
      "Epoch: 11520 Training Loss: 0.043696662818450166 Test Loss: 0.05682167983015312\n",
      "Epoch: 11530 Training Loss: 0.04369649256211092 Test Loss: 0.05682504873641942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11540 Training Loss: 0.04369611951066974 Test Loss: 0.05683037912308274\n",
      "Epoch: 11550 Training Loss: 0.04369629374728577 Test Loss: 0.05683195849690891\n",
      "Epoch: 11560 Training Loss: 0.04369657336172954 Test Loss: 0.0568312324944243\n",
      "Epoch: 11570 Training Loss: 0.043696779739080566 Test Loss: 0.0568454850695169\n",
      "Epoch: 11580 Training Loss: 0.04369521479375772 Test Loss: 0.0568477394982849\n",
      "Epoch: 11590 Training Loss: 0.04369518852393098 Test Loss: 0.056843370746491546\n",
      "Epoch: 11600 Training Loss: 0.043694782237178695 Test Loss: 0.0568505034024807\n",
      "Epoch: 11610 Training Loss: 0.04369480164102799 Test Loss: 0.056851560563993375\n",
      "Epoch: 11620 Training Loss: 0.043694832388666116 Test Loss: 0.05684808976264151\n",
      "Epoch: 11630 Training Loss: 0.04369533380403343 Test Loss: 0.05686273081274781\n",
      "Epoch: 11640 Training Loss: 0.043693807765916116 Test Loss: 0.056859533854438385\n",
      "Epoch: 11650 Training Loss: 0.04369350486685318 Test Loss: 0.056861565387706724\n",
      "Epoch: 11660 Training Loss: 0.04369285837039725 Test Loss: 0.05686820767359662\n",
      "Epoch: 11670 Training Loss: 0.04369165851596202 Test Loss: 0.05686242512749113\n",
      "Epoch: 11680 Training Loss: 0.04369179593501783 Test Loss: 0.05686397265910306\n",
      "Epoch: 11690 Training Loss: 0.04369196201206647 Test Loss: 0.05686842420065343\n",
      "Epoch: 11700 Training Loss: 0.043690792507241685 Test Loss: 0.056869054676495334\n",
      "Epoch: 11710 Training Loss: 0.043691865987889156 Test Loss: 0.05686806119941113\n",
      "Epoch: 11720 Training Loss: 0.04369130526639782 Test Loss: 0.05686581950752882\n",
      "Epoch: 11730 Training Loss: 0.04369075708277834 Test Loss: 0.05686129154466429\n",
      "Epoch: 11740 Training Loss: 0.043693095196865636 Test Loss: 0.05685852764046849\n",
      "Epoch: 11750 Training Loss: 0.043692388996258005 Test Loss: 0.05685773158511256\n",
      "Epoch: 11760 Training Loss: 0.043694866818060264 Test Loss: 0.056861559019263874\n",
      "Epoch: 11770 Training Loss: 0.04369019855043924 Test Loss: 0.05685847669292571\n",
      "Epoch: 11780 Training Loss: 0.04369137920003901 Test Loss: 0.05685772521666971\n",
      "Epoch: 11790 Training Loss: 0.04369079121365173 Test Loss: 0.056862310495519874\n",
      "Epoch: 11800 Training Loss: 0.043689710170478374 Test Loss: 0.05686187744140625\n",
      "Epoch: 11810 Training Loss: 0.043689858734309174 Test Loss: 0.05685637510678605\n",
      "Epoch: 11820 Training Loss: 0.0436897363407982 Test Loss: 0.05686578766531459\n",
      "Epoch: 11830 Training Loss: 0.04369003217486985 Test Loss: 0.05686279449717628\n",
      "Epoch: 11840 Training Loss: 0.04368998361549314 Test Loss: 0.05687102252533519\n",
      "Epoch: 11850 Training Loss: 0.04368954518800586 Test Loss: 0.056869016465838244\n",
      "Epoch: 11860 Training Loss: 0.04369033248675287 Test Loss: 0.05687937792235105\n",
      "Epoch: 11870 Training Loss: 0.04368919641625304 Test Loss: 0.05687067862942143\n",
      "Epoch: 11880 Training Loss: 0.04368897750103016 Test Loss: 0.05687560780418536\n",
      "Epoch: 11890 Training Loss: 0.04368773605270258 Test Loss: 0.056877626600568006\n",
      "Epoch: 11900 Training Loss: 0.04368794262906745 Test Loss: 0.05688793710953803\n",
      "Epoch: 11910 Training Loss: 0.04368741345126959 Test Loss: 0.0568790212895516\n",
      "Epoch: 11920 Training Loss: 0.043687609778421746 Test Loss: 0.05688333909380217\n",
      "Epoch: 11930 Training Loss: 0.04368767664707165 Test Loss: 0.056880632505592\n",
      "Epoch: 11940 Training Loss: 0.04368770132478768 Test Loss: 0.05688229466917519\n",
      "Epoch: 11950 Training Loss: 0.04368760092230591 Test Loss: 0.05689992251897694\n",
      "Epoch: 11960 Training Loss: 0.04368674635688132 Test Loss: 0.056900865048518365\n",
      "Epoch: 11970 Training Loss: 0.043686103840702165 Test Loss: 0.056899960729634026\n",
      "Epoch: 11980 Training Loss: 0.04368620702937768 Test Loss: 0.056904762535541005\n",
      "Epoch: 11990 Training Loss: 0.0436855694885445 Test Loss: 0.05690296663465803\n",
      "Epoch: 12000 Training Loss: 0.04368578213483145 Test Loss: 0.05689463671141355\n",
      "Epoch: 12010 Training Loss: 0.04368537107174703 Test Loss: 0.05688949100959281\n",
      "Epoch: 12020 Training Loss: 0.04368569078747936 Test Loss: 0.05689513344995566\n",
      "Epoch: 12030 Training Loss: 0.04368510459221663 Test Loss: 0.05688624310374061\n",
      "Epoch: 12040 Training Loss: 0.04368445312041472 Test Loss: 0.05688139035029085\n",
      "Epoch: 12050 Training Loss: 0.043685285794317026 Test Loss: 0.05687868376208068\n",
      "Epoch: 12060 Training Loss: 0.04368424534996682 Test Loss: 0.05687828255018129\n",
      "Epoch: 12070 Training Loss: 0.04368417460054706 Test Loss: 0.05687711712514021\n",
      "Epoch: 12080 Training Loss: 0.04368445988688525 Test Loss: 0.0568890197448221\n",
      "Epoch: 12090 Training Loss: 0.04368396006362864 Test Loss: 0.05688301430321695\n",
      "Epoch: 12100 Training Loss: 0.04368313813647364 Test Loss: 0.056887300265253286\n",
      "Epoch: 12110 Training Loss: 0.04368315604771915 Test Loss: 0.05689890356812135\n",
      "Epoch: 12120 Training Loss: 0.043682836033466066 Test Loss: 0.056896954824610026\n",
      "Epoch: 12130 Training Loss: 0.043682734237887426 Test Loss: 0.056896948456167176\n",
      "Epoch: 12140 Training Loss: 0.04368359318161648 Test Loss: 0.05689514618684135\n",
      "Epoch: 12150 Training Loss: 0.04368502916597166 Test Loss: 0.05690880649674914\n",
      "Epoch: 12160 Training Loss: 0.043681287407278016 Test Loss: 0.05691198434973002\n",
      "Epoch: 12170 Training Loss: 0.04368201181765192 Test Loss: 0.05691623846955212\n",
      "Epoch: 12180 Training Loss: 0.04368075444821722 Test Loss: 0.05690547580113992\n",
      "Epoch: 12190 Training Loss: 0.04368055344423985 Test Loss: 0.056912411035400796\n",
      "Epoch: 12200 Training Loss: 0.04368132750856658 Test Loss: 0.05689537545078386\n",
      "Epoch: 12210 Training Loss: 0.04368208296509935 Test Loss: 0.0569030303190865\n",
      "Epoch: 12220 Training Loss: 0.04368249701339136 Test Loss: 0.05691626394332351\n",
      "Epoch: 12230 Training Loss: 0.04368054936445615 Test Loss: 0.05691381846127008\n",
      "Epoch: 12240 Training Loss: 0.04368066966832182 Test Loss: 0.05690919497176283\n",
      "Epoch: 12250 Training Loss: 0.04367970654084806 Test Loss: 0.05690248900144446\n",
      "Epoch: 12260 Training Loss: 0.043680131534901204 Test Loss: 0.05690822696845002\n",
      "Epoch: 12270 Training Loss: 0.043680319602979044 Test Loss: 0.05690557769622548\n",
      "Epoch: 12280 Training Loss: 0.043679160247860054 Test Loss: 0.05690500453636921\n",
      "Epoch: 12290 Training Loss: 0.04367932493181181 Test Loss: 0.056908341600421276\n",
      "Epoch: 12300 Training Loss: 0.043679141938586866 Test Loss: 0.0569018903678168\n",
      "Epoch: 12310 Training Loss: 0.04367974415446363 Test Loss: 0.056912219982115375\n",
      "Epoch: 12320 Training Loss: 0.04367862350753631 Test Loss: 0.05690119620754643\n",
      "Epoch: 12330 Training Loss: 0.043677748244672465 Test Loss: 0.05689274528388786\n",
      "Epoch: 12340 Training Loss: 0.0436778939228026 Test Loss: 0.05689577666268325\n",
      "Epoch: 12350 Training Loss: 0.043677369720350724 Test Loss: 0.05690296663465803\n",
      "Epoch: 12360 Training Loss: 0.04367766137513175 Test Loss: 0.056892910863401897\n",
      "Epoch: 12370 Training Loss: 0.043677389721241536 Test Loss: 0.056901043364918094\n",
      "Epoch: 12380 Training Loss: 0.04367657515759858 Test Loss: 0.056899362096006365\n",
      "Epoch: 12390 Training Loss: 0.04367590836173107 Test Loss: 0.05690172478830277\n",
      "Epoch: 12400 Training Loss: 0.043675971747638785 Test Loss: 0.05690157831411728\n",
      "Epoch: 12410 Training Loss: 0.043677514204397826 Test Loss: 0.056898094775879725\n",
      "Epoch: 12420 Training Loss: 0.04367688422609052 Test Loss: 0.05691269761532893\n",
      "Epoch: 12430 Training Loss: 0.043675942194083696 Test Loss: 0.056916856208508324\n",
      "Epoch: 12440 Training Loss: 0.043674564620290256 Test Loss: 0.05690894023404894\n",
      "Epoch: 12450 Training Loss: 0.043674063503443696 Test Loss: 0.05690971718407632\n",
      "Epoch: 12460 Training Loss: 0.04367467905324767 Test Loss: 0.05691226456121531\n",
      "Epoch: 12470 Training Loss: 0.04367390289927564 Test Loss: 0.05691160224315917\n",
      "Epoch: 12480 Training Loss: 0.04367427804036212 Test Loss: 0.056907717493022224\n",
      "Epoch: 12490 Training Loss: 0.043673331829064674 Test Loss: 0.056906615752409614\n",
      "Epoch: 12500 Training Loss: 0.043673509647929806 Test Loss: 0.05690930960373409\n",
      "Epoch: 12510 Training Loss: 0.04367330973852855 Test Loss: 0.05689859788286467\n",
      "Epoch: 12520 Training Loss: 0.04367335182995549 Test Loss: 0.05690335510967172\n",
      "Epoch: 12530 Training Loss: 0.043673907277580096 Test Loss: 0.05690737996555131\n",
      "Epoch: 12540 Training Loss: 0.043673694531786225 Test Loss: 0.0569027755813726\n",
      "Epoch: 12550 Training Loss: 0.04367405594091782 Test Loss: 0.056903278688357546\n",
      "Epoch: 12560 Training Loss: 0.04367317699629795 Test Loss: 0.05690429763921314\n",
      "Epoch: 12570 Training Loss: 0.04367273419050621 Test Loss: 0.05690594706591063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12580 Training Loss: 0.04367350039378629 Test Loss: 0.056910908082888796\n",
      "Epoch: 12590 Training Loss: 0.04367331531091604 Test Loss: 0.056912538404257744\n",
      "Epoch: 12600 Training Loss: 0.04367311012764805 Test Loss: 0.056915741731010015\n",
      "Epoch: 12610 Training Loss: 0.04367339382187552 Test Loss: 0.056919078795062084\n",
      "Epoch: 12620 Training Loss: 0.04367242352990356 Test Loss: 0.056916505944151714\n",
      "Epoch: 12630 Training Loss: 0.04367225008934288 Test Loss: 0.05692296354519903\n",
      "Epoch: 12640 Training Loss: 0.04367149234415096 Test Loss: 0.05692686103222167\n",
      "Epoch: 12650 Training Loss: 0.043672404126054255 Test Loss: 0.056922065594757536\n",
      "Epoch: 12660 Training Loss: 0.043672114461411615 Test Loss: 0.05692144785580133\n",
      "Epoch: 12670 Training Loss: 0.04367225824891028 Test Loss: 0.05691299056369992\n",
      "Epoch: 12680 Training Loss: 0.043672755385480065 Test Loss: 0.056919792060661\n",
      "Epoch: 12690 Training Loss: 0.04367138517519867 Test Loss: 0.056924351865739777\n",
      "Epoch: 12700 Training Loss: 0.043670064419856255 Test Loss: 0.05692556186988079\n",
      "Epoch: 12710 Training Loss: 0.043669860430671295 Test Loss: 0.05693023630693082\n",
      "Epoch: 12720 Training Loss: 0.04366985187307622 Test Loss: 0.05693096230941543\n",
      "Epoch: 12730 Training Loss: 0.04366848166279482 Test Loss: 0.05693148452172892\n",
      "Epoch: 12740 Training Loss: 0.043668829041450766 Test Loss: 0.05691062150296066\n",
      "Epoch: 12750 Training Loss: 0.04366880914006687 Test Loss: 0.056933229475069125\n",
      "Epoch: 12760 Training Loss: 0.04366854266053647 Test Loss: 0.05694315150902546\n",
      "Epoch: 12770 Training Loss: 0.043667817454107215 Test Loss: 0.05693856623017529\n",
      "Epoch: 12780 Training Loss: 0.04366755465633284 Test Loss: 0.05693839428221841\n",
      "Epoch: 12790 Training Loss: 0.043667392161533314 Test Loss: 0.056940336657286886\n",
      "Epoch: 12800 Training Loss: 0.04366685442614038 Test Loss: 0.056936407328050005\n",
      "Epoch: 12810 Training Loss: 0.043667523908694716 Test Loss: 0.05693912028470302\n",
      "Epoch: 12820 Training Loss: 0.043667060803491406 Test Loss: 0.05694260382294058\n",
      "Epoch: 12830 Training Loss: 0.04366616325107759 Test Loss: 0.056946093729620985\n",
      "Epoch: 12840 Training Loss: 0.04366782710627841 Test Loss: 0.05694204976841285\n",
      "Epoch: 12850 Training Loss: 0.043667866312004686 Test Loss: 0.05694289677131156\n",
      "Epoch: 12860 Training Loss: 0.0436698486888548 Test Loss: 0.05693626722230736\n",
      "Epoch: 12870 Training Loss: 0.043669498922032784 Test Loss: 0.056941088133542886\n",
      "Epoch: 12880 Training Loss: 0.04366857111951545 Test Loss: 0.05694762852434722\n",
      "Epoch: 12890 Training Loss: 0.04366654217342702 Test Loss: 0.05694797878870383\n",
      "Epoch: 12900 Training Loss: 0.04366681731005941 Test Loss: 0.056948494632574474\n",
      "Epoch: 12910 Training Loss: 0.043665703429602624 Test Loss: 0.05695525155043562\n",
      "Epoch: 12920 Training Loss: 0.04366550869456118 Test Loss: 0.056966956748389246\n",
      "Epoch: 12930 Training Loss: 0.04366401081690207 Test Loss: 0.05697098160426883\n",
      "Epoch: 12940 Training Loss: 0.04366277324934435 Test Loss: 0.056972338082595345\n",
      "Epoch: 12950 Training Loss: 0.043661847436965405 Test Loss: 0.05698194806285215\n",
      "Epoch: 12960 Training Loss: 0.04366206973542355 Test Loss: 0.0569830689087933\n",
      "Epoch: 12970 Training Loss: 0.04366109715479244 Test Loss: 0.05698630407775981\n",
      "Epoch: 12980 Training Loss: 0.043659907251049165 Test Loss: 0.05699236683535058\n",
      "Epoch: 12990 Training Loss: 0.043659738984848304 Test Loss: 0.056994277368204824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size=int(x_variable.size()[0]/32)\n",
    "nb_of_epochs=13000\n",
    "# Train the network #\n",
    "my_conv_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_conv_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\n",
    "        #loss=torch.sqrt((out[0]-y_variable.narrow(0,b,batch_size)[0]).pow(2)+(out[1]-y_variable.narrow(0,b,batch_size)[1]).pow(2)) \n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_conv_net.eval()\n",
    "        test_loss=loss_func(my_conv_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_conv_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable=x_variable.cpu()\n",
    "my_conv_net=my_conv_net.cpu()\n",
    "x_variable_test=x_variable_test.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contour1=np.delete(polygons_reshaped[0],16)\n",
    "#contour1=contour1.reshape(8,2)\n",
    "#plot_contour(contour1)\n",
    "save_network('8_point_regression_CNN.pkl',my_conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 -0.299865726930685 0.2005578975693147 -0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x286f7faefd0>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_index=4\n",
    "\n",
    "contour1=np.delete(polygons_reshaped[sample_index],16)\n",
    "contour1=contour1.reshape(8,2)\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(contour1,.8\n",
    "                                                                              )\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "my_conv_net.eval()\n",
    "predictions=my_conv_net(x_variable_test)\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(1,2)\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "\n",
    "plot_contour(contour1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.04086411, -0.1030227 ,  0.42048059,  0.45060827, -0.07548485,\n",
       "        0.9862254 , -0.45504499,  0.62012031, -1.10505747,  0.2925037 ,\n",
       "       -0.62290614, -1.0617667 , -0.05984104, -0.56100069,  0.85698979,\n",
       "       -0.6236676 ,  0.4       ])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contour=polygons_reshaped[1]\n",
    "contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_tensor=torch.FloatTensor([[2.2,1.2],[3,3]])\n",
    "a_tensor=torch.FloatTensor([[1,2],[3,2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_variable=Variable(d_tensor)\n",
    "\n",
    "a_variable=Variable(a_tensor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_variable.requires_grad=False\n",
    "a_variable.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fun=torch.sqrt((a_variable[0]-d_variable[0]).pow(2)+(a_variable[1]-d_variable[1]).pow(2))\n",
    "#print(fun)\n",
    "\n",
    "    \n",
    "loss_result=my_torch_loss_function(a_variable,d_variable) \n",
    "\n",
    "loss_result.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_result.sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "another_loss_func=torch.nn.MSELoss(size_average=False)\n",
    "another_loss_result=another_loss_func(out,y_variable.view(0,b,batch_size))\n",
    "\n",
    "another_loss_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myloss=myLossfunction().apply(out,y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "myloss.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flush all variables in GPU\n",
    "del my_conv_net,my_net,x_variable_test,x_variable,y_variable,y_variable_test,loss_func\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Should I order the interior points ?\n",
    "\n",
    "For each interior point calculate distance with points of the polygon\n",
    "\n",
    "find which ti which point it is closer and sort it according to the index.\n",
    "\n",
    "If multiple points are closer to the same point of the contour then take into\n",
    "\n",
    "account the distance to the point.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing convoutional network with FNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CNN_net=load_network('12_point_regression_CNN.pkl')\n",
    "FNN_net=load_network('12_point_regression_FNN.pkl')\n",
    "FNN_net=FFN_net.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 -0.3493427200727041 -0.2353175009276941 0\n",
      "convolution prediction [-0.20658945 -0.05526882]\n",
      "feedforward prediction [-0.20658945 -0.05526882]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x288606c9ef0>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_index=63\n",
    "\n",
    "contour1=np.delete(polygons_reshaped[sample_index],24)\n",
    "contour1=contour1.reshape(12,2)\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(contour1,.3                                                                            )\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "\n",
    "\n",
    "predictions=CNN_net(x_variable_test)\n",
    "conv_predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "conv_predicted_inserted_points=predicted_inserted_points.reshape(1,2)\n",
    "print(\"convolution prediction\", conv_predicted_inserted_points[0])\n",
    "\n",
    "x_variable_test_resized=x_variable_test.resize(x_variable_test.size()[0],x_variable_test.size()[2]).cpu()\n",
    "predictions=FNN_net(x_variable_test_resized.cuda()).cpu()\n",
    "fnn_predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "fnn_predicted_inserted_points=predicted_inserted_points.reshape(1,2)\n",
    "print(\"feedforward prediction\",fnn_predicted_inserted_points[0])\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(fnn_predicted_inserted_points[:,0],fnn_predicted_inserted_points[:,1],label='fnn predicted points')\n",
    "plt.scatter(conv_predicted_inserted_points[:,0],conv_predicted_inserted_points[:,1],label='cnn predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "\n",
    "plot_contour(contour1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable_test_resized=x_variable_test.resize(x_variable_test.size()[0],x_variable_test.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0  ,.,.) = \n",
       "  0.8113  0.3627  2.3743  ...   0.8113  3.0093  0.8000\n",
       "\n",
       "(  1  ,.,.) = \n",
       "  0.8113  0.3627  2.3743  ...   0.8113  3.0093  0.9000\n",
       "\n",
       "(  2  ,.,.) = \n",
       "  0.8113  0.3627  2.3743  ...   0.8113  3.0093  1.0000\n",
       " ...  \n",
       "\n",
       "(43741,.,.) = \n",
       "  0.6288  0.3673  2.6870  ...   0.6288  2.5052  0.8000\n",
       "\n",
       "(43742,.,.) = \n",
       "  0.6288  0.3673  2.6870  ...   0.6288  2.5052  0.9000\n",
       "\n",
       "(43743,.,.) = \n",
       "  0.6288  0.3673  2.6870  ...   0.6288  2.5052  1.0000\n",
       "[torch.FloatTensor of size 43744x1x37]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (bn_input): BatchNorm1d(37, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc0): Linear(in_features=37, out_features=20, bias=True)\n",
       "  (bn0): BatchNorm1d(20, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (predict): Linear(in_features=20, out_features=2, bias=True)\n",
       "  (fc1): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (bn2): BatchNorm1d(20, eps=1e-05, momentum=0.5, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FNN_net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8113  0.3627  2.3743  ...   0.8113  3.0093  0.8000\n",
       " 0.8113  0.3627  2.3743  ...   0.8113  3.0093  0.9000\n",
       " 0.8113  0.3627  2.3743  ...   0.8113  3.0093  1.0000\n",
       "          ...                          ...          \n",
       " 0.6288  0.3673  2.6870  ...   0.6288  2.5052  0.8000\n",
       " 0.6288  0.3673  2.6870  ...   0.6288  2.5052  0.9000\n",
       " 0.6288  0.3673  2.6870  ...   0.6288  2.5052  1.0000\n",
       "[torch.FloatTensor of size 43744x37]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Checking jumps to insertion points ##\n",
    "\n",
    "contour3=apply_procrustes(generate_contour(12))\n",
    "plot_contour(contour3)\n",
    "\n",
    "\n",
    "inserted_points=[]\n",
    "nb_of_sampling=100\n",
    "contour=contour3.copy()\n",
    "plot_contour(contour3)\n",
    "\n",
    "for i in range(0,nb_of_sampling):\n",
    "    contour[0]=np.random.normal(contour3[0],0.08)\n",
    "    contour[11]=np.random.normal(contour3[11],0.08)\n",
    "\n",
    "    nb_of_points,point_coords=get_extrapoints_target_length(contour,0.2)\n",
    "    inserted_points.append(point_coords)\n",
    "    plt.scatter(contour[0][0],contour[0][1])\n",
    "    plt.scatter(contour[11][0],contour[11][1])\n",
    "\n",
    "\n",
    "inserted_points=[i for i in inserted_points if len(i)==1]\n",
    "inserted_points=np.array(inserted_points)\n",
    "inserted_points=inserted_points.reshape(len(inserted_points),2)\n",
    "\n",
    "plt.scatter(inserted_points[:,0],inserted_points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_variable_test=y_variable_test.cpu().resize(y_variable_test.size()[0],y_variable_test.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x286f8ba74a8>"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plot_contour(contour1)\n",
    "plt.scatter(predictions.data.numpy()[:,0],predictions.data.numpy()[:,1])\n",
    "plt.scatter(y_variable_test.data.numpy()[:,0],y_variable_test.data.numpy()[:,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2443 -0.2558\n",
       "-0.1168  0.3184\n",
       " 0.0416  0.2039\n",
       "               \n",
       "-0.1278 -0.3300\n",
       " 0.4185  0.0228\n",
       "-0.3816  0.2338\n",
       "[torch.FloatTensor of size 76672x2]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
