{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Triangulation import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from  Neural_network import *\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_nb_of_points(point_coordinates):\n",
    "    set_of_numbers=set()\n",
    "    for index,_ in enumerate(point_coordinates):\n",
    "        set_of_numbers.add(len(point_coordinates[index][0]))\n",
    "    return set_of_numbers\n",
    "\n",
    "\n",
    "def get_indices_nb_of_points(set_of_numbers,number_of_points,point_coordinates):\n",
    "    indices=[]\n",
    "    if number_of_points not in set_of_numbers:\n",
    "        return \"No such number of points for sample\"\n",
    "    else:\n",
    "        for index,_ in enumerate(point_coordinates):\n",
    "            if len(point_coordinates[index][0])==number_of_points:\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "    \n",
    "def get_polygons_nb_of_points(indices):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons=load_dataset('12_polygons.pkl')\n",
    "polygons=[i for i in polygons for j in range(10)]\n",
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(2,12))\n",
    "\n",
    "polygons_reshaped=np.array(polygons_reshaped)\n",
    "#polygons_reshaped=polygons_reshaped.reshape(60000,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=load_dataset('12_point_coordinates')\n",
    "number_of_insertion_points=load_dataset('12_nb_of_points.pkl')\n",
    "centers_of_mass=load_dataset('12_centers_of_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_points=get_set_nb_of_points(point_coordinates)        \n",
    "indices=get_indices_nb_of_points(set_of_points,1,point_coordinates)\n",
    "indices=np.asarray(indices)\n",
    "number_of_insertion_points=np.array(number_of_insertion_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped.resize(len(point_coordinates),2*12)\n",
    "\n",
    "polygons_reshaped=np.hstack([polygons_reshaped[indices],number_of_insertion_points[indices,1].reshape(len(indices),1) ])\n",
    "#polygons_reshaped=polygons_reshaped[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=[ point_coordinates[i][0] for i in indices]\n",
    "point_coordinates=np.array(point_coordinates)\n",
    "#barycenters,point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(polygons_reshaped),len(indices)\n",
    "#point_coordinates.shape\n",
    "#centers_of_mass=centers_of_mass.reshape(60000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# Project set of polygons in 2d and 3d space #\n",
    "# Dimensionality reduction using Isomap, PCA, kernel PCA ... #\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "\n",
    "Polygons_reshaped=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Isomap #\n",
    "#nb_components=8\n",
    "#iso=manifold.Isomap(n_neighbors=8,n_components=nb_components,n_jobs=-1)\n",
    "#iso.fit(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "\n",
    "                        # PCA   #\n",
    "pca=PCA(.999)\n",
    "pca.fit(polygons_reshaped)\n",
    "Polygons_projected=pca.transform(polygons_reshaped)\n",
    "nb_components=int(pca.n_components_)\n",
    "print(nb_components)\n",
    "# Fitting into lesser dimension\n",
    "\n",
    "#Polygons_projected=iso.fit_transform(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "#iso = KernelPCA(n_components=2,kernel=\"rbf\", fit_inverse_transform=True, gamma=1e-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(Polygons_projected[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(Polygons_projected[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n",
    "\n",
    "y_tensor=torch.from_numpy(point_coordinates[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(point_coordinates[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "shuffle=torch.randperm(x_variable.shape[0])\n",
    "x_variable = x_variable[shuffle]\n",
    "y_variable=y_variable[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 174978\n"
     ]
    }
   ],
   "source": [
    "my_net=Net(nb_components,1*2,nb_of_hidden_layers=2, nb_of_hidden_nodes=20,batch_normalization=True)\n",
    "\n",
    "print(\"Training data length:\",x_variable.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-3,weight_decay=0)\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "max_distance=0.6108970818704328\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=myLossfunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    x_variable , y_variable=x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test,y_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 0.19334205139360378 Test Loss: 0.18400412994139195\n",
      "Epoch: 10 Training Loss: 0.11567720410744493 Test Loss: 0.1155488824181602\n",
      "Epoch: 20 Training Loss: 0.10219234480548983 Test Loss: 0.10353034800129732\n",
      "Epoch: 30 Training Loss: 0.09842879689696704 Test Loss: 0.09985638432199273\n",
      "Epoch: 40 Training Loss: 0.09415261757906708 Test Loss: 0.09543940579708074\n",
      "Epoch: 50 Training Loss: 0.08902548865071609 Test Loss: 0.09023040458941511\n",
      "Epoch: 60 Training Loss: 0.0835252157146184 Test Loss: 0.08468092700034577\n",
      "Epoch: 70 Training Loss: 0.0781463029012933 Test Loss: 0.07947207090186655\n",
      "Epoch: 80 Training Loss: 0.07287419201896095 Test Loss: 0.07417404520834286\n",
      "Epoch: 90 Training Loss: 0.06853561925463487 Test Loss: 0.0698091776247171\n",
      "Epoch: 100 Training Loss: 0.06563329289521397 Test Loss: 0.06715303768445387\n",
      "Epoch: 110 Training Loss: 0.0633043421887323 Test Loss: 0.06487926603148432\n",
      "Epoch: 120 Training Loss: 0.06125940319104402 Test Loss: 0.06298593717852734\n",
      "Epoch: 130 Training Loss: 0.059603363529986624 Test Loss: 0.06148280692780724\n",
      "Epoch: 140 Training Loss: 0.05824541379844895 Test Loss: 0.06022077000917268\n",
      "Epoch: 150 Training Loss: 0.05700666711047246 Test Loss: 0.059080708523954716\n",
      "Epoch: 160 Training Loss: 0.0558443502317077 Test Loss: 0.058079968601051\n",
      "Epoch: 170 Training Loss: 0.054827447838978326 Test Loss: 0.057204826260101385\n",
      "Epoch: 180 Training Loss: 0.05392254055241945 Test Loss: 0.05650019837542006\n",
      "Epoch: 190 Training Loss: 0.05313008043779075 Test Loss: 0.055892726672157896\n",
      "Epoch: 200 Training Loss: 0.052421090638001634 Test Loss: 0.055281270047392216\n",
      "Epoch: 210 Training Loss: 0.05174276038451977 Test Loss: 0.0546318338836469\n",
      "Epoch: 220 Training Loss: 0.0511341375549712 Test Loss: 0.05402649416920035\n",
      "Epoch: 230 Training Loss: 0.05057760274684532 Test Loss: 0.05358565804203148\n",
      "Epoch: 240 Training Loss: 0.050122210676127855 Test Loss: 0.053133045746273776\n",
      "Epoch: 250 Training Loss: 0.04977053421090794 Test Loss: 0.052762838725025143\n",
      "Epoch: 260 Training Loss: 0.04945275151554767 Test Loss: 0.05243434501374474\n",
      "Epoch: 270 Training Loss: 0.04916339315246059 Test Loss: 0.052148061332339866\n",
      "Epoch: 280 Training Loss: 0.04887850521744162 Test Loss: 0.05188089857680482\n",
      "Epoch: 290 Training Loss: 0.04858809482477797 Test Loss: 0.051634073431856366\n",
      "Epoch: 300 Training Loss: 0.048301390255132075 Test Loss: 0.051396937115661574\n",
      "Epoch: 310 Training Loss: 0.04802633614565117 Test Loss: 0.05109280500432917\n",
      "Epoch: 320 Training Loss: 0.04775063742388329 Test Loss: 0.050783549422492226\n",
      "Epoch: 330 Training Loss: 0.047503251413213377 Test Loss: 0.05054729492366096\n",
      "Epoch: 340 Training Loss: 0.047273853154932335 Test Loss: 0.050318792604057125\n",
      "Epoch: 350 Training Loss: 0.04706637729546714 Test Loss: 0.050133744904658356\n",
      "Epoch: 360 Training Loss: 0.04686305374372064 Test Loss: 0.04998815016059345\n",
      "Epoch: 370 Training Loss: 0.04666990167156728 Test Loss: 0.049869785713469277\n",
      "Epoch: 380 Training Loss: 0.046489937641267756 Test Loss: 0.04975886090270951\n",
      "Epoch: 390 Training Loss: 0.04632066971008498 Test Loss: 0.04961285315557562\n",
      "Epoch: 400 Training Loss: 0.04615829159594349 Test Loss: 0.04952362216818307\n",
      "Epoch: 410 Training Loss: 0.04601386914310299 Test Loss: 0.049446089213663585\n",
      "Epoch: 420 Training Loss: 0.04588987615585959 Test Loss: 0.04939901802604357\n",
      "Epoch: 430 Training Loss: 0.04577418499352419 Test Loss: 0.049316473223470644\n",
      "Epoch: 440 Training Loss: 0.045672592980885454 Test Loss: 0.04924446558027958\n",
      "Epoch: 450 Training Loss: 0.04557910606083336 Test Loss: 0.049131196698047164\n",
      "Epoch: 460 Training Loss: 0.04549377027710269 Test Loss: 0.049057576110438005\n",
      "Epoch: 470 Training Loss: 0.04541395136995794 Test Loss: 0.048966497771480086\n",
      "Epoch: 480 Training Loss: 0.045335255650888966 Test Loss: 0.04889187816293377\n",
      "Epoch: 490 Training Loss: 0.04525685293740499 Test Loss: 0.04883012304187431\n",
      "Epoch: 500 Training Loss: 0.04517946737188032 Test Loss: 0.04878167889810603\n",
      "Epoch: 510 Training Loss: 0.045102905012523 Test Loss: 0.0487173508795492\n",
      "Epoch: 520 Training Loss: 0.045021291794986366 Test Loss: 0.04863195970447376\n",
      "Epoch: 530 Training Loss: 0.0449354034864383 Test Loss: 0.04855287519788428\n",
      "Epoch: 540 Training Loss: 0.044845425657082605 Test Loss: 0.04849821368359089\n",
      "Epoch: 550 Training Loss: 0.044755982214103346 Test Loss: 0.04840922826559071\n",
      "Epoch: 560 Training Loss: 0.044672620729909615 Test Loss: 0.04831837875265751\n",
      "Epoch: 570 Training Loss: 0.044584613014296795 Test Loss: 0.04822344385801482\n",
      "Epoch: 580 Training Loss: 0.04449063496107368 Test Loss: 0.0481634523716824\n",
      "Epoch: 590 Training Loss: 0.04439670155632065 Test Loss: 0.04805672456508321\n",
      "Epoch: 600 Training Loss: 0.04430539683024437 Test Loss: 0.04800006500891551\n",
      "Epoch: 610 Training Loss: 0.04422182466108239 Test Loss: 0.04789547477225448\n",
      "Epoch: 620 Training Loss: 0.044137822750395767 Test Loss: 0.04780129891027912\n",
      "Epoch: 630 Training Loss: 0.04404668824661172 Test Loss: 0.04769376184090961\n",
      "Epoch: 640 Training Loss: 0.043943463774285194 Test Loss: 0.04761453222513373\n",
      "Epoch: 650 Training Loss: 0.043849482930532695 Test Loss: 0.047526104919388946\n",
      "Epoch: 660 Training Loss: 0.04376519080479046 Test Loss: 0.04745266850882407\n",
      "Epoch: 670 Training Loss: 0.04368789593147067 Test Loss: 0.04743757715343819\n",
      "Epoch: 680 Training Loss: 0.043608873720464146 Test Loss: 0.04737169758281136\n",
      "Epoch: 690 Training Loss: 0.04353431356593757 Test Loss: 0.04732175769819861\n",
      "Epoch: 700 Training Loss: 0.04345890788100861 Test Loss: 0.047213796463515\n",
      "Epoch: 710 Training Loss: 0.04338146371436695 Test Loss: 0.04715342546084891\n",
      "Epoch: 720 Training Loss: 0.04330884995408351 Test Loss: 0.04713173163748171\n",
      "Epoch: 730 Training Loss: 0.04325120459339889 Test Loss: 0.047055019107977665\n",
      "Epoch: 740 Training Loss: 0.0431994360875903 Test Loss: 0.046992884470584535\n",
      "Epoch: 750 Training Loss: 0.043145193777485595 Test Loss: 0.04693111260615741\n",
      "Epoch: 760 Training Loss: 0.04309007245062593 Test Loss: 0.046875061392348094\n",
      "Epoch: 770 Training Loss: 0.04303311076963961 Test Loss: 0.04677200038326685\n",
      "Epoch: 780 Training Loss: 0.0429803558116949 Test Loss: 0.04672528917805156\n",
      "Epoch: 790 Training Loss: 0.04293734259181869 Test Loss: 0.04667182202398472\n",
      "Epoch: 800 Training Loss: 0.04290472688441547 Test Loss: 0.04660348396887287\n",
      "Epoch: 810 Training Loss: 0.042877523408746385 Test Loss: 0.04656836196464086\n",
      "Epoch: 820 Training Loss: 0.04285201936547023 Test Loss: 0.04654254369170629\n",
      "Epoch: 830 Training Loss: 0.042825718626055845 Test Loss: 0.04653752347196901\n",
      "Epoch: 840 Training Loss: 0.042798452363475695 Test Loss: 0.04650938624261327\n",
      "Epoch: 850 Training Loss: 0.04276664451432823 Test Loss: 0.04648057927855106\n",
      "Epoch: 860 Training Loss: 0.042732060088446835 Test Loss: 0.04645431451581217\n",
      "Epoch: 870 Training Loss: 0.04269719242383328 Test Loss: 0.0464594491485618\n",
      "Epoch: 880 Training Loss: 0.04266499111004312 Test Loss: 0.046458014800065436\n",
      "Epoch: 890 Training Loss: 0.042632904207957574 Test Loss: 0.04643853668235215\n",
      "Epoch: 900 Training Loss: 0.04260314181684626 Test Loss: 0.046407541918248786\n",
      "Epoch: 910 Training Loss: 0.04257592438853027 Test Loss: 0.04637096603159147\n",
      "Epoch: 920 Training Loss: 0.042550735675074156 Test Loss: 0.04637427563726597\n",
      "Epoch: 930 Training Loss: 0.04252719337395273 Test Loss: 0.046367840602961265\n",
      "Epoch: 940 Training Loss: 0.04250604116124598 Test Loss: 0.0463752048941712\n",
      "Epoch: 950 Training Loss: 0.04248437270060379 Test Loss: 0.04636998933514454\n",
      "Epoch: 960 Training Loss: 0.04246752487946699 Test Loss: 0.046383476117796155\n",
      "Epoch: 970 Training Loss: 0.04245397406879365 Test Loss: 0.046378690305206145\n",
      "Epoch: 980 Training Loss: 0.042440975782937714 Test Loss: 0.046390455311549866\n",
      "Epoch: 990 Training Loss: 0.042429167657862706 Test Loss: 0.046402488211776186\n",
      "Epoch: 1000 Training Loss: 0.04241884967547706 Test Loss: 0.046419678069242354\n",
      "Epoch: 1010 Training Loss: 0.042408587503679034 Test Loss: 0.04641720842251223\n",
      "Epoch: 1020 Training Loss: 0.042398559736349 Test Loss: 0.04642119334401575\n",
      "Epoch: 1030 Training Loss: 0.0423873320413852 Test Loss: 0.04642015525522072\n",
      "Epoch: 1040 Training Loss: 0.04237821398663332 Test Loss: 0.04639175571310494\n",
      "Epoch: 1050 Training Loss: 0.042370093546135086 Test Loss: 0.04640198870130761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1060 Training Loss: 0.042362782359157296 Test Loss: 0.04639402164886185\n",
      "Epoch: 1070 Training Loss: 0.04235579766411706 Test Loss: 0.04638987487480426\n",
      "Epoch: 1080 Training Loss: 0.04234849903452148 Test Loss: 0.04637795080646774\n",
      "Epoch: 1090 Training Loss: 0.04234091995672313 Test Loss: 0.04637704945517528\n",
      "Epoch: 1100 Training Loss: 0.04233297252904651 Test Loss: 0.04636428263733312\n",
      "Epoch: 1110 Training Loss: 0.042324249334202015 Test Loss: 0.04637321243341944\n",
      "Epoch: 1120 Training Loss: 0.04231662839846295 Test Loss: 0.04636910472721973\n",
      "Epoch: 1130 Training Loss: 0.042309237681397806 Test Loss: 0.04635572119533536\n",
      "Epoch: 1140 Training Loss: 0.042300373564819575 Test Loss: 0.04633287207959949\n",
      "Epoch: 1150 Training Loss: 0.04229117458471565 Test Loss: 0.04630713194238067\n",
      "Epoch: 1160 Training Loss: 0.04228304577262927 Test Loss: 0.046316491484903646\n",
      "Epoch: 1170 Training Loss: 0.04227484580204369 Test Loss: 0.046305256685202544\n",
      "Epoch: 1180 Training Loss: 0.042266663969899074 Test Loss: 0.04630371350481637\n",
      "Epoch: 1190 Training Loss: 0.04225842074610808 Test Loss: 0.0463035293277721\n",
      "Epoch: 1200 Training Loss: 0.04224937664038479 Test Loss: 0.04631360604454325\n",
      "Epoch: 1210 Training Loss: 0.04223969908449205 Test Loss: 0.04629799564475985\n",
      "Epoch: 1220 Training Loss: 0.04222882160096555 Test Loss: 0.046316340794594686\n",
      "Epoch: 1230 Training Loss: 0.042216914412097524 Test Loss: 0.04629883002258167\n",
      "Epoch: 1240 Training Loss: 0.04220478258561433 Test Loss: 0.04629737614015637\n",
      "Epoch: 1250 Training Loss: 0.042192017308961695 Test Loss: 0.046279890483194835\n",
      "Epoch: 1260 Training Loss: 0.04217958550057007 Test Loss: 0.04626061607645477\n",
      "Epoch: 1270 Training Loss: 0.04216724159385394 Test Loss: 0.04624433036084234\n",
      "Epoch: 1280 Training Loss: 0.04215553671836602 Test Loss: 0.04620498065627572\n",
      "Epoch: 1290 Training Loss: 0.04214337001026558 Test Loss: 0.04618530998983432\n",
      "Epoch: 1300 Training Loss: 0.04212983175697445 Test Loss: 0.046186811311801336\n",
      "Epoch: 1310 Training Loss: 0.04211523031198922 Test Loss: 0.046150620522600244\n",
      "Epoch: 1320 Training Loss: 0.042098854090317786 Test Loss: 0.0461424330158136\n",
      "Epoch: 1330 Training Loss: 0.042082913191229754 Test Loss: 0.04611085223384207\n",
      "Epoch: 1340 Training Loss: 0.042069443305908456 Test Loss: 0.04610692312356409\n",
      "Epoch: 1350 Training Loss: 0.042055532516944986 Test Loss: 0.046080259310562595\n",
      "Epoch: 1360 Training Loss: 0.04204319419128762 Test Loss: 0.04607326058287994\n",
      "Epoch: 1370 Training Loss: 0.04203299480640066 Test Loss: 0.04605780924508933\n",
      "Epoch: 1380 Training Loss: 0.04202325446359685 Test Loss: 0.0460451959081174\n",
      "Epoch: 1390 Training Loss: 0.04201158586499088 Test Loss: 0.04602474667507973\n",
      "Epoch: 1400 Training Loss: 0.042001791106864146 Test Loss: 0.04600354119993599\n",
      "Epoch: 1410 Training Loss: 0.04199215122311805 Test Loss: 0.046002597990224374\n",
      "Epoch: 1420 Training Loss: 0.04198325222492256 Test Loss: 0.04597399752769665\n",
      "Epoch: 1430 Training Loss: 0.04197376302976303 Test Loss: 0.04597832847879852\n",
      "Epoch: 1440 Training Loss: 0.04196667229460632 Test Loss: 0.045955144495709414\n",
      "Epoch: 1450 Training Loss: 0.04196137028878273 Test Loss: 0.04595964008992662\n",
      "Epoch: 1460 Training Loss: 0.04195704357297775 Test Loss: 0.045932828377177444\n",
      "Epoch: 1470 Training Loss: 0.04195362377922153 Test Loss: 0.045941367494684984\n",
      "Epoch: 1480 Training Loss: 0.041949763081823147 Test Loss: 0.04592647426914977\n",
      "Epoch: 1490 Training Loss: 0.04194618283262753 Test Loss: 0.04590096853907822\n",
      "Epoch: 1500 Training Loss: 0.04194292349431071 Test Loss: 0.04591123222345493\n",
      "Epoch: 1510 Training Loss: 0.041939253948174916 Test Loss: 0.04591222008214698\n",
      "Epoch: 1520 Training Loss: 0.04193533185913015 Test Loss: 0.04591428788805322\n",
      "Epoch: 1530 Training Loss: 0.04193061307394715 Test Loss: 0.045882140622142464\n",
      "Epoch: 1540 Training Loss: 0.041926314263435974 Test Loss: 0.04589191316773443\n",
      "Epoch: 1550 Training Loss: 0.04192183127798566 Test Loss: 0.045902291265123504\n",
      "Epoch: 1560 Training Loss: 0.041916963199480795 Test Loss: 0.0458937102891968\n",
      "Epoch: 1570 Training Loss: 0.04191272857114537 Test Loss: 0.045912677734196404\n",
      "Epoch: 1580 Training Loss: 0.04190878415786556 Test Loss: 0.04590303355442318\n",
      "Epoch: 1590 Training Loss: 0.04190411978800549 Test Loss: 0.045901562928630214\n",
      "Epoch: 1600 Training Loss: 0.041899728890024746 Test Loss: 0.04590226056894946\n",
      "Epoch: 1610 Training Loss: 0.041894982199547945 Test Loss: 0.04590555343125629\n",
      "Epoch: 1620 Training Loss: 0.041890552234155866 Test Loss: 0.04592360278159576\n",
      "Epoch: 1630 Training Loss: 0.04188653805764153 Test Loss: 0.04592625102424761\n",
      "Epoch: 1640 Training Loss: 0.0418832075608255 Test Loss: 0.04593081917305802\n",
      "Epoch: 1650 Training Loss: 0.04188011984006561 Test Loss: 0.04595368224160028\n",
      "Epoch: 1660 Training Loss: 0.04187709909201085 Test Loss: 0.0459466863044789\n",
      "Epoch: 1670 Training Loss: 0.04187314072608413 Test Loss: 0.04593252699655953\n",
      "Epoch: 1680 Training Loss: 0.0418685740247524 Test Loss: 0.04595778715723871\n",
      "Epoch: 1690 Training Loss: 0.04186456403403214 Test Loss: 0.04596812897733118\n",
      "Epoch: 1700 Training Loss: 0.041860801005162075 Test Loss: 0.04598084556507035\n",
      "Epoch: 1710 Training Loss: 0.041857262613907174 Test Loss: 0.04598174970692409\n",
      "Epoch: 1720 Training Loss: 0.041852761490015884 Test Loss: 0.04600098225524501\n",
      "Epoch: 1730 Training Loss: 0.041847618544367006 Test Loss: 0.04599365424133167\n",
      "Epoch: 1740 Training Loss: 0.04184288022547834 Test Loss: 0.04598966652926687\n",
      "Epoch: 1750 Training Loss: 0.04183682338145724 Test Loss: 0.046002623105275864\n",
      "Epoch: 1760 Training Loss: 0.04182958056244928 Test Loss: 0.04602392066894174\n",
      "Epoch: 1770 Training Loss: 0.04182180754285896 Test Loss: 0.04601833954638779\n",
      "Epoch: 1780 Training Loss: 0.04181476564196642 Test Loss: 0.04603805486180962\n",
      "Epoch: 1790 Training Loss: 0.0418086669400046 Test Loss: 0.046041844444023754\n",
      "Epoch: 1800 Training Loss: 0.04180411419131977 Test Loss: 0.046042117919028895\n",
      "Epoch: 1810 Training Loss: 0.04180059533377053 Test Loss: 0.04606354384851351\n",
      "Epoch: 1820 Training Loss: 0.041797368086541595 Test Loss: 0.04606729715343104\n",
      "Epoch: 1830 Training Loss: 0.04179301904650156 Test Loss: 0.04605136304853951\n",
      "Epoch: 1840 Training Loss: 0.04178895184992899 Test Loss: 0.04605591445398226\n",
      "Epoch: 1850 Training Loss: 0.0417854260160563 Test Loss: 0.04606312247376069\n",
      "Epoch: 1860 Training Loss: 0.04178284477637903 Test Loss: 0.04606798921262773\n",
      "Epoch: 1870 Training Loss: 0.041779560323297785 Test Loss: 0.04606288248549087\n",
      "Epoch: 1880 Training Loss: 0.04177602332730757 Test Loss: 0.046065031217674135\n",
      "Epoch: 1890 Training Loss: 0.04177264539149207 Test Loss: 0.046045363341794016\n",
      "Epoch: 1900 Training Loss: 0.04176948372170359 Test Loss: 0.046071296027740946\n",
      "Epoch: 1910 Training Loss: 0.041765780689215215 Test Loss: 0.04608159319885299\n",
      "Epoch: 1920 Training Loss: 0.041761483273968725 Test Loss: 0.04611549293724568\n",
      "Epoch: 1930 Training Loss: 0.04175689006260787 Test Loss: 0.0461040014059071\n",
      "Epoch: 1940 Training Loss: 0.041752503350421195 Test Loss: 0.04611941646640111\n",
      "Epoch: 1950 Training Loss: 0.04174850591708315 Test Loss: 0.04610911650472779\n",
      "Epoch: 1960 Training Loss: 0.04174532610885368 Test Loss: 0.046130679171714976\n",
      "Epoch: 1970 Training Loss: 0.0417426011569133 Test Loss: 0.046143172514551994\n",
      "Epoch: 1980 Training Loss: 0.04173943669659543 Test Loss: 0.0461331488184451\n",
      "Epoch: 1990 Training Loss: 0.0417360475986624 Test Loss: 0.046130684752837534\n",
      "Epoch: 2000 Training Loss: 0.0417339365631858 Test Loss: 0.04613554591058202\n",
      "Epoch: 2010 Training Loss: 0.04173172088285742 Test Loss: 0.04615134606853226\n",
      "Epoch: 2020 Training Loss: 0.041730038193640774 Test Loss: 0.046145430078625066\n",
      "Epoch: 2030 Training Loss: 0.04172831504174811 Test Loss: 0.04614400131125126\n",
      "Epoch: 2040 Training Loss: 0.041727071860908946 Test Loss: 0.046148376911333555\n",
      "Epoch: 2050 Training Loss: 0.04172605471294963 Test Loss: 0.04614385341150358\n",
      "Epoch: 2060 Training Loss: 0.04172452271231955 Test Loss: 0.046150054038661015\n",
      "Epoch: 2070 Training Loss: 0.04172382228944496 Test Loss: 0.046159575433738054\n",
      "Epoch: 2080 Training Loss: 0.04172294048216062 Test Loss: 0.04614597981919663\n",
      "Epoch: 2090 Training Loss: 0.041722040536435294 Test Loss: 0.04615590026453628\n",
      "Epoch: 2100 Training Loss: 0.041720455515746976 Test Loss: 0.04614484127019563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2110 Training Loss: 0.04171917187223179 Test Loss: 0.046133137656199995\n",
      "Epoch: 2120 Training Loss: 0.041718368199770116 Test Loss: 0.046139276891009336\n",
      "Epoch: 2130 Training Loss: 0.04171665760525966 Test Loss: 0.04613840344532964\n",
      "Epoch: 2140 Training Loss: 0.041714938639161064 Test Loss: 0.04611666497298201\n",
      "Epoch: 2150 Training Loss: 0.04171356569870569 Test Loss: 0.046117404471720407\n",
      "Epoch: 2160 Training Loss: 0.04171236577107194 Test Loss: 0.04612324232591184\n",
      "Epoch: 2170 Training Loss: 0.04171094260108771 Test Loss: 0.04612422181292006\n",
      "Epoch: 2180 Training Loss: 0.0417098961525699 Test Loss: 0.0461353784769054\n",
      "Epoch: 2190 Training Loss: 0.04170851344526169 Test Loss: 0.04611576083112827\n",
      "Epoch: 2200 Training Loss: 0.0417073470039805 Test Loss: 0.046110269006535184\n",
      "Epoch: 2210 Training Loss: 0.041706502868842796 Test Loss: 0.04611966203579348\n",
      "Epoch: 2220 Training Loss: 0.041705647571587574 Test Loss: 0.04612406275092727\n",
      "Epoch: 2230 Training Loss: 0.04170504760777069 Test Loss: 0.04611768631840938\n",
      "Epoch: 2240 Training Loss: 0.04170427602639689 Test Loss: 0.04612805604411462\n",
      "Epoch: 2250 Training Loss: 0.041703083075086585 Test Loss: 0.04612781884640608\n",
      "Epoch: 2260 Training Loss: 0.04170193198171699 Test Loss: 0.046126191949181605\n",
      "Epoch: 2270 Training Loss: 0.041700189296118656 Test Loss: 0.04613371251182305\n",
      "Epoch: 2280 Training Loss: 0.04169889030469188 Test Loss: 0.0461316949360198\n",
      "Epoch: 2290 Training Loss: 0.04169753410741279 Test Loss: 0.04613297580364593\n",
      "Epoch: 2300 Training Loss: 0.041697284355033205 Test Loss: 0.0461321916559271\n",
      "Epoch: 2310 Training Loss: 0.04169615698116334 Test Loss: 0.046138191362672595\n",
      "Epoch: 2320 Training Loss: 0.0416951593669097 Test Loss: 0.04612907738954199\n",
      "Epoch: 2330 Training Loss: 0.041694255235390304 Test Loss: 0.046148988044253214\n",
      "Epoch: 2340 Training Loss: 0.041693571555692 Test Loss: 0.04615400826399049\n",
      "Epoch: 2350 Training Loss: 0.04169297019661043 Test Loss: 0.04615288087723459\n",
      "Epoch: 2360 Training Loss: 0.04169228372638274 Test Loss: 0.04613733745092184\n",
      "Epoch: 2370 Training Loss: 0.04169200885923873 Test Loss: 0.04614997590294526\n",
      "Epoch: 2380 Training Loss: 0.04169137680433398 Test Loss: 0.04612714911169961\n",
      "Epoch: 2390 Training Loss: 0.041690409885903515 Test Loss: 0.04613674306136985\n",
      "Epoch: 2400 Training Loss: 0.04168959365605962 Test Loss: 0.046129741543125914\n",
      "Epoch: 2410 Training Loss: 0.041688969972743004 Test Loss: 0.04614411851482489\n",
      "Epoch: 2420 Training Loss: 0.04168819699610451 Test Loss: 0.046128265336210394\n",
      "Epoch: 2430 Training Loss: 0.0416870193927058 Test Loss: 0.046121807977415476\n",
      "Epoch: 2440 Training Loss: 0.041685441348340936 Test Loss: 0.04612383392490256\n",
      "Epoch: 2450 Training Loss: 0.04168399306359228 Test Loss: 0.04610976391494405\n",
      "Epoch: 2460 Training Loss: 0.04168310707051386 Test Loss: 0.04610440603729226\n",
      "Epoch: 2470 Training Loss: 0.04168262291366629 Test Loss: 0.04610302470946016\n",
      "Epoch: 2480 Training Loss: 0.041681879237586296 Test Loss: 0.046096383173620956\n",
      "Epoch: 2490 Training Loss: 0.041681311364857296 Test Loss: 0.046103680491360245\n",
      "Epoch: 2500 Training Loss: 0.041680636056747135 Test Loss: 0.0460881845045892\n",
      "Epoch: 2510 Training Loss: 0.04167902034023563 Test Loss: 0.04607778129214864\n",
      "Epoch: 2520 Training Loss: 0.041677228820373134 Test Loss: 0.04605557121494519\n",
      "Epoch: 2530 Training Loss: 0.0416752531255715 Test Loss: 0.04607462795790566\n",
      "Epoch: 2540 Training Loss: 0.041672624446894756 Test Loss: 0.046088725873476934\n",
      "Epoch: 2550 Training Loss: 0.04166993995763039 Test Loss: 0.04608555579586629\n",
      "Epoch: 2560 Training Loss: 0.041667503825480914 Test Loss: 0.046078021280418456\n",
      "Epoch: 2570 Training Loss: 0.04166597880117429 Test Loss: 0.04610995367311088\n",
      "Epoch: 2580 Training Loss: 0.041664395175750664 Test Loss: 0.046079692826623365\n",
      "Epoch: 2590 Training Loss: 0.04166291479991413 Test Loss: 0.046069610528729654\n",
      "Epoch: 2600 Training Loss: 0.04166153906892938 Test Loss: 0.04606860871723122\n",
      "Epoch: 2610 Training Loss: 0.041660293097560835 Test Loss: 0.04606474378986261\n",
      "Epoch: 2620 Training Loss: 0.04165927176380745 Test Loss: 0.04608085090955331\n",
      "Epoch: 2630 Training Loss: 0.0416587401679604 Test Loss: 0.046078437074048725\n",
      "Epoch: 2640 Training Loss: 0.04165785696541137 Test Loss: 0.04606095699820975\n",
      "Epoch: 2650 Training Loss: 0.04165678819265851 Test Loss: 0.04603988547000731\n",
      "Epoch: 2660 Training Loss: 0.04165518363826452 Test Loss: 0.046062729004620634\n",
      "Epoch: 2670 Training Loss: 0.04165404091648307 Test Loss: 0.0460480925107229\n",
      "Epoch: 2680 Training Loss: 0.04165323305822732 Test Loss: 0.04605538982846219\n",
      "Epoch: 2690 Training Loss: 0.041651804307184334 Test Loss: 0.04603136309586743\n",
      "Epoch: 2700 Training Loss: 0.04165076762551935 Test Loss: 0.04603701677301458\n",
      "Epoch: 2710 Training Loss: 0.04165003371629219 Test Loss: 0.04604430571907005\n",
      "Epoch: 2720 Training Loss: 0.041648946805098355 Test Loss: 0.04603536197017734\n",
      "Epoch: 2730 Training Loss: 0.04164835242234024 Test Loss: 0.04604443687545006\n",
      "Epoch: 2740 Training Loss: 0.04164605581665981 Test Loss: 0.046043393205532475\n",
      "Epoch: 2750 Training Loss: 0.041644308945267404 Test Loss: 0.04601664288513139\n",
      "Epoch: 2760 Training Loss: 0.04164312436554524 Test Loss: 0.046021188709451585\n",
      "Epoch: 2770 Training Loss: 0.04164228860199568 Test Loss: 0.04601615174634664\n",
      "Epoch: 2780 Training Loss: 0.041641200295537154 Test Loss: 0.04602140358266991\n",
      "Epoch: 2790 Training Loss: 0.04164007431693199 Test Loss: 0.04602632892332377\n",
      "Epoch: 2800 Training Loss: 0.04163832325974551 Test Loss: 0.046023354185002516\n",
      "Epoch: 2810 Training Loss: 0.041636028049329775 Test Loss: 0.04600303610834486\n",
      "Epoch: 2820 Training Loss: 0.04163452255872881 Test Loss: 0.04600573737166097\n",
      "Epoch: 2830 Training Loss: 0.04163327519209558 Test Loss: 0.04602343232071827\n",
      "Epoch: 2840 Training Loss: 0.04163133856470528 Test Loss: 0.04600129200754675\n",
      "Epoch: 2850 Training Loss: 0.04162943960946162 Test Loss: 0.04601368209961652\n",
      "Epoch: 2860 Training Loss: 0.041628327583503356 Test Loss: 0.046015886643025326\n",
      "Epoch: 2870 Training Loss: 0.04162745972886592 Test Loss: 0.04601685217722716\n",
      "Epoch: 2880 Training Loss: 0.041626064464175494 Test Loss: 0.046011594759781345\n",
      "Epoch: 2890 Training Loss: 0.04162490499921776 Test Loss: 0.04600392071626966\n",
      "Epoch: 2900 Training Loss: 0.04162410969834422 Test Loss: 0.046020438048468074\n",
      "Epoch: 2910 Training Loss: 0.04162297534815091 Test Loss: 0.04601069898961143\n",
      "Epoch: 2920 Training Loss: 0.041622029358690804 Test Loss: 0.04601269145036319\n",
      "Epoch: 2930 Training Loss: 0.04162024900094583 Test Loss: 0.04601306259501303\n",
      "Epoch: 2940 Training Loss: 0.04161883420254975 Test Loss: 0.04602900228102711\n",
      "Epoch: 2950 Training Loss: 0.04161737336041888 Test Loss: 0.046035144306397734\n",
      "Epoch: 2960 Training Loss: 0.04161678316345484 Test Loss: 0.046030143620589395\n",
      "Epoch: 2970 Training Loss: 0.041615650208526214 Test Loss: 0.04605175372711829\n",
      "Epoch: 2980 Training Loss: 0.041615260929677586 Test Loss: 0.04606036539921904\n",
      "Epoch: 2990 Training Loss: 0.04161459538842026 Test Loss: 0.04606592140672149\n",
      "Epoch: 3000 Training Loss: 0.04161367451372458 Test Loss: 0.046069507277962406\n",
      "Epoch: 3010 Training Loss: 0.04161287921285104 Test Loss: 0.04607385776299321\n",
      "Epoch: 3020 Training Loss: 0.041611855088568274 Test Loss: 0.04607534234159256\n",
      "Epoch: 3030 Training Loss: 0.04161126489160423 Test Loss: 0.04607106441115496\n",
      "Epoch: 3040 Training Loss: 0.04161063562722885 Test Loss: 0.0460593635877206\n",
      "Epoch: 3050 Training Loss: 0.041609416165889423 Test Loss: 0.046046150280074125\n",
      "Epoch: 3060 Training Loss: 0.041608746438838023 Test Loss: 0.04607302896629395\n",
      "Epoch: 3070 Training Loss: 0.04160759395020374 Test Loss: 0.04603898690927613\n",
      "Epoch: 3080 Training Loss: 0.0416066967950078 Test Loss: 0.046038085557983666\n",
      "Epoch: 3090 Training Loss: 0.04160575778187115 Test Loss: 0.046053955479965825\n",
      "Epoch: 3100 Training Loss: 0.041604961085732924 Test Loss: 0.046035412200280325\n",
      "Epoch: 3110 Training Loss: 0.0416039885862437 Test Loss: 0.04602790279988398\n",
      "Epoch: 3120 Training Loss: 0.041603203052222995 Test Loss: 0.04605182070058894\n",
      "Epoch: 3130 Training Loss: 0.04160200591511862 Test Loss: 0.046057092070841146\n",
      "Epoch: 3140 Training Loss: 0.04160159431203494 Test Loss: 0.04607970677942975\n",
      "Epoch: 3150 Training Loss: 0.04160115619892215 Test Loss: 0.04607256573312197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3160 Training Loss: 0.04160073064319157 Test Loss: 0.046081364372828275\n",
      "Epoch: 3170 Training Loss: 0.04159921817626716 Test Loss: 0.046089593738034076\n",
      "Epoch: 3180 Training Loss: 0.04159813545086739 Test Loss: 0.04607538978113427\n",
      "Epoch: 3190 Training Loss: 0.04159664670344272 Test Loss: 0.046083217305516185\n",
      "Epoch: 3200 Training Loss: 0.0415953100398693 Test Loss: 0.046077488283214556\n",
      "Epoch: 3210 Training Loss: 0.041593860359855954 Test Loss: 0.046080309540665575\n",
      "Epoch: 3220 Training Loss: 0.04159144934247091 Test Loss: 0.04609105878270448\n",
      "Epoch: 3230 Training Loss: 0.041589461090287064 Test Loss: 0.046090788098260616\n",
      "Epoch: 3240 Training Loss: 0.041586704047258796 Test Loss: 0.04607558512042366\n",
      "Epoch: 3250 Training Loss: 0.04158470323769274 Test Loss: 0.04607887798273049\n",
      "Epoch: 3260 Training Loss: 0.04158285590724262 Test Loss: 0.04607386613467704\n",
      "Epoch: 3270 Training Loss: 0.04158138111246485 Test Loss: 0.04608109368838441\n",
      "Epoch: 3280 Training Loss: 0.04157972493327733 Test Loss: 0.04606814548405924\n",
      "Epoch: 3290 Training Loss: 0.041578448266085595 Test Loss: 0.046078657528389605\n",
      "Epoch: 3300 Training Loss: 0.04157591725593718 Test Loss: 0.046069429142246654\n",
      "Epoch: 3310 Training Loss: 0.04157167844180768 Test Loss: 0.04604863387961063\n",
      "Epoch: 3320 Training Loss: 0.04156515976517405 Test Loss: 0.04603272768033188\n",
      "Epoch: 3330 Training Loss: 0.04155795461831273 Test Loss: 0.04602883205678922\n",
      "Epoch: 3340 Training Loss: 0.04155080528203903 Test Loss: 0.04599400864261385\n",
      "Epoch: 3350 Training Loss: 0.04154634183029438 Test Loss: 0.04600229660960646\n",
      "Epoch: 3360 Training Loss: 0.04154213510725277 Test Loss: 0.04601326630598625\n",
      "Epoch: 3370 Training Loss: 0.041535776886058536 Test Loss: 0.04600623688212955\n",
      "Epoch: 3380 Training Loss: 0.041526148164429955 Test Loss: 0.04599887538148089\n",
      "Epoch: 3390 Training Loss: 0.041515901340543525 Test Loss: 0.04597267480165137\n",
      "Epoch: 3400 Training Loss: 0.04150742650281392 Test Loss: 0.04598737547845847\n",
      "Epoch: 3410 Training Loss: 0.04150167522176002 Test Loss: 0.045937817900740675\n",
      "Epoch: 3420 Training Loss: 0.04149711549675173 Test Loss: 0.04593240421186334\n",
      "Epoch: 3430 Training Loss: 0.04149138793519757 Test Loss: 0.045907437060118246\n",
      "Epoch: 3440 Training Loss: 0.04148546224605736 Test Loss: 0.04591389720947444\n",
      "Epoch: 3450 Training Loss: 0.04147752318996888 Test Loss: 0.045909594163985344\n",
      "Epoch: 3460 Training Loss: 0.041470020851728503 Test Loss: 0.04587317454875955\n",
      "Epoch: 3470 Training Loss: 0.04146342962133097 Test Loss: 0.04586592746112324\n",
      "Epoch: 3480 Training Loss: 0.04145780251883451 Test Loss: 0.04584373970841001\n",
      "Epoch: 3490 Training Loss: 0.04145360835317511 Test Loss: 0.04583759768303939\n",
      "Epoch: 3500 Training Loss: 0.04144891607802124 Test Loss: 0.045815753169363226\n",
      "Epoch: 3510 Training Loss: 0.04144418473545603 Test Loss: 0.045807082895475665\n",
      "Epoch: 3520 Training Loss: 0.04143866367307605 Test Loss: 0.045795856467458394\n",
      "Epoch: 3530 Training Loss: 0.04143294029731595 Test Loss: 0.04578436493611981\n",
      "Epoch: 3540 Training Loss: 0.04142742342073003 Test Loss: 0.04578918702600642\n",
      "Epoch: 3550 Training Loss: 0.04142134006667981 Test Loss: 0.04579658201339041\n",
      "Epoch: 3560 Training Loss: 0.041414215845170535 Test Loss: 0.04576898894348368\n",
      "Epoch: 3570 Training Loss: 0.041407729259624784 Test Loss: 0.045789932105867376\n",
      "Epoch: 3580 Training Loss: 0.04140201565071752 Test Loss: 0.04578740106678916\n",
      "Epoch: 3590 Training Loss: 0.041397101528477864 Test Loss: 0.045785495113436984\n",
      "Epoch: 3600 Training Loss: 0.04139299665975866 Test Loss: 0.04582846696654112\n",
      "Epoch: 3610 Training Loss: 0.0413887383119235 Test Loss: 0.04582582430501183\n",
      "Epoch: 3620 Training Loss: 0.04138438787661877 Test Loss: 0.04581722100459491\n",
      "Epoch: 3630 Training Loss: 0.04137628417929683 Test Loss: 0.04583161471966155\n",
      "Epoch: 3640 Training Loss: 0.04136734053263125 Test Loss: 0.04580419187399272\n",
      "Epoch: 3650 Training Loss: 0.04136181807498657 Test Loss: 0.04579946466318952\n",
      "Epoch: 3660 Training Loss: 0.041356390495340845 Test Loss: 0.045787322931073404\n",
      "Epoch: 3670 Training Loss: 0.04134937929027149 Test Loss: 0.04575504729934391\n",
      "Epoch: 3680 Training Loss: 0.041343591732335636 Test Loss: 0.04574553148538942\n",
      "Epoch: 3690 Training Loss: 0.041335881499656384 Test Loss: 0.045748455993607696\n",
      "Epoch: 3700 Training Loss: 0.04133041345733464 Test Loss: 0.04573621659184688\n",
      "Epoch: 3710 Training Loss: 0.04132669507693467 Test Loss: 0.045728408601393905\n",
      "Epoch: 3720 Training Loss: 0.04132277298788991 Test Loss: 0.04572672310238261\n",
      "Epoch: 3730 Training Loss: 0.04132032848415229 Test Loss: 0.04573374415455548\n",
      "Epoch: 3740 Training Loss: 0.041317743058680946 Test Loss: 0.04572784211745468\n",
      "Epoch: 3750 Training Loss: 0.041313205657907706 Test Loss: 0.045745495208092825\n",
      "Epoch: 3760 Training Loss: 0.041306647913862744 Test Loss: 0.04576062005021403\n",
      "Epoch: 3770 Training Loss: 0.04130206865514879 Test Loss: 0.045753261340126646\n",
      "Epoch: 3780 Training Loss: 0.0412982958594259 Test Loss: 0.04574843925024003\n",
      "Epoch: 3790 Training Loss: 0.04129514395649025 Test Loss: 0.04574793694921018\n",
      "Epoch: 3800 Training Loss: 0.04129201019199556 Test Loss: 0.04571688916444255\n",
      "Epoch: 3810 Training Loss: 0.04128998984872384 Test Loss: 0.04575150328652215\n",
      "Epoch: 3820 Training Loss: 0.041287631851397034 Test Loss: 0.045742565118752\n",
      "Epoch: 3830 Training Loss: 0.041284796673546106 Test Loss: 0.04572826907333006\n",
      "Epoch: 3840 Training Loss: 0.04128170476699214 Test Loss: 0.0457169561379132\n",
      "Epoch: 3850 Training Loss: 0.04127955326883951 Test Loss: 0.045696395282424446\n",
      "Epoch: 3860 Training Loss: 0.04127742549018662 Test Loss: 0.04569234617801156\n",
      "Epoch: 3870 Training Loss: 0.041275233529357976 Test Loss: 0.04567388661516437\n",
      "Epoch: 3880 Training Loss: 0.04127324388190944 Test Loss: 0.04563901297088601\n",
      "Epoch: 3890 Training Loss: 0.041270917975670514 Test Loss: 0.04563888739562855\n",
      "Epoch: 3900 Training Loss: 0.041269578521567714 Test Loss: 0.04561936183837355\n",
      "Epoch: 3910 Training Loss: 0.04126834929337545 Test Loss: 0.04560118970333789\n",
      "Epoch: 3920 Training Loss: 0.041267456323973585 Test Loss: 0.04561163477419761\n",
      "Epoch: 3930 Training Loss: 0.041266080592988835 Test Loss: 0.045618611177390044\n",
      "Epoch: 3940 Training Loss: 0.0412650801882058 Test Loss: 0.04562036364987198\n",
      "Epoch: 3950 Training Loss: 0.04126468532829841 Test Loss: 0.045601856647483084\n",
      "Epoch: 3960 Training Loss: 0.041265060654500134 Test Loss: 0.04559602437441421\n",
      "Epoch: 3970 Training Loss: 0.04126481229738524 Test Loss: 0.04559517883434728\n",
      "Epoch: 3980 Training Loss: 0.04126333610734278 Test Loss: 0.0455966550412628\n",
      "Epoch: 3990 Training Loss: 0.041262591035998095 Test Loss: 0.04560420909063957\n",
      "Epoch: 4000 Training Loss: 0.04126215710867938 Test Loss: 0.04560879398281764\n",
      "Epoch: 4010 Training Loss: 0.041262482205352245 Test Loss: 0.04559827077624217\n",
      "Epoch: 4020 Training Loss: 0.04126189619418227 Test Loss: 0.045614520214557995\n",
      "Epoch: 4030 Training Loss: 0.0412615417969509 Test Loss: 0.04560659502053139\n",
      "Epoch: 4040 Training Loss: 0.041261186004454845 Test Loss: 0.04560366214062929\n",
      "Epoch: 4050 Training Loss: 0.04126048976737433 Test Loss: 0.04558059536111381\n",
      "Epoch: 4060 Training Loss: 0.041260259548700405 Test Loss: 0.045595996468801435\n",
      "Epoch: 4070 Training Loss: 0.04125966795647167 Test Loss: 0.04558315151524352\n",
      "Epoch: 4080 Training Loss: 0.04125954517317892 Test Loss: 0.04560861259633464\n",
      "Epoch: 4090 Training Loss: 0.04125938332247483 Test Loss: 0.04560136550869834\n",
      "Epoch: 4100 Training Loss: 0.04125932472135783 Test Loss: 0.045591450644481245\n",
      "Epoch: 4110 Training Loss: 0.04125937634615137 Test Loss: 0.045612946337997784\n",
      "Epoch: 4120 Training Loss: 0.041259386113004205 Test Loss: 0.04560157480079411\n",
      "Epoch: 4130 Training Loss: 0.04125891032774477 Test Loss: 0.04559415469835863\n",
      "Epoch: 4140 Training Loss: 0.04125862848427731 Test Loss: 0.045570150290254095\n",
      "Epoch: 4150 Training Loss: 0.04125852662995491 Test Loss: 0.045586274153312456\n",
      "Epoch: 4160 Training Loss: 0.04125850570098455 Test Loss: 0.04559459002591784\n",
      "Epoch: 4170 Training Loss: 0.041258015963078216 Test Loss: 0.04560846190602568\n",
      "Epoch: 4180 Training Loss: 0.04125786108869758 Test Loss: 0.04560065112501143\n",
      "Epoch: 4190 Training Loss: 0.04125772574802261 Test Loss: 0.04557980005114987\n",
      "Epoch: 4200 Training Loss: 0.041257280658586366 Test Loss: 0.04556412267789583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4210 Training Loss: 0.04125718159479334 Test Loss: 0.045591671098822124\n",
      "Epoch: 4220 Training Loss: 0.041257174618469894 Test Loss: 0.0455875773454288\n",
      "Epoch: 4230 Training Loss: 0.04125650628668318 Test Loss: 0.04558798755793652\n",
      "Epoch: 4240 Training Loss: 0.04125622165268634 Test Loss: 0.045579476346041745\n",
      "Epoch: 4250 Training Loss: 0.041256504891418494 Test Loss: 0.045577804799836835\n",
      "Epoch: 4260 Training Loss: 0.04125654814462389 Test Loss: 0.045585537445135334\n",
      "Epoch: 4270 Training Loss: 0.041255966319247994 Test Loss: 0.04558001213380692\n",
      "Epoch: 4280 Training Loss: 0.04125564819889858 Test Loss: 0.04558809638982632\n",
      "Epoch: 4290 Training Loss: 0.04125573610057407 Test Loss: 0.04558347801091293\n",
      "Epoch: 4300 Training Loss: 0.041255285430079065 Test Loss: 0.04559219572434219\n",
      "Epoch: 4310 Training Loss: 0.04125541239916589 Test Loss: 0.0456001823107169\n",
      "Epoch: 4320 Training Loss: 0.04125505381614046 Test Loss: 0.04559022279751938\n",
      "Epoch: 4330 Training Loss: 0.04125510404566931 Test Loss: 0.04557976098329199\n",
      "Epoch: 4340 Training Loss: 0.04125486824593663 Test Loss: 0.04559005257328148\n",
      "Epoch: 4350 Training Loss: 0.041254866850671944 Test Loss: 0.04558271897824559\n",
      "Epoch: 4360 Training Loss: 0.04125476081055547 Test Loss: 0.04559023675032576\n",
      "Epoch: 4370 Training Loss: 0.04125483475958406 Test Loss: 0.04559684758999091\n",
      "Epoch: 4380 Training Loss: 0.041254251538943465 Test Loss: 0.0455959880971176\n",
      "Epoch: 4390 Training Loss: 0.04125478173952583 Test Loss: 0.04558566022983152\n",
      "Epoch: 4400 Training Loss: 0.041254597564586694 Test Loss: 0.04558506584027953\n",
      "Epoch: 4410 Training Loss: 0.0412550328871701 Test Loss: 0.04561030367646849\n",
      "Epoch: 4420 Training Loss: 0.04125397109074069 Test Loss: 0.045603673302874397\n",
      "Epoch: 4430 Training Loss: 0.04125394318544688 Test Loss: 0.04559044604242153\n",
      "Epoch: 4440 Training Loss: 0.04125416224200328 Test Loss: 0.04558897541662857\n",
      "Epoch: 4450 Training Loss: 0.041253718547831726 Test Loss: 0.04559637598513511\n",
      "Epoch: 4460 Training Loss: 0.04125364599406783 Test Loss: 0.04559902422778695\n",
      "Epoch: 4470 Training Loss: 0.04125442873755915 Test Loss: 0.04561066086831194\n",
      "Epoch: 4480 Training Loss: 0.04125389574644741 Test Loss: 0.04562854836609735\n",
      "Epoch: 4490 Training Loss: 0.04125427944423728 Test Loss: 0.04559748662852334\n",
      "Epoch: 4500 Training Loss: 0.041254568264028194 Test Loss: 0.04562454670122617\n",
      "Epoch: 4510 Training Loss: 0.04125467709467404 Test Loss: 0.045620985945036746\n",
      "Epoch: 4520 Training Loss: 0.04125423060997311 Test Loss: 0.04561949299475357\n",
      "Epoch: 4530 Training Loss: 0.04125394318544688 Test Loss: 0.04558439610557305\n",
      "Epoch: 4540 Training Loss: 0.041253749243654915 Test Loss: 0.045618092132992526\n",
      "Epoch: 4550 Training Loss: 0.04125384830744794 Test Loss: 0.04558255712569152\n",
      "Epoch: 4560 Training Loss: 0.04125380365897784 Test Loss: 0.04559806985583023\n",
      "Epoch: 4570 Training Loss: 0.041253436704364266 Test Loss: 0.04559940095355935\n",
      "Epoch: 4580 Training Loss: 0.04125353018709852 Test Loss: 0.04559128879192718\n",
      "Epoch: 4590 Training Loss: 0.04125379807791908 Test Loss: 0.04559338171288491\n",
      "Epoch: 4600 Training Loss: 0.041253694828331994 Test Loss: 0.045604086305943384\n",
      "Epoch: 4610 Training Loss: 0.041253611112450565 Test Loss: 0.045576094185774046\n",
      "Epoch: 4620 Training Loss: 0.041253736686272705 Test Loss: 0.04558121207515602\n",
      "Epoch: 4630 Training Loss: 0.04125347577177559 Test Loss: 0.045602604517905314\n",
      "Epoch: 4640 Training Loss: 0.04125316602301432 Test Loss: 0.04559352124094876\n",
      "Epoch: 4650 Training Loss: 0.04125344228542303 Test Loss: 0.045602169190346105\n",
      "Epoch: 4660 Training Loss: 0.04125327345839548 Test Loss: 0.04560413932660765\n",
      "Epoch: 4670 Training Loss: 0.041253486933893115 Test Loss: 0.04558730945154621\n",
      "Epoch: 4680 Training Loss: 0.04125394458071158 Test Loss: 0.045579515413899624\n",
      "Epoch: 4690 Training Loss: 0.041253682270949776 Test Loss: 0.04558443517343093\n",
      "Epoch: 4700 Training Loss: 0.041253647389332515 Test Loss: 0.04559911631630909\n",
      "Epoch: 4710 Training Loss: 0.04125377435841934 Test Loss: 0.04558544814717447\n",
      "Epoch: 4720 Training Loss: 0.041253616693509326 Test Loss: 0.04559525697006304\n",
      "Epoch: 4730 Training Loss: 0.04125304044919218 Test Loss: 0.045564647303415896\n",
      "Epoch: 4740 Training Loss: 0.04125329299210115 Test Loss: 0.04560061763827611\n",
      "Epoch: 4750 Training Loss: 0.041253281829983626 Test Loss: 0.045567047186114096\n",
      "Epoch: 4760 Training Loss: 0.04125329578263053 Test Loss: 0.04559225990725157\n",
      "Epoch: 4770 Training Loss: 0.04125336415060036 Test Loss: 0.045611871971906144\n",
      "Epoch: 4780 Training Loss: 0.04125345763333462 Test Loss: 0.04559889865252949\n",
      "Epoch: 4790 Training Loss: 0.04125360274086242 Test Loss: 0.045606974536865054\n",
      "Epoch: 4800 Training Loss: 0.04125363901774437 Test Loss: 0.04560085483598465\n",
      "Epoch: 4810 Training Loss: 0.04125368506147916 Test Loss: 0.04559128321080463\n",
      "Epoch: 4820 Training Loss: 0.041253479957569665 Test Loss: 0.04560395514956337\n",
      "Epoch: 4830 Training Loss: 0.041253210671484415 Test Loss: 0.045600386021690115\n",
      "Epoch: 4840 Training Loss: 0.041253276248924865 Test Loss: 0.04558286408743199\n",
      "Epoch: 4850 Training Loss: 0.041252972081222354 Test Loss: 0.04559224037332263\n",
      "Epoch: 4860 Training Loss: 0.04125293161854633 Test Loss: 0.04557461239773598\n",
      "Epoch: 4870 Training Loss: 0.041252951152252 Test Loss: 0.04556711695014602\n",
      "Epoch: 4880 Training Loss: 0.041253048820780326 Test Loss: 0.04558727038368834\n",
      "Epoch: 4890 Training Loss: 0.04125305719236847 Test Loss: 0.04560652525649946\n",
      "Epoch: 4900 Training Loss: 0.04125296231436952 Test Loss: 0.04560815494428522\n",
      "Epoch: 4910 Training Loss: 0.04125306835448599 Test Loss: 0.04558042234631464\n",
      "Epoch: 4920 Training Loss: 0.04125325392468982 Test Loss: 0.04559400958917223\n",
      "Epoch: 4930 Training Loss: 0.041253269272601416 Test Loss: 0.045587158761237256\n",
      "Epoch: 4940 Training Loss: 0.04125325113416044 Test Loss: 0.045589539110006516\n",
      "Epoch: 4950 Training Loss: 0.04125360274086242 Test Loss: 0.045584984914002494\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0ea4af3e8d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# input x and predict based on x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# must be (1. nn output, 2. target), the target label is NOT one-hotted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m#loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \"\"\"\n\u001b[0;32m   1281\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[1;32m-> 1282\u001b[1;33m                            input, target, size_average, reduce)\n\u001b[0m\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[1;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=int(x_variable.size()[0]/2 )\n",
    "nb_of_epochs=13000\n",
    "my_net.cuda()\n",
    "\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_net.eval()\n",
    "        test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/(x_variable.size(0)),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour1=np.delete(polygons_reshaped[311],24)\n",
    "\n",
    "contour1=polygons[10].reshape(12,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.00646368949978321 -0.1199108451461814 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(contour1,.9)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x24214519c88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_contour(contour1)\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (bn_input): BatchNorm1d(22, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc0): Linear(in_features=22, out_features=20, bias=True)\n",
       "  (bn0): BatchNorm1d(20, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (predict): Linear(in_features=20, out_features=2, bias=True)\n",
       "  (fc1): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (bn1): BatchNorm1d(20, eps=1e-05, momentum=0.5, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net.eval()\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=my_net(x_variable).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_inserted_points=predictions.data.numpy()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_inserted_points=predicted_inserted_points.reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "#plt.scatter(polygons_reshaped[3112][0::2].sum()/12,polygons_reshaped[312][1::2].sum()/12,label='Original points')\n",
    "plot_contour(contour1)\n",
    "plt.legend()\n",
    "original_points=point_coordinates.reshape(218722,2)\n",
    "#plt.scatter(original_points[:,0],original_points[:,1],label='Original points')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "plt.scatter(centers_of_mass[10][0],centers_of_mass[10][1],label='Original points'\n",
    "           )\n",
    "plot_contour(polygons[2])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "points=centers_of_mass\n",
    "hull=ConvexHull(points)\n",
    "hull2=ConvexHull(original_points)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(points[:,0], points[:,1], 'o')\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n",
    "for simplex in hull2.simplices:\n",
    "    plt.plot(original_points[simplex, 0], original_points[simplex, 1], 'k-')\n",
    "\n",
    "    \n",
    "    \n",
    "convex_points=centers_of_mass[hull.vertices]\n",
    "convex_points2=original_points[hull2.vertices]\n",
    "\n",
    "distances=np.empty([convex_points.shape[0],convex_points.shape[0]])    \n",
    "distances2=np.empty([convex_points2.shape[0],convex_points2.shape[0]])    \n",
    "\n",
    "    \n",
    "for i,point_i in enumerate(convex_points):\n",
    "    for j,point_j in enumerate(convex_points):\n",
    "        distances[i][j]=np.linalg.norm(point_i-point_j)\n",
    "for i,point_i in enumerate(convex_points2):\n",
    "    for j,point_j in enumerate(convex_points2):\n",
    "        distances2[i][j]=np.linalg.norm(point_i-point_j)\n",
    "        \n",
    "def max_element(A):\n",
    "    r, (c, l) = max(map(lambda t: (t[0], max(enumerate(t[1]), key=lambda v: v[1])), enumerate(A)), key=lambda v: v[1][1])\n",
    "    return (l, r, c)\n",
    "\n",
    "print(max_element(distances))\n",
    "print(max_element(distances2))\n",
    "\n",
    "maximum_distance_points=convex_points[[0,11]]\n",
    "maximum_distance_points2=convex_points2[[3,11]]\n",
    "\n",
    "plt.plot(maximum_distance_points[:,0],maximum_distance_points[:,1])\n",
    "plt.plot(maximum_distance_points2[:,0],maximum_distance_points2[:,1])\n",
    "1.8508252250365/0.6108970818704328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distances=[np.linalg.norm(i-j) for i in centers_of_mass for j in predicted_inserted_points]\n",
    "distances=np.array(distances)\n",
    "100*distances.sum()/(x_variable.size(0) *max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable,x_variable_test=x_variable.resize(x_variable.size()[0],1,25),Variable(x_tensor_test.view(x_variable_test.size()[0],1,25),volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using convolution network\n",
    "# O: output dimension\n",
    "# I: Input dimensiion\n",
    "# S: Stride\n",
    "# P: padding\n",
    "# w: kernel size\n",
    "# O=(I-w-2*P)/S+1\n",
    "\n",
    "\n",
    "class Conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Conv_net,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,14,kernel_size=14,stride=1)\n",
    "        self.conv2=nn.Conv1d(14,28,kernel_size=2,stride=1)\n",
    "\n",
    "        self.bn1=nn.BatchNorm1d(num_features=28*10)\n",
    "        self.fc1=nn.Linear(28*10,12)\n",
    "        \n",
    "        self.bn2=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc2=nn.Linear(12,12)\n",
    "        \n",
    "        self.bn3=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc3=nn.Linear(12,12)\n",
    "        \n",
    "        \n",
    "        self.bn4=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc4=nn.Linear(12,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "\n",
    "        x=F.relu(x)\n",
    "        #x=self.conv2(x)\n",
    "\n",
    "        x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        x=F.relu(self.fc1(self.bn1(x.view(-1,28*10))))\n",
    "        x=F.relu(self.fc2(self.bn2(x)))\n",
    "        x=F.relu(self.fc3(self.bn3(x)))\n",
    "\n",
    "        x=self.fc4(self.bn4(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "my_conv_net=Conv_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_conv_net.parameters(), lr=1e-3,weight_decay=0)\n",
    "\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "\n",
    "#loss_func = torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=torch.nn.SmoothL1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    my_conv_net.cuda()\n",
    "    x_variable , y_variable,x_variable_test,y_variable_test= x_variable.cuda(), y_variable.cuda(),x_variable_test.cuda(),y_variable_test.cuda()\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[-1 x 280]' is invalid for input with 1905316 elements at C:\\Anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\TH\\THStorage.c:37",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-930af165e887>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msum_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_conv_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# input x and predict based on x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# must be (1. nn output, 2. target), the target label is NOT one-hotted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-25cc40eabcd0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 2: size '[-1 x 280]' is invalid for input with 1905316 elements at C:\\Anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\TH\\THStorage.c:37"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size=int(x_variable.size()[0]/18 )\n",
    "nb_of_epochs=13000\n",
    "# Train the network #\n",
    "my_conv_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_conv_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\n",
    "        #loss=torch.sqrt((out[0]-y_variable.narrow(0,b,batch_size)[0]).pow(2)+(out[1]-y_variable.narrow(0,b,batch_size)[1]).pow(2)) \n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_conv_net.eval()\n",
    "        test_loss=loss_func(my_conv_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_conv_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable=x_variable.cpu()\n",
    "my_conv_net=my_conv_net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 extra points\n",
      "13 -0.1925145994756827 -0.0867447465103373 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f958d13940>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8VFX6+PHPyaT3TklIIyGUAAESBCmKSLOAgqCuDcHeXde27u8rq7srtrWBuKKCHcUGIiJKQFBAjPROAiEkIJAESC8zOb8/JgkJSUggkynJ83695pWZO3fufSaTPPfMuec+R2mtEUII0b442ToAIYQQ1ifJXwgh2iFJ/kII0Q5J8hdCiHZIkr8QQrRDkvyFEKIdkuQvhBDtkCR/IYRohyT5CyFEO+Rs6wAaExwcrKOiomwdhhBCOJQ//vgjR2sd0tR6dpv8o6KiSE1NtXUYQgjhUJRSB5uznnT7CCFEOyTJXwhxXg4dOsSIESPo0aMHvXr14rXXXrN1SOIc2G23jxDCvjk7O/Pyyy/Tv39/CgoKGDBgAKNGjaJnz562Dk00g0Ml/4qKCrKysigtLbV1KMJC3N3dCQ8Px8XFxdahiHPUqVMnOnXqBICPjw89evQgOztbkr+DcKjkn5WVhY+PD1FRUSilbB2OaCGtNbm5uWRlZREdHW3rcEQLZGRksGnTJi644AJbhyKayaH6/EtLSwkKCpLE30YopQgKCpJvcg6usLCQSZMm8eqrr+Lr62vrcEQzOVTyByTxtzHyeTq2iooKJk2axA033MDEiRNtHY44Bw6X/IUQtrUx8wQpu4+itWb69On06NGDv/71r7YOS5wjSf7nyGAwkJiYSEJCApMnT6a4uPi8t7Vq1SquuOIKABYvXszMmTMbXffkyZO8+eab57yPGTNm8NJLL513jNVSU1N54IEHzrrO+cYoHMOx/FIe/mwzE99cy50f/kHKqtV8+OGHpKSkkJiYSGJiIkuXLrV1mKKZJPmfIw8PDzZv3sz27dtxdXXlrbfeqvO81prKyspz3u748eN54oknGn3e1ok1KSmJ119//azr2DpG0TrKjZX87+d0Rry0iu+2HuHSHh2oMGk8IxLQWrN161Y2b97M5s2bueyyy2wdrmimNp38v9mUzZCZKUQ/8R1DZqbwzaZsi25/2LBhpKWlkZGRQY8ePbjnnnvo378/hw4dYvny5QwePJj+/fszefJkCgsLAVi2bBndu3dn6NChfPXVVzXbmj9/Pvfddx8AR48e5eqrr6Zv37707duXtWvX8sQTT5Cenk5iYiKPPvooAC+++CLJycn06dOHp59+umZb//73v4mPj+fSSy9lz549DcY+depU7rrrLoYNG0a3bt1YsmQJYD6pfuutt9K7d2/69evHypUrgbrfUmbMmMG0adO4+OKLiYmJqTkonBnjkSNHGD58eM03pTVr1ljy1y+sYPXe44x9bTXPfb+bwV2D+PGvw3l5cl+UgtSMPFuHJ1rAoYZ6notvNmXz5FfbKKkwAZB9soQnv9oGwFX9wlq8faPRyPfff8/YsWMB2LNnD/PmzePNN98kJyeHf/3rX/z00094eXnx/PPP89///pfHHnuM22+/nZSUFGJjY7n22msb3PYDDzzARRddxNdff43JZKKwsJCZM2eyfft2Nm/eDMDy5cvZt28fGzZsQGvN+PHjWb16NV5eXixYsIBNmzZhNBrp378/AwYMaHA/GRkZ/Pzzz6SnpzNixAjS0tKYPXs2ANu2bWP37t2MHj2avXv31nvt7t27WblyJQUFBcTHx3P33XfXi/Hll19mzJgxPPXUU5hMphZ1kQnrOpRXzLNLdrJ851GigjyZNzWZEd1Da56P7+DD7wdP2DBC0VJtNvm/+MOemsRfraTCxIs/7GlR8i8pKSExMREwt/ynT5/O4cOHiYyMZNCgQQCsX7+enTt3MmTIEADKy8sZPHgwu3fvJjo6mri4OABuvPFG3n777Xr7SElJ4YMPPgDM5xj8/Pw4caLuP9ry5ctZvnw5/fr1A8zD7fbt20dBQQFXX301np6egLk7qTFTpkzBycmJuLg4YmJi2L17N7/88gv3338/AN27dycyMrLB5H/55Zfj5uaGm5sboaGhHD16tN46ycnJTJs2jYqKCq666qqa35uwXyXlJub8nM7/fk7H4KR4bGw804dG4+ZsqLNeUlQA32w6jKlSY3CSEVuOqM0m/8MnS85peXNV9/mfycvLq+a+1ppRo0bx6aef1lln8+bNFhvaqLXmySef5M4776yz/NVXX232Ps5cTymF1rpZr3Vzc6u5bzAYMBqN9dYZPnw4q1ev5rvvvuOmm27i0Ucf5eabb27W9oV1aa35YcefPLtkF9knSxjftzNPXtadTn4eDa6fHBXIR+sz2XUkn4QwPytHKyyhzfb5d/Zv+I+2seWWNGjQIH799VfS0tIAKC4uZu/evXTv3p0DBw6Qnp4OUO/gUG3kyJHMmTMHAJPJRH5+Pj4+PhQUFNSsM2bMGN57772acwnZ2dkcO3aM4cOH8/XXX1NSUkJBQQHffvtto3EuXLiQyspK0tPT2b9/P/Hx8QwfPpyPP/4YgL1795KZmUl8fHyz3veZMR48eJDQ0FBuv/12pk+fzsaNG5u1HWFdaccKuOndDdz10UZ83J1ZcMcgXr++X6OJHyApKhCQfn9H1mZb/o+Oia/T5w/g4WLg0THNS2QtERISwvz587n++uspKysD4F//+hfdunXj7bff5vLLLyc4OJihQ4eyffv2eq9/7bXXuOOOO3j33XcxGAzMmTOHwYMHM2TIEBISEhg3bhwvvvgiu3btYvDgwQB4e3vz0Ucf0b9/f6699loSExOJjIxk2LBhjcYZHx/PRRddxNGjR3nrrbdwd3fnnnvu4a677qJ37944Ozszf/78Oq38swkKCqoTY0JCAi+++CIuLi54e3vXdGUJ+1BQWsFrP+1j/toMPF0N/HN8L264IAJnQ9NtwjB/D8L8Pfj94AmmDpHSHI5INfdrvrUlJSXpMydz2bVrFz169Gj2Nr7ZlM2LP+zh8MkSOvt78OiYeIuc7G0Lpk6dyhVXXME111xj61DO+XMVLVNZqflqUzYzv99NblEZ1yV34W+j4wnybt5BvtqDCzaxfn8u658cKVdq2xGl1B9a66Sm1muzLX8wj+qRZC/EaduzT/F/i7azMfMkiV38eW9qEn3C/c9rW0lRgSzafJhDeSVEBHlaOFLR2tp08heNmz9/vq1DEFaUV1TOiz/sYcHvmQR5ufLiNX2Y1D8cpxaM1EmOCgDg94w8Sf4OSJK/EG2Y0VTJJxsyeXn5XgrLjEwbEs2Dl8bh697y+RO6hfrg6+5M6sE8Jg0It0C0wpok+QvRRm04kMfTi3ew60g+Q2KDmHFlL+I6+Fhs+05OiqSoQH7PkIu9HJEkfyHamD9PlfLc97tYtPkwYf4evHlDf8YldGyVk7JJUQGk7D5GXlE5gV6uFt++aD2S/IVoI8qMJt77JYM3UvZhrNQ8cEksd18ci4eroekXn6fkWuP9R/fq2Gr7EZbXZi/yai1ZWVlMmDCBuLg4unbtyoMPPkh5eXmD6x4+fLhZQykvu+wyTp48eV7xSMlmAbBqzzHGvrqG55ftZkhsMD89fBF/HR3fqokfoHeYH64GJ1Klzo/DkeR/DrTWTJw4kauuuop9+/axd+9eCgsLeeqpp+qtazQa6dy5M1988UWT2126dCn+/uc33M5SpGSzY8rMLea291OZOu93FDD/1mTm3pxktdE37i4G+nbx43e50tfhtO3kv/VzeCUBZvibf279vEWbS0lJwd3dnVtvvRUw17R55ZVXeO+99yguLmb+/PlMnjyZK6+8ktGjR5ORkUFCQgJgLvEwZcoU+vTpw7XXXssFF1xA9UVsUVFR5OTk1JSGvv322+nVqxejR4+mpMRci2ju3LkkJyfTt29fJk2a1GSFTCnZ3LaVlJt4efkeLn3lZ9al5/DEuO4se2g4F8eHNv1iC0uKCmRb1ilKyk1NryzsRtvt89/6OXz7AFRUFXI7dcj8GKDPlPPa5I4dO+qVR/b19SUiIqKmjs+6devYunUrgYGBZGRk1Kz35ptvEhAQwNatW9m+fXujFS737dvHp59+yty5c5kyZQpffvklN954IxMnTuT2228H4B//+AfvvvtuTfXNxkjJ5rZHa83SbX/y7+92cvhUKVcldubJy3rQwdfdZjElRwUwZ5Vm86GTDO4aZLM4xLmxSMtfKfWeUuqYUqp+oRrz80op9bpSKk0ptVUp1d8S+z2rFc+cTvzVKkrMy8+T1rrBERO1l48aNYrAwMB66/zyyy9cd911ACQkJNCnT58G9xEdHV1zYBgwYEDNAWT79u0MGzaM3r178/HHH7Njx44m422sZPNNN90ENK9kc3Bw8FlLNs+bN48ZM2awbds2fHwsN4xQ1Lf3aAE3vPMb936yET9PVz6/czCvXtfPpokfYEBEoEzu4oAs1e0zHxh7lufHAXFVtzuAORbab+NOZZ3b8mbo1asXZ9Ybys/P59ChQ3Tt2hWoW9q5tpaWSp46dSqzZs1i27ZtPP3005SWlja5LWuVbA4LC+Omm26Swm2tJL+0gme+3cm419aw43A+z07oxZL7hzIwun4jwxb8PF1kchcHZJHkr7VeDZztsD8B+ECbrQf8lVKdLLHvRvk1csVhY8ubYeTIkRQXF9ckOZPJxCOPPMLUqVNrJk9pzNChQ/n8c/M5h507d7Jt27Zz2ndBQQGdOnWioqKipuRyU6Rks2OrrNR8nnqIS15axby1B7g2uQsr/3YxNw2OsrsJVJKiAth48ASmSvssFCnqs9YJ3zDgUK3HWVXL6lBK3aGUSlVKpR4/frxlexz5f+ByRj1yFw/z8vOklOLrr79m4cKFxMXF0a1bN9zd3fnPf/7T5Gvvuecejh8/Tp8+fXj++efp06cPfn7NnwTj2Wef5YILLmDUqFF07969Wa+pLtk8bty4OiWbTSYTvXv35tprrz3vks2PPvooq1atIjExkX79+vHll1/y4IMPNvv9iLPbcugkE+es5bEvthIR6Mm39w3lP1f3ttsLqZKjAiksM7LrSL6tQxHNpbW2yA2IArY38tx3wNBaj1cAA862vQEDBugz7dy5s96ys9rymdb/7aX1037mn1s+O7fXW5DRaNQlJSVaa63T0tJ0ZGSkLisra7X93XLLLXrhwoWttn1LOufPtQ3LKSjVj3+xRUc9sUQPePZH/UXqIW0yVdo6rCZlnSjWkY8v0fN+2W/rUNo9IFU3I2dba7RPFtCl1uNw4HCr77XPlPMe2WNpxcXFjBgxgoqKCrTWzJkzB1dX+2zFCeszmir5aP1B/vvjXorLTdw2NJoHRsbhY4ECbNYgk7s4Hmsl/8XAfUqpBcAFwCmt9REr7dsu+Pj41DtZ3JqkZLPjWL8/lxmLd7D7zwKGxQXz9JU9iQ11vJFTSVEBrN+f2+ioOGFfLJL8lVKfAhcDwUqpLOBpwAVAa/0WsBS4DEgDioFbz3df8ofVtmg7nUnOGo6cKuE/S3fz7RZzAba3bhzAmF4dHPbvWyZ3cSwWSf5a6+ubeF4D97Z0P+7u7uTm5hIUFOSw/yDiNK01ubm5uLvbdpy6tZUZTbyz5gCzUtKo1JoHR8Zx98VdcXdp3To8rU0md3EsDnWFb3h4OFlZWbR4JJCwG+7u7oSHt5+JQFJ2H+WZb3eSkVvMmF4d+MflPekS2DYSpUzu4lgcKvm7uLgQHS0nk4Tjycgp4pklO0nZfYyuIV58OH0gw+JCbB2WRcnkLo7FoZK/EI6muNzI7JVpzF19ABeD4u+XdWfqhdG4OrfNmooyuYvjkOQvRCvQWrNk6xH+s3QXR06VMrFfGE+M606ojevwtDaZ3MVxSPIXwsJ2/5nPjMU7WL8/j16dfXnj+n4kRdlHHZ7W1ifcD1dn8+QukvztmyR/ISzkVEkFr/y4lw/XH8TH3Zl/X53AdckRdleHpzW5ORvoGy6TuzgCSf5CtFBlpWbhH4d4YdkeThSX85cLInhkVDwB7bTPOykqkLmr91NSbmr1aSTF+ZPkL0QLbD50kqcXbWdL1imSIgP4YMJAenVufsG+tkgmd3EMkvyFOA85hWW8sGw3n6dmEerjxqvXJjIhsbNcfEjdyV0k+dsvSf5CnIMKUyUfrjvIKz/tpbTCxJ3DY7h/ZBzebvKvVE0md3EM8hcrRDOtTc9hxuId7D1ayPBuITx9ZU+6hnjbOiy7lBQVwDebDmOq1O3qhLcjkeQvRBOyT5bwn+928d22I3QJ9ODtmwYwqqfjFmCzhuSoQD5an8muI/kkhLXvcyD2SpK/EI0orTDxzpr9zFqZhtbw8KXduPOiGIcvwGYNtS/2kuRvnyT5C3EGrTUrdh3jmSU7ycwrZlxCR566vAfhAW2jAJs1dJbJXeyeJH9Rj8lkIikpibCwMJYsWWLrcKzqQE4R//x2B6v2HCc21JuPpl/A0LhgW4flkJKiAliXLpO72CtJ/qKe1157jR49epCf334m4y4qMzJrZRrvrjmAq7MT/7i8B7dcGIWLoW0WYLMGmdzFvslftqgjKyuL7777jttuu83WobRYaWkpAwcOpG/fvvTq1Yunn3663jpaaxZtzmbkyz8zZ1U6V/btTMrfLuK2YTGS+FtoYFW/v5R6sE/S8hd1PPTQQ7zwwgsUFBTYOpQWc3NzIyUlBW9vbyoqKhg6dCjjxo1j0KBBAOw6ks/Ti3ew4UAevcP8mH1DfwZEBtg46rYjLtRbJnexY5L8RY0lS5YQGhrKgAEDWLVqla3DaTGlFN7e5nH4FRUVVFRUoJTiVHEF//1xDx+uP4ifhwvPTezNlKQuMh7dwqond9lwQFr+9kiSv6jx66+/snjxYpYuXUppaSn5+fnceOONfPTRR7YO7byZTCYGDBhAWload99zD/tVJ+5+eRUni8u5cVAkfx3VDX/P9lmAzRqqJ3fJLSwjyNvN1uGIWqRTU9R47rnnyMrKIiMjgwULFnDJJZc4dOIHMBgMbN68me/WbWfe1z/xyP++JTbEmyX3D+OZCQmS+FtZdb//H1Lqwe5Iy18A8P7aDNbsy8HdxQkPFwN/7jlA2rFCXl6+B3cXA27OTni4GnB3NuDuYsDdxemMn1U359OP7aEb5VhBKc9/v4cvN2bhHtGbUX5/8t6dg2TooZX0lsld7JYkfwHAos3ZbD+cT7i/ByUVJkorw1Bjnqi5uvV8uBqccKt9kHA21BxA3GofNKoPLFX33VxOH2A8at03v+7M5aefc6p1sKkwVTJr6R/M/SWTCmcPpg8K4/s16Uy6ZLIkfiuSyV3slyR/AUBsqDeZeSWk/O3iOsu11pSbKimtqKSswmQ+MFRUUlphMt+Mte7Xea6SUqOJknITZcYzXlNRSWGZkeMFZZQZ6y4vNZpadLCp/iZirNT8eWAPZSveIMTLhc8WwZQpU7jiiita/ssS50Qmd7FPkvwFAHGhPnyemsWJovI6M1AppXBzNuDmbAAPl1aPQ2tNmbGSsqoDQe0DSkmtg0RZg8+Z75cZTZQbNeMm9WHku/dKS9/GBkYFMmdVukzuYmck+QsAYjuYh0TuO1bIwGjbTTaulKrpzvGj9Q82ovX1jwiQyV3skIz2EYD5ghyAfccc/+IuYV9kchf7JMlfABDm74GXq4F9RwttHYpog5KiAth48ASmyvM8oSMsTpK/AMzdLbGh3qQdk+QvLC85KpDCMiO7jrSfYoH2TpK/qBEb6sPeo9LtIyyv9uQuwj5I8hc14jp4c6ygjFPFFbYORbQxtSd3EfZBkr+o0a1qxE/acWn9C8tLigrg9wN56PO9kENYlCR/USMu1AdATvqKVpEcFcixgjIO5ZXYOhSBhZK/UmqsUmqPUipNKfVEA89PVUodV0ptrro5/kwhbVCYvwfuLk7sk5O+ohUky+QudqXFyV8pZQBmA+OAnsD1SqmeDaz6mdY6ser2Tkv3KyzPyck84kdO+orWUHtyF2F7lmj5DwTStNb7tdblwAJgggW2K2ygW6iPDPcUrUImd7Evlkj+YcChWo+zqpadaZJSaqtS6gulVBcL7Fe0gtgO3hw5VUpBqYz4EZaXHBVI+vEicgvLbB1Ku2eJ5N9Q1awzT+d/C0RprfsAPwHvN7ghpe5QSqUqpVKPHz9ugdDEuao+6Sutf9EakqPMcyRXT+4ybdo0QkNDSUhIsGVY7ZIlkn8WULslHw4crr2C1jpXa119qJ8LDGhoQ1rrt7XWSVrrpJCQEAuEJs5VTY0fGfEjWkHtyV0Apk6dyrJly2wcVftkieT/OxCnlIpWSrkC1wGLa6+glOpU6+F4YJcF9itaQZdAT9ycnaTAm2gV1ZO7VPf7Dx8+nMBA21WRbc9anPy11kbgPuAHzEn9c631DqXUM0qp8VWrPaCU2qGU2gI8AExt6X5F6zA4KbqGeMtwT9FqkqMC2Z59ipJyk61DadcsMs5fa71Ua91Na91Va/3vqmX/p7VeXHX/Sa11L611X631CK31bkvsV7SOuA7e0u0jWk1yVCDGSs3mQydtHUq7Jlf4inriQr3JPllCUZnR1qGINqj25C7CdiT5i3piZcSPaEUyuYt9kOQv6ulWa0pHIVpDclQgf2TkceXEyQwePJg9e/YQHh7Ou+++a+vQ2g1J/qKeiEBPXA0y4ke0nilJXXA2OLGv1zSe+2ItZWXlZGVlMX36dFuH1m5I8hf1OBuciAnxkpO+otX0Dvdj+cPDGRQTxIxvd3Lju7+RdaLY1mG1K5L8RYNiQ72l5S9aVQdfd+ZNTea5ib3ZcugkY19dw+eph6Tev5VI8hcNigv1IetECcXlMuJHtB6lFNcPjGDZQ8Pp1dmXx77Yyu0fpHKsoNTWobV5kvxFg7p18EZr2H+8yNahiHagS6Ann94+iP93RU/W7Mth9CurWbL1cNMvFOdNkr9oUFzNiB/p+hHW4eSkmD40mu8eGEZkoCf3fbKJ+z/dxImicluH1iZJ8hcNigzywtlJsVdO+goriw315su7L+SRUd34ftsRRr+6mpTdR20dVpsjyV80yMXgRHSwjPgRtuFscOL+kXEsum8IQV6uTJufyuNfbJV5JixIkr9oVLcOPqRJt4+woV6d/Vh03xDuvrgrC/84xNhX17A2PcfWYbUJkvxFo2JDvcnMK6a0QqovCttxczbw+NjuLLzrQlydnfjL3N/457c7pCpoC0nyF42K6+BNpYb049L1I2xvQGQA3z0wlKkXRjHv1wwuf30NmzKlPtD5kuQvGtXQlI5RUVH07t2bxMREkpKSbBWaaKc8XZ2ZMb4XH992AaUVJibNWcuLP+ym3Fhp69AcjrOtAxD2KzrYC4OTqnfSd+XKlQQHB9soKiFgSGwwyx4ezrPf7mT2ynRW7DrGf6ck0rOzr61DcxjS8heNcnV2IirIU8b6C7vk6+7Ci5P78s7NSeQUljNh9i/MXpmG0STfAppDkr84q7hQnzotf6UUo0ePZsCAAbz99ts2jEwIs0t7dmD5w8MZ3bMjL/6wh2veWifnqZpBkr84q7gO3mTkFlFmNI+s+PXXX9m4cSPff/89s2fPZvXq1TaOUAgI9HJl9g39eeP6fmTkFnH562uY9+sBKiulSFxjJPmLs4oNNY/4OZBjrvHTuXNnAEJDQ7n66qvZsGGDLcMToo4r+3Zm+UPDGRwTxD+/3ckN70ip6MZI8hdn1a2DecTPvqOFFBUVUVBg7v8vKipi+fLlJCQk2DI8IeoJ9XXnvanJPD+pN1uzzKWiP/s9U0pFn0FG+4izig72wkmZh3v28inl6quvBsBoNPKXv/yFsWPH2jhCIepTSnFtcgQXdg3m0S+28PiX2/hhx1FmTuxNqK+7rcOzC5L8xVm5uxjwdnPmVEkFMTHd2LJli61DEqLZugR68sltg3h/XQYzv9/N6FdX8+yEBK7s29nWodmcdPuIJnm6OsukLsJhOTkpbh0SzdIHhxEV5MX9n27i3k82tvtS0ZL8RZM8XQ0USx0V4eC6hnjzxV2DeXRMPMt3/MnoV1ezYlf7LRUtyV80ycPVIEW0RJvgbHDi3hGxLLp3KEFerkx/P5XHvtjSLktFS/IXTZKWv2hrenb2ZdF9Q7h3RFe++CPLXCo6rX2VipbkL5rk4epMsZR1Fm2Mm7OBR8d054u7L8TN2Ym/vPMbMxa3n1LRkvxFkzxdDJTICV/RRvWPCOC7B4Yx9cIo5q/N4LLX17CxHZSKluQvmuTpaqCorH20hkT75OFqYMb4Xnxy+wWUGyu5Zs5aXli2u6asSVskyV80ydPNQIl0+4h24MKuwSx7aBiTB3ThzVXpTJj1KzsP59s6rFYhyV80Scb5i/bEx92F56/pw7u3JJFbZC4VPStlX5srFS3JXzTJw8VAaUWlVEgU7crIHh1Y/tBwxvTqyEvL9zLprXV1ZrVzdJL8RZM8XQ0A0vUj2p0AL1dm/cVcKvpgVanod39pG6WiLZL8lVJjlVJ7lFJpSqknGnjeTSn1WdXzvymloiyxX2Ed1clfxvqL9urKvp1Z/vBwhsYG8+ySnVw/dz2H8hy7VHSLk79SygDMBsYBPYHrlVI9z1htOnBCax0LvAI839L9CuvxcDXX/2sv45+FaEiojzvv3JLEC5P6sONwPmNfXc2CDY5bKtoSLf+BQJrWer/WuhxYAEw4Y50JwPtV978ARiqllAX2LaygpuVfISd9RfumlGJKcheWPTSMPuH+PPHVNqbN/52j+aW2Du2cWSL5hwGHaj3OqlrW4DpaayNwCgg6c0NKqTuUUqlKqdTjx49bIDRhCR5VyV/G+gthFh7gyce3XcCMK3uybn8uo19ZzeIthx3qW4Alkn9DLfgzfwPNWQet9dta6yStdVJISIgFQhOW4HVGt8/Jkye55ppr6N69Oz169GDdunW2DE8Im3ByUkwdEs3SB4YRE+LFA59u4r5PNpHnIKWiLZH8s4AutR6HA4cbW0cp5Qz4AXkW2LewgtMnfM3dPg8++CBjx45l9+7dbNmyhR49etgyPCFsKibEm4V3DuaxsfEs3/kno19ZzU877b+qmpdvAAAd2klEQVRUtCWS/+9AnFIqWinlClwHLD5jncXALVX3rwFStCN9P2rnPGoN9czPz2f16tVMnz4dAFdXV/z9/W0ZnhA252xw4p6LY1l831BCfNy47YNU/rZwC/l2XCq6xcm/qg//PuAHYBfwudZ6h1LqGaXU+KrV3gWClFJpwF+BesNBhf2qPdRz//79hISEcOutt9KvXz9uu+02ioqKbByhEPahRydfFt07hPtGxPLVxizGvrKaX+20VLRFxvlrrZdqrbtprbtqrf9dtez/tNaLq+6Xaq0na61jtdYDtdb7LbFfYR2eLuY+/+JyE0ajkY0bN3L33XezadMmvLy8mDlzpm0D3Po5vJIAM/zNP7d+btt4RLvm6uzE38bE8+XdF+LuauCGd37j6UXb7a5EilzhK5pU0+1TbiQ8PJzw8HAuuOACAK655ho2btxou+C2fg7fPgCnDgHa/PPbB+QAIGyuX0QASx8YxrQh0by/7iCXvbaGPw7az6lOSf6iSa7OTjg7KYrLTXTs2JEuXbqwZ88eAFasWEHPnmde02dFK56BipK6yypKzMuFsDF3FwP/d2VPPr19EBUmzeS31jHz++aXil62bBnx8fHExsZa/Bu2s0W3Jtqc9OOFvLkyHWOlxlRVz+SNN97ghhtuoLy8nJiYGObNm2e7AE9lndtyIWxgcNcgfnh4OP/+bidv/ZzOyt3HeHlKXxLC/Bp9jclk4t577+XHH38kPDyc5ORkxo8fb7HGliR/0aDdf+YzKyWN77Ydwc3ZielDo7nroq4AJCYmkpqaauMIq/iFV3X5NLBcCDvi7ebMcxP7MLpnRx7/citXzf6VB0bGcc/FXXE21O+E2bBhA7GxscTExABw3XXXsWjRIkn+onVszTrJGylp/LjzKN5uztx9UVemDY0m2NvN1qE1bOT/mfv4a3f9uHiYlwthh0Z0D2X5w8P5v0U7+O+Pe1mx6ygvT+lLbKhPnfWys7Pp0uX0JVTh4eH89ttvFotDkr8AIDUjjzdS0vh573F83Z156NI4pl4Yhb+nq61DO7s+U8w/Vzxj7urxCzcn/urlQtghf09XXr++H2N6deQf32zj8td/4dEx8UwbEo2Tk7kgQkOXQlmyJJok/3ZMa8269FxeT9nH+v15BHm58vjY7tw4KAIfdxdbh9d8faZIshcO6fI+nUiODuDvX23jX9/tYvnOo7w8uS9dAj0JDw/n0KHTXZpZWVl07tzZYvtW9nqhbVJSkrabfuU2RmvNqj3HeSNlHxszTxLq48adF3Xl+oFd8HSV9oAQ1lZSbuKl5Xt495cDAMSFerP0/guJj49nxYoVhIWFkZyczCeffEKvXr3Oui2l1B9a66Sm9in/6e1IZaVm+c6jzFq5j+3Z+YT5e/DsVQlMHhCOu4vB1uEJ0aZVVmoOnyph//Ei9h8vZH9OUc39w6fqloTed6wQZ2dnZs2axZgxYzCZTEybNq3JxH8upOXfDpgqNUu2Hmb2yjT2Hi0kKsiTe0bEcnW/MFwaGGUghDh/+aUVpxP88SL255h/Hsgposx4ehJ4bzdnYkK8iAn2IibEm5gQL6KDvYgK8qJS6/PuepWWv6DCVMk3m7J5c1U6B3KKiAv15rXrErm8d6cGh5YJIZqnwlRJZl4xB2ol9+pEn1N4uqSzwUnRJcCDmBBvhsYG1yT5mGAvQnzcLHoC91xJ8m+DyowmFqZmMWdVOtknS+jV2Ze3buzP6J4da0YSCCHOTmtNTmE5B3Jqd9OYE31mXjHGWpO4B3q5EhPsxSXdQ80Jvqo1HxHoiauzfTa0JPm3ISXlJj7ZkMnbq9M5ml9Gvwh//nVVAhfHh9i0hdFefLMpmxd/2MPhkyV09vfg0THxXNXvzEnthL0prTBxIKfodJI/XkR61f2C0tPF2FwNTkQFe9Ktgw9jEzrWacXb/ZDoBkjybwMKy4x8uO4g76zZT25ROYNiAvnvlEQu7BokSd9KvtmUzZNfbaOkwlyzJftkCU9+tQ1ADgB2oLJScyS/lP3HC6uSfBHpVYn+8KkSap/67OjrTkyIFxMSOxMTbE7wXUO86ezvgaENfXOW5O/AThVXMG/tAeb9msGpkgqGdwvh/ktiSY4KtHVo7c6LP+ypSfzVSipMvPjDHkn+VlRQfbI1p5ADNS34Ig7kFFJacfpkq5ergegQLwZEBjA5JLymqyY62Asvt/aRFtvHu2xjcgvLePeXA3yw7iCFZUZG9ezAfSNi6dtFZtSylcMnS85puTh/RlMlh06U1BpNc7pP/nhBWc16Tso80XpMiBeDY4LMXTRVrfhQG59stQeS/B3I0fxS3l69n09+y6TUaOKy3p24b0QsPTr52jq0dq+zvwfZDST6zv4eNojG8WmtySsqr3OStfp+Zl4xFabT/TQBni5EB3txUbeQqj54b7qGeBER5Imbs1y/0hhJ/g4g60Qx//t5P5+lHsJUqZmQ2Jl7Lo4lNtTb1qGJKo+Oia/T5w/g4WLg0THxNozK/pVWmDiYW1zTck+v1Sd/quT0/LeuBicigzzpGuLNqJ4dq1rw5kQf4OV4J1vtgSR/O5aRU8Sbq9L4amM2SsE1A8K566KuRAZ52To0cYbqfn17GO0zbdo0lixZQmhoKNu3bwdg4cKFzJgxg127drFhwwaSkpq8BshitNb8mV9ac+FTelUr/kBOIVkn6p5s7eDrRnSwF5f36URMsLmLJibEizB/D7k2xcLkCl87tO9oAbNXprF4y2FcDE5cPzCCO4bHSBeCaJbVq1fj7e3NzTffXJP8d+3ahZOTE3feeScvvfRSqyT/wjJjzUVP6bWucD2QU1TnG5Gnq4HoqnHw0cGnW/DRIV54t5OTra1JrvB1QNuzTzF7ZRrLdvyJh4uB24bFcNuwaEJ93G0dmnAgw4cPJyMjo86yHj16WGTbRlMl2SdLTg+VrOqHP5BTxNH80ydblYLwAA9igr0ZGB1oTvBVrfiOvu7t/mSrPZDkbwc2ZZ5gVkoaK3Yfw8fNmftGxHLrkGgC23Ff5iuvvMI777yDUorevXszb9483N3lIGgtJ4rKa7XgT4+mycwtptx0esikn4cLMSFeDI0NqVOnJjLIU4oF2jlJ/ja0fn8us1LS+CUtB39PFx4Z1Y2bL4zCz8OBaum3guzsbF5//XV27tyJh4cHU6ZMYcGCBUydOtXWobUpZUYTmbnFVX3whXWS/Mni0ydbXQyKiEBPYkK8Gdkj9HQhsmAvAr1cpRXvoCT5W5nWmjX7cpiVksaGjDyCvd34+2XdueGCyHZzcUlzGI1GSkpKcHFxobi42KKTWLQnWmuO5pfVJPeDucXMWLyD/JQCsk4UU6s8DSE+bsQEezEuoRNdqypMxoR40yVATra2RZJtrERrzU+7jjErZR9bsk7Ryc+dGVf25LqBEfL1+AxhYWH87W9/IyIiAg8PD0aPHs3o0aNtHZbdKzdW8tOuo+w9WsDmnfs4mFdMwtM/UFR++mTrsfxSOhWV06+7H1cldia61slWX0eavU20mCT/Vmaq1Czb/idvpOxj958FdAn04LmJvZnYP0wuQGnEiRMnWLRoEQcOHMDf35/Jkyfz0UcfceONN9o6NLu2fOef3PfJJnIWv0B51naMxac49OZUJt/xMDFhHZj1n6cw5eSQ/tE/8NmeyOwffrB1yMKGJPm3EqOpksVbzBOopB8vIibEi5cn92V8YmeZQKUJP/30E9HR0YSEhAAwceJE1q5dK8m/Cc5O5r+rX35YRGIDpT6euneqlSMS9kySv4WVGyv5amMWb65KJzOvmO4dfZj1l36MS+jUpioCtqaIiAjWr19PcXExHh4erFixwqoXJTXltddeY+7cuWituf3223nooYdsHRIAvh7mf+ficmMTawohyd9iSitMfPb7If73czqHT5XSJ9yPf1w+gEt7dJAJVBpRYarkyMlSDuaZJ8fIzCsmt7Cc8IAA+gwdQ0KfRDzcXOnfvx933HGHrcMFYPv27cydO5cNGzbg6urK2LFjufzyy4mLi7N1aDV99vklkvxF0yT5t1BRmZFPfsvk7TX7OV5QRlJkAM9N6sPwuGAZAoe57HR1YjffTif6wydLMdUabuJqcCLAy4VjBWXogJFwzUiKFez38+D2j7fQtaoio/lmm2nwdu3axaBBg/D09ATgoosu4uuvv+axxx6zahwNqR4inF9a0cSaQkjyP2/5pRV8sDaDd385wIniCobEBvH6df0YFBPYrpK+0VTJkVOlHMw9neAPVf08mFtEfmndVmiQlysRQZ70jwjgqkRPugR6EhHoSWSQJx183HFyUpRWmMjILSL9mPkq0upJNz7LyKO41sgVHzdnYkK9ax0UzD8jg7xabeq8hIQEnnrqKXJzc/Hw8GDp0qV20yV1uuUvyV80TZL/OTpRVM68Xw8wb20GBaVGLukeyr0jYhkQGWDr0FrNqZKKmoRuTuqnE3z2yZI6rXcXg6JLgDmpJ3bxJzLodILvEujZrNot7i4Gunf0pXvHuqWqqwuEnXlQWJeey1cbs2vWMziZL0qqLilQ+xtDSytA9ujRg8cff5xRo0bh7e1N3759cXa2j38jH3dnlKLeAVeIhtjHX60DOFZQyrtrDvDh+oMUl5sY26sj910SS0KYn61Da7Hq1nvd7hlzgj+YW1yntC6YJ6uOqEru4/t2rknskUGedPB1b7UT20opOvl50MnPg6FxwXWeqy4qZj4gmMsSpB8vZPW+HMqNp8sRVE+03TXEm66hpw8K4edwIdP06dOZPn06AH//+98JDw+33JtsAScnhbebs7T8RbO0KPkrpQKBz4AoIAOYorU+0cB6JmBb1cNMrfX4luzXmo6cKuF/P+/n0w2ZVJgqubKvuZZ+fEcfW4d2TvJLK8is1WI/WKt7JvtECcYzWu/hVa33PuF+RAZ61Wq9e+BjhxcDebs50zvcj97hdQ/GpkpN9omSmm8K1QeFFbuP8llqec16LgZFVFD9g0JMiFe993vs2DFCQ0PJzMzkq6++Yt26dVZ5j83h6+4iff6iWVra8n8CWKG1nqmUeqLq8eMNrFeitU5s4b6sKjO3mDk/p/PFH4fQGq7uF8bdF3clJsQ+J1AxVWqOnCohM7eB1ntecZ1aLWBuAZuTuz9X9OlERKAnEYHm2Y86tmLr3doMToqIIE8igjwZ0T20znMni8trSg9XHxT2Hivgp11H6xwMQ33c6BrizWV9OnHToEgmTZpEbm4uLi4uzJ49m4AA++ny8/VwkZa/aJaWJv8JwMVV998HVtFw8ncY6ccLeXNlOt9szsagFNcmd+HO4V3pEuhp69AoKK2o0x1TO8FnndF6d3ZShAd40CXQs1ZyN7fmuwR6yqX8gL+nKwMiXeudr6kwVZKZV0z6sdMHhe3Zp/h/32zHZKpkzZo1Noq4ab7uzjLUUzRLS5N/B631EQCt9RGlVGgj67krpVIBIzBTa/1NC/fbpD179nDttdfWPN6/fz/PPPNMoxfk7P4zn1kpaXy37Qhuzk7cMjiKO4bH0NHPemWEa1rvtbpkMvNKyMw1D488cUbrPcDThYhATxLC/Lisd1WCDzIn+U5+Hm2m9W5tLganmm6fakZTJXd/vJF/LtlJsI8bV/Sxz0Jzvh4uHMortnUYwgE0mfyVUj8BHRt46qlz2E+E1vqwUioGSFFKbdNapzewrzuAO8B8lWdLxMfHs3nzZgBMJhNhYWFcffXV9dbbmnWSN1LS+HHnUbxcDdw5vCu3DYsm2NutRftvTGGZsVbXTFFNgje33utOTO3spAgL8CAi0PN0cq9quUcESevdmpwNTrxxfT9uevc3Hv5sMwGergyJDW76hVbm6y7dPqJ5mkz+WutLG3tOKXVUKdWpqtXfCTjWyDYOV/3cr5RaBfQD6iV/rfXbwNtgnsaxWe+gGVasWEHXrl2JjIysWZaakccbKWn8vPc4vu7OPDgyjluHROHv2bKhgKZK83DE2idXa59gzSsqr7O+f1XrvWdnX8YmdDSPea9K8J383KWUrh1xdzHwzs3JTPnfOu788A8W3DHI7kZ7+Xm4yFBP0Swt7fZZDNwCzKz6uejMFZRSAUCx1rpMKRUMDAFeaOF+z8mCBQu4/vrr0VqzLj2X11P2sX5/HoFerjw6Jp6bB0ee0wiWwjLj6cTeQN977ZmODE6KMH8PIoM8a5J77RZ8e5+4xdH4ebrw/rSBTJqzlqnzfufLuwcTGeRl67Bq+Ho4U1hmxGiqlIaDOKuWJv+ZwOdKqelAJjAZQCmVBNyltb4N6AH8TylVCThh7vPf2cL9Nlt5eTmLFy/m8ml/ZdKctWzMPEmojxv/uLwHf7kgAk/X+r+CyurWe70rVs33c89ovft5mFvvPTr5MrpXRyKDTid4ab23PR393Hl/2kCueWstN7+3gS/uupAQn9bpJjxX1V2BhWXGFn+LFW1bi5K/1joXGNnA8lTgtqr7a4HeLdnP+aqs1Pz7rY9RwdE8suQgYf4ePDuhF5OTumCq1PVa7tW3rLz6rffO/u5EBnoxulfd1ntEoCd+ntJ6b29iQ72ZNzWZv8z9jVvnb2DBHYObdfVya/Ot+iZ5qqRCkr84K9v/tbYCU6VmyVZzLf1f356HR8wQOvm5k9jFn682ZfPain3kFNZtvfu4OxMZ5En3jj6M7lk3wXfyd5ca/KKefhEBvHljf257P5U7P0zlvanJNp+gx9fd/C8twz1FU9pc8jdVaibM/oXt2flUVpRSmrGZoLH3cTS/lK3ZJ4kI9GRUzw6nC4oFeknrXZy3EfGhvDCpD48s3MIjn2/h9ev62bSEt69U9hTN1OaSv8FJcUl8KENig80t97vSiQj0pLO/h7TeRauYNCCc44VlzPx+N8Hebjx9ZU+bVXatKesswz1FE9pc8gf46+h4W4cg2pk7h8dwvKCMd385QKivG/dcHGuTOKTlL5qrTSZ/IaxNKcVTl/Ugp7CMF5btIdjbjSlJXaweh/T5i+aS5C+EhTg5KV68pi95ReU8+dU2grxcGdmjg1Vj8HJ1xklJy180TTrBhbAgV2cn5tw4gF6dfbn3k438cTDPqvt3clL4SIkH0QyS/IWwMG83Z96bmkwnPw+mzU9l39ECq+7f18O53gQ8QpxJkr8QrSDY240Ppg3E1dmJm9/bwOGTJVbbt3lCF+nzF2cnyV+IVtIl0JP5tyZTWGrklvc2cLK4vOkXWYCfTOgimkGSvxCtqFdnP96+OYmDucVMfz+VknJTq+9TpnIUzSHJX4hWNrhrEK9el8jGzBPc/+lGjLXqRrUGXw+ZzUs0TZK/EFZwWe9OPDMhgZ92HePvX29Da4tNV1GPtPxFc8g4fyGs5KZBkRzPL+X1lDRCfdz525jWuRLd18OF4nITFaZKKWkiGiXJXwgrenhUN44XljFrZRrB3q5MHRJt8X2cvsq3gqBWmo5UOD5J/kJYkVKKZyckkFtY3mqTwZ+u72OU5C8aJd8JhbAyZ4MTr1/fj+TIQB7+bDO/puVYdPtS2VM0hyR/IWzA3cXA3JuTiAn25s4P/2B79imLbVsqe4rmkOQvhI1UTwbv5+HC1HkbOJhbZJHtVs/jK8M9xdlI8hfChqongzdVam5+bwPHC8pavE1fj6oTvtLyF2chyV8IG4sN9ea9qckcyy/j1vkbKCxrWYv9dMtfkr9onCR/IexA9WTwu44UcOeHqZQZz78MhKerAYOTksqe4qwk+QthJ6ong/81LZdHPt9CZeX5XQWslDIXd5NuH3EWMs5fCDsyaUA4OYVlPNfCyeB93aW+jzg7Sf5C2Jk7hsdwrIWTwftKy180QZK/EHbGEpPB+8pUjqIJkvyFsEMtnQze18OZP/NLWzFC4ejkhK8QdsrV2Ym3znMyeGn5i6ZI8hfCjnmd52Twvh4uMtRTnJUkfyHs3PlMBu/n4UKZsZLSitafNlI4Jkn+QjiAc50Mvrqmf0GpDPcUDZPkL4SDOJfJ4KWyp2iKJH8hHEhzJ4OX+j6iKZL8hXAwzZkM/nRlT+n2EQ1rUfJXSk1WSu1QSlUqpZLOst5YpdQepVSaUuqJluxTCGGeDP6BkXF8nprFS8v31HteWv6iKS1t+W8HJgKrG1tBKWUAZgPjgJ7A9Uqpni3crxDt3sOXxnH9wAhmr0xn/q8H6jxX3ed/qqSCadOmERoaSkJCQs3zeXl5jBo1iri4OEaNGsWJEyesGruwvRYlf631Lq11/WZHXQOBNK31fq11ObAAmNCS/QohqieD78Xonh3455KdfLvlcM1zfrVO+E6dOpVly5bVee3MmTMZOXIk+/btY+TIkcycOdOqsQvbs0affxhwqNbjrKpl9Sil7lBKpSqlUo8fP26F0IRwbLUng//r56cng3dzdsLV4ER+iZHhw4cTGBhY53WLFi3illtuAeCWW27hm2++sXrswraaTP5KqZ+UUtsbuDW39d5QPdoGC5Vrrd/WWidprZNCQkKauXkh2jd3FwNzb6k7GbxSCl8P50aHeh49epROnToB0KlTJ44dO2bNkIUdaDL5a60v1VonNHBb1Mx9ZAG1SxKGA4cbWVcIcR78POpPBi/1fcTZWKPb53cgTikVrZRyBa4DFlthv0K0K2dOBm/SutGhnh06dODIkSMAHDlyhNDQUGuGKuxAS4d6Xq2UygIGA98ppX6oWt5ZKbUUQGttBO4DfgB2AZ9rrXe0LGwhRENqTwZ/MLe40Zb/+PHjef/99wF4//33mTBBxmC0N6qhC0TsQVJSkk5NTbV1GEI4pJV7jnHb+6lEBnnSIfV/rFq1ipycHDp06MA///lPrrrqKqZMmUJmZiYREREsXLiw3klh4ZiUUn9orRu97qpmPUn+QrRNP+48yp/5pdw0KNLWoQgram7yl5m8hGijRvVs/sxfov2R2j5CCNEOSfIXQoh2SJK/EEK0Q5L8hRCiHZLkL4QQ7ZAkfyGEaIck+QshRDskyV8IIdohu73CVyl1HDho6zjOQTCQY+sgrEjeb9sm79dxRWqtm6yJb7fJ39EopVKbc0l1WyHvt22T99v2SbePEEK0Q5L8hRCiHZLkbzlv2zoAK5P327bJ+23jpM9fCCHaIWn5CyFEOyTJ/zwppSYrpXYopSqVUo2OElBKjVVK7VFKpSmlnrBmjJaklApUSv2olNpX9TOgkfVMSqnNVTeHmqu5qc9KKeWmlPqs6vnflFJR1o/ScprxfqcqpY7X+jxvs0WclqKUek8pdUwptb2R55VS6vWq38dWpVR/a8doTZL8z992YCKwurEVlFIGYDYwDugJXK+U6mmd8CzuCWCF1joOWFH1uCElWuvEqtt464XXMs38rKYDJ7TWscArwPPWjdJyzuFv87Nan+c7Vg3S8uYDY8/y/Dggrup2BzDHCjHZjCT/86S13qW13tPEagOBNK31fq11ObAAcNSZsicA71fdfx+4yoaxtIbmfFa1fwdfACOVUsqKMVpSW/rbbBat9Wog7yyrTAA+0GbrAX+lVCfrRGd9kvxbVxhwqNbjrKpljqiD1voIQNXP0EbWc1dKpSql1iulHOkA0ZzPqmYdrbUROAUEWSU6y2vu3+akqi6QL5RSXawTms20pf/XJskcvmehlPoJ6NjAU09prRc1ZxMNLLPb4VVne7/nsJkIrfVhpVQMkKKU2qa1TrdMhK2qOZ+VQ32eTWjOe/kW+FRrXaaUugvzt55LWj0y22lLn2+TJPmfhdb60hZuIguo3VoKBw63cJut5mzvVyl1VCnVSWt9pOqr8LFGtnG46ud+pdQqoB/gCMm/OZ9V9TpZSilnwI+zdyPYsybfr9Y6t9bDuTjwOY5mcqj/15aSbp/W9TsQp5SKVkq5AtcBDjUCppbFwC1V928B6n3zUUoFKKXcqu4HA0OAnVaLsGWa81nV/h1cA6Rox71Qpsn3e0Z/93hglxXjs4XFwM1Vo34GAaequzrbJK213M7jBlyNuaVQBhwFfqha3hlYWmu9y4C9mFu/T9k67ha83yDMo3z2Vf0MrFqeBLxTdf9CYBuwperndFvHfY7vsd5nBTwDjK+67w4sBNKADUCMrWNu5ff7HLCj6vNcCXS3dcwtfL+fAkeAiqr/3enAXcBdVc8rzCOg0qv+fpNsHXNr3uQKXyGEaIek20cIIdohSf5CCNEOSfIXQoh2SJK/EEK0Q5L8hRCiHZLkL4QQ7ZAkfyGEaIck+QshRDv0/wH1QtMMlr8dqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contour1=np.delete(polygons_reshaped[15],24)\n",
    "contour1=contour1.reshape(12,2)\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(contour1,.4\n",
    "                                                                              )\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "my_conv_net.eval()\n",
    "predictions=my_conv_net(x_variable).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[15]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(1,2)\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "\n",
    "plot_contour(contour1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09719837, -0.23911646]], dtype=float32)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_inserted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor=torch.FloatTensor([2.2,1.2])\n",
    "a_tensor=torch.FloatTensor([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_variable=Variable(d_tensor)\n",
    "\n",
    "a_variable=Variable(a_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_variable.requires_grad=False\n",
    "a_variable.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4422\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun=torch.sqrt((a_variable[0]-d_variable[0]).pow(2)+(a_variable[1]-d_variable[1]).pow(2))\n",
    "fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.8321\n",
       " 0.5547\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.function import Function\n",
    "from math import pow\n",
    "\n",
    "class myLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target):\n",
    "        self.save_for_backward(output,target) \n",
    "\n",
    "                        \n",
    "       # output=output.view(int(output.size()[0]/2),2)\n",
    "\n",
    "        #target=target.view(int(target.size()[0]/2),2)\n",
    "        distance=torch.nn.PairwiseDistance()\n",
    "        result=distance(output,target)\n",
    "\n",
    "        result=torch.FloatTensor(result)\n",
    "        #self.save_for_backward(result)\n",
    "\n",
    "\n",
    "        return  result \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self,grad_output1):\n",
    "        input1,target=self.saved_variables\n",
    "        \n",
    "        print(input1)\n",
    "        #distance=torch.nn.PairwiseDistance()(input1.view(int(input1.size()[0]/2),2),target.view(int(target.size()[0]/2),2))\n",
    "        distance=torch.nn.PairwiseDistance()(input1,target)\n",
    "\n",
    "        grad_output1=(input1-target)/distance\n",
    "\n",
    "        \n",
    "        return grad_output1,None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2042\n",
       " 0.4508\n",
       " 0.6575\n",
       "   ⋮    \n",
       " 0.3220\n",
       " 0.1622\n",
       " 1.0774\n",
       "[torch.FloatTensor of size 9721x1]"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myloss=myLossfunction().apply(out,y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "myloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.3953e-01 -1.6375e-01\n",
      "-1.9727e-01 -3.8089e-01\n",
      "-4.1456e-01 -9.3787e-02\n",
      "           ⋮            \n",
      "-7.2860e-02 -6.4023e-02\n",
      "-1.8527e-01 -2.7595e-01\n",
      "-2.6488e-01 -8.1565e-01\n",
      "[torch.FloatTensor of size 9721x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myloss.backward(torch.Tensor([1, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.8320  0.5547\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "backward() missing 2 required positional arguments: 'self' and 'grad_output1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-634-fd86ffc8917c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyLossfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: backward() missing 2 required positional arguments: 'self' and 'grad_output1'"
     ]
    }
   ],
   "source": [
    "myLossfunction.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       "[torch.FloatTensor of size 2]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush all variables in GPU\n",
    "del my_conv_net,my_net,x_variable_test,x_variable,y_variable,y_variable_test,loss_func\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2dust=torch.nn.PairwiseDistance(p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.4422\n",
       " 2.2472\n",
       "[torch.FloatTensor of size 2x1]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2dust(a_variable.resize(2,2),d_variable.resize(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.09185446,  0.06111541]],\n",
       "\n",
       "       [[ 0.09185446,  0.06111541]],\n",
       "\n",
       "       [[ 0.09185446,  0.06111541]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.29833109, -0.19828089]],\n",
       "\n",
       "       [[ 0.29833109, -0.19828089]],\n",
       "\n",
       "       [[ 0.29833109, -0.19828089]]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0   ,.,.) = \n",
       "  5.4394e-01 -1.2854e-01  1.2081e+00  ...   8.7166e-01 -3.9516e-01  4.0000e-01\n",
       "\n",
       "(  1   ,.,.) = \n",
       "  8.0653e-01 -7.0666e-02  7.9998e-01  ...   1.0453e+00 -4.1684e-01  4.0000e-01\n",
       "\n",
       "(  2   ,.,.) = \n",
       "  5.0649e-01  6.8832e-02  9.6722e-01  ...   8.5345e-01 -5.3134e-01  3.0000e-01\n",
       "  ...  \n",
       "\n",
       "(174975,.,.) = \n",
       "  1.1163e+00 -3.1378e-02  7.0993e-01  ...   7.3510e-01 -6.3782e-01  5.0000e-01\n",
       "\n",
       "(174976,.,.) = \n",
       "  7.6960e-01 -4.1094e-02  7.8531e-01  ...   9.1110e-01 -3.1011e-01  9.0000e-01\n",
       "\n",
       "(174977,.,.) = \n",
       "  1.2754e+00 -3.7225e-01  7.9667e-01  ...   5.6758e-01 -2.6036e-01  5.0000e-01\n",
       "[torch.FloatTensor of size 174978x1x25]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Should I order the interior points ?\n",
    "\n",
    "For each interior point calculate distance with points of the polygon\n",
    "\n",
    "find which ti which point it is closer and sort it according to the index.\n",
    "\n",
    "If multiple points are closer to the same point of the contour then take into\n",
    "\n",
    "account the distance to the point.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[4860 x 2]' is invalid for input with 19442 elements at C:\\Anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\TH\\THStorage.c:41",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-768-8090ffc56ab5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput_reshaped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 2: size '[4860 x 2]' is invalid for input with 19442 elements at C:\\Anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\TH\\THStorage.c:41"
     ]
    }
   ],
   "source": [
    " output_reshaped=out.view(int(out.size()[0]/2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " -1.4947e-01 -2.7472e-01\n",
       " -1.9165e-01 -2.2482e-01\n",
       " -1.3642e-01 -3.2461e-01\n",
       "            ⋮            \n",
       " -3.8810e-02 -2.6062e-01\n",
       " -1.4880e-01 -2.9060e-01\n",
       " -1.3283e-01 -3.1349e-01\n",
       " [torch.FloatTensor of size 9721x2], Variable containing:\n",
       " -0.2311  0.0403\n",
       "  0.1704 -0.1201\n",
       "  0.2371 -0.1810\n",
       "        ⋮        \n",
       " -0.0681  0.2579\n",
       " -0.1204 -0.1273\n",
       " -0.0528  0.2407\n",
       " [torch.FloatTensor of size 9721x2])"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out,y_variable.narrow(0,b,batch_size).resize(batch_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (  0   ,.,.) = \n",
       "  -0.2311  0.0403\n",
       " \n",
       " (  1   ,.,.) = \n",
       "   0.1704 -0.1201\n",
       " \n",
       " (  2   ,.,.) = \n",
       "   0.2371 -0.1810\n",
       "   ...  \n",
       " \n",
       " (174975,.,.) = \n",
       "  -0.1804  0.0393\n",
       " \n",
       " (174976,.,.) = \n",
       "   0.2417  0.0188\n",
       " \n",
       " (174977,.,.) = \n",
       "   0.2496 -0.1190\n",
       " [torch.FloatTensor of size 174978x1x2], Variable containing:\n",
       " -1.4947e-01 -2.7472e-01\n",
       " -1.9165e-01 -2.2482e-01\n",
       " -1.3642e-01 -3.2461e-01\n",
       "            ⋮            \n",
       " -3.8810e-02 -2.6062e-01\n",
       " -1.4880e-01 -2.9060e-01\n",
       " -1.3283e-01 -3.1349e-01\n",
       " [torch.FloatTensor of size 9721x2])"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_variable,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0177044392397268e-16"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons[0][:,0].sum()/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-826-db58e474dc7f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-826-db58e474dc7f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    for polygon\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_barycenters(polygons):\n",
    "    \n",
    "    for polygon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72589751,  0.84207899,  0.71337615,  0.16721335, -0.53593486,\n",
       "       -1.19396456, -0.99666214, -0.86514855, -0.06646145,  0.05744332,\n",
       "        0.52287723,  0.62928501])"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped[0][0::2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17159901,  0.40510667,  0.63783057,  0.58156785,  0.85478697,\n",
       "        0.50564378,  0.37817729, -0.73118862, -0.53079973, -1.1159677 ,\n",
       "       -0.86178872, -0.29496737])"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped[0][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.72589751,  0.17159901,  0.84207899, ..., -0.86178872,\n",
       "         0.62928501, -0.29496737],\n",
       "       [ 0.72589751,  0.17159901,  0.84207899, ..., -0.86178872,\n",
       "         0.62928501, -0.29496737],\n",
       "       [ 0.72589751,  0.17159901,  0.84207899, ..., -0.86178872,\n",
       "         0.62928501, -0.29496737],\n",
       "       ...,\n",
       "       [ 0.94168403, -0.21109   ,  0.66819129, ..., -0.98049631,\n",
       "         0.94108807, -0.3414368 ],\n",
       "       [ 0.94168403, -0.21109   ,  0.66819129, ..., -0.98049631,\n",
       "         0.94108807, -0.3414368 ],\n",
       "       [ 0.94168403, -0.21109   ,  0.66819129, ..., -0.98049631,\n",
       "         0.94108807, -0.3414368 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "barycenters=[np.array([i[0::2].sum()/12,i[1::2].sum()/12]) for i in polygons_reshaped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "barycenters=np.array(barycenters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218722"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(point_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218722, 24)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=point_coordinates.reshape(len(point_coordinates),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_with_points=np.hstack([polygons_reshaped,point_coordinates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_with_points=polygons_with_points.reshape(len(polygons_with_points),13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.72589751,  0.17159901],\n",
       "        [ 0.84207899,  0.40510667],\n",
       "        [ 0.71337615,  0.63783057],\n",
       "        ...,\n",
       "        [ 0.52287723, -0.86178872],\n",
       "        [ 0.62928501, -0.29496737],\n",
       "        [ 0.09185446,  0.06111541]],\n",
       "\n",
       "       [[ 0.72589751,  0.17159901],\n",
       "        [ 0.84207899,  0.40510667],\n",
       "        [ 0.71337615,  0.63783057],\n",
       "        ...,\n",
       "        [ 0.52287723, -0.86178872],\n",
       "        [ 0.62928501, -0.29496737],\n",
       "        [ 0.09185446,  0.06111541]],\n",
       "\n",
       "       [[ 0.72589751,  0.17159901],\n",
       "        [ 0.84207899,  0.40510667],\n",
       "        [ 0.71337615,  0.63783057],\n",
       "        ...,\n",
       "        [ 0.52287723, -0.86178872],\n",
       "        [ 0.62928501, -0.29496737],\n",
       "        [ 0.09185446,  0.06111541]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.94168403, -0.21109   ],\n",
       "        [ 0.66819129,  0.35508194],\n",
       "        [ 0.41427257,  0.62051448],\n",
       "        ...,\n",
       "        [ 0.46430839, -0.98049631],\n",
       "        [ 0.94108807, -0.3414368 ],\n",
       "        [ 0.29833109, -0.19828089]],\n",
       "\n",
       "       [[ 0.94168403, -0.21109   ],\n",
       "        [ 0.66819129,  0.35508194],\n",
       "        [ 0.41427257,  0.62051448],\n",
       "        ...,\n",
       "        [ 0.46430839, -0.98049631],\n",
       "        [ 0.94108807, -0.3414368 ],\n",
       "        [ 0.29833109, -0.19828089]],\n",
       "\n",
       "       [[ 0.94168403, -0.21109   ],\n",
       "        [ 0.66819129,  0.35508194],\n",
       "        [ 0.41427257,  0.62051448],\n",
       "        ...,\n",
       "        [ 0.46430839, -0.98049631],\n",
       "        [ 0.94108807, -0.3414368 ],\n",
       "        [ 0.29833109, -0.19828089]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_with_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-cd78c0aa7e0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolygons_with_points_unique\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolygons_with_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "polygons_with_points_unique=list(set(polygons_with_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2, 12)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_of_mass=np.array(centers_of_mass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers_of_mass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0  ,.,.) = \n",
       "  1.0804e+00  3.6440e-01  6.1404e-01  ...   4.9729e-01 -1.0542e+00  8.4189e-01\n",
       " -1.1037e+00 -1.0409e-01 -5.6104e-01  ...  -1.1438e+00  7.8898e-01 -5.4535e-01\n",
       "\n",
       "(  1  ,.,.) = \n",
       "  8.2030e-01 -9.6631e-02  1.1965e+00  ...   7.3245e-01 -1.1246e+00  6.5850e-01\n",
       " -9.8128e-01 -1.1748e-01 -4.0551e-01  ...  -7.3543e-01  5.2845e-01 -3.5533e-01\n",
       "\n",
       "(  2  ,.,.) = \n",
       "  8.8570e-01 -9.5701e-02  1.2094e+00  ...   5.8753e-01 -7.4392e-01  6.8171e-01\n",
       " -1.2722e+00 -1.4492e-01 -1.0465e+00  ...  -5.5213e-01  6.8863e-01 -5.0008e-01\n",
       " ...  \n",
       "\n",
       "(47997,.,.) = \n",
       "  1.0355e+00 -1.4388e-01  8.2466e-01  ...   7.1425e-01 -8.3453e-01  3.6346e-01\n",
       " -7.5888e-01  6.7694e-02 -8.8216e-01  ...  -1.0865e+00  3.7439e-01 -3.2188e-01\n",
       "\n",
       "(47998,.,.) = \n",
       "  6.8633e-01  5.9241e-02  9.9623e-01  ...   1.1396e+00 -7.3713e-01  8.0244e-02\n",
       " -8.3097e-01 -2.3101e-01 -8.2566e-01  ...  -7.4034e-01  6.7384e-01 -5.5374e-01\n",
       "\n",
       "(47999,.,.) = \n",
       "  8.7795e-01  6.0293e-02  8.4823e-01  ...   7.8716e-01 -7.3494e-01  3.3091e-01\n",
       " -1.1220e+00  3.2642e-01 -9.8364e-01  ...  -7.1744e-01  1.1813e+00 -3.0074e-01\n",
       "[torch.cuda.FloatTensor of size 48000x2x12 (GPU 0)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour3=apply_procrustes(generate_contour(12))\n",
    "plot_contour(contour3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 -0.1397529684620565 -0.03668817911622362 0\n",
      "14 -0.5556838673885416 0.006489134426013973 0\n",
      "13 -0.1506273310219623 -0.02890481750650344 0\n",
      "14 -0.5584024580285181 0.008434974828444014 0\n",
      "13 -0.1355795824036162 -0.0391512743253334 0\n",
      "14 -0.5546405208739316 0.005873360623736529 0\n",
      "13 -0.1606318248853285 -0.1507826396315696 0\n",
      "13 -0.1029359385443222 -0.01988941712108101 0\n",
      "14 -0.546479609909108 0.01068882492479963 0\n",
      "13 -0.1469844471794247 -0.05733317507392088 0\n",
      "14 0.7059926839926042 0.1405732385547066 0\n",
      "15 -0.5574917370678837 0.001327885436589659 0\n",
      "13 -0.06406265062238639 -0.0485721250445643 0\n",
      "13 -0.1236015407066197 -0.04107046060343363 0\n",
      "14 -0.5516460104496824 0.00539356405421147 0\n",
      "13 -0.14404267783239 -0.03219865149111428 0\n",
      "14 -0.556756294731125 0.00761151633229131 0\n",
      "13 -0.1415910047281703 -0.02934662034552328 0\n",
      "14 -0.5561433764550701 0.008324524118689058 0\n",
      "13 -0.1393203257012531 -0.04295809812348142 0\n",
      "14 -0.5555757066983406 0.004921654674199518 0\n",
      "13 -0.05551841490555372 -0.04630404540602545 0\n",
      "13 -0.1152941486290197 -0.03570669163350242 0\n",
      "14 -0.5495691624302824 0.006734506296694273 0\n",
      "13 -0.1481990857572459 -0.04522022789034128 0\n",
      "14 0.6966692085386884 0.2002444115280739 0\n",
      "15 -0.5577953967123389 0.00435612223248456 0\n",
      "13 -0.07055101030846371 -0.06536714467005897 0\n",
      "14 0.7254559917690627 0.1782028488186872 0\n",
      "13 0.668636742504867 0.1730424300096725 0\n",
      "14 -0.3420547800645272 -0.1011080068174328 0\n",
      "13 -0.2207103626209955 -0.1180203033517528 0\n",
      "14 -0.5759232159282763 -0.01384389663286831 0\n",
      "13 -0.05923516989966464 -0.05308195828137971 0\n",
      "13 -0.1290742129902545 -0.01990870138843091 0\n",
      "14 -0.5530141785205911 0.01068400385796215 0\n",
      "13 -0.06150263613039254 -0.02419357870215745 0\n",
      "13 -0.1118374092753558 -0.02796508085981035 0\n",
      "14 -0.5487049775918664 0.008669908990117291 0\n",
      "13 -0.06851715015816352 -0.03510044619353291 0\n",
      "13 -0.05297605435286387 -0.03865576811700715 0\n",
      "13 -0.3399600547589283 -0.09338459917915261 0\n",
      "13 -0.1467402633727014 -0.02403623576471525 0\n",
      "14 -0.5574306911162028 0.00965212026389106 0\n",
      "13 -0.06130916081936404 -0.03530915509763472 0\n",
      "13 -0.131623347546258 -0.01892871825386482 0\n",
      "14 -0.5536514621595919 0.01092899964160368 0\n",
      "13 -0.1358796847893088 -0.02270373409710344 0\n",
      "14 -0.5547155464703547 0.009985245680794019 0\n",
      "13 -0.1267787163622686 -0.01536690253090524 0\n",
      "14 -0.5524403043635946 0.01181945357234357 0\n",
      "13 -0.131539460607141 -0.0267662067652194 0\n",
      "14 -0.5536304904248127 0.008969627513765028 0\n",
      "13 -0.1365378918460694 -0.04084007827572396 0\n",
      "14 -0.5548800982345449 0.005451159636138889 0\n",
      "13 -0.3301168705830743 -0.08407409346101147 0\n",
      "13 -0.05559670217067636 -0.03394052144826785 0\n",
      "13 -0.04427075026552559 -0.01020100824648889 0\n",
      "13 -0.05258371292902656 -0.07324115685308397 0\n",
      "14 0.7498154331000679 0.1680858372150571 0\n",
      "13 -0.327337823060125 -0.08280235421318367 0\n",
      "13 -0.2280980501718821 -0.12173847705694 0\n",
      "14 -0.577770137815998 -0.01477344005916512 0\n",
      "13 -0.1474131678571638 -0.04244606622435481 0\n",
      "14 -0.5575989172373185 0.005049662648981176 0\n",
      "13 -0.03956674944986851 -0.02710795433156328 0\n",
      "13 -0.0668955770514985 -0.04266069365978505 0\n",
      "13 -0.132949276301365 -0.0122946015815628 0\n",
      "14 -0.5539829443483685 0.01258752880967917 0\n",
      "13 -0.1403216370137341 -0.047837722399509 0\n",
      "14 -0.555826034526461 0.003701748605192623 0\n",
      "13 -0.07562338668806432 -0.05991482889543128 0\n",
      "14 0.7265490209758726 0.1622457298735304 0\n",
      "13 -0.1468261710666018 -0.03980637410123573 0\n",
      "14 -0.5574521680396779 0.005709585679760947 0\n",
      "13 -0.2197673007388687 -0.09477857564662642 0\n",
      "14 -0.5756874504577447 -0.008033464706586727 0\n",
      "13 -0.140856607160155 -0.1568805282005062 0\n",
      "13 -0.05612086084599307 -0.02274351477224481 0\n",
      "13 -0.1306790362512654 -0.02395026286339837 0\n",
      "14 -0.5534153843358438 0.009673613489220287 0\n",
      "13 -0.1466484840583014 -0.03078314879845298 0\n",
      "14 -0.5574077462876028 0.007965392005456633 0\n",
      "13 0.6781525585907586 0.1781686756011178 0\n",
      "14 -0.3398428873609275 -0.1112426127964096 0\n",
      "13 -0.1286765556329334 -0.03834700299175541 0\n",
      "14 -0.5529147641812608 0.006074428457131026 0\n",
      "13 -0.1425208477071837 -0.1480949726841848 0\n",
      "13 -0.2295247198802254 -0.122490114835207 0\n",
      "14 -0.5781268052430838 -0.01496134950373186 0\n",
      "15 0.6657881575066984 0.2005219709940744 0\n",
      "13 -0.1401382249124987 -0.04438696191526676 0\n",
      "14 -0.5557801815011522 0.004564438726253188 0\n",
      "13 -0.3331606528800971 -0.07990415180608197 0\n",
      "13 -0.1526067429149884 -0.04102509893687514 0\n",
      "14 0.6797516074813463 0.2115398151852108 0\n",
      "15 -0.5588973110017745 0.005404904470851093 0\n",
      "13 -0.1366886318782331 -0.01417230109403478 0\n",
      "14 -0.5549177832425858 0.01211810393156118 0\n",
      "13 -0.05981068413271194 -0.04720046188510983 0\n",
      "13 -0.2125650283954261 -0.1049708062633488 0\n",
      "14 -0.573886882371884 -0.01058152236076733 0\n",
      "13 -0.1500018396124621 -0.03656289705671551 0\n",
      "14 -0.5582460851761431 0.006520454940891001 0\n",
      "13 -0.1289985351318677 -0.02400084491377941 0\n",
      "14 -0.5529952590559945 0.009660967976625027 0\n",
      "13 -0.1394876835303045 -0.03746581650992508 0\n",
      "14 -0.5556175461556037 0.006294725077588608 0\n",
      "13 -0.118664770504445 -0.0127507651422367 0\n",
      "14 -0.5504118178991387 0.0124734879195107 0\n",
      "13 -0.1341437017418671 -0.01698123672938327 0\n",
      "14 -0.5542815507084941 0.01141587002272406 0\n",
      "13 -0.1478384116236053 -0.05920448952487276 0\n",
      "14 0.6838935371480942 0.1919000591336481 0\n",
      "15 -0.5577052281789288 0.0008600568238516897 0\n",
      "13 -0.1413124189310576 -0.03188163586906931 0\n",
      "14 -0.5560737300057919 0.007690770237802545 0\n",
      "13 -0.2242051176715144 -0.08375400982115525 0\n",
      "14 -0.5767969046909061 -0.005277323250218933 0\n",
      "13 -0.1515301104572981 -0.05879096046795171 0\n",
      "14 0.689944160006159 0.1719409864438256 0\n",
      "15 -0.558628152887352 0.0009634390880819514 0\n",
      "13 -0.1614356047742784 -0.1424951007962882 0\n",
      "13 -0.1524243529220074 -0.1699834645643355 0\n",
      "14 0.7252994759466482 0.2092065714876347 0\n",
      "13 -0.03151777023174163 -0.05092264937644676 0\n",
      "13 -0.189640727251055 -0.02570138845168707 0\n",
      "13 -0.07358253211827795 -0.06678878014543131 0\n",
      "14 0.7294955491776982 0.1763605516738522 0\n",
      "13 -0.1232421838936155 -0.01819063288795197 0\n",
      "14 -0.5515561712464313 0.01111352098308189 0\n",
      "13 -0.1408574837019793 -0.1242156432237974 0\n",
      "13 -0.1480153563676902 -0.1428787241247025 0\n",
      "13 -0.03666884231416678 -0.03799211108395292 0\n",
      "13 -0.2247757957830015 -0.1238216139338256 0\n",
      "14 -0.5769395742187778 -0.01529422427838652 0\n",
      "13 -0.2167594209378544 -0.1056574500741192 0\n",
      "14 -0.5749354805074911 -0.01075318331345993 0\n",
      "13 -0.1249749877229445 -0.03861279751962351 0\n",
      "14 -0.5519893722037637 0.006007979825164002 0\n",
      "13 -0.06750263578351787 -0.03580310179476102 0\n",
      "13 -0.1345067389077143 -0.1404675268215443 0\n",
      "13 -0.1264632831340571 -0.04099417656393815 0\n",
      "14 -0.5523614460565417 0.005412635064085342 0\n",
      "13 -0.1464945834086179 -0.05695040544565277 0\n",
      "14 0.6905684970705867 0.187069267302955 0\n",
      "15 -0.557369271125182 0.001423577843656685 0\n",
      "13 -0.1487693391512562 -0.1419271050077581 0\n",
      "13 -0.1234861307451766 -0.02284691760734195 0\n",
      "14 -0.5516171579593217 0.009949449803234391 0\n",
      "13 0.6848397681433996 0.1766069557976743 0\n",
      "14 -0.1742528694427543 -0.1793763288269057 0\n",
      "13 -0.1317130355441832 -0.01877169288191783 0\n",
      "14 -0.5536738841590733 0.01096825598459042 0\n",
      "13 -0.2259633919170091 -0.119115970462218 0\n",
      "14 -0.5772364732522796 -0.01411781341048462 0\n",
      "13 -0.04801941787163085 -0.05048921835696252 0\n",
      "13 -0.1305435506635666 -0.02483942972507727 0\n",
      "14 -0.5533815129389191 0.009451321773800561 0\n",
      "13 -0.05310040463287966 -0.04999466596033805 0\n",
      "13 -0.1270756280915445 -0.03395358713439253 0\n",
      "14 -0.5525145322959136 0.007172782421471739 0\n",
      "13 -0.1428323311334145 -0.1547412602474058 0\n",
      "13 -0.1336810424955752 -0.0412095563716969 0\n",
      "14 -0.5541658858969213 0.005358790112145654 0\n",
      "13 -0.2177818512972035 -0.09961526651157587 0\n",
      "14 -0.5751910880973283 -0.00924263742282409 0\n",
      "13 -0.1450776076669155 -0.1407926389674559 0\n",
      "13 -0.1309676650605889 -0.02008444381223736 0\n",
      "14 -0.5534875415381747 0.01064006825201053 0\n",
      "13 -0.2172699141104487 -0.1033083044113232 0\n",
      "14 -0.5750631038006396 -0.01016589689776093 0\n",
      "13 -0.07739642030705787 -0.04701983303141259 0\n",
      "14 0.7058805647781584 0.1897875196112926 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1bd48fd9630>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inserted_points=[]\n",
    "nb_of_sampling=100\n",
    "contour=contour3.copy()\n",
    "plot_contour(contour3)\n",
    "\n",
    "for i in range(0,nb_of_sampling):\n",
    "    contour[0]=np.random.normal(contour3[0],0.08)\n",
    "    contour[11]=np.random.normal(contour3[11],0.08)\n",
    "\n",
    "    nb_of_points,point_coords=get_extrapoints_target_length(contour,0.2)\n",
    "    inserted_points.append(point_coords)\n",
    "    plt.scatter(contour[0][0],contour[0][1])\n",
    "    plt.scatter(contour[11][0],contour[11][1])\n",
    "\n",
    "\n",
    "inserted_points=[i for i in inserted_points if len(i)==1]\n",
    "inserted_points=np.array(inserted_points)\n",
    "inserted_points=inserted_points.reshape(len(inserted_points),2)\n",
    "\n",
    "plt.scatter(inserted_points[:,0],inserted_points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
