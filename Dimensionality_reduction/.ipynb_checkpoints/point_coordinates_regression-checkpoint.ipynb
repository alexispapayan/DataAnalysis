{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Triangulation import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from  Neural_network import *\n",
    "\n",
    "%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_nb_of_points(point_coordinates):\n",
    "    set_of_numbers=set()\n",
    "    for index,_ in enumerate(point_coordinates):\n",
    "        set_of_numbers.add(len(point_coordinates[index][0]))\n",
    "    return set_of_numbers\n",
    "\n",
    "\n",
    "def get_indices_nb_of_points(set_of_numbers,number_of_points,point_coordinates):\n",
    "    indices=[]\n",
    "    if number_of_points not in set_of_numbers:\n",
    "        return \"No such number of points for sample\"\n",
    "    else:\n",
    "        for index,_ in enumerate(point_coordinates):\n",
    "            if len(point_coordinates[index][0])==number_of_points:\n",
    "                indices.append(index)\n",
    "        return indices\n",
    "    \n",
    "def get_polygons_nb_of_points(indices):\n",
    "    \n",
    "    pass\n",
    "\n",
    "def get_edge_lengths(polygon):\n",
    "    polygon_edge_lengths=np.empty([polygon.shape[0]])\n",
    "    for index,_ in enumerate(polygon):\n",
    "        polygon_edge_lengths[index]=np.linalg.norm(polygon[(index+1)%(polygon.shape[0])]-polygon[index])\n",
    "    return polygon_edge_lengths\n",
    "    \n",
    "\n",
    "def extract_lengths_angles(polygon_set):\n",
    "    lengths=[]\n",
    "    angles=[]\n",
    "    target_edge_length=[]\n",
    "    for polygons in polygons_reshaped:\n",
    "        polygon=np.delete(polygons,24).reshape(12,2)\n",
    "        lengths.append(get_edge_lengths(polygon))\n",
    "        angles.append(np.array(np.multiply(pi/180,get_polygon_angles(polygon))))\n",
    "        target_edge_length.append([polygons[24]])\n",
    "    data=np.hstack([lengths,angles,target_edge_length])\n",
    "    return data\n",
    "    \n",
    "def extract_lengths_angles_in_triangle_form(polygon_set):\n",
    "    lengths=[]\n",
    "    angles=[]\n",
    "    target_edge_length=[]\n",
    "    for polygons in polygons_reshaped:\n",
    "        polygon=np.delete(polygons,24).reshape(12,2)\n",
    "        lengths.append(get_edge_lengths(polygon))\n",
    "        angles.append(np.array(np.multiply(pi/180,get_polygon_angles(polygon))))\n",
    "        target_edge_length.append([polygons[24]])\n",
    "    data=np.hstack([lengths,angles,target_edge_length])\n",
    "    return data    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.function import Function\n",
    "from math import pow\n",
    "\n",
    "class myLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target):\n",
    "        self.save_for_backward(output,target) \n",
    "\n",
    "                        \n",
    "       # output=output.view(int(output.size()[0]/2),2)\n",
    "\n",
    "        #target=target.view(int(target.size()[0]/2),2)\n",
    "        distance=torch.nn.PairwiseDistance()\n",
    "        result=distance(output,target)\n",
    "\n",
    "        result=torch.FloatTensor(result)\n",
    "        #self.save_for_backward(result)\n",
    "\n",
    "\n",
    "        return  result \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self,grad_output1):\n",
    "        input1,target=self.saved_variables\n",
    "        \n",
    "        print(input1)\n",
    "        #distance=torch.nn.PairwiseDistance()(input1.view(int(input1.size()[0]/2),2),target.view(int(target.size()[0]/2),2))\n",
    "        distance=torch.nn.PairwiseDistance()(input1,target)\n",
    "\n",
    "        grad_output1=(input1-target)/distance\n",
    "\n",
    "        \n",
    "        return grad_output1,None\n",
    "    \n",
    "    \n",
    "class myOtherLossfunction(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self,output,target)->Variable:\n",
    "        \n",
    "        torch_sum=0\n",
    "        for i in range(0,output.size()[1],2):\n",
    "            euclidean_distance=(output[:,i]-target[:,i]).pow(2)+(output[:,i+1]-target[:,i+1]).pow(2)\n",
    "            torch_sum+=euclidean_distance\n",
    "        return  torch_sum\n",
    "\n",
    "def my_torch_loss_function(a,b)->Variable:\n",
    "    torch_sum=0\n",
    "    for i in range(0,a.size()[1],2):\n",
    "        euclidean_distance=torch.sqrt((a[:,i]-b[:,i]).pow(2)+(a[:,i+1]-b[:,i+1]).pow(2))\n",
    "        torch_sum+=euclidean_distance\n",
    "    return  torch_sum   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using convolution network\n",
    "# O: output dimension\n",
    "# I: Input dimensiion\n",
    "# S: Stride\n",
    "# P: padding\n",
    "# w: kernel size\n",
    "# O=(I-w-2*P)/S+1\n",
    "\n",
    "# Included batch normalization with dropout layers\n",
    "\n",
    "nb_of_points_output=1\n",
    "\n",
    "class Conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Conv_net,self).__init__()\n",
    "        self.conv1=nn.Conv1d(1,14,kernel_size=14,stride=1)\n",
    "        self.conv2=nn.Conv1d(14,28,kernel_size=2,stride=1)\n",
    "\n",
    "        self.bn1=nn.BatchNorm1d(num_features=28*9)\n",
    "        self.fc1=nn.Linear(28*9,12)\n",
    "        self.dp_1=nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.bn2=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc2=nn.Linear(12,12)\n",
    "        self.dp_2=nn.Dropout(p=0.5)\n",
    "\n",
    "        \n",
    "        self.bn3=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc3=nn.Linear(12,12)\n",
    "        self.dp_3=nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.bn4=nn.BatchNorm1d(num_features=12)\n",
    "        self.fc4=nn.Linear(12,nb_of_points_output*2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(F.max_pool1d(self.conv1(x),kernel_size=2,stride=1))\n",
    "        \n",
    "        #x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        #x=self.conv2(x)\n",
    "\n",
    "        x=F.relu(F.max_pool1d(self.conv2(x),kernel_size=2,stride=1))\n",
    "        x=F.relu(self.fc1(self.bn1(x.view(-1,28*9))))\n",
    "        x=F.relu(self.fc2( self.dp_1(self.bn2(x))))\n",
    "        x=F.relu(self.fc3(self.dp_2(self.bn3(x))))\n",
    "\n",
    "        x=self.fc4(self.bn4(self.dp_3(x)))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another convolution network involving the pairing of lengths and angles through convolution and then \n",
    "# fully connectiong in a  linear fashion\n",
    "# Input : a matrix X where lengths take X/2 and angles X/2 and the target edge length\n",
    "\n",
    "class alt_conv_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,nb_of_filters,nb_of_hidden_nodes,out_dimension):\n",
    "        super(alt_conv_net,self).__init__()\n",
    "        \n",
    "        self.nb_of_filters=nb_of_filters\n",
    "        \n",
    "        self.conv1=nn.Sequential(nn.Conv1d(in_channels=1,out_channels=nb_of_filters,stride=1,kernel_size=2),\n",
    "                                nn.MaxPool1d(stride=1,kernel_size=2),nn.ReLU(inplace=True))\n",
    "        self.conv2=nn.Sequential(nn.Conv1d(in_channels=1,out_channels=nb_of_filters,stride=1,kernel_size=2),\n",
    "                                nn.MaxPool1d(stride=1,kernel_size=2),nn.ReLU(inplace=True))\n",
    "        # The linear connections are fixed for 12 polygon example\n",
    "   \n",
    "        self.fc=nn.Sequential(  nn.BatchNorm1d(num_features=nb_of_filters*10+nb_of_filters*10+1),\n",
    "\n",
    "                                nn.Linear(nb_of_filters*10+nb_of_filters*10+1,nb_of_hidden_nodes),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                               nn.BatchNorm1d(num_features=nb_of_hidden_nodes),\n",
    "                               nn.Linear(nb_of_hidden_nodes,out_dimension) )                                                                        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        lenghts=x.narrow(1,0,1).narrow(2,0,12)\n",
    "        angles=x.narrow(1,0,1).narrow(2,12,12)\n",
    "        target_edge_length=x.narrow(1,0,1).narrow(2,24,1).resize(x.size()[0],1)\n",
    "\n",
    "       \n",
    "        conv_result1=self.conv1(lenghts)\n",
    "        conv_result2=self.conv2(angles)        \n",
    "        \n",
    "        # reshape the convolution results\n",
    "        \n",
    "        conv_result1=conv_result1.view(-1,self.nb_of_filters*10)\n",
    "        conv_result2=conv_result2.view(-1,self.nb_of_filters*10)\n",
    "        \n",
    "        concat_tensor=torch.cat([conv_result1,conv_result2,target_edge_length],1)\n",
    "        output=self.fc(concat_tensor)\n",
    "        return output\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another convolutional network. The input of the network is : Length[index], Length[index+1], angle[index] e.g the triangles\n",
    "# formed by connecting the neighbouring edges of the contour.\n",
    "# The input is convoluted by a kernel of size 3 with a stride of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons=load_dataset('12_polygons.pkl')\n",
    "polygons=[i for i in polygons for j in range(10)]\n",
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(2,12))\n",
    "\n",
    "polygons_reshaped=np.array(polygons_reshaped)\n",
    "#polygons_reshaped=polygons_reshaped.reshape(60000,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=load_dataset('12_point_coordinates')\n",
    "number_of_insertion_points=load_dataset('12_nb_of_points.pkl')\n",
    "#centers_of_mass=load_dataset('12_centers_of_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_points=3\n",
    "set_of_points=get_set_nb_of_points(point_coordinates)        \n",
    "indices=get_indices_nb_of_points(set_of_points,nb_of_points,point_coordinates)\n",
    "indices=np.asarray(indices)\n",
    "number_of_insertion_points=np.array(number_of_insertion_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped.resize(len(point_coordinates),2*12)\n",
    "\n",
    "polygons_reshaped=np.hstack([polygons_reshaped[indices],number_of_insertion_points[indices,1].reshape(len(indices),1) ])\n",
    "#polygons_reshaped=polygons_reshaped[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_coordinates=[ point_coordinates[i][0]for i in indices]\n",
    "point_coordinates=np.array(point_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(polygons_reshaped),len(indices)\n",
    "#point_coordinates.shape\n",
    "#centers_of_mass=centers_of_mass.reshape(60000,2)\n",
    "point_coordinates=point_coordinates.reshape(polygons_reshaped.shape[0],1,2*nb_of_points)\n",
    "#point_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting angles and length of edges\n",
    "\n",
    "data=extract_lengths_angles(polygons_reshaped)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Project set of polygons in 2d and 3d space #\n",
    "# Dimensionality reduction using Isomap, PCA, kernel PCA ... #\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "\n",
    "Polygons_reshaped=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Isomap #\n",
    "#nb_components=8\n",
    "#iso=manifold.Isomap(n_neighbors=8,n_components=nb_components,n_jobs=-1)\n",
    "#iso.fit(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "\n",
    "                        # PCA   #\n",
    "pca=PCA(.999)\n",
    "pca.fit(polygons_reshaped)\n",
    "Polygons_projected=pca.transform(polygons_reshaped)\n",
    "nb_components=int(pca.n_components_)\n",
    "print(nb_components)\n",
    "# Fitting into lesser dimension\n",
    "\n",
    "#Polygons_projected=iso.fit_transform(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "#iso = KernelPCA(n_components=2,kernel=\"rbf\", fit_inverse_transform=True, gamma=1e-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15554, 1, 6]) torch.Size([15554, 25])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_tensor=torch.from_numpy(data[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "#mixed_tensor=torch.cat((lengths_tensor[:nb_of_training_data],angles_tensor[:nb_of_training_data]),1)\n",
    "#double_mixed_tensor=angles_tensor[:nb_of_training_data]\n",
    "\n",
    "x_tensor_test=torch.from_numpy(data[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "#mixed_tensor_test=torch.cat((lengths_tensor[nb_of_training_data:],angles_tensor[nb_of_training_data:]),1)\n",
    "#double_mixed_tensor_test=angles_tensor[nb_of_training_data:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n",
    "#x_variable,x_variable_test=Variable(mixed_tensor),Variable(mixed_tensor_test)\n",
    "#x_variable,x_variable_test=Variable(double_mixed_tensor),Variable(double_mixed_tensor_test)\n",
    "\n",
    "y_tensor=torch.from_numpy(point_coordinates[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(point_coordinates[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "shuffle=torch.randperm(x_variable.shape[0])\n",
    "x_variable = x_variable[shuffle]\n",
    "y_variable=y_variable[shuffle]\n",
    "print(y_variable.size(),x_variable.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 25 6\n"
     ]
    }
   ],
   "source": [
    "my_net=Net(x_variable.size()[1],y_variable.size()[2],nb_of_hidden_layers=3, nb_of_hidden_nodes=20,batch_normalization=True)\n",
    "\n",
    "print(\"Training data length:\",x_variable_test.size()[1],y_variable.size()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=0)\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "#max_distance=0.6108970818704328\n",
    "loss_func =torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=myOtherLossfunction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "        \n",
    "    x_variable , y_variable=x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test,y_variable_test= Variable(x_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "    #x_variable_test,y_variable_test= Variable(double_mixed_tensor_test.cuda(),volatile=True),Variable(y_tensor_test.cuda(),volatile=True)\n",
    "\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 1.3357752627941366 Test Loss: 0.992444952819573\n",
      "Epoch: 10 Training Loss: 1.3062170753905749 Test Loss: 1.2964369534465021\n",
      "Epoch: 20 Training Loss: 1.278080723808988 Test Loss: 1.2852196713043338\n",
      "Epoch: 30 Training Loss: 1.2513837879323646 Test Loss: 1.2592331372170782\n",
      "Epoch: 40 Training Loss: 1.226089977859393 Test Loss: 1.2347704726482125\n",
      "Epoch: 50 Training Loss: 1.202145172664588 Test Loss: 1.2114313070666152\n",
      "Epoch: 60 Training Loss: 1.179442888485277 Test Loss: 1.1892500512393904\n",
      "Epoch: 70 Training Loss: 1.1579533650909735 Test Loss: 1.1681266125337577\n",
      "Epoch: 80 Training Loss: 1.1375706209013758 Test Loss: 1.1480592327353396\n",
      "Epoch: 90 Training Loss: 1.118249952785457 Test Loss: 1.1288764859423226\n",
      "Epoch: 100 Training Loss: 1.0998873882924007 Test Loss: 1.110534165621785\n",
      "Epoch: 110 Training Loss: 1.0824043202311302 Test Loss: 1.0931284712175282\n",
      "Epoch: 120 Training Loss: 1.0657508970763148 Test Loss: 1.0764351048096708\n",
      "Epoch: 130 Training Loss: 1.0498592479225601 Test Loss: 1.060571490001286\n",
      "Epoch: 140 Training Loss: 1.0347055771425357 Test Loss: 1.0454030605991191\n",
      "Epoch: 150 Training Loss: 1.0202525274848913 Test Loss: 1.030953238530414\n",
      "Epoch: 160 Training Loss: 1.0064715944250675 Test Loss: 1.0170998278959298\n",
      "Epoch: 170 Training Loss: 0.9933766535135656 Test Loss: 1.0039435492621527\n",
      "Epoch: 180 Training Loss: 0.9809197367839463 Test Loss: 0.9914251884805813\n",
      "Epoch: 190 Training Loss: 0.9690759812588402 Test Loss: 0.9795409779489777\n",
      "Epoch: 200 Training Loss: 0.9578212773844349 Test Loss: 0.9682002440401556\n",
      "Epoch: 210 Training Loss: 0.9471223489536453 Test Loss: 0.9575025770399306\n",
      "Epoch: 220 Training Loss: 0.9369599208804809 Test Loss: 0.9473290463043339\n",
      "Epoch: 230 Training Loss: 0.9273085651199049 Test Loss: 0.9377194000369727\n",
      "Epoch: 240 Training Loss: 0.9181329963353478 Test Loss: 0.9286431834530928\n",
      "Epoch: 250 Training Loss: 0.9093927807959368 Test Loss: 0.9200416847511574\n",
      "Epoch: 260 Training Loss: 0.9010595395477048 Test Loss: 0.9118484685450424\n",
      "Epoch: 270 Training Loss: 0.8931190831136685 Test Loss: 0.9040590765054334\n",
      "Epoch: 280 Training Loss: 0.8855596706434036 Test Loss: 0.8966701177903164\n",
      "Epoch: 290 Training Loss: 0.8783593900684711 Test Loss: 0.8896438535839442\n",
      "Epoch: 300 Training Loss: 0.8715005987205864 Test Loss: 0.8829966729560507\n",
      "Epoch: 310 Training Loss: 0.8649548548604861 Test Loss: 0.8766095196759259\n",
      "Epoch: 320 Training Loss: 0.8587137452584545 Test Loss: 0.8705179975847158\n",
      "Epoch: 330 Training Loss: 0.8527628292963225 Test Loss: 0.8647013220767426\n",
      "Epoch: 340 Training Loss: 0.8470940076708564 Test Loss: 0.8592004344296553\n",
      "Epoch: 350 Training Loss: 0.8416886959343256 Test Loss: 0.8539272983378343\n",
      "Epoch: 360 Training Loss: 0.83653421145686 Test Loss: 0.8489219131783693\n",
      "Epoch: 370 Training Loss: 0.8316068213964254 Test Loss: 0.8441242484889403\n",
      "Epoch: 380 Training Loss: 0.8268986775909734 Test Loss: 0.8395623729062178\n",
      "Epoch: 390 Training Loss: 0.8223913211633663 Test Loss: 0.8352232254091113\n",
      "Epoch: 400 Training Loss: 0.8180809222105246 Test Loss: 0.8311064292373971\n",
      "Epoch: 410 Training Loss: 0.8139624579087373 Test Loss: 0.8271061775615677\n",
      "Epoch: 420 Training Loss: 0.8100164648161244 Test Loss: 0.8233014644418725\n",
      "Epoch: 430 Training Loss: 0.8062266815409219 Test Loss: 0.8196726983466757\n",
      "Epoch: 440 Training Loss: 0.802587959688826 Test Loss: 0.8161984667365934\n",
      "Epoch: 450 Training Loss: 0.7990924510977884 Test Loss: 0.81292724609375\n",
      "Epoch: 460 Training Loss: 0.7957425416090717 Test Loss: 0.8097929777922453\n",
      "Epoch: 470 Training Loss: 0.792540365922753 Test Loss: 0.8068389264644419\n",
      "Epoch: 480 Training Loss: 0.7894790804415263 Test Loss: 0.8040477611400463\n",
      "Epoch: 490 Training Loss: 0.786552092709271 Test Loss: 0.8013796708220807\n",
      "Epoch: 500 Training Loss: 0.7837568913141314 Test Loss: 0.798834718303916\n",
      "Epoch: 510 Training Loss: 0.7811051543212357 Test Loss: 0.79638772344393\n",
      "Epoch: 520 Training Loss: 0.7785907915568343 Test Loss: 0.7940613546489198\n",
      "Epoch: 530 Training Loss: 0.7762079639883631 Test Loss: 0.7918826758616255\n",
      "Epoch: 540 Training Loss: 0.7739423565682461 Test Loss: 0.7898145134066358\n",
      "Epoch: 550 Training Loss: 0.7717848654285071 Test Loss: 0.7878576208043981\n",
      "Epoch: 560 Training Loss: 0.769714206353671 Test Loss: 0.7859777128745499\n",
      "Epoch: 570 Training Loss: 0.7677391692852321 Test Loss: 0.7841640519507137\n",
      "Epoch: 580 Training Loss: 0.7658655304704578 Test Loss: 0.7824450206364133\n",
      "Epoch: 590 Training Loss: 0.7640991917272084 Test Loss: 0.7808401476699138\n",
      "Epoch: 600 Training Loss: 0.7624282238491706 Test Loss: 0.7793510656788516\n",
      "Epoch: 610 Training Loss: 0.7608479179391153 Test Loss: 0.7779554830166538\n",
      "Epoch: 620 Training Loss: 0.759346658717211 Test Loss: 0.7766334941848315\n",
      "Epoch: 630 Training Loss: 0.7579196117156358 Test Loss: 0.7753388832626029\n",
      "Epoch: 640 Training Loss: 0.7565567940722644 Test Loss: 0.7741021050347222\n",
      "Epoch: 650 Training Loss: 0.7552636681078823 Test Loss: 0.7729515421047132\n",
      "Epoch: 660 Training Loss: 0.7540379735518195 Test Loss: 0.771863395785108\n",
      "Epoch: 670 Training Loss: 0.752872552880288 Test Loss: 0.7708202095188722\n",
      "Epoch: 680 Training Loss: 0.7517672177373987 Test Loss: 0.7698341652199074\n",
      "Epoch: 690 Training Loss: 0.7507149989552526 Test Loss: 0.7689024999799061\n",
      "Epoch: 700 Training Loss: 0.7497112504219172 Test Loss: 0.7680054966804912\n",
      "Epoch: 710 Training Loss: 0.7487626273788093 Test Loss: 0.7671533906410751\n",
      "Epoch: 720 Training Loss: 0.7478609677373987 Test Loss: 0.7663531519257973\n",
      "Epoch: 730 Training Loss: 0.7470022532387167 Test Loss: 0.7656037758407279\n",
      "Epoch: 740 Training Loss: 0.7461847258944645 Test Loss: 0.7649107254091113\n",
      "Epoch: 750 Training Loss: 0.7454051208692298 Test Loss: 0.7642428551191165\n",
      "Epoch: 760 Training Loss: 0.7446575363451524 Test Loss: 0.7635886109905479\n",
      "Epoch: 770 Training Loss: 0.7439378912779671 Test Loss: 0.7629466115692516\n",
      "Epoch: 780 Training Loss: 0.7432498900001607 Test Loss: 0.762346306946052\n",
      "Epoch: 790 Training Loss: 0.7425914605969526 Test Loss: 0.7617718103981803\n",
      "Epoch: 800 Training Loss: 0.7419629797801208 Test Loss: 0.7612413947964892\n",
      "Epoch: 810 Training Loss: 0.7413604920759933 Test Loss: 0.7607385454845036\n",
      "Epoch: 820 Training Loss: 0.7407824906374566 Test Loss: 0.7602594320666153\n",
      "Epoch: 830 Training Loss: 0.7402306078822168 Test Loss: 0.7598144782423483\n",
      "Epoch: 840 Training Loss: 0.7396969956482256 Test Loss: 0.7593916276845422\n",
      "Epoch: 850 Training Loss: 0.7391805238001479 Test Loss: 0.7589972853170011\n",
      "Epoch: 860 Training Loss: 0.7386840176763212 Test Loss: 0.758625862529739\n",
      "Epoch: 870 Training Loss: 0.7382035218030731 Test Loss: 0.7582840154200424\n",
      "Epoch: 880 Training Loss: 0.7377380316156615 Test Loss: 0.757959185313786\n",
      "Epoch: 890 Training Loss: 0.7372891167464961 Test Loss: 0.7576438370064943\n",
      "Epoch: 900 Training Loss: 0.7368534495748682 Test Loss: 0.7573429939678177\n",
      "Epoch: 910 Training Loss: 0.7364308417448888 Test Loss: 0.7570528258021476\n",
      "Epoch: 920 Training Loss: 0.7360131939533239 Test Loss: 0.7567600203149113\n",
      "Epoch: 930 Training Loss: 0.7356062824474412 Test Loss: 0.7564635728121785\n",
      "Epoch: 940 Training Loss: 0.7352105467243153 Test Loss: 0.7561832632056971\n",
      "Epoch: 950 Training Loss: 0.7348232242309052 Test Loss: 0.7559170193142362\n",
      "Epoch: 960 Training Loss: 0.7344433104024688 Test Loss: 0.7556584990073624\n",
      "Epoch: 970 Training Loss: 0.7340666614094445 Test Loss: 0.7554145467624743\n",
      "Epoch: 980 Training Loss: 0.7336949724548347 Test Loss: 0.7551843462657536\n",
      "Epoch: 990 Training Loss: 0.7333294992445674 Test Loss: 0.754960362312725\n",
      "Epoch: 1000 Training Loss: 0.7329674792256011 Test Loss: 0.7547391412680041\n",
      "Epoch: 1010 Training Loss: 0.7326113610244953 Test Loss: 0.7545206203382202\n",
      "Epoch: 1020 Training Loss: 0.7322586960146907 Test Loss: 0.7542843916779193\n",
      "Epoch: 1030 Training Loss: 0.7319079773490742 Test Loss: 0.7540403138462899\n",
      "Epoch: 1040 Training Loss: 0.7315578237511251 Test Loss: 0.75379818260915\n",
      "Epoch: 1050 Training Loss: 0.7312079212943616 Test Loss: 0.7535590026604295\n",
      "Epoch: 1060 Training Loss: 0.7308578932670052 Test Loss: 0.7533238414874293\n",
      "Epoch: 1070 Training Loss: 0.7305111300750611 Test Loss: 0.7530820242171425\n",
      "Epoch: 1080 Training Loss: 0.7301678828597146 Test Loss: 0.7528448536562822\n",
      "Epoch: 1090 Training Loss: 0.7298309769593031 Test Loss: 0.7526230674712255\n",
      "Epoch: 1100 Training Loss: 0.7294963313295615 Test Loss: 0.7524132748199589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1110 Training Loss: 0.7291672735911984 Test Loss: 0.7522066846305941\n",
      "Epoch: 1120 Training Loss: 0.7288433014618426 Test Loss: 0.7519983990202225\n",
      "Epoch: 1130 Training Loss: 0.7285214012472676 Test Loss: 0.7517985277215149\n",
      "Epoch: 1140 Training Loss: 0.7282048377828855 Test Loss: 0.751611026716821\n",
      "Epoch: 1150 Training Loss: 0.7278890905273563 Test Loss: 0.7514291143221129\n",
      "Epoch: 1160 Training Loss: 0.7275722131364922 Test Loss: 0.751245318126286\n",
      "Epoch: 1170 Training Loss: 0.7272553357456282 Test Loss: 0.7510686803747106\n",
      "Epoch: 1180 Training Loss: 0.7269373910047254 Test Loss: 0.7508917286562822\n",
      "Epoch: 1190 Training Loss: 0.7266198229756011 Test Loss: 0.7506851384669174\n",
      "Epoch: 1200 Training Loss: 0.7262977971904333 Test Loss: 0.7504608405470358\n",
      "Epoch: 1210 Training Loss: 0.7259744529140414 Test Loss: 0.7502362286603009\n",
      "Epoch: 1220 Training Loss: 0.725648597225794 Test Loss: 0.7500058397834684\n",
      "Epoch: 1230 Training Loss: 0.7253239972434743 Test Loss: 0.7497838024249293\n",
      "Epoch: 1240 Training Loss: 0.7249943116521473 Test Loss: 0.7495741981537745\n",
      "Epoch: 1250 Training Loss: 0.7246569662546611 Test Loss: 0.7493479536394033\n",
      "Epoch: 1260 Training Loss: 0.7243162304511701 Test Loss: 0.7491281768422068\n",
      "Epoch: 1270 Training Loss: 0.7239753690770863 Test Loss: 0.7489104094328703\n",
      "Epoch: 1280 Training Loss: 0.7236355122677446 Test Loss: 0.7487138661828061\n",
      "Epoch: 1290 Training Loss: 0.7232981668702585 Test Loss: 0.7485274954587834\n",
      "Epoch: 1300 Training Loss: 0.7229630817434421 Test Loss: 0.7483361640584812\n",
      "Epoch: 1310 Training Loss: 0.7226280594019223 Test Loss: 0.748126120233732\n",
      "Epoch: 1320 Training Loss: 0.7222949834045904 Test Loss: 0.747912497186857\n",
      "Epoch: 1330 Training Loss: 0.7219656745250418 Test Loss: 0.7477038348162616\n",
      "Epoch: 1340 Training Loss: 0.7216390654132377 Test Loss: 0.7474979353539738\n",
      "Epoch: 1350 Training Loss: 0.7213134608661759 Test Loss: 0.7472996338895319\n",
      "Epoch: 1360 Training Loss: 0.7209840264160344 Test Loss: 0.7471083024892297\n",
      "Epoch: 1370 Training Loss: 0.7206543408247075 Test Loss: 0.7469015239197531\n",
      "Epoch: 1380 Training Loss: 0.7203211392567829 Test Loss: 0.7467024061414931\n",
      "Epoch: 1390 Training Loss: 0.7199907002418992 Test Loss: 0.7465020952891911\n",
      "Epoch: 1400 Training Loss: 0.7196581265269384 Test Loss: 0.746321187588413\n",
      "Epoch: 1410 Training Loss: 0.7193235436824933 Test Loss: 0.7461507035871592\n",
      "Epoch: 1420 Training Loss: 0.718988772482159 Test Loss: 0.7459784613715278\n",
      "Epoch: 1430 Training Loss: 0.7186558220554198 Test Loss: 0.7457879462850437\n",
      "Epoch: 1440 Training Loss: 0.7183284595200592 Test Loss: 0.7456059083035944\n",
      "Epoch: 1450 Training Loss: 0.71800298054359 Test Loss: 0.7454359266493056\n",
      "Epoch: 1460 Training Loss: 0.7176795734819018 Test Loss: 0.7452388810522762\n",
      "Epoch: 1470 Training Loss: 0.7173579871938087 Test Loss: 0.745028837227527\n",
      "Epoch: 1480 Training Loss: 0.7170362125498264 Test Loss: 0.7448296566558964\n",
      "Epoch: 1490 Training Loss: 0.7167111730704321 Test Loss: 0.7446232548466435\n",
      "Epoch: 1500 Training Loss: 0.7163833082527002 Test Loss: 0.7444177321445795\n",
      "Epoch: 1510 Training Loss: 0.7160526180966311 Test Loss: 0.7442361965100952\n",
      "Epoch: 1520 Training Loss: 0.715719981596374 Test Loss: 0.7440584284778485\n",
      "Epoch: 1530 Training Loss: 0.7153818199900347 Test Loss: 0.743873376414609\n",
      "Epoch: 1540 Training Loss: 0.715039514554134 Test Loss: 0.7436763936109504\n",
      "Epoch: 1550 Training Loss: 0.7146962673387874 Test Loss: 0.7434644031917117\n",
      "Epoch: 1560 Training Loss: 0.7143490018644721 Test Loss: 0.7432281117380402\n",
      "Epoch: 1570 Training Loss: 0.7140012341077858 Test Loss: 0.7430290567531507\n",
      "Epoch: 1580 Training Loss: 0.713652901283432 Test Loss: 0.7428171919206533\n",
      "Epoch: 1590 Training Loss: 0.7133017431207407 Test Loss: 0.7425828470614712\n",
      "Epoch: 1600 Training Loss: 0.7129517150933843 Test Loss: 0.7423882504058964\n",
      "Epoch: 1610 Training Loss: 0.7126008080718786 Test Loss: 0.7422020680619856\n",
      "Epoch: 1620 Training Loss: 0.7122432458089559 Test Loss: 0.7419740653332368\n",
      "Epoch: 1630 Training Loss: 0.7118835488459561 Test Loss: 0.7417250896186985\n",
      "Epoch: 1640 Training Loss: 0.711527242288961 Test Loss: 0.7414879818512089\n",
      "Epoch: 1650 Training Loss: 0.7111730704320433 Test Loss: 0.7412518159842786\n",
      "Epoch: 1660 Training Loss: 0.7108247376076894 Test Loss: 0.7410370626567323\n",
      "Epoch: 1670 Training Loss: 0.7104833739512344 Test Loss: 0.740830221293885\n",
      "Epoch: 1680 Training Loss: 0.7101471586890832 Test Loss: 0.7406454831974987\n",
      "Epoch: 1690 Training Loss: 0.709808997082744 Test Loss: 0.7404482492203575\n",
      "Epoch: 1700 Training Loss: 0.7094685752057348 Test Loss: 0.7402139043611754\n",
      "Epoch: 1710 Training Loss: 0.7091282788993185 Test Loss: 0.7399713335704411\n",
      "Epoch: 1720 Training Loss: 0.7087908707165359 Test Loss: 0.7397389980991191\n",
      "Epoch: 1730 Training Loss: 0.7084549693808666 Test Loss: 0.7395354847849152\n",
      "Epoch: 1740 Training Loss: 0.7081202609658287 Test Loss: 0.7393306528099279\n",
      "Epoch: 1750 Training Loss: 0.7077809064388582 Test Loss: 0.739104219915445\n",
      "Epoch: 1760 Training Loss: 0.7074369685852514 Test Loss: 0.7388899689348637\n",
      "Epoch: 1770 Training Loss: 0.7070929051610518 Test Loss: 0.7386750900205762\n",
      "Epoch: 1780 Training Loss: 0.7067505369398547 Test Loss: 0.7384916077916024\n",
      "Epoch: 1790 Training Loss: 0.7064064735156551 Test Loss: 0.7383361941992991\n",
      "Epoch: 1800 Training Loss: 0.706059710323711 Test Loss: 0.7381854273164223\n",
      "Epoch: 1810 Training Loss: 0.7057046594726437 Test Loss: 0.7380113012996721\n",
      "Epoch: 1820 Training Loss: 0.7053496086215765 Test Loss: 0.7378209117999293\n",
      "Epoch: 1830 Training Loss: 0.7049956879058441 Test Loss: 0.7376828291779193\n",
      "Epoch: 1840 Training Loss: 0.7046484224315289 Test Loss: 0.7375318739149306\n",
      "Epoch: 1850 Training Loss: 0.7043107631075608 Test Loss: 0.7373683599778164\n",
      "Epoch: 1860 Training Loss: 0.7039804496632699 Test Loss: 0.7372095555434992\n",
      "Epoch: 1870 Training Loss: 0.7036486921571622 Test Loss: 0.7370975321702996\n",
      "Epoch: 1880 Training Loss: 0.7033188809952424 Test Loss: 0.7369831854423868\n",
      "Epoch: 1890 Training Loss: 0.7029889442627298 Test Loss: 0.7369008005401234\n",
      "Epoch: 1900 Training Loss: 0.7026607655185161 Test Loss: 0.7367728276507844\n",
      "Epoch: 1910 Training Loss: 0.7023332774125627 Test Loss: 0.7366811493296682\n",
      "Epoch: 1920 Training Loss: 0.7020038429624212 Test Loss: 0.7365441969883295\n",
      "Epoch: 1930 Training Loss: 0.7016730272357593 Test Loss: 0.7364337434493956\n",
      "Epoch: 1940 Training Loss: 0.7013403907355021 Test Loss: 0.73631110799656\n",
      "Epoch: 1950 Training Loss: 0.7010106423588788 Test Loss: 0.736175537109375\n",
      "Epoch: 1960 Training Loss: 0.7006839704617783 Test Loss: 0.7360519597559799\n",
      "Epoch: 1970 Training Loss: 0.7003572985646779 Test Loss: 0.7359626047895769\n",
      "Epoch: 1980 Training Loss: 0.7000371563384017 Test Loss: 0.7359123700930749\n",
      "Epoch: 1990 Training Loss: 0.6997252389859522 Test Loss: 0.735830236364294\n",
      "Epoch: 2000 Training Loss: 0.6994174026777678 Test Loss: 0.7357506771637089\n",
      "Epoch: 2010 Training Loss: 0.6991093780136942 Test Loss: 0.7356800346217528\n",
      "Epoch: 2020 Training Loss: 0.6988041786879581 Test Loss: 0.7356123433682163\n",
      "Epoch: 2030 Training Loss: 0.698500611779928 Test Loss: 0.7355663786209169\n",
      "Epoch: 2040 Training Loss: 0.6982026327632763 Test Loss: 0.7354936638977302\n",
      "Epoch: 2050 Training Loss: 0.6979074162996657 Test Loss: 0.7354004157423483\n",
      "Epoch: 2060 Training Loss: 0.6976159041685418 Test Loss: 0.7353570255232446\n",
      "Epoch: 2070 Training Loss: 0.6973292892905362 Test Loss: 0.7353383130987976\n",
      "Epoch: 2080 Training Loss: 0.697047948377427 Test Loss: 0.7352856294608411\n",
      "Epoch: 2090 Training Loss: 0.6967672353172817 Test Loss: 0.7352517210407021\n",
      "Epoch: 2100 Training Loss: 0.6964948727015559 Test Loss: 0.7352102774160879\n",
      "Epoch: 2110 Training Loss: 0.696224456430018 Test Loss: 0.7351776248633616\n",
      "Epoch: 2120 Training Loss: 0.6959556725761862 Test Loss: 0.7350902792848186\n",
      "Epoch: 2130 Training Loss: 0.6956872654341327 Test Loss: 0.7350243462456597\n",
      "Epoch: 2140 Training Loss: 0.6954194233597467 Test Loss: 0.7349582876197596\n",
      "Epoch: 2150 Training Loss: 0.6951520207824354 Test Loss: 0.7348948663154257\n",
      "Epoch: 2160 Training Loss: 0.6948822951491578 Test Loss: 0.7348352126133295\n",
      "Epoch: 2170 Training Loss: 0.6946138252218079 Test Loss: 0.7347775682990934\n",
      "Epoch: 2180 Training Loss: 0.694346171503311 Test Loss: 0.7346401136107896\n",
      "Epoch: 2190 Training Loss: 0.694080589699595 Test Loss: 0.7345982932259516\n",
      "Epoch: 2200 Training Loss: 0.6938177704489199 Test Loss: 0.7345974141187629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2210 Training Loss: 0.6935576509659894 Test Loss: 0.7345363161691423\n",
      "Epoch: 2220 Training Loss: 0.6933018008832134 Test Loss: 0.7344683109487525\n",
      "Epoch: 2230 Training Loss: 0.6930489644946638 Test Loss: 0.734442440080054\n",
      "Epoch: 2240 Training Loss: 0.6927990790150443 Test Loss: 0.7344003057283629\n",
      "Epoch: 2250 Training Loss: 0.6925549697826926 Test Loss: 0.7343869307404193\n",
      "Epoch: 2260 Training Loss: 0.6923124301827505 Test Loss: 0.7343460522561407\n",
      "Epoch: 2270 Training Loss: 0.6920735321299987 Test Loss: 0.7343000247154707\n",
      "Epoch: 2280 Training Loss: 0.6918363920655458 Test Loss: 0.7343012805828832\n",
      "Epoch: 2290 Training Loss: 0.6916006332776135 Test Loss: 0.7342905429165059\n",
      "Epoch: 2300 Training Loss: 0.6913608562307123 Test Loss: 0.7343050481851209\n",
      "Epoch: 2310 Training Loss: 0.691119258410216 Test Loss: 0.7342543111416538\n",
      "Epoch: 2320 Training Loss: 0.6908752119631606 Test Loss: 0.7341885664826068\n",
      "Epoch: 2330 Training Loss: 0.6906336141426642 Test Loss: 0.7342003716362847\n",
      "Epoch: 2340 Training Loss: 0.6903896304809052 Test Loss: 0.7341745007675862\n",
      "Epoch: 2350 Training Loss: 0.6901440144014401 Test Loss: 0.734120749642329\n",
      "Epoch: 2360 Training Loss: 0.6899006585926449 Test Loss: 0.7340591493457433\n",
      "Epoch: 2370 Training Loss: 0.6896596258398161 Test Loss: 0.734012493871367\n",
      "Epoch: 2380 Training Loss: 0.6894184047310981 Test Loss: 0.733945556138278\n",
      "Epoch: 2390 Training Loss: 0.6891843411461682 Test Loss: 0.7339764504766269\n",
      "Epoch: 2400 Training Loss: 0.6889612022028095 Test Loss: 0.7340097309630594\n",
      "Epoch: 2410 Training Loss: 0.6887472926980198 Test Loss: 0.7340385531201775\n",
      "Epoch: 2420 Training Loss: 0.6885384688022373 Test Loss: 0.7340521792816036\n",
      "Epoch: 2430 Training Loss: 0.6883359234360936 Test Loss: 0.7340727755071695\n",
      "Epoch: 2440 Training Loss: 0.6881390915319211 Test Loss: 0.7340907972045396\n",
      "Epoch: 2450 Training Loss: 0.6879451477513823 Test Loss: 0.7341307965816294\n",
      "Epoch: 2460 Training Loss: 0.6877492576266555 Test Loss: 0.7341787707167888\n",
      "Epoch: 2470 Training Loss: 0.6875517978695191 Test Loss: 0.7341962272738233\n",
      "Epoch: 2480 Training Loss: 0.6873560333153851 Test Loss: 0.734229319380144\n",
      "Epoch: 2490 Training Loss: 0.6871646009467018 Test Loss: 0.7342373569315843\n",
      "Epoch: 2500 Training Loss: 0.6869695898161244 Test Loss: 0.7341911410108025\n",
      "Epoch: 2510 Training Loss: 0.6867711882795422 Test Loss: 0.7341228846169303\n",
      "Epoch: 2520 Training Loss: 0.6865698986193263 Test Loss: 0.7340512373810443\n",
      "Epoch: 2530 Training Loss: 0.6863691112414813 Test Loss: 0.7340054610138568\n",
      "Epoch: 2540 Training Loss: 0.6861726560490871 Test Loss: 0.7340362925588348\n",
      "Epoch: 2550 Training Loss: 0.6859811608951074 Test Loss: 0.7340695730452675\n",
      "Epoch: 2560 Training Loss: 0.6857897285264241 Test Loss: 0.7340271875200939\n",
      "Epoch: 2570 Training Loss: 0.6855987984401118 Test Loss: 0.7340691334916731\n",
      "Epoch: 2580 Training Loss: 0.6854059847949081 Test Loss: 0.7340627285678691\n",
      "Epoch: 2590 Training Loss: 0.6852110992349235 Test Loss: 0.7340364809389468\n",
      "Epoch: 2600 Training Loss: 0.6850189134426835 Test Loss: 0.7340099821365419\n",
      "Epoch: 2610 Training Loss: 0.6848291762770027 Test Loss: 0.7340436393831983\n",
      "Epoch: 2620 Training Loss: 0.684642703946734 Test Loss: 0.7341073746543852\n",
      "Epoch: 2630 Training Loss: 0.6844572989665038 Test Loss: 0.7342100418153613\n",
      "Epoch: 2640 Training Loss: 0.684274719324611 Test Loss: 0.7343037295243378\n",
      "Epoch: 2650 Training Loss: 0.6840908211914941 Test Loss: 0.7343764442475245\n",
      "Epoch: 2660 Training Loss: 0.6839050394994857 Test Loss: 0.7344843232582626\n",
      "Epoch: 2670 Training Loss: 0.6837225854281856 Test Loss: 0.7344909165621785\n",
      "Epoch: 2680 Training Loss: 0.6835391895774399 Test Loss: 0.7345327369470165\n",
      "Epoch: 2690 Training Loss: 0.6833551030884338 Test Loss: 0.7346170684437693\n",
      "Epoch: 2700 Training Loss: 0.6831705143170567 Test Loss: 0.7346816828221451\n",
      "Epoch: 2710 Training Loss: 0.6829878091045711 Test Loss: 0.7347799544471773\n",
      "Epoch: 2720 Training Loss: 0.6828052922479748 Test Loss: 0.7349029666602366\n",
      "Epoch: 2730 Training Loss: 0.6826256635150122 Test Loss: 0.7349984753769612\n",
      "Epoch: 2740 Training Loss: 0.6824479183409412 Test Loss: 0.7350720064139661\n",
      "Epoch: 2750 Training Loss: 0.6822680384667931 Test Loss: 0.7351319112895448\n",
      "Epoch: 2760 Training Loss: 0.6820906072192041 Test Loss: 0.7351337950906636\n",
      "Epoch: 2770 Training Loss: 0.6819160640952489 Test Loss: 0.7351444699636702\n",
      "Epoch: 2780 Training Loss: 0.6817414581859972 Test Loss: 0.7351989118160044\n",
      "Epoch: 2790 Training Loss: 0.6815676684855986 Test Loss: 0.7352126007708012\n",
      "Epoch: 2800 Training Loss: 0.681391116232159 Test Loss: 0.735277717496142\n",
      "Epoch: 2810 Training Loss: 0.6812136849845699 Test Loss: 0.73533655488442\n",
      "Epoch: 2820 Training Loss: 0.6810385140076508 Test Loss: 0.7354203212408372\n",
      "Epoch: 2830 Training Loss: 0.6808647243072522 Test Loss: 0.7355022665895061\n",
      "Epoch: 2840 Training Loss: 0.6806925042392632 Test Loss: 0.735552375699267\n",
      "Epoch: 2850 Training Loss: 0.6805222933007586 Test Loss: 0.7355746045524691\n",
      "Epoch: 2860 Training Loss: 0.680353526424071 Test Loss: 0.7356611966105645\n",
      "Epoch: 2870 Training Loss: 0.6801833782708628 Test Loss: 0.7357212270728846\n",
      "Epoch: 2880 Training Loss: 0.6800104047793172 Test Loss: 0.7357644917052469\n",
      "Epoch: 2890 Training Loss: 0.6798383102819211 Test Loss: 0.7357548215261702\n",
      "Epoch: 2900 Training Loss: 0.6796660902139321 Test Loss: 0.7357781806600437\n",
      "Epoch: 2910 Training Loss: 0.6794938701459432 Test Loss: 0.7358324969256366\n",
      "Epoch: 2920 Training Loss: 0.6793202688014337 Test Loss: 0.7358155427155672\n",
      "Epoch: 2930 Training Loss: 0.679145976818664 Test Loss: 0.735823140713413\n",
      "Epoch: 2940 Training Loss: 0.6789745101742317 Test Loss: 0.7358340667599023\n",
      "Epoch: 2950 Training Loss: 0.6788037341680596 Test Loss: 0.7358446160461677\n",
      "Epoch: 2960 Training Loss: 0.6786298816823647 Test Loss: 0.7358663425524048\n",
      "Epoch: 2970 Training Loss: 0.6784518853671081 Test Loss: 0.7359153213814943\n",
      "Epoch: 2980 Training Loss: 0.6782776561696349 Test Loss: 0.735985022022891\n",
      "Epoch: 2990 Training Loss: 0.6781019829103446 Test Loss: 0.7360502015416024\n",
      "Epoch: 3000 Training Loss: 0.6779271258599074 Test Loss: 0.7361555688175154\n",
      "Epoch: 3010 Training Loss: 0.6777552197184004 Test Loss: 0.7362112037438915\n",
      "Epoch: 3020 Training Loss: 0.6775838786445609 Test Loss: 0.7363095381622943\n",
      "Epoch: 3030 Training Loss: 0.6774148606266877 Test Loss: 0.7364371970847801\n",
      "Epoch: 3040 Training Loss: 0.6772491074442266 Test Loss: 0.7365304452401621\n",
      "Epoch: 3050 Training Loss: 0.6770834798323583 Test Loss: 0.7365243542832112\n",
      "Epoch: 3060 Training Loss: 0.6769190451411212 Test Loss: 0.7365853894394612\n",
      "Epoch: 3070 Training Loss: 0.6767515967556577 Test Loss: 0.7366277121712641\n",
      "Epoch: 3080 Training Loss: 0.6765828926642664 Test Loss: 0.7367087156193737\n",
      "Epoch: 3090 Training Loss: 0.6764099819580173 Test Loss: 0.736731446819541\n",
      "Epoch: 3100 Training Loss: 0.6762349365516909 Test Loss: 0.7367936122564622\n",
      "Epoch: 3110 Training Loss: 0.6760609584954032 Test Loss: 0.7369794178401492\n",
      "Epoch: 3120 Training Loss: 0.675882711038961 Test Loss: 0.7370930738409851\n",
      "Epoch: 3130 Training Loss: 0.6756994407588081 Test Loss: 0.7372518154819316\n",
      "Epoch: 3140 Training Loss: 0.6755132195697249 Test Loss: 0.7373147344393004\n",
      "Epoch: 3150 Training Loss: 0.675318459580333 Test Loss: 0.7373410448615934\n",
      "Epoch: 3160 Training Loss: 0.6751237623762376 Test Loss: 0.7374055336532279\n",
      "Epoch: 3170 Training Loss: 0.6749320788663687 Test Loss: 0.7374945118594072\n",
      "Epoch: 3180 Training Loss: 0.6747435974066157 Test Loss: 0.7375779642489712\n",
      "Epoch: 3190 Training Loss: 0.6745588830646457 Test Loss: 0.7376830175580311\n",
      "Epoch: 3200 Training Loss: 0.6743777474845699 Test Loss: 0.7377191865395126\n",
      "Epoch: 3210 Training Loss: 0.674195795695641 Test Loss: 0.7377718701774691\n",
      "Epoch: 3220 Training Loss: 0.6740124626301915 Test Loss: 0.7377714934172453\n",
      "Epoch: 3230 Training Loss: 0.6738266809381831 Test Loss: 0.7378939404899691\n",
      "Epoch: 3240 Training Loss: 0.6736439757256976 Test Loss: 0.7379902655205118\n",
      "Epoch: 3250 Training Loss: 0.673461270513212 Test Loss: 0.7381218804253472\n",
      "Epoch: 3260 Training Loss: 0.6732832741979555 Test Loss: 0.7382435111842528\n",
      "Epoch: 3270 Training Loss: 0.6731096100681496 Test Loss: 0.7383554717640818\n",
      "Epoch: 3280 Training Loss: 0.6729392735590524 Test Loss: 0.7384786723572531\n",
      "Epoch: 3290 Training Loss: 0.6727669279204707 Test Loss: 0.7386548077618634\n",
      "Epoch: 3300 Training Loss: 0.6725949589936672 Test Loss: 0.738868493602109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3310 Training Loss: 0.6724256270493121 Test Loss: 0.739098631305459\n",
      "Epoch: 3320 Training Loss: 0.672255981178475 Test Loss: 0.7392531029972029\n",
      "Epoch: 3330 Training Loss: 0.6720890350753825 Test Loss: 0.7393727871616191\n",
      "Epoch: 3340 Training Loss: 0.6719209588369551 Test Loss: 0.7395059719007202\n",
      "Epoch: 3350 Training Loss: 0.6717503084013758 Test Loss: 0.7395897382571374\n",
      "Epoch: 3360 Training Loss: 0.6715777116216086 Test Loss: 0.7396692346643519\n",
      "Epoch: 3370 Training Loss: 0.6714068728301401 Test Loss: 0.7397367375377765\n",
      "Epoch: 3380 Training Loss: 0.6712365363210429 Test Loss: 0.7398340044688786\n",
      "Epoch: 3390 Training Loss: 0.6710613025588273 Test Loss: 0.7399170173048483\n",
      "Epoch: 3400 Training Loss: 0.6708858176554262 Test Loss: 0.7399967020921746\n",
      "Epoch: 3410 Training Loss: 0.6707117768138421 Test Loss: 0.7401420687451775\n",
      "Epoch: 3420 Training Loss: 0.6705379243281471 Test Loss: 0.7402274049358604\n",
      "Epoch: 3430 Training Loss: 0.6703633184188955 Test Loss: 0.7403183925298997\n",
      "Epoch: 3440 Training Loss: 0.670186954521345 Test Loss: 0.7403640433103459\n",
      "Epoch: 3450 Training Loss: 0.6700074513589752 Test Loss: 0.7404368208269033\n",
      "Epoch: 3460 Training Loss: 0.6698279481966054 Test Loss: 0.7404579193994342\n",
      "Epoch: 3470 Training Loss: 0.6696521493667223 Test Loss: 0.7406415900085198\n",
      "Epoch: 3480 Training Loss: 0.6694798037281406 Test Loss: 0.7407668627829218\n",
      "Epoch: 3490 Training Loss: 0.6693112252073422 Test Loss: 0.7408878028147505\n",
      "Epoch: 3500 Training Loss: 0.6691460998778449 Test Loss: 0.7410199200665509\n",
      "Epoch: 3510 Training Loss: 0.6689866252250225 Test Loss: 0.7411107192804784\n",
      "Epoch: 3520 Training Loss: 0.6688338685989135 Test Loss: 0.7412724122098444\n",
      "Epoch: 3530 Training Loss: 0.6686864487229973 Test Loss: 0.7413882031852816\n",
      "Epoch: 3540 Training Loss: 0.6685400334118233 Test Loss: 0.7415623919954025\n",
      "Epoch: 3550 Training Loss: 0.6683936808859458 Test Loss: 0.7416972093621399\n",
      "Epoch: 3560 Training Loss: 0.6682485840659959 Test Loss: 0.7418209123022762\n",
      "Epoch: 3570 Training Loss: 0.6681041150990099 Test Loss: 0.7419515225131816\n",
      "Epoch: 3580 Training Loss: 0.6679576997878359 Test Loss: 0.7420342213822981\n",
      "Epoch: 3590 Training Loss: 0.6678119123296258 Test Loss: 0.7421437958140432\n",
      "Epoch: 3600 Training Loss: 0.6676691385656423 Test Loss: 0.7423006536538709\n",
      "Epoch: 3610 Training Loss: 0.6675265531575479 Test Loss: 0.7424897872862012\n",
      "Epoch: 3620 Training Loss: 0.6673829631847114 Test Loss: 0.74261474609375\n",
      "Epoch: 3630 Training Loss: 0.667239247641282 Test Loss: 0.7427820276331019\n",
      "Epoch: 3640 Training Loss: 0.6670982946508937 Test Loss: 0.7429800151306906\n",
      "Epoch: 3650 Training Loss: 0.6669588485076187 Test Loss: 0.7431562133286715\n",
      "Epoch: 3660 Training Loss: 0.6668124959817411 Test Loss: 0.7433344837078832\n",
      "Epoch: 3670 Training Loss: 0.6666650133205285 Test Loss: 0.7435145750948431\n",
      "Epoch: 3680 Training Loss: 0.6665111265590845 Test Loss: 0.7436866917237333\n",
      "Epoch: 3690 Training Loss: 0.6663551050975633 Test Loss: 0.743818746182163\n",
      "Epoch: 3700 Training Loss: 0.666200025415488 Test Loss: 0.7439822601192773\n",
      "Epoch: 3710 Training Loss: 0.6660498429865308 Test Loss: 0.74411060976884\n",
      "Epoch: 3720 Training Loss: 0.6659046833812846 Test Loss: 0.7441607188786008\n",
      "Epoch: 3730 Training Loss: 0.6657651116674167 Test Loss: 0.7442407176327803\n",
      "Epoch: 3740 Training Loss: 0.6656291815007394 Test Loss: 0.7443337146146798\n",
      "Epoch: 3750 Training Loss: 0.6654953232488428 Test Loss: 0.7444449216740612\n",
      "Epoch: 3760 Training Loss: 0.6653634113411341 Test Loss: 0.7445183271243249\n",
      "Epoch: 3770 Training Loss: 0.6652326295687605 Test Loss: 0.7445462701742541\n",
      "Epoch: 3780 Training Loss: 0.6651033546435 Test Loss: 0.7446123288001543\n",
      "Epoch: 3790 Training Loss: 0.6649761516330205 Test Loss: 0.744642218444573\n",
      "Epoch: 3800 Training Loss: 0.6648459349283142 Test Loss: 0.7447408040364584\n",
      "Epoch: 3810 Training Loss: 0.6647123278176031 Test Loss: 0.7447498462818287\n",
      "Epoch: 3820 Training Loss: 0.664579348559856 Test Loss: 0.7447549953382202\n",
      "Epoch: 3830 Training Loss: 0.6644472482962582 Test Loss: 0.7449264840334041\n",
      "Epoch: 3840 Training Loss: 0.6643152736032532 Test Loss: 0.7450341746640303\n",
      "Epoch: 3850 Training Loss: 0.6641838011926192 Test Loss: 0.7452020213437178\n",
      "Epoch: 3860 Training Loss: 0.6640535217026167 Test Loss: 0.7452378135649755\n",
      "Epoch: 3870 Training Loss: 0.6639217981507972 Test Loss: 0.7453178123191551\n",
      "Epoch: 3880 Training Loss: 0.6637895095313103 Test Loss: 0.7453964924125515\n",
      "Epoch: 3890 Training Loss: 0.6636575976236017 Test Loss: 0.7453997576678241\n",
      "Epoch: 3900 Training Loss: 0.663524053298187 Test Loss: 0.7454618603113747\n",
      "Epoch: 3910 Training Loss: 0.6633881859168059 Test Loss: 0.7455264118963799\n",
      "Epoch: 3920 Training Loss: 0.6632520673942395 Test Loss: 0.7456879792390047\n",
      "Epoch: 3930 Training Loss: 0.6631170790070079 Test Loss: 0.7457792180065265\n",
      "Epoch: 3940 Training Loss: 0.6629804582020702 Test Loss: 0.7458062191558964\n",
      "Epoch: 3950 Training Loss: 0.6628430211882795 Test Loss: 0.7459041768140754\n",
      "Epoch: 3960 Training Loss: 0.662706588739231 Test Loss: 0.7460088533629116\n",
      "Epoch: 3970 Training Loss: 0.6625712236402211 Test Loss: 0.7461017247580697\n",
      "Epoch: 3980 Training Loss: 0.6624391233766234 Test Loss: 0.7461298561881109\n",
      "Epoch: 3990 Training Loss: 0.6623076509659894 Test Loss: 0.7462688807106803\n",
      "Epoch: 4000 Training Loss: 0.6621790038936929 Test Loss: 0.746369412897055\n",
      "Epoch: 4010 Training Loss: 0.6620492266860615 Test Loss: 0.7464201499405221\n",
      "Epoch: 4020 Training Loss: 0.6619174403489456 Test Loss: 0.7465174796649948\n",
      "Epoch: 4030 Training Loss: 0.661785528441237 Test Loss: 0.7466345265078447\n",
      "Epoch: 4040 Training Loss: 0.6616507284098946 Test Loss: 0.7467458591539673\n",
      "Epoch: 4050 Training Loss: 0.6615151749549955 Test Loss: 0.7468154970019933\n",
      "Epoch: 4060 Training Loss: 0.6613800609971712 Test Loss: 0.7468401120032793\n",
      "Epoch: 4070 Training Loss: 0.6612463911011637 Test Loss: 0.7470184451758616\n",
      "Epoch: 4080 Training Loss: 0.661111214358043 Test Loss: 0.7470326364776234\n",
      "Epoch: 4090 Training Loss: 0.66097277277951 Test Loss: 0.7470887737509645\n",
      "Epoch: 4100 Training Loss: 0.6608362147598689 Test Loss: 0.7471349268783758\n",
      "Epoch: 4110 Training Loss: 0.6606973336842613 Test Loss: 0.7472116603772827\n",
      "Epoch: 4120 Training Loss: 0.660558013111579 Test Loss: 0.7472855681745113\n",
      "Epoch: 4130 Training Loss: 0.6604206388830848 Test Loss: 0.7473707159850823\n",
      "Epoch: 4140 Training Loss: 0.6602811927398097 Test Loss: 0.7475226759420011\n",
      "Epoch: 4150 Training Loss: 0.6601472089173203 Test Loss: 0.747701574254919\n",
      "Epoch: 4160 Training Loss: 0.6600134762360165 Test Loss: 0.7479518058368698\n",
      "Epoch: 4170 Training Loss: 0.6598779227811173 Test Loss: 0.7480888209715792\n",
      "Epoch: 4180 Training Loss: 0.6597432483203678 Test Loss: 0.7481130592126415\n",
      "Epoch: 4190 Training Loss: 0.6596146640333677 Test Loss: 0.7482714868867348\n",
      "Epoch: 4200 Training Loss: 0.6594892817964833 Test Loss: 0.7483545625160751\n",
      "Epoch: 4210 Training Loss: 0.6593689223833098 Test Loss: 0.7484545295621142\n",
      "Epoch: 4220 Training Loss: 0.6592549670703678 Test Loss: 0.7486340558087384\n",
      "Epoch: 4230 Training Loss: 0.6591399444073872 Test Loss: 0.7487615263511124\n",
      "Epoch: 4240 Training Loss: 0.6590275587268548 Test Loss: 0.7488901899675283\n",
      "Epoch: 4250 Training Loss: 0.6589178728140671 Test Loss: 0.7489911617074975\n",
      "Epoch: 4260 Training Loss: 0.6588091914660216 Test Loss: 0.7490681463798868\n",
      "Epoch: 4270 Training Loss: 0.6587023308915713 Test Loss: 0.7492163387345679\n",
      "Epoch: 4280 Training Loss: 0.6585949052494535 Test Loss: 0.7492929466467335\n",
      "Epoch: 4290 Training Loss: 0.6584876679632249 Test Loss: 0.7493108427573624\n",
      "Epoch: 4300 Training Loss: 0.658379740038736 Test Loss: 0.7493478280526621\n",
      "Epoch: 4310 Training Loss: 0.658273570102546 Test Loss: 0.7494167751736112\n",
      "Epoch: 4320 Training Loss: 0.6581648887545004 Test Loss: 0.7494419553152327\n",
      "Epoch: 4330 Training Loss: 0.6580547005593417 Test Loss: 0.7493936044198496\n",
      "Epoch: 4340 Training Loss: 0.6579487817643371 Test Loss: 0.7494872293354552\n",
      "Epoch: 4350 Training Loss: 0.6578463789459303 Test Loss: 0.7495249681512024\n",
      "Epoch: 4360 Training Loss: 0.6577455457599332 Test Loss: 0.749588263868795\n",
      "Epoch: 4370 Training Loss: 0.6576440219356757 Test Loss: 0.7495275426793981\n",
      "Epoch: 4380 Training Loss: 0.6575380403553748 Test Loss: 0.7496081065739133\n",
      "Epoch: 4390 Training Loss: 0.6574316192779992 Test Loss: 0.7496661276483731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4400 Training Loss: 0.657322812359361 Test Loss: 0.7498221691743827\n",
      "Epoch: 4410 Training Loss: 0.6572153239319468 Test Loss: 0.7499471279819316\n",
      "Epoch: 4420 Training Loss: 0.6571090912104603 Test Loss: 0.7500163262763632\n",
      "Epoch: 4430 Training Loss: 0.6570041141949017 Test Loss: 0.7500064677171746\n",
      "Epoch: 4440 Training Loss: 0.6569022136588659 Test Loss: 0.7500594025286137\n",
      "Epoch: 4450 Training Loss: 0.6568001247669409 Test Loss: 0.7501395268695344\n",
      "Epoch: 4460 Training Loss: 0.6567013007104282 Test Loss: 0.7502602157278807\n",
      "Epoch: 4470 Training Loss: 0.6566020371568407 Test Loss: 0.7503352538057806\n",
      "Epoch: 4480 Training Loss: 0.6565054733709978 Test Loss: 0.7505295364945023\n",
      "Epoch: 4490 Training Loss: 0.6564094746528224 Test Loss: 0.7506350921505273\n",
      "Epoch: 4500 Training Loss: 0.6563121574434229 Test Loss: 0.7507478690441743\n",
      "Epoch: 4510 Training Loss: 0.6562133961722065 Test Loss: 0.7508205837673612\n",
      "Epoch: 4520 Training Loss: 0.6561148232568793 Test Loss: 0.7508729534384645\n",
      "Epoch: 4530 Training Loss: 0.6560127971502507 Test Loss: 0.7509359979825746\n",
      "Epoch: 4540 Training Loss: 0.6559086991288414 Test Loss: 0.7509725437242798\n",
      "Epoch: 4550 Training Loss: 0.6558034709720972 Test Loss: 0.7510566240475501\n",
      "Epoch: 4560 Training Loss: 0.6556956058329048 Test Loss: 0.7510766551327803\n",
      "Epoch: 4570 Training Loss: 0.6555822783729266 Test Loss: 0.7511384438094779\n",
      "Epoch: 4580 Training Loss: 0.655470018262987 Test Loss: 0.75123671543451\n",
      "Epoch: 4590 Training Loss: 0.6553558118088595 Test Loss: 0.7515082967624743\n",
      "Epoch: 4600 Training Loss: 0.655240475219397 Test Loss: 0.751685374067644\n",
      "Epoch: 4610 Training Loss: 0.6551315427301658 Test Loss: 0.751820756574717\n",
      "Epoch: 4620 Training Loss: 0.6550300189059084 Test Loss: 0.7519614765182935\n",
      "Epoch: 4630 Training Loss: 0.6549303786405426 Test Loss: 0.7519758562001672\n",
      "Epoch: 4640 Training Loss: 0.6548265317603189 Test Loss: 0.7520836724175347\n",
      "Epoch: 4650 Training Loss: 0.6547187921917191 Test Loss: 0.7520799676086677\n",
      "Epoch: 4660 Training Loss: 0.654610801481934 Test Loss: 0.7522983629517104\n",
      "Epoch: 4670 Training Loss: 0.6545066406752282 Test Loss: 0.7524519555362654\n",
      "Epoch: 4680 Training Loss: 0.6544062469863058 Test Loss: 0.752539866255144\n",
      "Epoch: 4690 Training Loss: 0.65431181790054 Test Loss: 0.752764792108732\n",
      "Epoch: 4700 Training Loss: 0.6542222860678925 Test Loss: 0.7528742409537359\n",
      "Epoch: 4710 Training Loss: 0.6541362074265462 Test Loss: 0.7529520419399434\n",
      "Epoch: 4720 Training Loss: 0.6540489358645686 Test Loss: 0.7530582883230452\n",
      "Epoch: 4730 Training Loss: 0.653957269331844 Test Loss: 0.753245161394033\n",
      "Epoch: 4740 Training Loss: 0.6538624007490035 Test Loss: 0.7533330721129116\n",
      "Epoch: 4750 Training Loss: 0.6537663392455317 Test Loss: 0.7535007932058577\n",
      "Epoch: 4760 Training Loss: 0.6536750494245853 Test Loss: 0.753610681604456\n",
      "Epoch: 4770 Training Loss: 0.6535850153095667 Test Loss: 0.7537773980034722\n",
      "Epoch: 4780 Training Loss: 0.6534961741151794 Test Loss: 0.7539795298635223\n",
      "Epoch: 4790 Training Loss: 0.6534065167119391 Test Loss: 0.7541275966314622\n",
      "Epoch: 4800 Training Loss: 0.6533150385351035 Test Loss: 0.7542289451316551\n",
      "Epoch: 4810 Training Loss: 0.6532155238403304 Test Loss: 0.7543266516163516\n",
      "Epoch: 4820 Training Loss: 0.653114627869037 Test Loss: 0.7544188322844328\n",
      "Epoch: 4830 Training Loss: 0.6530136063271506 Test Loss: 0.7546070240162037\n",
      "Epoch: 4840 Training Loss: 0.6529124592146714 Test Loss: 0.7548641000755529\n",
      "Epoch: 4850 Training Loss: 0.6528108098198213 Test Loss: 0.7550441914625129\n",
      "Epoch: 4860 Training Loss: 0.6527113579103446 Test Loss: 0.7551820229110404\n",
      "Epoch: 4870 Training Loss: 0.6526059413977112 Test Loss: 0.7553632445786715\n",
      "Epoch: 4880 Training Loss: 0.6524977623320368 Test Loss: 0.7555070413974087\n",
      "Epoch: 4890 Training Loss: 0.6523898971928442 Test Loss: 0.7556787184727045\n",
      "Epoch: 4900 Training Loss: 0.6522852341037675 Test Loss: 0.7557744783629116\n",
      "Epoch: 4910 Training Loss: 0.6521776828910569 Test Loss: 0.7557649965639468\n",
      "Epoch: 4920 Training Loss: 0.6520726430902019 Test Loss: 0.7558096426504629\n",
      "Epoch: 4930 Training Loss: 0.6519698635600167 Test Loss: 0.7558900809582368\n",
      "Epoch: 4940 Training Loss: 0.6518690303740196 Test Loss: 0.7560270960929463\n",
      "Epoch: 4950 Training Loss: 0.6517696412498393 Test Loss: 0.7561671881028164\n",
      "Epoch: 4960 Training Loss: 0.6516694359168059 Test Loss: 0.7562738740395126\n",
      "Epoch: 4970 Training Loss: 0.6515642077600617 Test Loss: 0.7564452371479552\n",
      "Epoch: 4980 Training Loss: 0.6514563426208693 Test Loss: 0.7565031326356738\n",
      "Epoch: 4990 Training Loss: 0.651346091640414 Test Loss: 0.7566957826967593\n",
      "Epoch: 5000 Training Loss: 0.6512342710275492 Test Loss: 0.7568816510738169\n",
      "Epoch: 5010 Training Loss: 0.6511210063528674 Test Loss: 0.7570817735460069\n",
      "Epoch: 5020 Training Loss: 0.6510083695311495 Test Loss: 0.7571949899932484\n",
      "Epoch: 5030 Training Loss: 0.6509008811037353 Test Loss: 0.7573496500651041\n",
      "Epoch: 5040 Training Loss: 0.6507940833145814 Test Loss: 0.7573995080013824\n",
      "Epoch: 5050 Training Loss: 0.6506913665696927 Test Loss: 0.7574884234141911\n",
      "Epoch: 5060 Training Loss: 0.6505892776777678 Test Loss: 0.7577051233362269\n",
      "Epoch: 5070 Training Loss: 0.6504851796563585 Test Loss: 0.7579191859366963\n",
      "Epoch: 5080 Training Loss: 0.6503824629114697 Test Loss: 0.7581628870081019\n",
      "Epoch: 5090 Training Loss: 0.6502818808666581 Test Loss: 0.758301283596965\n",
      "Epoch: 5100 Training Loss: 0.6501819894601067 Test Loss: 0.7585159113377701\n",
      "Epoch: 5110 Training Loss: 0.6500849861771891 Test Loss: 0.7586430051199202\n",
      "Epoch: 5120 Training Loss: 0.6499859109794908 Test Loss: 0.7587937092094265\n",
      "Epoch: 5130 Training Loss: 0.6498928631702456 Test Loss: 0.7589246961805556\n",
      "Epoch: 5140 Training Loss: 0.6497991247227402 Test Loss: 0.7590704395937822\n",
      "Epoch: 5150 Training Loss: 0.649703502716343 Test Loss: 0.759096436049222\n",
      "Epoch: 5160 Training Loss: 0.6496034229539025 Test Loss: 0.7592377839265047\n",
      "Epoch: 5170 Training Loss: 0.6495061057445031 Test Loss: 0.7593101846828382\n",
      "Epoch: 5180 Training Loss: 0.6494064026938409 Test Loss: 0.7593744223009903\n",
      "Epoch: 5190 Training Loss: 0.6493021791018387 Test Loss: 0.7595099931881751\n",
      "Epoch: 5200 Training Loss: 0.6491950673862029 Test Loss: 0.759677777074492\n",
      "Epoch: 5210 Training Loss: 0.6490873906028995 Test Loss: 0.7598344465342078\n",
      "Epoch: 5220 Training Loss: 0.6489815345931914 Test Loss: 0.7599633613241062\n",
      "Epoch: 5230 Training Loss: 0.6488776877129677 Test Loss: 0.7599685103804977\n",
      "Epoch: 5240 Training Loss: 0.6487719572738524 Test Loss: 0.7601288846490805\n",
      "Epoch: 5250 Training Loss: 0.6486546115549054 Test Loss: 0.7603401843412423\n",
      "Epoch: 5260 Training Loss: 0.6485385843271827 Test Loss: 0.7605492862654321\n",
      "Epoch: 5270 Training Loss: 0.6484249429407226 Test Loss: 0.7606437902882266\n",
      "Epoch: 5280 Training Loss: 0.6483114899101518 Test Loss: 0.7608250747492284\n",
      "Epoch: 5290 Training Loss: 0.6481975345972097 Test Loss: 0.7610914442274306\n",
      "Epoch: 5300 Training Loss: 0.6480811306577087 Test Loss: 0.7613810472527649\n",
      "Epoch: 5310 Training Loss: 0.6479664847065064 Test Loss: 0.7615355189445088\n",
      "Epoch: 5320 Training Loss: 0.6478577405731645 Test Loss: 0.761675924921232\n",
      "Epoch: 5330 Training Loss: 0.6477567818165745 Test Loss: 0.7618905526620371\n",
      "Epoch: 5340 Training Loss: 0.6476551324217243 Test Loss: 0.7621030454282407\n",
      "Epoch: 5350 Training Loss: 0.6475548015180982 Test Loss: 0.7622285693761253\n",
      "Epoch: 5360 Training Loss: 0.6474561658174746 Test Loss: 0.7624151912736304\n",
      "Epoch: 5370 Training Loss: 0.6473554582020702 Test Loss: 0.7626619692201968\n",
      "Epoch: 5380 Training Loss: 0.6472508578982898 Test Loss: 0.7628217783484439\n",
      "Epoch: 5390 Training Loss: 0.6471433694708757 Test Loss: 0.763058320975598\n",
      "Epoch: 5400 Training Loss: 0.6470383924553169 Test Loss: 0.7633096828382202\n",
      "Epoch: 5410 Training Loss: 0.646930464530828 Test Loss: 0.7634184409561471\n",
      "Epoch: 5420 Training Loss: 0.6468273082888646 Test Loss: 0.7634554262514468\n",
      "Epoch: 5430 Training Loss: 0.6467279191646843 Test Loss: 0.7636401643478331\n",
      "Epoch: 5440 Training Loss: 0.6466311670229523 Test Loss: 0.7637825169190458\n",
      "Epoch: 5450 Training Loss: 0.6465357961577408 Test Loss: 0.7640708640769676\n",
      "Epoch: 5460 Training Loss: 0.6464449458338691 Test Loss: 0.764247501828543\n",
      "Epoch: 5470 Training Loss: 0.646359557830783 Test Loss: 0.7644620667759774\n",
      "Epoch: 5480 Training Loss: 0.6462776230189984 Test Loss: 0.7646768828968943\n",
      "Epoch: 5490 Training Loss: 0.6461956254219172 Test Loss: 0.7648762518486368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5500 Training Loss: 0.6461155113837277 Test Loss: 0.7650029060771927\n",
      "Epoch: 5510 Training Loss: 0.6460404201692491 Test Loss: 0.765120015713413\n",
      "Epoch: 5520 Training Loss: 0.6459630686841006 Test Loss: 0.7653269826630016\n",
      "Epoch: 5530 Training Loss: 0.645883770854764 Test Loss: 0.7654484250417952\n",
      "Epoch: 5540 Training Loss: 0.6458030289636106 Test Loss: 0.7657082012160815\n",
      "Epoch: 5550 Training Loss: 0.6457234172077922 Test Loss: 0.765820915316358\n",
      "Epoch: 5560 Training Loss: 0.6456437426666773 Test Loss: 0.7658786224239648\n",
      "Epoch: 5570 Training Loss: 0.6455680235992349 Test Loss: 0.7660369245113169\n",
      "Epoch: 5580 Training Loss: 0.6454943136612769 Test Loss: 0.7662382400575488\n",
      "Epoch: 5590 Training Loss: 0.6454233662763598 Test Loss: 0.7663210645134066\n",
      "Epoch: 5600 Training Loss: 0.6453524188914427 Test Loss: 0.7664891623665766\n",
      "Epoch: 5610 Training Loss: 0.6452820365741931 Test Loss: 0.7666504785357189\n",
      "Epoch: 5620 Training Loss: 0.6452107752627941 Test Loss: 0.7667497548546811\n",
      "Epoch: 5630 Training Loss: 0.6451398906631735 Test Loss: 0.7668768486368313\n",
      "Epoch: 5640 Training Loss: 0.6450682526399961 Test Loss: 0.7670797968106996\n",
      "Epoch: 5650 Training Loss: 0.6449997538816382 Test Loss: 0.7671876758214378\n",
      "Epoch: 5660 Training Loss: 0.6449323224733188 Test Loss: 0.7673367472833076\n",
      "Epoch: 5670 Training Loss: 0.6448650794208886 Test Loss: 0.7675152060426311\n",
      "Epoch: 5680 Training Loss: 0.6448003477803137 Test Loss: 0.76766917538741\n",
      "Epoch: 5690 Training Loss: 0.6447387554045583 Test Loss: 0.7678085138768326\n",
      "Epoch: 5700 Training Loss: 0.6446752166846149 Test Loss: 0.7679842725212191\n",
      "Epoch: 5710 Training Loss: 0.6446137498794522 Test Loss: 0.7680527172952032\n",
      "Epoch: 5720 Training Loss: 0.6445544177743667 Test Loss: 0.7681749131944444\n",
      "Epoch: 5730 Training Loss: 0.6444964669458018 Test Loss: 0.7683040791578254\n",
      "Epoch: 5740 Training Loss: 0.6444378882642728 Test Loss: 0.7685232908146862\n",
      "Epoch: 5750 Training Loss: 0.6443810675710429 Test Loss: 0.7687351556471836\n",
      "Epoch: 5760 Training Loss: 0.644325565369037 Test Loss: 0.7687884672188464\n",
      "Epoch: 5770 Training Loss: 0.6442700003817347 Test Loss: 0.7689154982076261\n",
      "Epoch: 5780 Training Loss: 0.6442168840209914 Test Loss: 0.7689909130457497\n",
      "Epoch: 5790 Training Loss: 0.6441663418574001 Test Loss: 0.7689629699958205\n",
      "Epoch: 5800 Training Loss: 0.6441130371407676 Test Loss: 0.769093705793467\n",
      "Epoch: 5810 Training Loss: 0.644059355712357 Test Loss: 0.7691916634516461\n",
      "Epoch: 5820 Training Loss: 0.644006616063392 Test Loss: 0.7692817091451261\n",
      "Epoch: 5830 Training Loss: 0.6439561366850971 Test Loss: 0.7693902788829411\n",
      "Epoch: 5840 Training Loss: 0.6439053433803201 Test Loss: 0.7695357083293145\n",
      "Epoch: 5850 Training Loss: 0.6438573754138807 Test Loss: 0.7696133209354102\n",
      "Epoch: 5860 Training Loss: 0.6438119188592967 Test Loss: 0.7696908707481352\n",
      "Epoch: 5870 Training Loss: 0.6437652693840813 Test Loss: 0.7697757045918532\n",
      "Epoch: 5880 Training Loss: 0.6437181176264948 Test Loss: 0.7697852491841886\n",
      "Epoch: 5890 Training Loss: 0.6436709658689084 Test Loss: 0.7698864093042695\n",
      "Epoch: 5900 Training Loss: 0.6436243791789893 Test Loss: 0.7699409767433449\n",
      "Epoch: 5910 Training Loss: 0.6435753438625113 Test Loss: 0.7701934688866384\n",
      "Epoch: 5920 Training Loss: 0.6435238599194741 Test Loss: 0.770267376683867\n",
      "Epoch: 5930 Training Loss: 0.6434726899029188 Test Loss: 0.7704214088220165\n",
      "Epoch: 5940 Training Loss: 0.6434217082422528 Test Loss: 0.7705074357397762\n",
      "Epoch: 5950 Training Loss: 0.6433697220168445 Test Loss: 0.7705643893269355\n",
      "Epoch: 5960 Training Loss: 0.6433174846502507 Test Loss: 0.7706450160148213\n",
      "Epoch: 5970 Training Loss: 0.6432651217130642 Test Loss: 0.7708398638438786\n",
      "Epoch: 5980 Training Loss: 0.6432157724701042 Test Loss: 0.7709491871021412\n",
      "Epoch: 5990 Training Loss: 0.6431681184301465 Test Loss: 0.7710287463027263\n",
      "Epoch: 6000 Training Loss: 0.6431209038872637 Test Loss: 0.7710425608442644\n",
      "Epoch: 6010 Training Loss: 0.6430751334061977 Test Loss: 0.7710563753858025\n",
      "Epoch: 6020 Training Loss: 0.6430309953428378 Test Loss: 0.7711197338967657\n",
      "Epoch: 6030 Training Loss: 0.6429859782853286 Test Loss: 0.7712210196035879\n",
      "Epoch: 6040 Training Loss: 0.6429410240131156 Test Loss: 0.7713667630168145\n",
      "Epoch: 6050 Training Loss: 0.6428952535320497 Test Loss: 0.771478472423161\n",
      "Epoch: 6060 Training Loss: 0.6428487924127234 Test Loss: 0.7715230557163066\n",
      "Epoch: 6070 Training Loss: 0.64280245686399 Test Loss: 0.7715948913323045\n",
      "Epoch: 6080 Training Loss: 0.642754237756365 Test Loss: 0.7715442798755787\n",
      "Epoch: 6090 Training Loss: 0.6427038211633663 Test Loss: 0.7716421119470165\n",
      "Epoch: 6100 Training Loss: 0.6426520860791436 Test Loss: 0.7717247480227624\n",
      "Epoch: 6110 Training Loss: 0.6425994720007715 Test Loss: 0.771776238586677\n",
      "Epoch: 6120 Training Loss: 0.6425479880577344 Test Loss: 0.7718692983619471\n",
      "Epoch: 6130 Training Loss: 0.6424911045792079 Test Loss: 0.7719714003825875\n",
      "Epoch: 6140 Training Loss: 0.6424335304624212 Test Loss: 0.7720211955254951\n",
      "Epoch: 6150 Training Loss: 0.6423777771192298 Test Loss: 0.7721175833494084\n",
      "Epoch: 6160 Training Loss: 0.6423279255938987 Test Loss: 0.7722233273855452\n",
      "Epoch: 6170 Training Loss: 0.6422805226951267 Test Loss: 0.7723992744100437\n",
      "Epoch: 6180 Training Loss: 0.6422316129492414 Test Loss: 0.7726179837199395\n",
      "Epoch: 6190 Training Loss: 0.6421838961239874 Test Loss: 0.772660746005337\n",
      "Epoch: 6200 Training Loss: 0.6421385651399961 Test Loss: 0.772717385625643\n",
      "Epoch: 6210 Training Loss: 0.6420939875795615 Test Loss: 0.7727695041232638\n",
      "Epoch: 6220 Training Loss: 0.6420528004251318 Test Loss: 0.7729078379187564\n",
      "Epoch: 6230 Training Loss: 0.6420123666942588 Test Loss: 0.7728994864004629\n",
      "Epoch: 6240 Training Loss: 0.6419714934663109 Test Loss: 0.7729904112011317\n",
      "Epoch: 6250 Training Loss: 0.6419306202383631 Test Loss: 0.7730128912278164\n",
      "Epoch: 6260 Training Loss: 0.641890500433972 Test Loss: 0.7730924504284015\n",
      "Epoch: 6270 Training Loss: 0.6418500667030989 Test Loss: 0.773152041337127\n",
      "Epoch: 6280 Training Loss: 0.6418112653899318 Test Loss: 0.773149529602302\n",
      "Epoch: 6290 Training Loss: 0.6417722129355793 Test Loss: 0.7731440665790573\n",
      "Epoch: 6300 Training Loss: 0.6417312769223351 Test Loss: 0.7731337056729038\n",
      "Epoch: 6310 Training Loss: 0.6416897758414234 Test Loss: 0.7733029338067451\n",
      "Epoch: 6320 Training Loss: 0.6416464539869166 Test Loss: 0.7733186949427726\n",
      "Epoch: 6330 Training Loss: 0.64160382277067 Test Loss: 0.7733068897890947\n",
      "Epoch: 6340 Training Loss: 0.6415610031985342 Test Loss: 0.7733402958622685\n",
      "Epoch: 6350 Training Loss: 0.6415146676498007 Test Loss: 0.773299919724955\n",
      "Epoch: 6360 Training Loss: 0.6414699645187734 Test Loss: 0.7733679249453447\n",
      "Epoch: 6370 Training Loss: 0.6414225616200013 Test Loss: 0.773413387345679\n",
      "Epoch: 6380 Training Loss: 0.6413747192241546 Test Loss: 0.7733898398316936\n",
      "Epoch: 6390 Training Loss: 0.6413218540045968 Test Loss: 0.7733839372548547\n",
      "Epoch: 6400 Training Loss: 0.6412719396939695 Test Loss: 0.7734009542582948\n",
      "Epoch: 6410 Training Loss: 0.6412221509539346 Test Loss: 0.7733718181343235\n",
      "Epoch: 6420 Training Loss: 0.6411724877844928 Test Loss: 0.7734778133439429\n",
      "Epoch: 6430 Training Loss: 0.6411201248473062 Test Loss: 0.7735340762040253\n",
      "Epoch: 6440 Training Loss: 0.6410673224130449 Test Loss: 0.773640510967239\n",
      "Epoch: 6450 Training Loss: 0.6410147083346728 Test Loss: 0.7737011693632652\n",
      "Epoch: 6460 Training Loss: 0.6409624081827826 Test Loss: 0.7735662892031572\n",
      "Epoch: 6470 Training Loss: 0.6409102336014851 Test Loss: 0.7735980626486947\n",
      "Epoch: 6480 Training Loss: 0.6408562382465925 Test Loss: 0.7736241846908758\n",
      "Epoch: 6490 Training Loss: 0.6408000454063264 Test Loss: 0.7737140420042439\n",
      "Epoch: 6500 Training Loss: 0.640743978136653 Test Loss: 0.773742173434285\n",
      "Epoch: 6510 Training Loss: 0.6406909245612061 Test Loss: 0.7737063812130273\n",
      "Epoch: 6520 Training Loss: 0.640640256827022 Test Loss: 0.7738211674945344\n",
      "Epoch: 6530 Training Loss: 0.6405903425163945 Test Loss: 0.7737701792775848\n",
      "Epoch: 6540 Training Loss: 0.6405436930411791 Test Loss: 0.7737719374919625\n",
      "Epoch: 6550 Training Loss: 0.6404922718834384 Test Loss: 0.7737302426938657\n",
      "Epoch: 6560 Training Loss: 0.6404380253873602 Test Loss: 0.7738040876977238\n",
      "Epoch: 6570 Training Loss: 0.6403835905353928 Test Loss: 0.7738872889138053\n",
      "Epoch: 6580 Training Loss: 0.640328590615758 Test Loss: 0.7739751368393133\n",
      "Epoch: 6590 Training Loss: 0.640274532475569 Test Loss: 0.7741345064139661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6600 Training Loss: 0.6402207254765655 Test Loss: 0.7742659957320602\n",
      "Epoch: 6610 Training Loss: 0.640166918477562 Test Loss: 0.7743988665043081\n",
      "Epoch: 6620 Training Loss: 0.6401147438962647 Test Loss: 0.7745432912567516\n",
      "Epoch: 6630 Training Loss: 0.6400639505914877 Test Loss: 0.7747203685619213\n",
      "Epoch: 6640 Training Loss: 0.6400129689308216 Test Loss: 0.7749895009484311\n",
      "Epoch: 6650 Training Loss: 0.6399621128407483 Test Loss: 0.7751089967327354\n",
      "Epoch: 6660 Training Loss: 0.6399126380271956 Test Loss: 0.7752263575424383\n",
      "Epoch: 6670 Training Loss: 0.6398646072754597 Test Loss: 0.7754241566599152\n",
      "Epoch: 6680 Training Loss: 0.6398167020943166 Test Loss: 0.7755178443688915\n",
      "Epoch: 6690 Training Loss: 0.639769864263212 Test Loss: 0.77567978847174\n",
      "Epoch: 6700 Training Loss: 0.6397213940144014 Test Loss: 0.7757741041144226\n",
      "Epoch: 6710 Training Loss: 0.6396714797037739 Test Loss: 0.7759170846193416\n",
      "Epoch: 6720 Training Loss: 0.6396199329754404 Test Loss: 0.7760608186447081\n",
      "Epoch: 6730 Training Loss: 0.6395673816823647 Test Loss: 0.7761617275913066\n",
      "Epoch: 6740 Training Loss: 0.6395096819949853 Test Loss: 0.7762403448913323\n",
      "Epoch: 6750 Training Loss: 0.6394523590193841 Test Loss: 0.7763420073583783\n",
      "Epoch: 6760 Training Loss: 0.639392775773113 Test Loss: 0.7765465253665124\n",
      "Epoch: 6770 Training Loss: 0.639331748465025 Test Loss: 0.776598518277392\n",
      "Epoch: 6780 Training Loss: 0.6392708467275299 Test Loss: 0.7765885969248328\n",
      "Epoch: 6790 Training Loss: 0.6392112006959625 Test Loss: 0.7767193327224794\n",
      "Epoch: 6800 Training Loss: 0.6391533126526938 Test Loss: 0.776718893168885\n",
      "Epoch: 6810 Training Loss: 0.6390964291741674 Test Loss: 0.7768056736070923\n",
      "Epoch: 6820 Training Loss: 0.639037097069082 Test Loss: 0.7769338976699138\n",
      "Epoch: 6830 Training Loss: 0.6389777649639964 Test Loss: 0.7768823443126286\n",
      "Epoch: 6840 Training Loss: 0.6389200024913205 Test Loss: 0.7768738044142233\n",
      "Epoch: 6850 Training Loss: 0.638864563074611 Test Loss: 0.7768226906105324\n",
      "Epoch: 6860 Training Loss: 0.6388096259402726 Test Loss: 0.7767783584908693\n",
      "Epoch: 6870 Training Loss: 0.6387536214558955 Test Loss: 0.7768057991938336\n",
      "Epoch: 6880 Training Loss: 0.6387011957334127 Test Loss: 0.7768977914818029\n",
      "Epoch: 6890 Training Loss: 0.638648330513855 Test Loss: 0.776927304365998\n",
      "Epoch: 6900 Training Loss: 0.6385969721414105 Test Loss: 0.7769419352213541\n",
      "Epoch: 6910 Training Loss: 0.6385485018925999 Test Loss: 0.7769772250956468\n",
      "Epoch: 6920 Training Loss: 0.638504991682204 Test Loss: 0.777005858872653\n",
      "Epoch: 6930 Training Loss: 0.6384590956305451 Test Loss: 0.7770582285437564\n",
      "Epoch: 6940 Training Loss: 0.638413450720072 Test Loss: 0.7771348364559221\n",
      "Epoch: 6950 Training Loss: 0.6383698777243796 Test Loss: 0.777258602189429\n",
      "Epoch: 6960 Training Loss: 0.638325300163945 Test Loss: 0.7773836865837191\n",
      "Epoch: 6970 Training Loss: 0.6382814132417706 Test Loss: 0.7774551454394933\n",
      "Epoch: 6980 Training Loss: 0.6382377146754854 Test Loss: 0.7775333859792952\n",
      "Epoch: 6990 Training Loss: 0.6381903117767134 Test Loss: 0.7775261647416731\n",
      "Epoch: 7000 Training Loss: 0.6381409625337534 Test Loss: 0.777585190510063\n",
      "Epoch: 7010 Training Loss: 0.6380928062114247 Test Loss: 0.7776322855380337\n",
      "Epoch: 7020 Training Loss: 0.6380459683803201 Test Loss: 0.7776087380240484\n",
      "Epoch: 7030 Training Loss: 0.637997246990324 Test Loss: 0.7776901182323817\n",
      "Epoch: 7040 Training Loss: 0.6379494045944774 Test Loss: 0.777672536088606\n",
      "Epoch: 7050 Training Loss: 0.6379040736104861 Test Loss: 0.7777262244204926\n",
      "Epoch: 7060 Training Loss: 0.6378620702472033 Test Loss: 0.7778501157407407\n",
      "Epoch: 7070 Training Loss: 0.637821762086923 Test Loss: 0.7778343546047132\n",
      "Epoch: 7080 Training Loss: 0.6377791936559727 Test Loss: 0.7778857195818866\n",
      "Epoch: 7090 Training Loss: 0.6377351183779092 Test Loss: 0.7779316843291859\n",
      "Epoch: 7100 Training Loss: 0.6376898501792143 Test Loss: 0.7780037083252958\n",
      "Epoch: 7110 Training Loss: 0.6376421333539604 Test Loss: 0.7781187457802855\n",
      "Epoch: 7120 Training Loss: 0.6375971162964511 Test Loss: 0.7781070034199782\n",
      "Epoch: 7130 Training Loss: 0.6375478298387874 Test Loss: 0.7781690432701582\n",
      "Epoch: 7140 Training Loss: 0.6374974760310852 Test Loss: 0.7782498583381559\n",
      "Epoch: 7150 Training Loss: 0.6374401530554841 Test Loss: 0.7782929345904064\n",
      "Epoch: 7160 Training Loss: 0.6373692684558634 Test Loss: 0.7782508630320859\n",
      "Epoch: 7170 Training Loss: 0.6372943028119776 Test Loss: 0.7781826694315843\n",
      "Epoch: 7180 Training Loss: 0.6372210323710943 Test Loss: 0.7782851482124485\n",
      "Epoch: 7190 Training Loss: 0.6371482014272856 Test Loss: 0.7783834826308513\n",
      "Epoch: 7200 Training Loss: 0.6370754960540697 Test Loss: 0.7783988042132844\n",
      "Epoch: 7210 Training Loss: 0.6370091319957889 Test Loss: 0.7784504831573109\n",
      "Epoch: 7220 Training Loss: 0.6369474768347371 Test Loss: 0.7785495083027907\n",
      "Epoch: 7230 Training Loss: 0.6368884586561334 Test Loss: 0.7785628832907343\n",
      "Epoch: 7240 Training Loss: 0.6368283103421949 Test Loss: 0.7787141525205762\n",
      "Epoch: 7250 Training Loss: 0.6367655250458082 Test Loss: 0.7788084681632588\n",
      "Epoch: 7260 Training Loss: 0.6367029281053105 Test Loss: 0.7789341176978846\n",
      "Epoch: 7270 Training Loss: 0.6366451028473383 Test Loss: 0.7790141164520641\n",
      "Epoch: 7280 Training Loss: 0.636588407724701 Test Loss: 0.7791481175049833\n",
      "Epoch: 7290 Training Loss: 0.636528510551948 Test Loss: 0.7792841279457626\n",
      "Epoch: 7300 Training Loss: 0.6364670437467854 Test Loss: 0.7794954276379243\n",
      "Epoch: 7310 Training Loss: 0.6364114787594831 Test Loss: 0.7795531347455311\n",
      "Epoch: 7320 Training Loss: 0.6363576089751832 Test Loss: 0.7796955501101145\n",
      "Epoch: 7330 Training Loss: 0.6363055599644786 Test Loss: 0.779887007097158\n",
      "Epoch: 7340 Training Loss: 0.6362556456538511 Test Loss: 0.7798996285646541\n",
      "Epoch: 7350 Training Loss: 0.6362034710725537 Test Loss: 0.7800489512000064\n",
      "Epoch: 7360 Training Loss: 0.6361482200117333 Test Loss: 0.7801187146347737\n",
      "Epoch: 7370 Training Loss: 0.6360945385833227 Test Loss: 0.7801766101224923\n",
      "Epoch: 7380 Training Loss: 0.636042991854989 Test Loss: 0.7801763589490097\n",
      "Epoch: 7390 Training Loss: 0.6359915706972483 Test Loss: 0.7803518664199138\n",
      "Epoch: 7400 Training Loss: 0.6359402123248039 Test Loss: 0.7804969818994342\n",
      "Epoch: 7410 Training Loss: 0.635885840258133 Test Loss: 0.7806566026475694\n",
      "Epoch: 7420 Training Loss: 0.6358282033560498 Test Loss: 0.7807687516075102\n",
      "Epoch: 7430 Training Loss: 0.6357740824305644 Test Loss: 0.7809899098588606\n",
      "Epoch: 7440 Training Loss: 0.6357264911759033 Test Loss: 0.7810461099255722\n",
      "Epoch: 7450 Training Loss: 0.6356813485478012 Test Loss: 0.7812481789922519\n",
      "Epoch: 7460 Training Loss: 0.6356402241786678 Test Loss: 0.781321835615998\n",
      "Epoch: 7470 Training Loss: 0.6355977813183104 Test Loss: 0.7814118813094779\n",
      "Epoch: 7480 Training Loss: 0.6355593567169217 Test Loss: 0.7814863542470422\n",
      "Epoch: 7490 Training Loss: 0.6355214971832005 Test Loss: 0.7815547990210262\n",
      "Epoch: 7500 Training Loss: 0.6354835120788864 Test Loss: 0.7815601364575295\n",
      "Epoch: 7510 Training Loss: 0.6354496080188377 Test Loss: 0.7816594755698624\n",
      "Epoch: 7520 Training Loss: 0.6354173363764948 Test Loss: 0.7816970260054977\n",
      "Epoch: 7530 Training Loss: 0.6353863204400797 Test Loss: 0.7817346392345036\n",
      "Epoch: 7540 Training Loss: 0.6353577531302237 Test Loss: 0.7819406642835327\n",
      "Epoch: 7550 Training Loss: 0.6353300648145171 Test Loss: 0.7820213537647891\n",
      "Epoch: 7560 Training Loss: 0.6353012463634756 Test Loss: 0.7821454334651492\n",
      "Epoch: 7570 Training Loss: 0.6352721767712486 Test Loss: 0.7821347585921425\n",
      "Epoch: 7580 Training Loss: 0.6352411608348335 Test Loss: 0.7822578963919432\n",
      "Epoch: 7590 Training Loss: 0.6352052476453003 Test Loss: 0.7822913652584876\n",
      "Epoch: 7600 Training Loss: 0.6351704018058055 Test Loss: 0.782335320617927\n",
      "Epoch: 7610 Training Loss: 0.6351368116722387 Test Loss: 0.7824284431865677\n",
      "Epoch: 7620 Training Loss: 0.6351042888887103 Test Loss: 0.7825059302059221\n",
      "Epoch: 7630 Training Loss: 0.6350698825462904 Test Loss: 0.7825851126462834\n",
      "Epoch: 7640 Training Loss: 0.6350275024712293 Test Loss: 0.7826631648059735\n",
      "Epoch: 7650 Training Loss: 0.6349798484312718 Test Loss: 0.7827359423225309\n",
      "Epoch: 7660 Training Loss: 0.6349318176795358 Test Loss: 0.7827709810233411\n",
      "Epoch: 7670 Training Loss: 0.6348885586103253 Test Loss: 0.7827594898365162\n",
      "Epoch: 7680 Training Loss: 0.6348455506823004 Test Loss: 0.7828296928248778\n",
      "Epoch: 7690 Training Loss: 0.6348030450366465 Test Loss: 0.7828710108627508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7700 Training Loss: 0.6347627996616626 Test Loss: 0.7828028172622492\n",
      "Epoch: 7710 Training Loss: 0.6347245006308666 Test Loss: 0.7828848881976594\n",
      "Epoch: 7720 Training Loss: 0.6346867038824419 Test Loss: 0.7829155313625257\n",
      "Epoch: 7730 Training Loss: 0.6346468352192363 Test Loss: 0.7829427836853781\n",
      "Epoch: 7740 Training Loss: 0.6346092268267005 Test Loss: 0.782857133527842\n",
      "Epoch: 7750 Training Loss: 0.6345694837340877 Test Loss: 0.7829323599858539\n",
      "Epoch: 7760 Training Loss: 0.6345314358444774 Test Loss: 0.7829842901033629\n",
      "Epoch: 7770 Training Loss: 0.6344984107785778 Test Loss: 0.7830210870185507\n",
      "Epoch: 7780 Training Loss: 0.6344672064862736 Test Loss: 0.7830234103732638\n",
      "Epoch: 7790 Training Loss: 0.6344346837027453 Test Loss: 0.783026424455054\n",
      "Epoch: 7800 Training Loss: 0.6344036049810338 Test Loss: 0.782964886951839\n",
      "Epoch: 7810 Training Loss: 0.6343693242092067 Test Loss: 0.7831246332867156\n",
      "Epoch: 7820 Training Loss: 0.6343312763195963 Test Loss: 0.7831913198463221\n",
      "Epoch: 7830 Training Loss: 0.6342941702094317 Test Loss: 0.7831652605975116\n",
      "Epoch: 7840 Training Loss: 0.6342532341961875 Test Loss: 0.7832394195682227\n",
      "Epoch: 7850 Training Loss: 0.6342092844887167 Test Loss: 0.7832714441872428\n",
      "Epoch: 7860 Training Loss: 0.6341674066960268 Test Loss: 0.7832804236392426\n",
      "Epoch: 7870 Training Loss: 0.634125403332744 Test Loss: 0.7834389769000771\n",
      "Epoch: 7880 Training Loss: 0.6340828349017937 Test Loss: 0.7835292737670396\n",
      "Epoch: 7890 Training Loss: 0.634042463956217 Test Loss: 0.7834160573197981\n",
      "Epoch: 7900 Training Loss: 0.6340070530490549 Test Loss: 0.7833569059646669\n",
      "Epoch: 7910 Training Loss: 0.633972646706635 Test Loss: 0.7833456659513246\n",
      "Epoch: 7920 Training Loss: 0.6339386170759933 Test Loss: 0.7834249111850566\n",
      "Epoch: 7930 Training Loss: 0.633906910501318 Test Loss: 0.7833760579427084\n",
      "Epoch: 7940 Training Loss: 0.6338740737913077 Test Loss: 0.783331600236304\n",
      "Epoch: 7950 Training Loss: 0.6338401697312589 Test Loss: 0.7833290257081083\n",
      "Epoch: 7960 Training Loss: 0.6338046960388003 Test Loss: 0.7835097450287745\n",
      "Epoch: 7970 Training Loss: 0.6337722360405683 Test Loss: 0.7835081124011382\n",
      "Epoch: 7980 Training Loss: 0.6337411573188569 Test Loss: 0.7834860719280479\n",
      "Epoch: 7990 Training Loss: 0.6337103925236274 Test Loss: 0.7835962115001286\n",
      "Epoch: 8000 Training Loss: 0.6336699587927542 Test Loss: 0.7837859102727945\n",
      "Epoch: 8010 Training Loss: 0.6336343595297029 Test Loss: 0.7836824267980003\n",
      "Epoch: 8020 Training Loss: 0.6336052899374759 Test Loss: 0.7836637143735532\n",
      "Epoch: 8030 Training Loss: 0.633574148430468 Test Loss: 0.7836778428819444\n",
      "Epoch: 8040 Training Loss: 0.6335454555500193 Test Loss: 0.7838132253890175\n",
      "Epoch: 8050 Training Loss: 0.6335151302518645 Test Loss: 0.7837702119301376\n",
      "Epoch: 8060 Training Loss: 0.6334861234449338 Test Loss: 0.7838738837850437\n",
      "Epoch: 8070 Training Loss: 0.6334547935820368 Test Loss: 0.7839381841965664\n",
      "Epoch: 8080 Training Loss: 0.6334240915721037 Test Loss: 0.7839042757764275\n",
      "Epoch: 8090 Training Loss: 0.6333945824828019 Test Loss: 0.7840616987565908\n",
      "Epoch: 8100 Training Loss: 0.6333655756758712 Test Loss: 0.7840275391629694\n",
      "Epoch: 8110 Training Loss: 0.6333373850777935 Test Loss: 0.7840497680161715\n",
      "Epoch: 8120 Training Loss: 0.6333118942474605 Test Loss: 0.784125559614519\n",
      "Epoch: 8130 Training Loss: 0.6332848965700142 Test Loss: 0.7841111799326453\n",
      "Epoch: 8140 Training Loss: 0.6332577105366787 Test Loss: 0.7840713689356674\n",
      "Epoch: 8150 Training Loss: 0.6332299594356757 Test Loss: 0.7841104892055684\n",
      "Epoch: 8160 Training Loss: 0.6332015804817089 Test Loss: 0.7840833624694573\n",
      "Epoch: 8170 Training Loss: 0.6331699366923299 Test Loss: 0.7841889181254823\n",
      "Epoch: 8180 Training Loss: 0.6331369116264305 Test Loss: 0.784230424543467\n",
      "Epoch: 8190 Training Loss: 0.6331062096164973 Test Loss: 0.7841249316808128\n",
      "Epoch: 8200 Training Loss: 0.633074000759451 Test Loss: 0.7841571446799447\n",
      "Epoch: 8210 Training Loss: 0.6330451823084094 Test Loss: 0.7841099240652327\n",
      "Epoch: 8220 Training Loss: 0.6330153592926256 Test Loss: 0.7841398765030221\n",
      "Epoch: 8230 Training Loss: 0.6329851595650636 Test Loss: 0.7842390900286137\n",
      "Epoch: 8240 Training Loss: 0.6329564038993185 Test Loss: 0.7842557930652007\n",
      "Epoch: 8250 Training Loss: 0.6329289667247975 Test Loss: 0.7844025411723573\n",
      "Epoch: 8260 Training Loss: 0.6329014039796836 Test Loss: 0.7844507664809992\n",
      "Epoch: 8270 Training Loss: 0.6328747830140157 Test Loss: 0.7843678164384003\n",
      "Epoch: 8280 Training Loss: 0.6328497944660537 Test Loss: 0.784450138547293\n",
      "Epoch: 8290 Training Loss: 0.6328265639063907 Test Loss: 0.7844414730621464\n",
      "Epoch: 8300 Training Loss: 0.6328025171378745 Test Loss: 0.7844699184590407\n",
      "Epoch: 8310 Training Loss: 0.6327785959399511 Test Loss: 0.7845784254034851\n",
      "Epoch: 8320 Training Loss: 0.6327565583009194 Test Loss: 0.7846193666811343\n",
      "Epoch: 8330 Training Loss: 0.6327330138147743 Test Loss: 0.7848004627620242\n",
      "Epoch: 8340 Training Loss: 0.6327117923845956 Test Loss: 0.7848939620908887\n",
      "Epoch: 8350 Training Loss: 0.6326890641073036 Test Loss: 0.7849811820826903\n",
      "Epoch: 8360 Training Loss: 0.6326661474741224 Test Loss: 0.7850620599440586\n",
      "Epoch: 8370 Training Loss: 0.6326414728526424 Test Loss: 0.7852160292888375\n",
      "Epoch: 8380 Training Loss: 0.632616735445866 Test Loss: 0.7852946465888632\n",
      "Epoch: 8390 Training Loss: 0.6325958907274656 Test Loss: 0.7854812056929977\n",
      "Epoch: 8400 Training Loss: 0.6325752971502507 Test Loss: 0.7855821146395962\n",
      "Epoch: 8410 Training Loss: 0.6325518782346985 Test Loss: 0.7857729436929334\n",
      "Epoch: 8420 Training Loss: 0.6325276431102932 Test Loss: 0.7858921883037552\n",
      "Epoch: 8430 Training Loss: 0.6325056054712614 Test Loss: 0.7859060656386638\n",
      "Epoch: 8440 Training Loss: 0.632483128335155 Test Loss: 0.7859728777850116\n",
      "Epoch: 8450 Training Loss: 0.6324601489166773 Test Loss: 0.7861006622942387\n",
      "Epoch: 8460 Training Loss: 0.6324386135600167 Test Loss: 0.7861308031121399\n",
      "Epoch: 8470 Training Loss: 0.6324144412209078 Test Loss: 0.7862384309493956\n",
      "Epoch: 8480 Training Loss: 0.6323873179728687 Test Loss: 0.7863633897569444\n",
      "Epoch: 8490 Training Loss: 0.6323603202954224 Test Loss: 0.7863873140311536\n",
      "Epoch: 8500 Training Loss: 0.6323304344943423 Test Loss: 0.7864135616600759\n",
      "Epoch: 8510 Training Loss: 0.6322955258695513 Test Loss: 0.78648200643406\n",
      "Epoch: 8520 Training Loss: 0.632259612680018 Test Loss: 0.7865754429695537\n",
      "Epoch: 8530 Training Loss: 0.6322234483492992 Test Loss: 0.7865276572145061\n",
      "Epoch: 8540 Training Loss: 0.6321897954304359 Test Loss: 0.7865302945360725\n",
      "Epoch: 8550 Training Loss: 0.6321582144263533 Test Loss: 0.7865871853298612\n",
      "Epoch: 8560 Training Loss: 0.6321234313721551 Test Loss: 0.7867448594835069\n",
      "Epoch: 8570 Training Loss: 0.6320890250297351 Test Loss: 0.7867773236561214\n",
      "Epoch: 8580 Training Loss: 0.6320560627491321 Test Loss: 0.7868874004348315\n",
      "Epoch: 8590 Training Loss: 0.6320226609714543 Test Loss: 0.7870371626237783\n",
      "Epoch: 8600 Training Loss: 0.6319864966407355 Test Loss: 0.7871264547968106\n",
      "Epoch: 8610 Training Loss: 0.6319492649599782 Test Loss: 0.7871459835350759\n",
      "Epoch: 8620 Training Loss: 0.6319152353293365 Test Loss: 0.7871646959595229\n",
      "Epoch: 8630 Training Loss: 0.6318825869752154 Test Loss: 0.7871109448342657\n",
      "Epoch: 8640 Training Loss: 0.6318505664740581 Test Loss: 0.7871994206934799\n",
      "Epoch: 8650 Training Loss: 0.6318197388935322 Test Loss: 0.7872813032487783\n",
      "Epoch: 8660 Training Loss: 0.6317915482954546 Test Loss: 0.787238038616416\n",
      "Epoch: 8670 Training Loss: 0.6317681293799022 Test Loss: 0.787319293238008\n",
      "Epoch: 8680 Training Loss: 0.6317464684526488 Test Loss: 0.7874287420830118\n",
      "Epoch: 8690 Training Loss: 0.6317225472547254 Test Loss: 0.7875624919624485\n",
      "Epoch: 8700 Training Loss: 0.6316984377009129 Test Loss: 0.7874750207971644\n",
      "Epoch: 8710 Training Loss: 0.6316726957293943 Test Loss: 0.7875320999710648\n",
      "Epoch: 8720 Training Loss: 0.6316453213401697 Test Loss: 0.7875462912728266\n",
      "Epoch: 8730 Training Loss: 0.6316185120186125 Test Loss: 0.7876869484230324\n",
      "Epoch: 8740 Training Loss: 0.6315920794088338 Test Loss: 0.7878086419753086\n",
      "Epoch: 8750 Training Loss: 0.6315668397196863 Test Loss: 0.7878774635095165\n",
      "Epoch: 8760 Training Loss: 0.6315402187540182 Test Loss: 0.7878503995667759\n",
      "Epoch: 8770 Training Loss: 0.6315125932236081 Test Loss: 0.7879391265994727\n",
      "Epoch: 8780 Training Loss: 0.6314842142696413 Test Loss: 0.7881261252572016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8790 Training Loss: 0.6314597280040504 Test Loss: 0.7882906438882459\n",
      "Epoch: 8800 Training Loss: 0.6314372508679439 Test Loss: 0.788343578699685\n",
      "Epoch: 8810 Training Loss: 0.6314148365171338 Test Loss: 0.7883555722334747\n",
      "Epoch: 8820 Training Loss: 0.6313938662281406 Test Loss: 0.7883909876945088\n",
      "Epoch: 8830 Training Loss: 0.631372644797962 Test Loss: 0.7883914900414738\n",
      "Epoch: 8840 Training Loss: 0.6313529930001929 Test Loss: 0.7883676913540059\n",
      "Epoch: 8850 Training Loss: 0.63133045307879 Test Loss: 0.7883369226023984\n",
      "Epoch: 8860 Training Loss: 0.631308541010351 Test Loss: 0.7883402506510416\n",
      "Epoch: 8870 Training Loss: 0.6312846825977241 Test Loss: 0.7884000927332497\n",
      "Epoch: 8880 Training Loss: 0.6312593801232802 Test Loss: 0.7884156654891654\n",
      "Epoch: 8890 Training Loss: 0.6312326335870194 Test Loss: 0.7886233859592013\n",
      "Epoch: 8900 Training Loss: 0.6312056986948695 Test Loss: 0.7887246716660237\n",
      "Epoch: 8910 Training Loss: 0.6311802706498328 Test Loss: 0.7886454264322916\n",
      "Epoch: 8920 Training Loss: 0.6311545914636106 Test Loss: 0.7886670273517875\n",
      "Epoch: 8930 Training Loss: 0.6311319887569115 Test Loss: 0.7886627574025848\n",
      "Epoch: 8940 Training Loss: 0.6311107045414363 Test Loss: 0.7887409979423868\n",
      "Epoch: 8950 Training Loss: 0.6310884785465154 Test Loss: 0.788825266645769\n",
      "Epoch: 8960 Training Loss: 0.6310643062074065 Test Loss: 0.7887391769346387\n",
      "Epoch: 8970 Training Loss: 0.6310378108123312 Test Loss: 0.7888545283564815\n",
      "Epoch: 8980 Training Loss: 0.6310122571967018 Test Loss: 0.7888200547960069\n",
      "Epoch: 8990 Training Loss: 0.6309863268692941 Test Loss: 0.7888854226948302\n",
      "Epoch: 9000 Training Loss: 0.6309579479153272 Test Loss: 0.7889144960254308\n",
      "Epoch: 9010 Training Loss: 0.6309308246672881 Test Loss: 0.7889965041674705\n",
      "Epoch: 9020 Training Loss: 0.630902069001543 Test Loss: 0.7891222792888375\n",
      "Epoch: 9030 Training Loss: 0.6308724343416484 Test Loss: 0.7892569710688336\n",
      "Epoch: 9040 Training Loss: 0.6308420462581973 Test Loss: 0.7892382586443866\n",
      "Epoch: 9050 Training Loss: 0.6308106536100039 Test Loss: 0.7895230893735532\n",
      "Epoch: 9060 Training Loss: 0.6307802027412562 Test Loss: 0.7894493699564364\n",
      "Epoch: 9070 Training Loss: 0.6307503797254725 Test Loss: 0.7895973739310057\n",
      "Epoch: 9080 Training Loss: 0.6307273375216986 Test Loss: 0.7895900899000129\n",
      "Epoch: 9090 Training Loss: 0.6307027884708114 Test Loss: 0.7894931997291346\n",
      "Epoch: 9100 Training Loss: 0.6306786161317025 Test Loss: 0.7894567167807999\n",
      "Epoch: 9110 Training Loss: 0.6306563901367815 Test Loss: 0.7895430576654128\n",
      "Epoch: 9120 Training Loss: 0.6306370522654944 Test Loss: 0.789556683826839\n",
      "Epoch: 9130 Training Loss: 0.6306195351678024 Test Loss: 0.7895778451927404\n",
      "Epoch: 9140 Training Loss: 0.6306022064259997 Test Loss: 0.7897405428160366\n",
      "Epoch: 9150 Training Loss: 0.6305836847635656 Test Loss: 0.7897448755586098\n",
      "Epoch: 9160 Training Loss: 0.6305666699482448 Test Loss: 0.7896768075448496\n",
      "Epoch: 9170 Training Loss: 0.6305502829858879 Test Loss: 0.7897731953687629\n",
      "Epoch: 9180 Training Loss: 0.6305338960235309 Test Loss: 0.7897716883278678\n",
      "Epoch: 9190 Training Loss: 0.630516378925839 Test Loss: 0.7897917822064686\n",
      "Epoch: 9200 Training Loss: 0.6304968526986627 Test Loss: 0.7897414219232253\n",
      "Epoch: 9210 Training Loss: 0.6304778287538575 Test Loss: 0.7897861935964827\n",
      "Epoch: 9220 Training Loss: 0.6304569840354571 Test Loss: 0.7898812627596129\n",
      "Epoch: 9230 Training Loss: 0.6304343813287578 Test Loss: 0.7900150126390496\n",
      "Epoch: 9240 Training Loss: 0.6304152318133599 Test Loss: 0.7901004744164738\n",
      "Epoch: 9250 Training Loss: 0.6303963334391475 Test Loss: 0.7902319009411972\n",
      "Epoch: 9260 Training Loss: 0.6303797581209014 Test Loss: 0.7902657465679656\n",
      "Epoch: 9270 Training Loss: 0.6303643757232866 Test Loss: 0.7902506761590149\n",
      "Epoch: 9280 Training Loss: 0.6303506885286743 Test Loss: 0.7902835798852238\n",
      "Epoch: 9290 Training Loss: 0.6303368757634692 Test Loss: 0.7903759489334169\n",
      "Epoch: 9300 Training Loss: 0.6303246954159701 Test Loss: 0.7903627623255851\n",
      "Epoch: 9310 Training Loss: 0.6303129545655458 Test Loss: 0.7904909863884066\n",
      "Epoch: 9320 Training Loss: 0.6302986395179696 Test Loss: 0.790582476329411\n",
      "Epoch: 9330 Training Loss: 0.6302840733292079 Test Loss: 0.7905307345920138\n",
      "Epoch: 9340 Training Loss: 0.6302698838522245 Test Loss: 0.7906347204137731\n",
      "Epoch: 9350 Training Loss: 0.6302578290753182 Test Loss: 0.7907940899884259\n",
      "Epoch: 9360 Training Loss: 0.6302445813777806 Test Loss: 0.7908674326453189\n",
      "Epoch: 9370 Training Loss: 0.6302329660979491 Test Loss: 0.790911262418017\n",
      "Epoch: 9380 Training Loss: 0.6302201578974862 Test Loss: 0.790950382687918\n",
      "Epoch: 9390 Training Loss: 0.6302079775499871 Test Loss: 0.7910890304502637\n",
      "Epoch: 9400 Training Loss: 0.6301954204907098 Test Loss: 0.791035530498489\n",
      "Epoch: 9410 Training Loss: 0.6301804148048733 Test Loss: 0.7912146171915188\n",
      "Epoch: 9420 Training Loss: 0.6301641534131092 Test Loss: 0.791268870663741\n",
      "Epoch: 9430 Training Loss: 0.6301456317506751 Test Loss: 0.7913537045074589\n",
      "Epoch: 9440 Training Loss: 0.6301276123706121 Test Loss: 0.7914520389258616\n",
      "Epoch: 9450 Training Loss: 0.630111853261219 Test Loss: 0.7915256327562371\n",
      "Epoch: 9460 Training Loss: 0.630098542778385 Test Loss: 0.7915765581798161\n",
      "Epoch: 9470 Training Loss: 0.6300847300131799 Test Loss: 0.79167200410317\n",
      "Epoch: 9480 Training Loss: 0.6300709172479748 Test Loss: 0.7916887071397569\n",
      "Epoch: 9490 Training Loss: 0.6300587369004758 Test Loss: 0.791758156607671\n",
      "Epoch: 9500 Training Loss: 0.6300454892029381 Test Loss: 0.7917169013631687\n",
      "Epoch: 9510 Training Loss: 0.6300321787201042 Test Loss: 0.7917552681126222\n",
      "Epoch: 9520 Training Loss: 0.6300218819314968 Test Loss: 0.7917007006735468\n",
      "Epoch: 9530 Training Loss: 0.6300126524929278 Test Loss: 0.7916745158379951\n",
      "Epoch: 9540 Training Loss: 0.6300027952013951 Test Loss: 0.7916814231087641\n",
      "Epoch: 9550 Training Loss: 0.6299938796893082 Test Loss: 0.7917178432637282\n",
      "Epoch: 9560 Training Loss: 0.6299849013919249 Test Loss: 0.7918182498633616\n",
      "Epoch: 9570 Training Loss: 0.6299766137328019 Test Loss: 0.7918917809003665\n",
      "Epoch: 9580 Training Loss: 0.6299677610060113 Test Loss: 0.7919734122821823\n",
      "Epoch: 9590 Training Loss: 0.6299602267704449 Test Loss: 0.7918377786016269\n",
      "Epoch: 9600 Training Loss: 0.6299513740436543 Test Loss: 0.7919044651612333\n",
      "Epoch: 9610 Training Loss: 0.6299435886669024 Test Loss: 0.7919822661474408\n",
      "Epoch: 9620 Training Loss: 0.6299341708724444 Test Loss: 0.7921305840888632\n",
      "Epoch: 9630 Training Loss: 0.629924376366208 Test Loss: 0.7920174304349923\n",
      "Epoch: 9640 Training Loss: 0.6299138284364151 Test Loss: 0.7920381522472993\n",
      "Epoch: 9650 Training Loss: 0.6299031549360293 Test Loss: 0.7921330330303177\n",
      "Epoch: 9660 Training Loss: 0.6298947417063135 Test Loss: 0.7921885423699524\n",
      "Epoch: 9670 Training Loss: 0.6298854494824483 Test Loss: 0.7923127476570537\n",
      "Epoch: 9680 Training Loss: 0.6298762828291757 Test Loss: 0.7923413186406894\n",
      "Epoch: 9690 Training Loss: 0.6298679951700528 Test Loss: 0.7923720245989262\n",
      "Epoch: 9700 Training Loss: 0.6298587029461875 Test Loss: 0.7923905486432613\n",
      "Epoch: 9710 Training Loss: 0.6298493479370258 Test Loss: 0.7924758848339442\n",
      "Epoch: 9720 Training Loss: 0.6298401812837534 Test Loss: 0.7926108277874229\n",
      "Epoch: 9730 Training Loss: 0.6298327098334834 Test Loss: 0.7926053647641782\n",
      "Epoch: 9740 Training Loss: 0.6298244849596567 Test Loss: 0.7926521458252958\n",
      "Epoch: 9750 Training Loss: 0.6298170762946831 Test Loss: 0.7926717373569316\n",
      "Epoch: 9760 Training Loss: 0.6298094792738202 Test Loss: 0.7926461804550862\n",
      "Epoch: 9770 Training Loss: 0.6298010660441045 Test Loss: 0.7927274978700488\n",
      "Epoch: 9780 Training Loss: 0.6297914598937573 Test Loss: 0.7927133065682871\n",
      "Epoch: 9790 Training Loss: 0.6297794679021473 Test Loss: 0.7927553781266075\n",
      "Epoch: 9800 Training Loss: 0.6297643366457182 Test Loss: 0.7926993664400077\n",
      "Epoch: 9810 Training Loss: 0.6297514028746625 Test Loss: 0.7927460847077547\n",
      "Epoch: 9820 Training Loss: 0.6297377784653465 Test Loss: 0.7927301979849859\n",
      "Epoch: 9830 Training Loss: 0.6297239657001414 Test Loss: 0.7926858030719521\n",
      "Epoch: 9840 Training Loss: 0.6297120992791243 Test Loss: 0.7926933382764275\n",
      "Epoch: 9850 Training Loss: 0.6297006723551819 Test Loss: 0.7926895706741898\n",
      "Epoch: 9860 Training Loss: 0.6296891198606468 Test Loss: 0.792741124031475\n",
      "Epoch: 9870 Training Loss: 0.6296778185072972 Test Loss: 0.79278664922518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9880 Training Loss: 0.6296663287980584 Test Loss: 0.7927696950151106\n",
      "Epoch: 9890 Training Loss: 0.6296525788181496 Test Loss: 0.7928262090486754\n",
      "Epoch: 9900 Training Loss: 0.6296417169618748 Test Loss: 0.7928537753383809\n",
      "Epoch: 9910 Training Loss: 0.6296325503086023 Test Loss: 0.7928928956082819\n",
      "Epoch: 9920 Training Loss: 0.6296252672142214 Test Loss: 0.7928831626358346\n",
      "Epoch: 9930 Training Loss: 0.6296156610638742 Test Loss: 0.7930196126302084\n",
      "Epoch: 9940 Training Loss: 0.6296078129018259 Test Loss: 0.7930050445682227\n",
      "Epoch: 9950 Training Loss: 0.6295990229603318 Test Loss: 0.7930467393663194\n",
      "Epoch: 9960 Training Loss: 0.6295903585894304 Test Loss: 0.7931843824347351\n",
      "Epoch: 9970 Training Loss: 0.6295809407949724 Test Loss: 0.7931829381872106\n",
      "Epoch: 9980 Training Loss: 0.6295715857858107 Test Loss: 0.7932112579973637\n",
      "Epoch: 9990 Training Loss: 0.6295619796354636 Test Loss: 0.7931878360701196\n",
      "Epoch: 10000 Training Loss: 0.6295507410674103 Test Loss: 0.7931966271420074\n",
      "Epoch: 10010 Training Loss: 0.6295379956522438 Test Loss: 0.7932527016219779\n",
      "Epoch: 10020 Training Loss: 0.6295253130223737 Test Loss: 0.7933651017554012\n",
      "Epoch: 10030 Training Loss: 0.6295122536807252 Test Loss: 0.7933765929422261\n",
      "Epoch: 10040 Training Loss: 0.6294995082655587 Test Loss: 0.7934393863128536\n",
      "Epoch: 10050 Training Loss: 0.6294887719798765 Test Loss: 0.7934354931238747\n",
      "Epoch: 10060 Training Loss: 0.629475210355857 Test Loss: 0.7935652242275913\n",
      "Epoch: 10070 Training Loss: 0.6294627160818761 Test Loss: 0.7935367788306971\n",
      "Epoch: 10080 Training Loss: 0.6294521053667866 Test Loss: 0.7935539214208783\n",
      "Epoch: 10090 Training Loss: 0.6294401761604732 Test Loss: 0.7937823009098508\n",
      "Epoch: 10100 Training Loss: 0.6294288748071236 Test Loss: 0.7938190978250386\n",
      "Epoch: 10110 Training Loss: 0.6294198965097403 Test Loss: 0.7938972755714699\n",
      "Epoch: 10120 Training Loss: 0.6294120483476919 Test Loss: 0.7939865677445023\n",
      "Epoch: 10130 Training Loss: 0.6294035723326797 Test Loss: 0.794026253154739\n",
      "Epoch: 10140 Training Loss: 0.6293932755440723 Test Loss: 0.7940677595727238\n",
      "Epoch: 10150 Training Loss: 0.6293799650612383 Test Loss: 0.7941311808770576\n",
      "Epoch: 10160 Training Loss: 0.629367910284332 Test Loss: 0.794281005859375\n",
      "Epoch: 10170 Training Loss: 0.6293544114456089 Test Loss: 0.794349450633359\n",
      "Epoch: 10180 Training Loss: 0.629340975392182 Test Loss: 0.794504927019033\n",
      "Epoch: 10190 Training Loss: 0.6293264092034203 Test Loss: 0.7946465260697981\n",
      "Epoch: 10200 Training Loss: 0.6293097083145814 Test Loss: 0.7947127730758102\n",
      "Epoch: 10210 Training Loss: 0.6292921284315932 Test Loss: 0.7948416878657086\n",
      "Epoch: 10220 Training Loss: 0.6292759298251254 Test Loss: 0.7949072441446438\n",
      "Epoch: 10230 Training Loss: 0.6292588522245082 Test Loss: 0.7949188609182098\n",
      "Epoch: 10240 Training Loss: 0.6292405189179632 Test Loss: 0.7950123602470743\n",
      "Epoch: 10250 Training Loss: 0.6292236296732352 Test Loss: 0.7950884030189043\n",
      "Epoch: 10260 Training Loss: 0.6292063009314325 Test Loss: 0.7950887169857574\n",
      "Epoch: 10270 Training Loss: 0.6291879676248875 Test Loss: 0.7951412750369727\n",
      "Epoch: 10280 Training Loss: 0.6291711411654558 Test Loss: 0.7950974452642747\n",
      "Epoch: 10290 Training Loss: 0.6291546914178024 Test Loss: 0.7950762211050025\n",
      "Epoch: 10300 Training Loss: 0.6291395601613733 Test Loss: 0.7951835977687757\n",
      "Epoch: 10310 Training Loss: 0.6291216035666066 Test Loss: 0.7951888724119084\n",
      "Epoch: 10320 Training Loss: 0.6291038353277292 Test Loss: 0.7953034075199331\n",
      "Epoch: 10330 Training Loss: 0.6290841835299601 Test Loss: 0.795491410871592\n",
      "Epoch: 10340 Training Loss: 0.6290660385793043 Test Loss: 0.795674516340342\n",
      "Epoch: 10350 Training Loss: 0.6290459472844606 Test Loss: 0.7956517851401749\n",
      "Epoch: 10360 Training Loss: 0.6290259187749132 Test Loss: 0.7957717204780735\n",
      "Epoch: 10370 Training Loss: 0.6290090295301851 Test Loss: 0.7958136036562822\n",
      "Epoch: 10380 Training Loss: 0.6289935215619776 Test Loss: 0.7959750454121657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-0c45f6750811>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# input x and predict based on x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mloss2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# must be (1. nn output, 2. target), the target label is NOT one-hotted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m#loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \"\"\"\n\u001b[0;32m   1281\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[1;32m-> 1282\u001b[1;33m                            input, target, size_average, reduce)\n\u001b[0m\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[1;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size=int(x_variable.size()[0])\n",
    "nb_of_epochs=13000\n",
    "my_net.cuda()\n",
    "#my_net.cpu()\n",
    "\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss1=0\n",
    "    sum_loss2=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss2= loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "        \n",
    "        #loss1=my_torch_loss_function(out, y_variable.narrow(0,b,batch_size).resize(batch_size,nb_of_points*2)).sum()\n",
    "        #sum_loss1+=loss1.data[0]\n",
    "        sum_loss2+=loss2.data[0]\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss2.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss1.data[0],loss2.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0: \n",
    "        my_net.eval()\n",
    "        test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "        #my_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss2/(x_variable.size(0)),\"Test Loss:\",test_loss/x_variable_test.size(0))\n",
    "       # print(\"Epoch:\",t,\"Training Loss:\",sum_loss1/(x_variable.size(0)),sum_loss2/(x_variable.size(0)))\n",
    "        my_net.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 -0.1835132056838366 -0.1300538270048559 0\n",
      "14 0.5652917122930424 -0.04600978073820392 0\n",
      "15 -0.1768214242382707 -0.673074468116609 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2d80be07e80>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization of error\n",
    "\n",
    "sample_index=233\n",
    "sample_contour=np.delete(polygons_reshaped[sample_index],24)\n",
    "#sample_contour=polygons_reshaped[sample_index]\n",
    "sample_contour=sample_contour.reshape(12,2)\n",
    "_,inserted_point_coordinates=get_extrapoints_target_length(sample_contour,.3)\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "\n",
    "my_net.eval()\n",
    "predictions=my_net(x_variable_test).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(nb_of_points,2)\n",
    "\n",
    "\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "#plt.scatter(polygons_reshaped[3112][0::2].sum()/12,polygons_reshaped[312][1::2].sum()/12,label='Original points')\n",
    "\n",
    "plot_contour(sample_contour)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "#original_points=point_coordinates.reshape(218722,2)\n",
    "#plt.scatter(original_points[:,0],original_points[:,1],label='Original points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Plotting the convex hull of inserted points \n",
    "\n",
    "plt.scatter(centers_of_mass[10][0],centers_of_mass[10][1],label='Original points'\n",
    "           )\n",
    "plot_contour(polygons[2])\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "points=centers_of_mass\n",
    "hull=ConvexHull(points)\n",
    "hull2=ConvexHull(original_points)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(points[:,0], points[:,1], 'o')\n",
    "for simplex in hull.simplices:\n",
    "    plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n",
    "for simplex in hull2.simplices:\n",
    "    plt.plot(original_points[simplex, 0], original_points[simplex, 1], 'k-')\n",
    "\n",
    "    \n",
    "    \n",
    "convex_points=centers_of_mass[hull.vertices]\n",
    "convex_points2=original_points[hull2.vertices]\n",
    "\n",
    "distances=np.empty([convex_points.shape[0],convex_points.shape[0]])    \n",
    "distances2=np.empty([convex_points2.shape[0],convex_points2.shape[0]])    \n",
    "\n",
    "    \n",
    "for i,point_i in enumerate(convex_points):\n",
    "    for j,point_j in enumerate(convex_points):\n",
    "        distances[i][j]=np.linalg.norm(point_i-point_j)\n",
    "for i,point_i in enumerate(convex_points2):\n",
    "    for j,point_j in enumerate(convex_points2):\n",
    "        distances2[i][j]=np.linalg.norm(point_i-point_j)\n",
    "        \n",
    "def max_element(A):\n",
    "    r, (c, l) = max(map(lambda t: (t[0], max(enumerate(t[1]), key=lambda v: v[1])), enumerate(A)), key=lambda v: v[1][1])\n",
    "    return (l, r, c)\n",
    "\n",
    "print(max_element(distances))\n",
    "print(max_element(distances2))\n",
    "\n",
    "maximum_distance_points=convex_points[[0,11]]\n",
    "maximum_distance_points2=convex_points2[[3,11]]\n",
    "\n",
    "plt.plot(maximum_distance_points[:,0],maximum_distance_points[:,1])\n",
    "plt.plot(maximum_distance_points2[:,0],maximum_distance_points2[:,1])\n",
    "1.8508252250365/0.6108970818704328"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distances=[np.linalg.norm(i-j) for i in centers_of_mass for j in predicted_inserted_points]\n",
    "distances=np.array(distances)\n",
    "100*distances.sum()/(x_variable.size(0) *max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15554, 25])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable,x_variable_test=x_variable.resize(x_variable.size()[0],1,x_variable.size()[1]),Variable(x_tensor_test.view(x_variable_test.size()[0],1,x_variable_test.size()[1]),volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15554, 1, 6])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_variable.size()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt_conv_net(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(1, 6, kernel_size=(2,), stride=(1,))\n",
      "    (1): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(1, 6, kernel_size=(2,), stride=(1,))\n",
      "    (1): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): BatchNorm1d(121, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): Linear(in_features=121, out_features=70, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): Linear(in_features=70, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "my_conv_net=alt_conv_net(nb_of_filters=6,out_dimension=2*nb_of_points,nb_of_hidden_nodes=70)\n",
    "print(my_conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15554, 15554)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(my_conv_net.parameters(), lr=1e-3,weight_decay=0)\n",
    "\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False) \n",
    "#loss_func=torch.nn.SmoothL1Loss()\n",
    "x_variable.size()[0],y_variable.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    loss_func.cuda()\n",
    "    my_conv_net.cuda()\n",
    "    x_variable , y_variable,x_variable_test,y_variable_test= x_variable.cuda(), y_variable.cuda(),x_variable_test.cuda(),y_variable_test.cuda()\n",
    "    print(\"cuda activated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 1.3447757861723673 Test Loss: 0.8073009600855195\n",
      "Epoch: 10 Training Loss: 0.8710827625731323 Test Loss: 0.7759311503343621\n",
      "Epoch: 20 Training Loss: 0.7696625968400411 Test Loss: 0.7522398395302855\n",
      "Epoch: 30 Training Loss: 0.7286550083579787 Test Loss: 0.7319482873987269\n",
      "Epoch: 40 Training Loss: 0.7065460452295229 Test Loss: 0.7254253120579347\n",
      "Epoch: 50 Training Loss: 0.6940771365082937 Test Loss: 0.72413610136558\n",
      "Epoch: 60 Training Loss: 0.6855037415013823 Test Loss: 0.7215659059124229\n",
      "Epoch: 70 Training Loss: 0.6791330430476084 Test Loss: 0.7196419798297646\n",
      "Epoch: 80 Training Loss: 0.6735303971245339 Test Loss: 0.7186378510400592\n",
      "Epoch: 90 Training Loss: 0.6681073171491256 Test Loss: 0.7180696966226209\n",
      "Epoch: 100 Training Loss: 0.6627495338819597 Test Loss: 0.7181440439734439\n",
      "Epoch: 110 Training Loss: 0.6574327494133342 Test Loss: 0.7186332671240033\n",
      "Epoch: 120 Training Loss: 0.6522169864865951 Test Loss: 0.7203791112075617\n",
      "Epoch: 130 Training Loss: 0.6470338719139771 Test Loss: 0.7229925084997106\n",
      "Epoch: 140 Training Loss: 0.6420366646039604 Test Loss: 0.7261691623263888\n",
      "Epoch: 150 Training Loss: 0.6371019286638485 Test Loss: 0.7294033976739326\n",
      "Epoch: 160 Training Loss: 0.6320538652637585 Test Loss: 0.7323457066414287\n",
      "Epoch: 170 Training Loss: 0.6268894607094638 Test Loss: 0.7354588135770319\n",
      "Epoch: 180 Training Loss: 0.6218336119326219 Test Loss: 0.7405561910244663\n",
      "Epoch: 190 Training Loss: 0.6170746120370966 Test Loss: 0.7464197103869278\n",
      "Epoch: 200 Training Loss: 0.6125678834624534 Test Loss: 0.7503599316004372\n",
      "Epoch: 210 Training Loss: 0.6082624445480262 Test Loss: 0.7534279528959298\n",
      "Epoch: 220 Training Loss: 0.6040976446975055 Test Loss: 0.7571357130513761\n",
      "Epoch: 230 Training Loss: 0.6000375707213579 Test Loss: 0.7603696972254372\n",
      "Epoch: 240 Training Loss: 0.5961601391723994 Test Loss: 0.7648040394724152\n",
      "Epoch: 250 Training Loss: 0.5924602644416227 Test Loss: 0.768296041606385\n",
      "Epoch: 260 Training Loss: 0.5888457149286357 Test Loss: 0.7728319207336677\n",
      "Epoch: 270 Training Loss: 0.5854601433915713 Test Loss: 0.7778327847704475\n",
      "Epoch: 280 Training Loss: 0.5821878365291886 Test Loss: 0.7822540032029642\n",
      "Epoch: 290 Training Loss: 0.5791048901207085 Test Loss: 0.7853163730951003\n",
      "Epoch: 300 Training Loss: 0.5761745747677446 Test Loss: 0.7871053562242798\n",
      "Epoch: 310 Training Loss: 0.573389732946509 Test Loss: 0.7900455302171746\n",
      "Epoch: 320 Training Loss: 0.5707526877129677 Test Loss: 0.7922831091861176\n",
      "Epoch: 330 Training Loss: 0.5681029598495564 Test Loss: 0.7950200838316616\n",
      "Epoch: 340 Training Loss: 0.5655437055982384 Test Loss: 0.7975827440803434\n",
      "Epoch: 350 Training Loss: 0.5630216830276777 Test Loss: 0.8002616348580568\n",
      "Epoch: 360 Training Loss: 0.5605767608010801 Test Loss: 0.8033535804277585\n",
      "Epoch: 370 Training Loss: 0.5582676431705671 Test Loss: 0.8062327192644033\n",
      "Epoch: 380 Training Loss: 0.5560470528079594 Test Loss: 0.8082022961275077\n",
      "Epoch: 390 Training Loss: 0.5539068276247268 Test Loss: 0.8109752513744213\n",
      "Epoch: 400 Training Loss: 0.5517529152468819 Test Loss: 0.8125710193021798\n",
      "Epoch: 410 Training Loss: 0.5497515711392568 Test Loss: 0.8154035026644483\n",
      "Epoch: 420 Training Loss: 0.5478167166605054 Test Loss: 0.8178300268856096\n",
      "Epoch: 430 Training Loss: 0.545981062950045 Test Loss: 0.8210088784802597\n",
      "Epoch: 440 Training Loss: 0.5442312367397454 Test Loss: 0.8232204609937629\n",
      "Epoch: 450 Training Loss: 0.5425457026729459 Test Loss: 0.8249997739438657\n",
      "Epoch: 460 Training Loss: 0.5409257164555741 Test Loss: 0.8270528659899048\n",
      "Epoch: 470 Training Loss: 0.539315901456217 Test Loss: 0.8295853226273148\n",
      "Epoch: 480 Training Loss: 0.5377814915978205 Test Loss: 0.8291333359455375\n",
      "Epoch: 490 Training Loss: 0.5361537827889932 Test Loss: 0.8316882098162616\n",
      "Epoch: 500 Training Loss: 0.5345813250409862 Test Loss: 0.8331413739993249\n",
      "Epoch: 510 Training Loss: 0.5329905339864344 Test Loss: 0.8363411985797646\n",
      "Epoch: 520 Training Loss: 0.5313456847916934 Test Loss: 0.8364709924768519\n",
      "Epoch: 530 Training Loss: 0.5297098138943359 Test Loss: 0.8371561309437693\n",
      "Epoch: 540 Training Loss: 0.5281745878270863 Test Loss: 0.8404728139869471\n",
      "Epoch: 550 Training Loss: 0.5267235568422913 Test Loss: 0.8418169688786008\n",
      "Epoch: 560 Training Loss: 0.5252846748123473 Test Loss: 0.844644617151331\n",
      "Epoch: 570 Training Loss: 0.5239120312700913 Test Loss: 0.8476869559582368\n",
      "Epoch: 580 Training Loss: 0.5227005889763083 Test Loss: 0.8484525955262988\n",
      "Epoch: 590 Training Loss: 0.5214013100528803 Test Loss: 0.8501599472736625\n",
      "Epoch: 600 Training Loss: 0.5202983921439019 Test Loss: 0.8511584874533822\n",
      "Epoch: 610 Training Loss: 0.5193024603799665 Test Loss: 0.8520723821694959\n",
      "Epoch: 620 Training Loss: 0.5182360521208371 Test Loss: 0.8531039516621657\n",
      "Epoch: 630 Training Loss: 0.5173116328094863 Test Loss: 0.8541250346619406\n",
      "Epoch: 640 Training Loss: 0.5163945279851646 Test Loss: 0.8551137790758423\n",
      "Epoch: 650 Training Loss: 0.5155290326744728 Test Loss: 0.8574293474111047\n",
      "Epoch: 660 Training Loss: 0.5146573530120869 Test Loss: 0.8598188235435957\n",
      "Epoch: 670 Training Loss: 0.5138796001530956 Test Loss: 0.861598576047293\n",
      "Epoch: 680 Training Loss: 0.5134094952463997 Test Loss: 0.863885196638696\n",
      "Epoch: 690 Training Loss: 0.5123455983991256 Test Loss: 0.8639820240162037\n",
      "Epoch: 700 Training Loss: 0.511683841375209 Test Loss: 0.8626481672373328\n",
      "Epoch: 710 Training Loss: 0.5109797042762312 Test Loss: 0.864352567696277\n",
      "Epoch: 720 Training Loss: 0.5103479586239874 Test Loss: 0.8659599523976016\n",
      "Epoch: 730 Training Loss: 0.5096717295892536 Test Loss: 0.8659472681367348\n",
      "Epoch: 740 Training Loss: 0.5090638423496368 Test Loss: 0.8652588644145448\n",
      "Epoch: 750 Training Loss: 0.5083977845329176 Test Loss: 0.8652166044761124\n",
      "Epoch: 760 Training Loss: 0.5076867410513373 Test Loss: 0.8679026536980774\n",
      "Epoch: 770 Training Loss: 0.5071471642141893 Test Loss: 0.8692603091644161\n",
      "Epoch: 780 Training Loss: 0.5065437033379677 Test Loss: 0.8709407225557806\n",
      "Epoch: 790 Training Loss: 0.5058656221369905 Test Loss: 0.8695187666779193\n",
      "Epoch: 800 Training Loss: 0.5054440502644014 Test Loss: 0.871584731364937\n",
      "Epoch: 810 Training Loss: 0.5046339943704192 Test Loss: 0.8703132284030993\n",
      "Epoch: 820 Training Loss: 0.5040186042878841 Test Loss: 0.8742925698865097\n",
      "Epoch: 830 Training Loss: 0.5034172467190916 Test Loss: 0.8740113811728395\n",
      "Epoch: 840 Training Loss: 0.5028170820709303 Test Loss: 0.8756389225461355\n",
      "Epoch: 850 Training Loss: 0.5024954016048926 Test Loss: 0.8753766974303948\n",
      "Epoch: 860 Training Loss: 0.5018195178892889 Test Loss: 0.8775764747902199\n",
      "Epoch: 870 Training Loss: 0.5010599727662498 Test Loss: 0.8767649332682291\n",
      "Epoch: 880 Training Loss: 0.5006074477425421 Test Loss: 0.8783743273574138\n",
      "Epoch: 890 Training Loss: 0.5001073314641732 Test Loss: 0.8794706996085713\n",
      "Epoch: 900 Training Loss: 0.499623727718754 Test Loss: 0.8793814702289094\n",
      "Epoch: 910 Training Loss: 0.49906311780731644 Test Loss: 0.8814797107084298\n",
      "Epoch: 920 Training Loss: 0.49869626332052847 Test Loss: 0.879123577855742\n",
      "Epoch: 930 Training Loss: 0.4982744716994021 Test Loss: 0.8800517894603588\n",
      "Epoch: 940 Training Loss: 0.49770880757160535 Test Loss: 0.8812407819331919\n",
      "Epoch: 950 Training Loss: 0.49732145368554714 Test Loss: 0.8836674317410944\n",
      "Epoch: 960 Training Loss: 0.4968366570194966 Test Loss: 0.8841774394973315\n",
      "Epoch: 970 Training Loss: 0.4964589720690819 Test Loss: 0.8858311029128086\n",
      "Epoch: 980 Training Loss: 0.496250713240967 Test Loss: 0.8871412865909529\n",
      "Epoch: 990 Training Loss: 0.4956339104892632 Test Loss: 0.8847327212737911\n",
      "Epoch: 1000 Training Loss: 0.4951347359903401 Test Loss: 0.8861958068094136\n",
      "Epoch: 1010 Training Loss: 0.49491589783978396 Test Loss: 0.8873927740403164\n",
      "Epoch: 1020 Training Loss: 0.4944426222756204 Test Loss: 0.8877513241865997\n",
      "Epoch: 1030 Training Loss: 0.49435083017230297 Test Loss: 0.8901128572691616\n",
      "Epoch: 1040 Training Loss: 0.4939072834459785 Test Loss: 0.8896034774466307\n",
      "Epoch: 1050 Training Loss: 0.4934136026604893 Test Loss: 0.8906589084201388\n",
      "Epoch: 1060 Training Loss: 0.4933267392029381 Test Loss: 0.892737306194541\n",
      "Epoch: 1070 Training Loss: 0.492875281529269 Test Loss: 0.8929670671376672\n",
      "Epoch: 1080 Training Loss: 0.4925536010632313 Test Loss: 0.8914389276700746\n",
      "Epoch: 1090 Training Loss: 0.4922635957792208 Test Loss: 0.8929877261566036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1100 Training Loss: 0.49202805673982575 Test Loss: 0.8950959507820537\n",
      "Epoch: 1110 Training Loss: 0.49175861364038187 Test Loss: 0.8939581976996528\n",
      "Epoch: 1120 Training Loss: 0.49129488144126915 Test Loss: 0.89683557832192\n",
      "Epoch: 1130 Training Loss: 0.49094852635375147 Test Loss: 0.8951446784376608\n",
      "Epoch: 1140 Training Loss: 0.4913312027352289 Test Loss: 0.8980588559751157\n",
      "Epoch: 1150 Training Loss: 0.4905435297994085 Test Loss: 0.8972522123360339\n",
      "Epoch: 1160 Training Loss: 0.4902437614018098 Test Loss: 0.8973256805796682\n",
      "Epoch: 1170 Training Loss: 0.49008686094613924 Test Loss: 0.899389447498714\n",
      "Epoch: 1180 Training Loss: 0.48988667002861 Test Loss: 0.9009508674527392\n",
      "Epoch: 1190 Training Loss: 0.4894899925260383 Test Loss: 0.9009585282439557\n",
      "Epoch: 1200 Training Loss: 0.4892375014566189 Test Loss: 0.8990774900334362\n",
      "Epoch: 1210 Training Loss: 0.489108634635785 Test Loss: 0.9009120611496914\n",
      "Epoch: 1220 Training Loss: 0.4891218823333226 Test Loss: 0.9037486888744213\n",
      "Epoch: 1230 Training Loss: 0.48867529052012343 Test Loss: 0.9061732665010931\n",
      "Epoch: 1240 Training Loss: 0.4883186386439983 Test Loss: 0.905310862348894\n",
      "Epoch: 1250 Training Loss: 0.4880755967616851 Test Loss: 0.9052357614776234\n",
      "Epoch: 1260 Training Loss: 0.48808539126792144 Test Loss: 0.9043163409448945\n",
      "Epoch: 1270 Training Loss: 0.48786743211151473 Test Loss: 0.9082578180258166\n",
      "Epoch: 1280 Training Loss: 0.4874861997918542 Test Loss: 0.9058678395463606\n",
      "Epoch: 1290 Training Loss: 0.48715663977111995 Test Loss: 0.9062624958807549\n",
      "Epoch: 1300 Training Loss: 0.4871266597920953 Test Loss: 0.9074663075890561\n",
      "Epoch: 1310 Training Loss: 0.48676153190095794 Test Loss: 0.9071485731336806\n",
      "Epoch: 1320 Training Loss: 0.48725439647759416 Test Loss: 0.9106123810442387\n",
      "Epoch: 1330 Training Loss: 0.4867178333346728 Test Loss: 0.9069173051496592\n",
      "Epoch: 1340 Training Loss: 0.4862298660111547 Test Loss: 0.9091276317957497\n",
      "Epoch: 1350 Training Loss: 0.48613794833724444 Test Loss: 0.910298037430877\n",
      "Epoch: 1360 Training Loss: 0.4860930568503279 Test Loss: 0.9104155866206919\n",
      "Epoch: 1370 Training Loss: 0.48567286625425937 Test Loss: 0.9083176601080247\n",
      "Epoch: 1380 Training Loss: 0.4857079318422914 Test Loss: 0.9087211703076775\n",
      "Epoch: 1390 Training Loss: 0.4853923415500032 Test Loss: 0.9077906981417181\n",
      "Epoch: 1400 Training Loss: 0.4854649213526263 Test Loss: 0.9122802357614777\n",
      "Epoch: 1410 Training Loss: 0.48535746431786037 Test Loss: 0.9090234575938786\n",
      "Epoch: 1420 Training Loss: 0.4850869538683779 Test Loss: 0.9115004048916538\n",
      "Epoch: 1430 Training Loss: 0.48495325257972227 Test Loss: 0.909871984411169\n",
      "Epoch: 1440 Training Loss: 0.4846928819556063 Test Loss: 0.9120926091700424\n",
      "Epoch: 1450 Training Loss: 0.4845364837823068 Test Loss: 0.9114220387651106\n",
      "Epoch: 1460 Training Loss: 0.48425837631196156 Test Loss: 0.9129158302589699\n",
      "Epoch: 1470 Training Loss: 0.48426173532531824 Test Loss: 0.9122649769724152\n",
      "Epoch: 1480 Training Loss: 0.4841126202463996 Test Loss: 0.9152108651620371\n",
      "Epoch: 1490 Training Loss: 0.4841034849857754 Test Loss: 0.9146947036554784\n",
      "Epoch: 1500 Training Loss: 0.4839337135443455 Test Loss: 0.9166641549318416\n",
      "Epoch: 1510 Training Loss: 0.4836027722470908 Test Loss: 0.9190482308344586\n",
      "Epoch: 1520 Training Loss: 0.48349170505778255 Test Loss: 0.9162264851385674\n",
      "Epoch: 1530 Training Loss: 0.4839496610096278 Test Loss: 0.9160840069806134\n",
      "Epoch: 1540 Training Loss: 0.48334563506573874 Test Loss: 0.916699319219393\n",
      "Epoch: 1550 Training Loss: 0.4831765856552173 Test Loss: 0.9172299859945666\n",
      "Epoch: 1560 Training Loss: 0.48295388620893337 Test Loss: 0.9187636512787745\n",
      "Epoch: 1570 Training Loss: 0.48300778738588146 Test Loss: 0.9186093679671425\n",
      "Epoch: 1580 Training Loss: 0.48291455122074706 Test Loss: 0.9171040852864584\n",
      "Epoch: 1590 Training Loss: 0.48278637503817345 Test Loss: 0.9180652006172839\n",
      "Epoch: 1600 Training Loss: 0.48240316498167674 Test Loss: 0.9191463768727495\n",
      "Epoch: 1610 Training Loss: 0.4824118607452263 Test Loss: 0.9208803530092593\n",
      "Epoch: 1620 Training Loss: 0.48234329920157193 Test Loss: 0.9204168123472866\n",
      "Epoch: 1630 Training Loss: 0.4826484043493635 Test Loss: 0.9188479827755273\n",
      "Epoch: 1640 Training Loss: 0.48216141019793946 Test Loss: 0.9199038533026299\n",
      "Epoch: 1650 Training Loss: 0.48197792016924906 Test Loss: 0.9194642369148662\n",
      "Epoch: 1660 Training Loss: 0.48197019757779347 Test Loss: 0.920562744140625\n",
      "Epoch: 1670 Training Loss: 0.4818463849733188 Test Loss: 0.9191016679928626\n",
      "Epoch: 1680 Training Loss: 0.4815538996701009 Test Loss: 0.9190889837319959\n",
      "Epoch: 1690 Training Loss: 0.48163322889208565 Test Loss: 0.9181411178023727\n",
      "Epoch: 1700 Training Loss: 0.4817640420571075 Test Loss: 0.9180264571076068\n",
      "Epoch: 1710 Training Loss: 0.48155060344204065 Test Loss: 0.9218510129324202\n",
      "Epoch: 1720 Training Loss: 0.48145516979153274 Test Loss: 0.9222489345220872\n",
      "Epoch: 1730 Training Loss: 0.4812176530153015 Test Loss: 0.9205694002379116\n",
      "Epoch: 1740 Training Loss: 0.4813735802988781 Test Loss: 0.9202826229142554\n",
      "Epoch: 1750 Training Loss: 0.4811439116846953 Test Loss: 0.9226931348259066\n",
      "Epoch: 1760 Training Loss: 0.48089167175646136 Test Loss: 0.9199706654489777\n",
      "Epoch: 1770 Training Loss: 0.4807758328846278 Test Loss: 0.9218461150495113\n",
      "Epoch: 1780 Training Loss: 0.48080138650025717 Test Loss: 0.9224775651845422\n",
      "Epoch: 1790 Training Loss: 0.48077768505087115 Test Loss: 0.9233104564525463\n",
      "Epoch: 1800 Training Loss: 0.4806558815758808 Test Loss: 0.9228605419519997\n",
      "Epoch: 1810 Training Loss: 0.4805969575752218 Test Loss: 0.9234195285373263\n",
      "Epoch: 1820 Training Loss: 0.480478167794458 Test Loss: 0.9218172300990226\n",
      "Epoch: 1830 Training Loss: 0.480359503584287 Test Loss: 0.9227981881349666\n",
      "Epoch: 1840 Training Loss: 0.48078716563062557 Test Loss: 0.92486572265625\n",
      "Epoch: 1850 Training Loss: 0.4803032165660763 Test Loss: 0.9238511701670203\n",
      "Epoch: 1860 Training Loss: 0.4801362076776874 Test Loss: 0.9238405580873843\n",
      "Epoch: 1870 Training Loss: 0.4801992755079079 Test Loss: 0.924802489732028\n",
      "Epoch: 1880 Training Loss: 0.4802767839562974 Test Loss: 0.9241894380545911\n",
      "Epoch: 1890 Training Loss: 0.4800499720731002 Test Loss: 0.92459156680009\n",
      "Epoch: 1900 Training Loss: 0.4799537222137392 Test Loss: 0.9250256573712384\n",
      "Epoch: 1910 Training Loss: 0.47997698416605056 Test Loss: 0.9242684949082112\n",
      "Epoch: 1920 Training Loss: 0.47982218701780893 Test Loss: 0.9260925167382009\n",
      "Epoch: 1930 Training Loss: 0.4796867591335026 Test Loss: 0.9245699658805941\n",
      "Epoch: 1940 Training Loss: 0.4798293759342452 Test Loss: 0.9266650038982125\n",
      "Epoch: 1950 Training Loss: 0.47983534053740196 Test Loss: 0.9277651437516075\n",
      "Epoch: 1960 Training Loss: 0.4797073527107175 Test Loss: 0.9275124632282021\n",
      "Epoch: 1970 Training Loss: 0.47952110012898613 Test Loss: 0.9265482082288452\n",
      "Epoch: 1980 Training Loss: 0.4795081977505786 Test Loss: 0.9266629945103524\n",
      "Epoch: 1990 Training Loss: 0.4793640427100746 Test Loss: 0.9241538342134452\n",
      "Epoch: 2000 Training Loss: 0.4793515484360936 Test Loss: 0.9264994805732382\n",
      "Epoch: 2010 Training Loss: 0.4792675417095281 Test Loss: 0.9264309102125129\n",
      "Epoch: 2020 Training Loss: 0.47919496190690497 Test Loss: 0.9261067708333334\n",
      "Epoch: 2030 Training Loss: 0.4791235122396168 Test Loss: 0.9265663555129565\n",
      "Epoch: 2040 Training Loss: 0.47898274760511766 Test Loss: 0.9257483462737911\n",
      "Epoch: 2050 Training Loss: 0.4790048166367976 Test Loss: 0.9269627700617284\n",
      "Epoch: 2060 Training Loss: 0.4792987773944805 Test Loss: 0.9274260595502186\n",
      "Epoch: 2070 Training Loss: 0.4791201846189083 Test Loss: 0.9308410142184285\n",
      "Epoch: 2080 Training Loss: 0.47887532196299987 Test Loss: 0.9266195414978781\n",
      "Epoch: 2090 Training Loss: 0.4789203704131574 Test Loss: 0.9291481049463092\n",
      "Epoch: 2100 Training Loss: 0.47878409492734986 Test Loss: 0.929144086170589\n",
      "Epoch: 2110 Training Loss: 0.47849719751551045 Test Loss: 0.927946993352945\n",
      "Epoch: 2120 Training Loss: 0.47850159248625757 Test Loss: 0.92730813360018\n",
      "Epoch: 2130 Training Loss: 0.4790910522413849 Test Loss: 0.9295576433095422\n",
      "Epoch: 2140 Training Loss: 0.4786533131549762 Test Loss: 0.9287046581629372\n",
      "Epoch: 2150 Training Loss: 0.4784391525090009 Test Loss: 0.9263804243425283\n",
      "Epoch: 2160 Training Loss: 0.47840380438713515 Test Loss: 0.9276231679406186\n",
      "Epoch: 2170 Training Loss: 0.47828645866818825 Test Loss: 0.9264338615009323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2180 Training Loss: 0.4782483479932815 Test Loss: 0.9279540890038259\n",
      "Epoch: 2190 Training Loss: 0.4782585819965925 Test Loss: 0.9289725974754051\n",
      "Epoch: 2200 Training Loss: 0.4786506133872316 Test Loss: 0.9287088025253987\n",
      "Epoch: 2210 Training Loss: 0.4783042269070657 Test Loss: 0.926317756558642\n",
      "Epoch: 2220 Training Loss: 0.47817824820986565 Test Loss: 0.9268723476080247\n",
      "Epoch: 2230 Training Loss: 0.4779358341805163 Test Loss: 0.9290418585632073\n",
      "Epoch: 2240 Training Loss: 0.47803283746343384 Test Loss: 0.9295622900189686\n",
      "Epoch: 2250 Training Loss: 0.47797058584206636 Test Loss: 0.9278883443447788\n",
      "Epoch: 2260 Training Loss: 0.47783183033705157 Test Loss: 0.9280568189581726\n",
      "Epoch: 2270 Training Loss: 0.47775746115348144 Test Loss: 0.9270826426062564\n",
      "Epoch: 2280 Training Loss: 0.4778806459049923 Test Loss: 0.9260265209056713\n",
      "Epoch: 2290 Training Loss: 0.477628500154703 Test Loss: 0.9288605113088348\n",
      "Epoch: 2300 Training Loss: 0.4775913940445384 Test Loss: 0.9267299322434414\n",
      "Epoch: 2310 Training Loss: 0.47755767834037866 Test Loss: 0.9300422825440458\n",
      "Epoch: 2320 Training Loss: 0.4774633120399093 Test Loss: 0.9302349326051311\n",
      "Epoch: 2330 Training Loss: 0.47756310926851614 Test Loss: 0.9304554629227753\n",
      "Epoch: 2340 Training Loss: 0.47736652850552913 Test Loss: 0.9295164508584105\n",
      "Epoch: 2350 Training Loss: 0.4773048733444773 Test Loss: 0.9289486104078254\n",
      "Epoch: 2360 Training Loss: 0.47757478733364406 Test Loss: 0.9282996409223894\n",
      "Epoch: 2370 Training Loss: 0.47719013321533044 Test Loss: 0.9282941778991448\n",
      "Epoch: 2380 Training Loss: 0.4772677986269609 Test Loss: 0.9259814352655606\n",
      "Epoch: 2390 Training Loss: 0.47739214490645493 Test Loss: 0.9283812723042052\n",
      "Epoch: 2400 Training Loss: 0.47724977924689793 Test Loss: 0.9320900371535815\n",
      "Epoch: 2410 Training Loss: 0.4771775133707567 Test Loss: 0.9304420251414609\n",
      "Epoch: 2420 Training Loss: 0.4769528047949884 Test Loss: 0.9313048060538838\n",
      "Epoch: 2430 Training Loss: 0.47713635760897516 Test Loss: 0.9293810055579668\n",
      "Epoch: 2440 Training Loss: 0.4769437009270123 Test Loss: 0.9302069895552019\n",
      "Epoch: 2450 Training Loss: 0.47697773055765397 Test Loss: 0.93208488809719\n",
      "Epoch: 2460 Training Loss: 0.476765892967645 Test Loss: 0.9319041059831533\n",
      "Epoch: 2470 Training Loss: 0.4769257443322457 Test Loss: 0.9319365701557677\n",
      "Epoch: 2480 Training Loss: 0.4768488323441719 Test Loss: 0.9292972392015496\n",
      "Epoch: 2490 Training Loss: 0.4768422712806995 Test Loss: 0.9295249279634452\n",
      "Epoch: 2500 Training Loss: 0.4768283329449016 Test Loss: 0.9301637249228395\n",
      "Epoch: 2510 Training Loss: 0.47680764518974217 Test Loss: 0.9325241905181005\n",
      "Epoch: 2520 Training Loss: 0.47663671222032916 Test Loss: 0.9314171433939364\n",
      "Epoch: 2530 Training Loss: 0.4765077512215507 Test Loss: 0.9299487204218106\n",
      "Epoch: 2540 Training Loss: 0.4767539951539797 Test Loss: 0.9305489622516396\n",
      "Epoch: 2550 Training Loss: 0.47646144706546545 Test Loss: 0.930806038310989\n",
      "Epoch: 2560 Training Loss: 0.47648803663848527 Test Loss: 0.9320657989125193\n",
      "Epoch: 2570 Training Loss: 0.4762770780426257 Test Loss: 0.9303050100067516\n",
      "Epoch: 2580 Training Loss: 0.47696972543236466 Test Loss: 0.9317945943447788\n",
      "Epoch: 2590 Training Loss: 0.476393513374775 Test Loss: 0.9328666027681327\n",
      "Epoch: 2600 Training Loss: 0.47636657848262504 Test Loss: 0.9313907701782729\n",
      "Epoch: 2610 Training Loss: 0.4762533452005915 Test Loss: 0.933369828840342\n",
      "Epoch: 2620 Training Loss: 0.47639266577327377 Test Loss: 0.9341495969167952\n",
      "Epoch: 2630 Training Loss: 0.4761648493253343 Test Loss: 0.9340125189887153\n",
      "Epoch: 2640 Training Loss: 0.47614981224684966 Test Loss: 0.9337582686310443\n",
      "Epoch: 2650 Training Loss: 0.476124384201813 Test Loss: 0.9326588195047261\n",
      "Epoch: 2660 Training Loss: 0.4761773749919635 Test Loss: 0.9359176070601852\n",
      "Epoch: 2670 Training Loss: 0.47614846236297736 Test Loss: 0.934689682697563\n",
      "Epoch: 2680 Training Loss: 0.47609456118602933 Test Loss: 0.9320490958759323\n",
      "Epoch: 2690 Training Loss: 0.4760328746323293 Test Loss: 0.9348614225662294\n",
      "Epoch: 2700 Training Loss: 0.4758419759386653 Test Loss: 0.9327162754388503\n",
      "Epoch: 2710 Training Loss: 0.4760049979607336 Test Loss: 0.9332713060418274\n",
      "Epoch: 2720 Training Loss: 0.4758025153798862 Test Loss: 0.9328240288628472\n",
      "Epoch: 2730 Training Loss: 0.47606492652613475 Test Loss: 0.931936695742509\n",
      "Epoch: 2740 Training Loss: 0.475994198889755 Test Loss: 0.9313662807637282\n",
      "Epoch: 2750 Training Loss: 0.4760506114785586 Test Loss: 0.9322840058754501\n",
      "Epoch: 2760 Training Loss: 0.4756448928933072 Test Loss: 0.9317692258230452\n",
      "Epoch: 2770 Training Loss: 0.4755349872319821 Test Loss: 0.9334987436302404\n",
      "Epoch: 2780 Training Loss: 0.4756306406310274 Test Loss: 0.9367636221426504\n",
      "Epoch: 2790 Training Loss: 0.47550343762054775 Test Loss: 0.9348613597728588\n",
      "Epoch: 2800 Training Loss: 0.47560292092267265 Test Loss: 0.9351476347495499\n",
      "Epoch: 2810 Training Loss: 0.47557008421266234 Test Loss: 0.9366640946502057\n",
      "Epoch: 2820 Training Loss: 0.475546979223592 Test Loss: 0.9368242805386767\n",
      "Epoch: 2830 Training Loss: 0.4754121791922496 Test Loss: 0.9362585122693222\n",
      "Epoch: 2840 Training Loss: 0.47551740734899384 Test Loss: 0.9353708651821309\n",
      "Epoch: 2850 Training Loss: 0.4753541027930918 Test Loss: 0.935682257507073\n",
      "Epoch: 2860 Training Loss: 0.4752254871134435 Test Loss: 0.9358741540477109\n",
      "Epoch: 2870 Training Loss: 0.47545587775853476 Test Loss: 0.9365661369920267\n",
      "Epoch: 2880 Training Loss: 0.4752467085436222 Test Loss: 0.9383513525189686\n",
      "Epoch: 2890 Training Loss: 0.47541170830252666 Test Loss: 0.9398652378914287\n",
      "Epoch: 2900 Training Loss: 0.47534769869286037 Test Loss: 0.9382283403059092\n",
      "Epoch: 2910 Training Loss: 0.47549408261138615 Test Loss: 0.9368639031555427\n",
      "Epoch: 2920 Training Loss: 0.47524507612591615 Test Loss: 0.9364823706356096\n",
      "Epoch: 2930 Training Loss: 0.47501634929117914 Test Loss: 0.9371125649032279\n",
      "Epoch: 2940 Training Loss: 0.47514819841359135 Test Loss: 0.9370659722222222\n",
      "Epoch: 2950 Training Loss: 0.4750446026745532 Test Loss: 0.9393597512578769\n",
      "Epoch: 2960 Training Loss: 0.4752423449655233 Test Loss: 0.9369508091804912\n",
      "Epoch: 2970 Training Loss: 0.4750545855366787 Test Loss: 0.9376510808497299\n",
      "Epoch: 2980 Training Loss: 0.4748147457044812 Test Loss: 0.936669118119856\n",
      "Epoch: 2990 Training Loss: 0.47496448863636365 Test Loss: 0.936780387972608\n",
      "Epoch: 3000 Training Loss: 0.47526557552518645 Test Loss: 0.9371548876350309\n",
      "Epoch: 3010 Training Loss: 0.47501911184422013 Test Loss: 0.938225012257266\n",
      "Epoch: 3020 Training Loss: 0.47471692621271055 Test Loss: 0.9369310920621142\n",
      "Epoch: 3030 Training Loss: 0.4749546313448309 Test Loss: 0.9382852938930684\n",
      "Epoch: 3040 Training Loss: 0.47460585902340235 Test Loss: 0.9412203816229424\n",
      "Epoch: 3050 Training Loss: 0.4747896943712228 Test Loss: 0.9400371033468364\n",
      "Epoch: 3060 Training Loss: 0.47457195496335347 Test Loss: 0.9389929751800411\n",
      "Epoch: 3070 Training Loss: 0.4746071461219783 Test Loss: 0.9394956361119149\n",
      "Epoch: 3080 Training Loss: 0.4744887016603446 Test Loss: 0.9393135981304656\n",
      "Epoch: 3090 Training Loss: 0.47474747125940275 Test Loss: 0.9399962248625578\n",
      "Epoch: 3100 Training Loss: 0.47455729459664714 Test Loss: 0.937658490467464\n",
      "Epoch: 3110 Training Loss: 0.4743532423833901 Test Loss: 0.9401393937475887\n",
      "Epoch: 3120 Training Loss: 0.47449240599283143 Test Loss: 0.9399849848492156\n",
      "Epoch: 3130 Training Loss: 0.47451896417320305 Test Loss: 0.9393744449066036\n",
      "Epoch: 3140 Training Loss: 0.47438683251695707 Test Loss: 0.9391418582617991\n",
      "Epoch: 3150 Training Loss: 0.4744250059771602 Test Loss: 0.9391181223677019\n",
      "Epoch: 3160 Training Loss: 0.47423401310555163 Test Loss: 0.9381702564380787\n",
      "Epoch: 3170 Training Loss: 0.4742848378029767 Test Loss: 0.9390738530414094\n",
      "Epoch: 3180 Training Loss: 0.4741333996680918 Test Loss: 0.9391882625626929\n",
      "Epoch: 3190 Training Loss: 0.4747513325551305 Test Loss: 0.9369001349303948\n",
      "Epoch: 3200 Training Loss: 0.47433522300332714 Test Loss: 0.9394916801295653\n",
      "Epoch: 3210 Training Loss: 0.47414834256863186 Test Loss: 0.936642807697563\n",
      "Epoch: 3220 Training Loss: 0.4739961510101903 Test Loss: 0.939710075472608\n",
      "Epoch: 3230 Training Loss: 0.4740460653208178 Test Loss: 0.9386159637827932\n",
      "Epoch: 3240 Training Loss: 0.47416658169723225 Test Loss: 0.9395079436125578\n",
      "Epoch: 3250 Training Loss: 0.4740096812415617 Test Loss: 0.9397094475389017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3260 Training Loss: 0.4739783513786647 Test Loss: 0.9388479224938915\n",
      "Epoch: 3270 Training Loss: 0.4738865906679954 Test Loss: 0.9386815828550991\n",
      "Epoch: 3280 Training Loss: 0.4740786822822907 Test Loss: 0.9397210015190972\n",
      "Epoch: 3290 Training Loss: 0.4739700323268934 Test Loss: 0.9382909452964249\n",
      "Epoch: 3300 Training Loss: 0.47386634240991066 Test Loss: 0.9385809878753536\n",
      "Epoch: 3310 Training Loss: 0.47418221523603254 Test Loss: 0.9400043252073689\n",
      "Epoch: 3320 Training Loss: 0.47379564616617914 Test Loss: 0.9405587906700102\n",
      "Epoch: 3330 Training Loss: 0.4738712082703806 Test Loss: 0.9414119014033565\n",
      "Epoch: 3340 Training Loss: 0.4737527638087469 Test Loss: 0.9413389355066872\n",
      "Epoch: 3350 Training Loss: 0.47385629676248875 Test Loss: 0.9412822958863811\n",
      "Epoch: 3360 Training Loss: 0.4737037284922689 Test Loss: 0.9444534238964442\n",
      "Epoch: 3370 Training Loss: 0.4738621357950527 Test Loss: 0.9439016585487398\n",
      "Epoch: 3380 Training Loss: 0.4738014851987431 Test Loss: 0.943454130196277\n",
      "Epoch: 3390 Training Loss: 0.47374526096582875 Test Loss: 0.9418127742814429\n",
      "Epoch: 3400 Training Loss: 0.4735169422355182 Test Loss: 0.943274415569541\n",
      "Epoch: 3410 Training Loss: 0.47374042649800696 Test Loss: 0.9443984796971451\n",
      "Epoch: 3420 Training Loss: 0.473586382773322 Test Loss: 0.9420422840510867\n",
      "Epoch: 3430 Training Loss: 0.4736840453018516 Test Loss: 0.9435650232888053\n",
      "Epoch: 3440 Training Loss: 0.47342552684397904 Test Loss: 0.942817468211484\n",
      "Epoch: 3450 Training Loss: 0.473494339528819 Test Loss: 0.9448510943126286\n",
      "Epoch: 3460 Training Loss: 0.47335831518419696 Test Loss: 0.9435465620378408\n",
      "Epoch: 3470 Training Loss: 0.4733126074884274 Test Loss: 0.9423859521685314\n",
      "Epoch: 3480 Training Loss: 0.4733616428049055 Test Loss: 0.9448417381004051\n",
      "Epoch: 3490 Training Loss: 0.47354243306585125 Test Loss: 0.9414007241833847\n",
      "Epoch: 3500 Training Loss: 0.47406333127732414 Test Loss: 0.9464672072924705\n",
      "Epoch: 3510 Training Loss: 0.4735316026022245 Test Loss: 0.9403533935546875\n",
      "Epoch: 3520 Training Loss: 0.47323811273426447 Test Loss: 0.9426049754452803\n",
      "Epoch: 3530 Training Loss: 0.4732085722523145 Test Loss: 0.9420952188625257\n",
      "Epoch: 3540 Training Loss: 0.4731228075374502 Test Loss: 0.9436450220429848\n",
      "Epoch: 3550 Training Loss: 0.4731512806693616 Test Loss: 0.9417973271122685\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-a3bb14a545da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_conv_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                 \u001b[1;31m# input x and predict based on x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# must be (1. nn output, 2. target), the target label is NOT one-hotted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m#loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#loss=torch.sqrt((out[0]-y_variable.narrow(0,b,batch_size)[0]).pow(2)+(out[1]-y_variable.narrow(0,b,batch_size)[1]).pow(2))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \"\"\"\n\u001b[0;32m   1281\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[1;32m-> 1282\u001b[1;33m                            input, target, size_average, reduce)\n\u001b[0m\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[1;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size=int(x_variable.size()[0])\n",
    "nb_of_epochs=13000\n",
    "# Train the network #\n",
    "my_conv_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_conv_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        #loss=loss_func.apply(out, y_variable.narrow(0,b,batch_size))\n",
    "        #loss=torch.sqrt((out[0]-y_variable.narrow(0,b,batch_size)[0]).pow(2)+(out[1]-y_variable.narrow(0,b,batch_size)[1]).pow(2)) \n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_conv_net.eval()\n",
    "        test_loss=loss_func(my_conv_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_conv_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable=x_variable.cpu()\n",
    "my_conv_net=my_conv_net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour1=np.delete(polygons_reshaped[0],16)\n",
    "contour1=contour1.reshape(8,2)\n",
    "plot_contour(contour1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.392270715649738 -0.1321869205956847 0\n",
      "14 0.5224069280092827 0.3559939700162692 0\n",
      "15 0.4179047172785241 -0.5898560627234799 0\n",
      "16 -0.5408761102342659 0.08821852929451014 0\n",
      "17 -0.1569743994224644 -0.1506476053101568 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2d9088e7550>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_index=122\n",
    "\n",
    "contour1=np.delete(polygons_reshaped[sample_index],24)\n",
    "contour1=contour1.reshape(12,2)\n",
    "nb_of_inserted_points,inserted_point_coordinates=get_extrapoints_target_length(contour1,.2\n",
    "                                                                              )\n",
    "inserted_point_coordinates=np.array(inserted_point_coordinates)\n",
    "my_conv_net.eval()\n",
    "predictions=my_conv_net(x_variable).cpu()\n",
    "predicted_inserted_points=predictions.data.numpy()[sample_index]\n",
    "predicted_inserted_points=predicted_inserted_points.reshape(3,2)\n",
    "plt.scatter(predicted_inserted_points[:,0],predicted_inserted_points[:,1],label='Predicted points')\n",
    "plt.scatter(inserted_point_coordinates[:,0],inserted_point_coordinates[:,1],label='Original points')\n",
    "\n",
    "plot_contour(contour1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour=polygons_reshaped[1]\n",
    "contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor=torch.FloatTensor([[2.2,1.2],[3,3]])\n",
    "a_tensor=torch.FloatTensor([[1,2],[3,2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_variable=Variable(d_tensor)\n",
    "\n",
    "a_variable=Variable(a_tensor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_variable.requires_grad=False\n",
    "a_variable.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fun=torch.sqrt((a_variable[0]-d_variable[0]).pow(2)+(a_variable[1]-d_variable[1]).pow(2))\n",
    "#print(fun)\n",
    "\n",
    "    \n",
    "loss_result=my_torch_loss_function(a_variable,d_variable) \n",
    "\n",
    "loss_result.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_result.sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_loss_func=torch.nn.MSELoss(size_average=False)\n",
    "another_loss_result=another_loss_func(out,y_variable.view(0,b,batch_size))\n",
    "\n",
    "another_loss_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myloss=myLossfunction().apply(out,y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "myloss.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush all variables in GPU\n",
    "del my_conv_net,my_net,x_variable_test,x_variable,y_variable,y_variable_test,loss_func\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Should I order the interior points ?\n",
    "\n",
    "For each interior point calculate distance with points of the polygon\n",
    "\n",
    "find which ti which point it is closer and sort it according to the index.\n",
    "\n",
    "If multiple points are closer to the same point of the contour then take into\n",
    "\n",
    "account the distance to the point.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Checking jumps to insertion points ##\n",
    "\n",
    "contour3=apply_procrustes(generate_contour(12))\n",
    "plot_contour(contour3)\n",
    "\n",
    "\n",
    "inserted_points=[]\n",
    "nb_of_sampling=100\n",
    "contour=contour3.copy()\n",
    "plot_contour(contour3)\n",
    "\n",
    "for i in range(0,nb_of_sampling):\n",
    "    contour[0]=np.random.normal(contour3[0],0.08)\n",
    "    contour[11]=np.random.normal(contour3[11],0.08)\n",
    "\n",
    "    nb_of_points,point_coords=get_extrapoints_target_length(contour,0.2)\n",
    "    inserted_points.append(point_coords)\n",
    "    plt.scatter(contour[0][0],contour[0][1])\n",
    "    plt.scatter(contour[11][0],contour[11][1])\n",
    "\n",
    "\n",
    "inserted_points=[i for i in inserted_points if len(i)==1]\n",
    "inserted_points=np.array(inserted_points)\n",
    "inserted_points=inserted_points.reshape(len(inserted_points),2)\n",
    "\n",
    "plt.scatter(inserted_points[:,0],inserted_points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_er(n,r)->'joules':\n",
    "    return n*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_er.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:,},{}\".format(get_er(4,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewew=my_torch_loss_function(out,y_variable.narrow(0,b,batch_size).resize(batch_size,2))\n",
    "ewew.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(out,y_variable.narrow(0,b,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon=polygons[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_polygon_angles(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_edge_lengths(polygon),get_polygon_angles(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_polygon_angles(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon=np.delete(polygons_reshaped[0],24).reshape(12,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge2elem(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[]\n",
    "angles=[]\n",
    "for polygons in polygons_reshaped:\n",
    "    polygon=np.delete(polygons,24).reshape(12,2)\n",
    "    lengths.append(get_edge_lengths(polygon))\n",
    "    angles.append(np.array(np.multiply(pi/180,get_polygon_angles(polygon))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles=np.array(angles)\n",
    "angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.empty(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.hstack([lengths[0],angles[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.empty([polygons_reshaped.shape[0],24])\n",
    "for index,_ in enumerate(lengths):\n",
    "    data[index]=np.hstack([lengths[index],angles[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(data[:12]).type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "angles_normalised=preprocessing.scale(angles,axis=1)\n",
    "lengths_normalised=preprocessing.scale(angles,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalised=np.empty([polygons_reshaped.shape[0],24])\n",
    "for index,_ in enumerate(lengths):\n",
    "    data_normalised[index]=np.hstack([lengths[index],angles[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(pi/180,get_polygon_angles(polygon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_tensor=torch.from_numpy(angles).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_tensor=torch.cat((x_tensor,angles_tensor[:nb_of_training_data]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_tensor.size()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_tensor=torch.from_numpy(np.array(lengths)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_tensor,lengths_tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_tensor,lengths_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_tensor[3],angles_tensor[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_tensor=torch.from_numpy(polygons_reshaped).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor=torch.from_numpy(data).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor=torch.cat((polygon_tensor,data_tensor),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor.view(a_tensor.size()[1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor=torch.randn([2,1,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_variable=Variable(random_tensor)\n",
    "random_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=F.relu\n",
    "conv=F.conv1d(random_variable,weight=Variable(torch.rand([3,1,6])))\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y(conv)\n",
    "#conv=F.conv1d(input=random_variable,weight=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(F.conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_variable.narrow(1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_net=alt_conv_net(nb_of_filters=15,nb_of_hidden_nodes=22,out_dimension=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=a_net(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_variable.narrow(1,0,1).narrow(2,12,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor_reshaped=x_tensor.resize_(x_tensor.size()[0],0,(x_tensor.size()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=my_conv_net(x_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable.narrow(1,0,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.8000\n",
       " 0.7000\n",
       " 0.4000\n",
       "   ⋮    \n",
       " 0.6000\n",
       " 0.7000\n",
       " 1.0000\n",
       "[torch.cuda.FloatTensor of size 174978x1 (GPU 0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenghts=x_variable.narrow(1,0,1).narrow(2,0,12)\n",
    "angles=x_variable.narrow(1,0,1).narrow(2,12,12)\n",
    "target_edge_length=x_variable.narrow(1,0,1).narrow(2,24,1).resize(x_variable.size()[0],1)\n",
    "target_edge_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_tensor=torch.cat([lenghts,angles],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat_tensor=concat_tensor.view(-1,2*12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_tensor=torch.cat([concat_tensor,target_edge_length],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5737  0.6448  1.4051  ...   3.2888  0.9418  1.0000\n",
       " 0.7179  0.4341  1.0819  ...   1.7742  3.2546  0.8000\n",
       " 0.7109  0.4569  1.1069  ...   4.0847  0.8838  0.7000\n",
       "          ...             ⋱             ...          \n",
       " 0.8995  0.9301  0.2706  ...   1.4332  3.4208  0.9000\n",
       " 0.6886  0.3743  0.6793  ...   3.3749  1.7661  0.9000\n",
       " 0.8210  1.0848  0.3460  ...   2.5856  3.2940  0.8000\n",
       "[torch.cuda.FloatTensor of size 76672x17 (GPU 0)]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0000\n",
       " 0.8000\n",
       " 0.7000\n",
       "   ⋮   \n",
       " 0.9000\n",
       " 0.9000\n",
       " 0.8000\n",
       "[torch.cuda.FloatTensor of size 76672 (GPU 0)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_edge_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-2f29d43d3c4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlengths\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lengths' is not defined"
     ]
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
