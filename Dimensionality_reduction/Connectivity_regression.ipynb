{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.spatial\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA,KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from itertools import permutations\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import triplot\n",
    "from numpy import pi,sin,cos,tan,sqrt\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "import triangle as tri\n",
    "import triangle\n",
    "import triangle.plot as plot\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.path import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "import cTimer as ctimer\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  ======================================\n",
    "  Main idea of finding the connectivity:\n",
    "  ======================================\n",
    "  \n",
    "1) Build set of elements and edges\n",
    "2) Sort edges according to maximum quality\n",
    "3) for each pair of the edge put into set of edges  of the maximum quality\n",
    "4) Put also to the set of elements the element that is formed\n",
    "5) Proceed to the next edge and check if the edge formed from each pair already exist\n",
    "6) If a pair of edges already exists proceed to next edge\n",
    "7) If yes proceed to the next element\n",
    "\n",
    "(*) Add function computing the quality of the mesh given that every point of a contour is connected \n",
    "    to a point of the mesh -> normalize the qualities for each point -> Add as parameter to neural network"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                             Issues:\n",
    "\n",
    "(1) Avoiding the consideration of triangles that are invalid ( by setting quality to 0):\n",
    "     \n",
    "     1st Approach (Failed):\n",
    "     See if formed triangle contains points of the polygon\n",
    "     \n",
    "     2nd Approach :\n",
    "     Calculate the angles of the polygon. Once that is done, any edge departing from a point in the polygon\n",
    "     can have a greater angle from the departed edge greater than the edge has with the pre-existing edges\n",
    "     \n",
    "     \n",
    "(2) Avoiding interseactions when creating elements:\n",
    "\n",
    "    Here the main idea is whenever a new element is created to check if it includes a forabideen vertex. If it does \n",
    "    the new element can't be formed and we porceed to check the connection with the second smallest quality.\n",
    "    \n",
    "   ==========================================================================\n",
    "      How to check if the vertex is locked ( So no new connections with it.)\n",
    "   ==========================================================================\n",
    "    \n",
    "    \n",
    "    So the idea is that given  a vertex we check the edges and the elements that include those edges.\n",
    "    If start from an edge of the contour and end up to an edge of the contour again then the vertex\n",
    "    \n",
    "    \n",
    "    + For every vertex of the element that is to be created  and\n",
    "        \n",
    "        for vtx in element:\n",
    "        \n",
    "            if closed_ring(vtx,adj_vertices,elements):\n",
    "            \n",
    "                  don't created element\n",
    "                  proceed to connection with second most great quality\n",
    "                  if doesn't exist:\n",
    "                  proceeed  to next edge\n",
    "                  \n",
    "    + for the vtx and the adjacent v1 look for the element \n",
    "        (vtx,v1,v2) ->  (vtx, v2) -> (vtx,v3,v2) -> (vtx,v3)-> ... -> (vtx,vn) \n",
    "        Check if end is edge of contour if yes then vtx is forbidden -> insert to forbidden vertices\n",
    "                \n",
    "               \n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BCE_accuracy(model,variable,labels):\n",
    "    net.eval()\n",
    "    predictions=model(variable).data.numpy()\n",
    "    predictions[np.where(predictions>0.5)]=1\n",
    "    predictions[np.where(predictions<=0.5)]=0\n",
    "    diff=labels-predictions\n",
    "    correct_prediction=0\n",
    "    for i in diff:\n",
    "        if (not i.any()):\n",
    "            correct_prediction+=1\n",
    "    net.train()\n",
    "    return  100*correct_prediction/variable.size()[0],diff\n",
    "\n",
    "def plot_contour(contour):    \n",
    "    plot_coords=np.vstack([contour,contour[0]])\n",
    "    (s,t)=zip(*plot_coords)\n",
    "    plt.plot(s,t)\n",
    "    indices=[i for i in range(contour.shape[0])]\n",
    "    for index,i in enumerate(indices):\n",
    "        plt.annotate(str(i),(s[index],t[index]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def connectivity_information(triangulation,print_info=False):\n",
    "    \n",
    "    segments= tuple(triangulation['segments'].tolist())\n",
    "    triangles=tuple(triangulation['triangles'].tolist())\n",
    "    vertices=triangulation['vertices']   \n",
    "    \n",
    "    connect_info={str(r):[0 for i in range(len(vertices))] for r in tuple(triangulation['segments'].tolist())}\n",
    "    for segment in segments:\n",
    "        for triangle in triangles:\n",
    "            if set(segment).issubset(set(triangle)):\n",
    "                connection=set(triangle)-set(segment)\n",
    "                if print_info: print(\"segment:\",segment,\"is connected to:\",connection,\"to form triangle:\",triangle)\n",
    "                connect_info[str(segment)][tuple(connection)[0]]=1    \n",
    "    return connect_info\n",
    "\n",
    "\n",
    "\n",
    "def get_labels(triangulation,connect_info):\n",
    "    indices=[]\n",
    "    vertices=list(range(triangulation['vertices'].shape[0]))\n",
    "    for i in triangulation['segments']:\n",
    "           indices.append(set(vertices)-set(i)) \n",
    "    labels=[]\n",
    "    list_values=list(connect_info.values())\n",
    "    for i in range(len(list_values)):\n",
    "        for j in indices[i]:\n",
    "            labels.append(list_values[i][j])\n",
    "    return  labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def rot(theta):\n",
    "    return np.array([[cos(theta),-sin(theta)],     \n",
    "                     [sin(theta),cos(theta)]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_reference_polygon(nb_of_points,plot=False):\n",
    "    angles=np.empty(nb_of_points)\n",
    "    points=np.empty([nb_of_points,2])\n",
    "    plot_coords=np.empty([nb_of_points,2])\n",
    "    indices=[]\n",
    "    angle_division=2*pi/nb_of_points\n",
    "   \n",
    "    for i in range(nb_of_points):\n",
    "        angle=i*angle_division\n",
    "        angles[i]=angle\n",
    "        point=np.array([1,0]) #pick edge length of 1\n",
    "        points[i]=np.dot(rot(angle),point.T)  #rotate it according to the  chosen angle\n",
    "        indices.append(i)\n",
    "   \n",
    "    if plot==True:\n",
    "        plot_coords=np.vstack([points,points[0]])\n",
    "        (s,t)=zip(*plot_coords)\n",
    "        plt.plot(s,t)\n",
    "        for index,i in enumerate(indices):\n",
    "            plt.annotate(str(i),(s[index],t[index]))\n",
    "    \n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "def generate_contour(nb_of_points,plot=False):\n",
    "    \n",
    "    angles=np.empty(nb_of_points)\n",
    "    points=np.empty([nb_of_points,2])\n",
    "    plot_coords=np.empty([nb_of_points,2])\n",
    "    indices=[]\n",
    "    angle_division=2*pi/nb_of_points\n",
    "   \n",
    "    for i in range(nb_of_points):\n",
    "        angle=((i+1)*angle_division-i*angle_division)*np.random.random_sample()+i*angle_division\n",
    "        angles[i]=angle\n",
    "        point=np.array([np.random.uniform(0.3,1),0]) #pick random point at (1,0)\n",
    "       #point=np.array([1,0]) #pick edge length of 1\n",
    "\n",
    "        points[i]=np.dot(rot(angle),point.T)  #rotate it according to the  chosen angle\n",
    "        indices.append(i)\n",
    "   \n",
    "    if plot==True:\n",
    "        plot_coords=np.vstack([points,points[0]])\n",
    "        (s,t)=zip(*plot_coords)\n",
    "        plt.plot(s,t)\n",
    "        for index,i in enumerate(indices):\n",
    "            plt.annotate(str(i),(s[index],t[index]))\n",
    "    \n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "def get_barycenter(contour):\n",
    "    return np.array([contour[:,0].sum()/contour.shape[0],contour[:,1].sum()/contour.shape[0]])\n",
    "\n",
    "\n",
    "def apply_procrustes(polygon_points,plot=False):  \n",
    "    \n",
    "    # Get reference polygona and adjust any random poygon to that\n",
    "    ref_polygon=get_reference_polygon(polygon_points.shape[0])\n",
    "    \n",
    "    \n",
    "    #Mean of each coordinate\n",
    "    mu_polygon = polygon_points.mean(0)\n",
    "    mu_ref_polygon = ref_polygon.mean(0)\n",
    "    \n",
    "    #Centralize data to the mean \n",
    "    centralised_ref_polygon_points = ref_polygon-mu_ref_polygon\n",
    "    centralised_polygon_points = polygon_points-mu_polygon\n",
    "    \n",
    "    #Squared sum of X-mean(X)\n",
    "    ss_ref_polygon_points = (centralised_ref_polygon_points**2.).sum()\n",
    "    ss_polygon_points = (centralised_polygon_points**2.).sum()\n",
    "\n",
    "       \n",
    "    #Frobenius norm of X\n",
    "    norm_ss_ref_polygon_points = np.sqrt(ss_ref_polygon_points)\n",
    "    norm_ss_polygon_points = np.sqrt(ss_polygon_points)\n",
    "\n",
    "    \n",
    "    # scale to equal (unit) norm\n",
    "    centralised_ref_polygon_points /=norm_ss_ref_polygon_points     \n",
    "    centralised_polygon_points /=norm_ss_polygon_points\n",
    "        \n",
    "    \n",
    "    #Finding best rotation to superimpose on regular triangle\n",
    "    #Applying SVD to the  matrix \n",
    "    A = np.dot(centralised_ref_polygon_points.T, centralised_polygon_points)\n",
    "    U,s,Vt = np.linalg.svd(A,full_matrices=False)\n",
    "    V=Vt.T\n",
    "    R = np.dot(V,U.T)\n",
    "    \n",
    "  \n",
    "    traceTA = s.sum()\n",
    "    d = 1 - traceTA**2\n",
    "    b = traceTA * norm_ss_ref_polygon_points / norm_ss_polygon_points    \n",
    "    indices=[i for i in range(polygon_points.shape[0])]\n",
    "    \n",
    "   \n",
    "\n",
    "    polygon_transformed =norm_ss_ref_polygon_points*traceTA*np.dot(centralised_polygon_points,R)+mu_ref_polygon\n",
    "\n",
    "    if plot==True:\n",
    "        plot_coords=np.vstack([polygon_transformed,polygon_transformed[0]])\n",
    "        (s,t)=zip(*plot_coords)\n",
    "        plt.plot(s,t)\n",
    "        for index,i in enumerate(indices):\n",
    "            plt.annotate(str(i),(s[index],t[index]))\n",
    "    \n",
    "    return polygon_transformed\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def contains_points(triangle,polygon):\n",
    "    hull=ConvexHull(triangle)\n",
    "    hull_path=Path(triangle[hull.vertices])\n",
    "    set_polygon=set(tuple(i) for i in polygon)\n",
    "    set_triangle=set(tuple(i) for i in triangle)\n",
    "    #print(set_polygon,set_triangle)\n",
    "    difference=set_polygon-set_triangle\n",
    "    \n",
    "    if len(difference)==0:\n",
    "        return False\n",
    "\n",
    "    for i in difference:\n",
    "        if hull_path.contains_point(i):\n",
    "            return True\n",
    "            break\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_edge_lengths(pt1,pt2):\n",
    "    return np.linalg.norm(pt1-pt2)\n",
    "\n",
    "def compute_edge_lengths2(triangle):\n",
    "    edgelengths2=np.empty([2,3])\n",
    "    for i in range(2):\n",
    "        for j in range(i+1,3):\n",
    "            eij=triangle[j]-triangle[i]\n",
    "            edgelengths2[i][j]=np.dot(eij,eij)\n",
    "    return edgelengths2\n",
    "    \n",
    "\n",
    "\n",
    "def compute_triangle_normals(triangle):\n",
    "   \n",
    "    e01=triangle[1]-triangle[0]\n",
    "    e02=triangle[2]-triangle[0]\n",
    "    \n",
    "    e01_cross_e02=np.cross(e01,e02)\n",
    "    \n",
    "    return e01_cross_e02\n",
    "\n",
    "\n",
    "def compute_triangle_area(triangle):\n",
    "   \n",
    "    e01=triangle[1]-triangle[0]\n",
    "    e02=triangle[2]-triangle[0]\n",
    "    \n",
    "    e01_cross_e02=np.cross(e01,e02)\n",
    "    \n",
    "    # Omit triangles that are inverted (out of the domain)\n",
    "    if e01_cross_e02<0:\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "    e01_cross_e02_norm=np.linalg.norm(e01_cross_e02)\n",
    "\n",
    "        \n",
    "    return e01_cross_e02_norm/2\n",
    "\n",
    "def compute_triangle_quality(triangle,polygon=None):\n",
    "    \n",
    "    if polygon is None:\n",
    "        polygon=triangle\n",
    "    \n",
    "    # The incoming triangle has edged [p0,p1] which is an edge and p2 is the connection\n",
    "    polygon_angles=get_polygon_angles(polygon)\n",
    "    \n",
    "    indices=[]\n",
    "\n",
    "\n",
    "    for point in triangle: \n",
    "        for index,point_in_polygon in enumerate(polygon):\n",
    "              if np.allclose(point,point_in_polygon):\n",
    "                    indices.append(index)\n",
    "                \n",
    "    p0,p1,p2=indices[0],indices[1],indices[2]\n",
    "    \n",
    "    neighbor_points=connection_indices(p2,get_contour_edges(polygon))\n",
    "    \n",
    "    # Checking if edges of connected poiints form an angle bigger than the polygon angles\n",
    "    if (polygon_angles[p0]<calculate_angle(polygon[p0],polygon[p1],polygon[p2]) \n",
    "        or polygon_angles[p1]<calculate_angle(polygon[p1],polygon[p0],polygon[p2])):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p0])\n",
    "        or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p1])\n",
    "    ):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p0])\n",
    "        or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p1])):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    factor=4/sqrt(3)\n",
    "    area=compute_triangle_area(triangle)\n",
    "   \n",
    "    if area==0:\n",
    "        return 0\n",
    "    \n",
    "    if contains_points(triangle,polygon):\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    #edgelengths=np.empty([triangle.shape[0],1])\n",
    "    \n",
    "    #for i,_ in enumerate(triangle):\n",
    "     #   edgelengths[i]=compute_edge_length(triangle[i],triangle[(i+1)%3])\n",
    "    \n",
    "    #sum_edge_lengths=edgelengths.sum() \n",
    "    #factor=4/sqrt(3)*3/sum_edge_lengths\n",
    "    \n",
    "    sum_edge_lengths=0\n",
    "    edge_length2=compute_edge_lengths2(triangle)\n",
    "    for i in range(2):\n",
    "        for j in range(i+1,3):\n",
    "            sum_edge_lengths+=edge_length2[i][j]\n",
    "    \n",
    "    \n",
    "    \n",
    "    lrms=sqrt(sum_edge_lengths/3)\n",
    "    lrms2=lrms**2\n",
    "    quality=area/lrms2\n",
    "    \n",
    "    return quality*factor\n",
    "\n",
    "\n",
    "\n",
    "def compute_minimum_quality_triangle(triangle,polygon=None,get_mean=False):\n",
    "    if polygon is None:\n",
    "        polygon=triangle\n",
    "    \n",
    "    # The incoming triangle has edged [p0,p1] which is an edge and p2 is the connection\n",
    "    polygon_angles=get_polygon_angles(polygon)\n",
    "    \n",
    "    indices=[]\n",
    "\n",
    "\n",
    "    for point in triangle: \n",
    "        for index,point_in_polygon in enumerate(polygon):\n",
    "              if np.allclose(point,point_in_polygon):\n",
    "                    indices.append(index)\n",
    "                \n",
    "    p0,p1,p2=indices[0],indices[1],indices[2]\n",
    "    \n",
    "    neighbor_points=connection_indices(p2,get_contour_edges(polygon))\n",
    "    \n",
    "    # Checking if edges of connected poiints form an angle bigger than the polygon angles\n",
    "    if (polygon_angles[p0]<calculate_angle(polygon[p0],polygon[p1],polygon[p2]) \n",
    "        or polygon_angles[p1]<calculate_angle(polygon[p1],polygon[p0],polygon[p2])):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p0])\n",
    "        or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p1])\n",
    "    ):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p0])\n",
    "        or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p1])):\n",
    "        #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "        return 0\n",
    "    \n",
    "    area=compute_triangle_area(triangle)\n",
    "   \n",
    "    if area==0:\n",
    "        return 0\n",
    "    \n",
    "    if contains_points(triangle,polygon):\n",
    "        return 0\n",
    "    \n",
    "    triangles_in_mesh=[]\n",
    "    triangles_in_mesh.append(triangle)\n",
    "    contour_connectivity=get_contour_edges(polygon)\n",
    "    contour_connectivity=np.vstack([contour_connectivity,[p0,p2],[p1,p2]])\n",
    "    hole=np.array([(triangle.sum(0))/3])\n",
    "    shape=dict(holes=hole,vertices=polygon,segments=contour_connectivity)\n",
    "    t = tri.triangulate(shape, 'pq0')\n",
    "    \n",
    "    Invalid_triangulation=False\n",
    "\n",
    "    try:   \n",
    "        for triangle_index in t['triangles']:\n",
    "            triangles_in_mesh.append(polygon[np.asarray([triangle_index])])\n",
    "    except :\n",
    "        print(\"Invalid triangulation\",p0,p1,p2)\n",
    "        Invalid_triangulation=True\n",
    "        \n",
    "    triangle_qualities=[]\n",
    "    for triangle in triangles_in_mesh:\n",
    "        triangle.resize(3,2)\n",
    "        triangle_quality=compute_triangle_quality(triangle)\n",
    "        triangle_qualities.append(triangle_quality)\n",
    "    \n",
    "    if Invalid_triangulation:\n",
    "        mean_quality,minimum_quality=0,0\n",
    "    else:\n",
    "        triangle_qualities=np.array(triangle_qualities)\n",
    "        mean_quality=triangle_qualities.mean()\n",
    "        minimum_quality=triangle_qualities.min()\n",
    "\n",
    "    \n",
    "\n",
    "    if get_mean:\n",
    "        return mean_quality\n",
    "    else:\n",
    "        return minimum_quality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Quality of elements formed by connecting each edge with one of the other points of the contour\n",
    "\n",
    "def quality_matrix(polygon,compute_minimum=True ,normalize=False,mean=False):\n",
    "    polygon=apply_procrustes(polygon,False)\n",
    "\n",
    "    contour_connectivity=np.array(list(tuple(i) for i in get_contour_edges(polygon)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    quality_matrix=np.zeros([contour_connectivity.shape[0],polygon.shape[0]])\n",
    "    area_matrix=np.zeros([contour_connectivity.shape[0],polygon.shape[0]])\n",
    "    normals_matrix=np.zeros([contour_connectivity.shape[0],polygon.shape[0]])\n",
    "\n",
    "    list_of_triangles=[]\n",
    "    \n",
    "    for index,edge in enumerate(contour_connectivity):\n",
    "        # Not omitting non triangles because either way their quality is zero\n",
    "        triangles_to_edge_indices=[[*edge,i] for i in range(polygon.shape[0]) ]\n",
    "        \n",
    "        \n",
    "\n",
    "        #print(triangles_to_edge_indices)\n",
    "        triangles_to_edge_indices=np.asarray(triangles_to_edge_indices)\n",
    "        triangles=polygon[triangles_to_edge_indices]\n",
    "        list_of_triangles.append(triangles)\n",
    "        \n",
    "    \n",
    "    list_of_triangles=np.array(list_of_triangles)\n",
    "    \n",
    "    if compute_minimum:\n",
    "        for i,triangles in enumerate(list_of_triangles):\n",
    "            for j,triangle in enumerate(triangles):\n",
    "                quality_matrix[i,j]=compute_minimum_quality_triangle(triangle,polygon)\n",
    "    else:\n",
    "         for i,triangles in enumerate(list_of_triangles):\n",
    "            for j,triangle in enumerate(triangles):\n",
    "                if mean:\n",
    "                    quality_matrix[i,j]=compute_triangle_quality(triangle,polygon,get_mean=True)\n",
    "                else: \n",
    "                    quality_matrix[i,j]=compute_triangle_quality(triangle,polygon)\n",
    "    \n",
    "            #area_matrix[i,j]=compute_triangle_area(triangle)\n",
    "            #normals_matrix[i,j]=compute_triangle_normals(triangle)\n",
    "\n",
    "            \n",
    "    \n",
    "    sum_of_qualities=quality_matrix.sum(1)\n",
    "\n",
    "    if normalize is True:\n",
    "        for i,_ in enumerate(quality_matrix):\n",
    "            quality_matrix[i]/=sum_of_qualities[i]\n",
    "    \n",
    "    return quality_matrix,normals_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_edge_validity(edge,polygon,set_edges,interior_edges):        \n",
    "    # Check if new edges are already in the set\n",
    "    found_in_set=False\n",
    "    found_in_interior_set=False\n",
    "    for index in range(len(polygon)):\n",
    "        occuring_index=index\n",
    "\n",
    "        edge1,edge2=tuple(permutations((edge[0],index))),tuple(permutations((edge[1],index)))\n",
    "        condition1= edge1[0] in set_edges or edge1[1] in set_edges\n",
    "        condition2= edge2[0] in set_edges or edge2[1] in set_edges\n",
    "        condition3= edge1[0] in interior_edges or edge1[1] in interior_edges\n",
    "        condition4= edge2[0] in interior_edges or edge2[1] in interior_edges\n",
    "    \n",
    "        \n",
    "            # both edges are found in the list of set of edges (Invalid)\n",
    "        if (condition1 and condition2): \n",
    "            found_in_set=True\n",
    "            occuring_index=index\n",
    "            \n",
    "        \n",
    "        # both edges are found in the list of interior edges created\n",
    "        if (condition3 and condition4):\n",
    "            found_in_interior_set=True\n",
    "            occuring_index=index\n",
    "            \n",
    "        if found_in_interior_set or found_in_set:\n",
    "            break\n",
    "    return found_in_interior_set,found_in_set,occuring_index\n",
    "\n",
    "def triangulate(polygon,ordered_quality_matrix,recursive=True):\n",
    "    set_edges=set(tuple(i) for i in get_contour_edges(polygon))\n",
    "    interior_edges=set()\n",
    "    set_elements=set()\n",
    "    set_locked_vertices=set()\n",
    "    set_forbidden_intersections=set()\n",
    "    polygon_angles=get_polygon_angles(polygon)\n",
    "\n",
    "    print(\"initial set edges:\", set_edges)\n",
    "    \n",
    "\n",
    "\n",
    "    for edge in ordered_quality_matrix.keys():\n",
    "        \n",
    "        \n",
    "        \n",
    "        found_in_interior_set,found_in_set,index=check_edge_validity(edge,polygon,set_edges,interior_edges)\n",
    "\n",
    "        for qualities_with_edges in ordered_quality_matrix[edge][0]:\n",
    "            \n",
    "            element_created=False\n",
    "           \n",
    "            target_vtx=qualities_with_edges[1]\n",
    "            \n",
    "            if target_vtx==edge[0] or target_vtx==edge[1]:\n",
    "                continue\n",
    "           \n",
    "            print(\"Edge:\",edge,\"targeting:\",target_vtx)\n",
    "        \n",
    "            if found_in_interior_set:\n",
    "                element=(edge[0],edge[1],index)  \n",
    "                set_elements.add(element)\n",
    "                print(\"Element inserted:\",element)\n",
    "                continue\n",
    "        \n",
    "            if found_in_set and not found_in_interior_set:    \n",
    "                if(index != target_vtx):\n",
    "                    print('found',(edge[0],index),(edge[1],index),\"Canceling creation\")\n",
    "                    continue        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            # Passed edges checking \n",
    "            # Proceed to check vertices\n",
    "            temp_element=(edge[0],edge[1],target_vtx)\n",
    "            \n",
    "            # Cehcking element normals to avoid inverted elements\n",
    "            element_indices=np.asarray([edge[0],edge[1],target_vtx])\n",
    "            if compute_triangle_normals(polygon[element_indices])<0:\n",
    "                continue\n",
    "            \n",
    "            # Checking if the element contains other points of the polygon inside\n",
    "            if contains_points(polygon[element_indices],polygon):\n",
    "                continue\n",
    "            \n",
    "                            \n",
    "            p0,p1,p2=edge[0],edge[1],target_vtx\n",
    "    \n",
    "            neighbor_points=connection_indices(p2,get_contour_edges(polygon))\n",
    "    \n",
    "            # Checking if edges of connected poiints form an angle bigger than the polygon angles\n",
    "            if (polygon_angles[p0]<calculate_angle(polygon[p0],polygon[p1],polygon[p2]) \n",
    "            or polygon_angles[p1]<calculate_angle(polygon[p1],polygon[p0],polygon[p2])):\n",
    "            #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "                 continue\n",
    "    \n",
    "    \n",
    "            if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p0])\n",
    "           or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[0]],polygon[p1])\n",
    "            ):\n",
    "                continue\n",
    "    \n",
    "            if( polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p0])\n",
    "            or polygon_angles[p2]<calculate_angle(polygon[p2],polygon[neighbor_points[1]],polygon[p1])):\n",
    "            #print(\"Spotted inverted triangle: {}\".format([p0,p1,p2]))\n",
    "                continue\n",
    "         \n",
    "        \n",
    "        \n",
    "            quality=compute_minimum_quality_triangle(polygon[element_indices],polygon)\n",
    "            if quality==0 or quality<1e-2:\n",
    "                continue\n",
    "            \n",
    "            print(temp_element)\n",
    "            existing_element=False\n",
    "            for element in set_elements:\n",
    "                if set(temp_element)==set(element):\n",
    "                    print(\"Element {} already in set\".format(element))\n",
    "                    existing_element=True\n",
    "                    break\n",
    "            if existing_element:\n",
    "                break\n",
    "            \n",
    "            \n",
    "            \n",
    "            if target_vtx in set_locked_vertices:\n",
    "                print(\" Target vertex {} is locked\".format(target_vtx))\n",
    "                continue\n",
    "            set_elements.add(temp_element)\n",
    "\n",
    "        \n",
    "    \n",
    "            # Check if a locked vertex was created after the creation of the element\n",
    "            # If so, add it to the list\n",
    "            #Tracer()()\n",
    "            Found_locked_vertex=False\n",
    "            for vertex in temp_element:\n",
    "                _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                if isclosed and vertex not in set_locked_vertices:\n",
    "                    print(\"Vertex locked:\",vertex)\n",
    "                    Found_locked_vertex=True\n",
    "                    set_locked_vertices.add(vertex)\n",
    "            set_elements.remove(temp_element)\n",
    "            \n",
    "        \n",
    "        \n",
    "            # Locking the vertices and checking if the connection is with a locked vertex has been checked/\n",
    "            # Proceeding to check if both internal edges intersect with other internal edges\n",
    "            internal_edge1=(edge[0],target_vtx)\n",
    "            internal_edge2=(edge[1],target_vtx)\n",
    "            set_a,set_b=get_intermediate_indices(target_vtx,polygon,edge[0],edge[1])\n",
    "        \n",
    "            internal_condition1= internal_edge1 in set_forbidden_intersections or tuple(reversed(internal_edge1)) in set_forbidden_intersections\n",
    "                                                                        \n",
    "            internal_condition2=internal_edge2 in set_forbidden_intersections or tuple(reversed(internal_edge2)) in set_forbidden_intersections\n",
    "                                                                            \n",
    "    \n",
    "                                                                                   \n",
    "            internal_intersection=False\n",
    "            if internal_condition1 or  internal_condition2:\n",
    "                print(\"edges :\",internal_edge1, \"and\",internal_edge2,\"intersecting\")\n",
    "                print(\"Abandoning creation of element\",temp_element)\n",
    "                internal_intersection=True\n",
    "        \n",
    "     \n",
    "            if internal_intersection:\n",
    "                for vtx in temp_element:\n",
    "                    if Found_locked_vertex and vtx in set_locked_vertices:\n",
    "                        print(\"Unlocking vertex\",vtx)\n",
    "                        set_locked_vertices.remove(vtx)                    \n",
    "                continue\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Create the element\n",
    "            element=temp_element\n",
    "        \n",
    "            # Add to set of edges all the forbidden intersections after the creation of the element\n",
    "            \n",
    "            for i in set_a:\n",
    "                for j in set_b:\n",
    "                    set_forbidden_intersections.add((i,j))\n",
    "            #print(\"set of forbidden inter section edges updated:\",set_forbidden_intersections)\n",
    "    \n",
    "                \n",
    "        \n",
    "        \n",
    "        # New edges after creation of the element\n",
    "   \n",
    "            new_edge1=(edge[0],target_vtx)\n",
    "            new_edge2=(edge[1],target_vtx)\n",
    "        \n",
    "            if new_edge1 not in set_edges and tuple(reversed(new_edge1)) not in set_edges:\n",
    "                set_edges.add(new_edge1)\n",
    "                interior_edges.add(new_edge1)\n",
    "                print(\"edges inserted:\",new_edge1)\n",
    "                print(\"set of interior edges updated:\",interior_edges)\n",
    "                print(\"set of edges updated:\",set_edges)\n",
    "            if new_edge2 not in set_edges and tuple(reversed(new_edge2)) not in set_edges:    \n",
    "                set_edges.add(new_edge2)\n",
    "                interior_edges.add(new_edge2)\n",
    "                print(\"edges inserted:\",new_edge2)\n",
    "                print(\"set of interior edges updated:\",interior_edges)\n",
    "                print(\"set of edges updated:\",set_edges)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "            # Checking list of elements to see whether the were created or were already there\n",
    "            #for element_permutation in tuple(permutations(element)):\n",
    "                #if element_permutation in set_elements:\n",
    "                 #   print(\"Element {} already in set\".format(element))\n",
    "                  #  break\n",
    "            #else:\n",
    "            set_elements.add(element)\n",
    "            indices=np.asarray(element)\n",
    "            print(\"element inserted:\",element)\n",
    "            element_created=True\n",
    "            break\n",
    "        if element_created:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    triangulated={'segment_markers': np.ones([polygon.shape[0]]), 'segments':np.array(get_contour_edges(polygon)), 'triangles': np.array(list( list(i) for i in set_elements)),\n",
    "                  'vertex_markers': np.ones([polygon.shape[0]]), 'vertices': polygon}\n",
    "    plot.plot(plt.axes(), **triangulated)\n",
    "    print(\"Final edges:\",set_edges)\n",
    "    print(\"Elements created:\",set_elements)\n",
    "    print(\"Set of locked vertices:\", set_locked_vertices)\n",
    "    \n",
    "    \n",
    "    # find open vertices\n",
    "    for element in set_elements:\n",
    "        for vertex in  element:\n",
    "                    _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                    if isclosed and vertex not in set_locked_vertices:\n",
    "                        print(\"Vertex locked:\",vertex)\n",
    "                        Found_locked_vertex=True\n",
    "                        set_locked_vertices.add(vertex)\n",
    "    set_open_vertices=set(range(len(polygon)))-set_locked_vertices\n",
    "    print(\"Set of open vertices:\", set_open_vertices)\n",
    "    set_edges.clear(),set_locked_vertices.clear(),set_forbidden_intersections.clear\n",
    "    if recursive:\n",
    "        sub_polygon_list=check_for_sub_polygon(set_open_vertices,interior_edges,set_elements,polygon)\n",
    "\n",
    "        for sub_polygon_indices in sub_polygon_list:\n",
    "            if len(sub_polygon_indices)>=4:\n",
    "                polygon_copy=polygon.copy()\n",
    "                sub_polygon=np.array(polygon_copy[sub_polygon_indices])\n",
    "                sub_quality,_=quality_matrix(sub_polygon,compute_minimum=True,normalize=False)\n",
    "                sub_order_matrix=order_quality_matrix(sub_quality,sub_polygon)\n",
    "                print(sub_quality,sub_order_matrix)\n",
    "                triangulate(sub_polygon,sub_order_matrix)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def pure_triangulate(polygon,ordered_quality_matrix,recursive=True):\n",
    "    set_edges=set(tuple(i) for i in get_contour_edges(polygon))\n",
    "    interior_edges=set()\n",
    "    set_elements=set()\n",
    "    set_locked_vertices=set()\n",
    "    set_forbidden_intersections=set()\n",
    "    polygon_angles=get_polygon_angles(polygon)\n",
    "    check_edge_validity_time,check_for_existing_element=0,0\n",
    "    #print(\"initial set edges:\", set_edges)\n",
    "    \n",
    "\n",
    "   \n",
    "    for edge in ordered_quality_matrix.keys():\n",
    "        \n",
    "        start=timer()\n",
    "        found_in_interior_set,found_in_set,index=check_edge_validity(edge,polygon,set_edges,interior_edges)\n",
    "        stop=timer()\n",
    "        check_edge_validity_time+=stop-start\n",
    "        \n",
    "        for qualities_with_edges in ordered_quality_matrix[edge][0]:\n",
    "            \n",
    "            element_created=False\n",
    "           \n",
    "            target_vtx=qualities_with_edges[1]\n",
    "            \n",
    "\n",
    "        \n",
    "            if found_in_interior_set:\n",
    "                element=(edge[0],edge[1],index)  \n",
    "                set_elements.add(element)\n",
    "                #print(\"Element inserted:\",element)\n",
    "                continue\n",
    "        \n",
    "            if found_in_set and not found_in_interior_set:    \n",
    "                if(index != target_vtx):\n",
    "                    continue        \n",
    "        \n",
    "        \n",
    "        \n",
    "            # Passed edges checking \n",
    "            # Proceed to check vertices\n",
    "            temp_element=(edge[0],edge[1],target_vtx)\n",
    "\n",
    "            existing_element=False\n",
    "           \n",
    "            for element in set_elements:\n",
    "                if set(temp_element)==set(element):\n",
    "                    existing_element=True\n",
    "                    break\n",
    "            \n",
    "            if existing_element:\n",
    "                break\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            if target_vtx in set_locked_vertices:\n",
    "                #print(\" Target vertex {} is locked\".format(target_vtx))\n",
    "                continue\n",
    "            set_elements.add(temp_element)\n",
    "\n",
    "        \n",
    "    \n",
    "            # Check if a locked vertex was created after the creation of the element\n",
    "            # If so, add it to the list\n",
    "            #Tracer()()\n",
    "            \n",
    "            Found_locked_vertex=False\n",
    "#            timer1=timer()\n",
    "            for vertex in temp_element:\n",
    "                _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                if isclosed and vertex not in set_locked_vertices:\n",
    "                    #print(\"Vertex locked:\",vertex)\n",
    "                    Found_locked_vertex=True\n",
    "                    set_locked_vertices.add(vertex)\n",
    "  #          timer2=timer()\n",
    "            checking_locked_vertices=time2-time1\n",
    "            set_elements.remove(temp_element)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "            # Locking the vertices and checking if the connection is with a locked vertex has been checked/\n",
    "            # Proceeding to check if both internal edges intersect with other internal edges\n",
    "            internal_edge1=(edge[0],target_vtx)\n",
    "            internal_edge2=(edge[1],target_vtx)\n",
    "            set_a,set_b=get_intermediate_indices(target_vtx,polygon,edge[0],edge[1])\n",
    "        \n",
    "            internal_condition1= internal_edge1 in set_forbidden_intersections or tuple(reversed(internal_edge1)) in set_forbidden_intersections\n",
    "                                                                        \n",
    "            internal_condition2=internal_edge2 in set_forbidden_intersections or tuple(reversed(internal_edge2)) in set_forbidden_intersections\n",
    "                                                                            \n",
    "    \n",
    "                                                                                   \n",
    "            internal_intersection=False\n",
    "            \n",
    "\n",
    "            if internal_condition1 or  internal_condition2:\n",
    "                #print(\"edges :\",internal_edge1, \"and\",internal_edge2,\"intersecting\")\n",
    "                #print(\"Abandoning creation of element\",temp_element)\n",
    "                internal_intersection=True\n",
    "        \n",
    "     \n",
    "            if internal_intersection:\n",
    "                for vtx in temp_element:\n",
    "                    if Found_locked_vertex and vtx in set_locked_vertices:\n",
    "                        #print(\"Unlocking vertex\",vtx)\n",
    "                        set_locked_vertices.remove(vtx)                    \n",
    "                continue\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Create the element\n",
    "            element=temp_element\n",
    "        \n",
    "            # Add to set of edges all the forbidden intersections after the creation of the element\n",
    "            \n",
    "            for i in set_a:\n",
    "                for j in set_b:\n",
    "                    set_forbidden_intersections.add((i,j))\n",
    "            #print(\"set of forbidden inter section edges updated:\",set_forbidden_intersections)\n",
    "    \n",
    "                \n",
    "        \n",
    "        \n",
    "        # New edges after creation of the element\n",
    "   \n",
    "            new_edge1=(edge[0],target_vtx)\n",
    "            new_edge2=(edge[1],target_vtx)\n",
    "        \n",
    "            if new_edge1 not in set_edges and tuple(reversed(new_edge1)) not in set_edges:\n",
    "                set_edges.add(new_edge1)\n",
    "                interior_edges.add(new_edge1)\n",
    "                #print(\"edges inserted:\",new_edge1)\n",
    "                #print(\"set of interior edges updated:\",interior_edges)\n",
    "                #print(\"set of edges updated:\",set_edges)\n",
    "            if new_edge2 not in set_edges and tuple(reversed(new_edge2)) not in set_edges:    \n",
    "                set_edges.add(new_edge2)\n",
    "                interior_edges.add(new_edge2)\n",
    "                #print(\"edges inserted:\",new_edge2)\n",
    "                #print(\"set of interior edges updated:\",interior_edges)\n",
    "                #print(\"set of edges updated:\",set_edges)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "         \n",
    "            set_elements.add(element)\n",
    "            indices=np.asarray(element)\n",
    "                #print(\"element inserted:\",element)\n",
    "            element_created=True\n",
    "            \n",
    "            if element_created:\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "           \n",
    "    \n",
    "#    triangulated={'segment_markers': np.ones([polygon.shape[0]]), 'segments':np.array(get_contour_edges(polygon)), 'triangles': np.array(list( list(i) for i in set_elements)),\n",
    "#                  'vertex_markers': np.ones([polygon.shape[0]]), 'vertices': polygon}\n",
    "#    plot.plot(plt.axes(), **triangulated)\n",
    "    #print(\"Final edges:\",set_edges)\n",
    "    #print(\"Elements created:\",set_elements)\n",
    "    #print(\"Set of locked vertices:\", set_locked_vertices)\n",
    "    \n",
    "    \n",
    "    # find open vertices\n",
    "    for element in set_elements:\n",
    "        for vertex in  element:\n",
    "                _ ,isclosed = is_closed_ring(vertex,set_elements,*connection_indices(vertex,get_contour_edges(polygon)))\n",
    "                if isclosed and vertex not in set_locked_vertices:\n",
    "                        #print(\"Vertex locked:\",vertex)\n",
    "                        Found_locked_vertex=True\n",
    "                        set_locked_vertices.add(vertex)\n",
    "    set_open_vertices=set(range(len(polygon)))-set_locked_vertices\n",
    "    #print(\"Set of open vertices:\", set_open_vertices)\n",
    "    set_edges.clear(),set_locked_vertices.clear(),set_forbidden_intersections.clear\n",
    "    if recursive and len(set_open_vertices)>=4:\n",
    "        sub_polygon_list=check_for_sub_polygon(set_open_vertices,interior_edges,set_elements,polygon)\n",
    "#\n",
    "        for sub_polygon_indices in sub_polygon_list:\n",
    "            if len(sub_polygon_indices)>=4:\n",
    "                polygon_copy=polygon\n",
    "                sub_polygon=np.array(polygon_copy[sub_polygon_indices])\n",
    "                sub_quality,_=quality_matrix(sub_polygon,compute_minimum=True,normalize=False)\n",
    "                sub_order_matrix=order_quality_matrix(sub_quality,sub_polygon)\n",
    "                #print(sub_quality,sub_order_matrix)\n",
    "                pure_triangulate(sub_polygon,sub_order_matrix)\n",
    "#\n",
    "\n",
    "    \n",
    "def order_quality_matrix(_quality_matrix,_polygon):\n",
    "\n",
    "    #  Create the quality matrix in accordance with the edges\n",
    "    quality_board=[(q,index)  for qualities in _quality_matrix for index,q in enumerate(qualities)]\n",
    "    quality_board=np.array(quality_board)\n",
    "    #print(\"Quality board not resized:\",quality_board)\n",
    "\n",
    "    quality_board.resize(len(get_contour_edges(_polygon)),len(_polygon),2)\n",
    "    quality_board=dict(zip(list(tuple(i) for i in get_contour_edges(_polygon)),quality_board))\n",
    "    \n",
    "    \n",
    "    #sorted_quality_board={i[0]:i[1] for i in sorted(board.items(),key=lambda x: max(x[1]),reverse=True)}\n",
    "    #print(\"Quality board\")\n",
    "    #for keys,items in quality_board.items():\n",
    "    #    print(keys,items)\n",
    "    edge_quality=quality_board[(0,1)]\n",
    "    edge_quality=edge_quality[np.lexsort(np.fliplr(edge_quality).T)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in quality_board.keys():\n",
    "        quality_board[i]=quality_board[i][np.lexsort(np.fliplr(quality_board[i]).T)]\n",
    "        quality_board[i]=quality_board[i][::-1]\n",
    "        quality_board[i][:,1]=quality_board[i][:,1].astype(int)\n",
    "    \n",
    "    edge=[]\n",
    "    max_quality=[]\n",
    "    listing=[]\n",
    "    for keys,values in quality_board.items():\n",
    "        listing.append([keys,max(values[:,0])])\n",
    "    \n",
    "    listing=np.array(listing)\n",
    "    listing=listing[np.lexsort(np.transpose(listing)[::-3]).T]\n",
    "    listing=listing[::-1]\n",
    "    ordered_indices=listing[:,0]\n",
    "\n",
    "    ordered_quality_matrix={}\n",
    "\n",
    "    for i in ordered_indices:\n",
    "        ordered_quality_matrix[i]=[tuple(zip(quality_board[i][:,0],quality_board[i][:,1].astype(int)))]\n",
    "    \n",
    "    return ordered_quality_matrix    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get the list of edges of a polygon\n",
    "def get_contour_edges(polygon):\n",
    "    contour_connectivity=np.array([[i,(i+1)%polygon.shape[0]] for i in range(polygon.shape[0])])\n",
    "    return contour_connectivity\n",
    "\n",
    "\n",
    "\n",
    "# Function to return indices that are connected to a vertex\n",
    "def connection_indices(vertex,edges):   \n",
    "    indices=[]\n",
    "    for edge in edges:\n",
    "        if vertex in edge:\n",
    "\n",
    "            if edge[0] == vertex:\n",
    "                indices.append(edge[1])\n",
    "            else:\n",
    "                indices.append(edge[0])\n",
    "\n",
    "    return indices\n",
    "\n",
    "# Function to calculate and angle:\n",
    "def calculate_angle(p0,p1,p2):\n",
    "    v0 = p1 - p0\n",
    "    v1 = p2 - p0\n",
    "    \n",
    "    \n",
    "    normal=compute_triangle_normals([p0,p1,p2])\n",
    "    angle = np.math.atan2(np.linalg.det([v0,v1]),np.dot(v0,v1))\n",
    "    angle=abs(angle)\n",
    "    #unit_v0=v0 / np.linalg.norm(v0)\n",
    "    #unit_v1=v1 / np.linalg.norm(v1)\n",
    "    #angle=np.arccos(np.clip(np.dot(unit_v0, unit_v1), -1.0, 1.0))\n",
    "    \n",
    "    return np.degrees(angle)\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate the angles of a polygon\n",
    "def get_polygon_angles(polygon):\n",
    "    angles=[]\n",
    "    for index,point in enumerate(polygon):\n",
    "        p0=point        \n",
    "        neighbor_points=connection_indices(index,get_contour_edges(polygon))\n",
    "        #print(\"neighbor points\",neighbor_points)\n",
    "        indices=np.asarray(neighbor_points)\n",
    "        p1,p2=polygon[indices]\n",
    "        angle=calculate_angle(p0,p1,p2)\n",
    "        if index !=0:\n",
    "            triangle_normal=compute_triangle_normals([p0,p1,p2])\n",
    "        else:\n",
    "            triangle_normal=compute_triangle_normals([p1,p0,p2])\n",
    "\n",
    "            \n",
    "        if triangle_normal>0:\n",
    "            angle=360-angle\n",
    "        \n",
    "        angles.append(angle)\n",
    "    return angles\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def is_closed_ring(vtx,set_of_elements,*adj_vtx):\n",
    "    contour_edge1=(vtx,adj_vtx[0])\n",
    "    contour_edge2=(vtx,adj_vtx[1])\n",
    "    visited_elements=set_of_elements.copy()\n",
    "    \n",
    "    target_edge=contour_edge1\n",
    "    \n",
    "    edges_found=[]\n",
    "    edges_found.append(contour_edge1)\n",
    "\n",
    "    proceed=True\n",
    "    \n",
    "    while proceed:\n",
    "        \n",
    "        if not visited_elements:\n",
    "            break\n",
    "        \n",
    "        remaining_edge,found_element=edge2elem(target_edge,visited_elements)\n",
    "        \n",
    "        if found_element is None:\n",
    "            #print(\"stopped\")\n",
    "            proceed=False\n",
    "            break\n",
    "            \n",
    "        visited_elements.remove(found_element)\n",
    "        edges_found.append(remaining_edge)\n",
    "        target_edge=remaining_edge\n",
    "    \n",
    "    \n",
    "                \n",
    "    #print(set(edges_found))\n",
    "    found_contour_edge1,found_contour_edge2=False,False\n",
    "    found_contour_edges=False\n",
    "\n",
    "    # Checking if both contour edges area contained in the set of edges acquired\n",
    "    \n",
    "    for edge in edges_found:\n",
    "        condition1= contour_edge1[0] in set(edge) and contour_edge1[1] in set((edge))\n",
    "        condition2= contour_edge2[0] in set(edge) and contour_edge2[1] in set((edge))\n",
    "        if condition1:\n",
    "            #print(\"found \",contour_edge1)\n",
    "            found_contour_edge1=True\n",
    "        if condition2:\n",
    "            #print(\"found\",contour_edge2)\n",
    "            found_contour_edge2=True\n",
    "            \n",
    "    if found_contour_edge1 and found_contour_edge2:\n",
    "        found_contour_edges=True\n",
    "        #print(\"found both of contour edges in set\")\n",
    "    \n",
    "    visited_elements.clear()\n",
    "    return edges_found,found_contour_edges\n",
    "    \n",
    "    \n",
    "    \n",
    "# Finds element containing the edge and exits (does not give the full list of elements)   \n",
    "# Serve is_one_ring function\n",
    "def edge2elem(edge,set_of_elements):\n",
    "    found=False\n",
    "    Found_element=()\n",
    "    Remaining_edge=()\n",
    "  \n",
    "    for element in set_of_elements.copy():\n",
    "        \n",
    "        if edge[0] in  set(element) and edge[1] in set(element): \n",
    "            #print(\"Edge {} is part of element {}\".format(edge,element))\n",
    "            Found_element=element\n",
    "            Remaining_index=set(element)-set(edge)\n",
    "            Remaining_index=list(Remaining_index)\n",
    "            Remaining_edge=(edge[0],Remaining_index[0])\n",
    "            #print(\" Remaining edge is {}\".format(Remaining_edge))\n",
    "            break \n",
    "        else:\n",
    "            Found_element=None\n",
    "            Remaining_edge=None\n",
    "    return  Remaining_edge,Found_element \n",
    "\n",
    "# Departing from a target vertex connected with and edge get all intermediate  indices from one side and other\n",
    "def get_intermediate_indices(target_vtx,polygon,*edge):\n",
    "    \n",
    "    set_1=set()\n",
    "    set_2=set()\n",
    "    \n",
    "    \n",
    "    contour_edges=get_contour_edges(polygon)\n",
    "    \n",
    "    \n",
    "    # Depart from target vertex and get neighbor indices\n",
    "    neighbors=connection_indices(target_vtx,contour_edges)\n",
    "    found_vertex1,found_vertex2=neighbors[0],neighbors[1]\n",
    "    #print(\"found vertices:\",found_vertex1,found_vertex2)\n",
    "\n",
    "    \n",
    "    # Include them into seperate lists\n",
    "    set_1.add(found_vertex1)\n",
    "    set_2.add(found_vertex2)\n",
    "    \n",
    "    visited_vertex=target_vtx\n",
    "      \n",
    "    \n",
    "    while found_vertex1!=edge[0] and found_vertex1!=edge[1]:\n",
    "        visiting_vertex=found_vertex1\n",
    "        neighbors=connection_indices(visiting_vertex,contour_edges)\n",
    "        for index in neighbors:\n",
    "            if index !=  visited_vertex:\n",
    "                set_1.add(index)\n",
    "                found_vertex1=index\n",
    "                #print(\"Found vertex:\",found_vertex1)     \n",
    "        visited_vertex=visiting_vertex\n",
    "        \n",
    "    #print(\"Start  looking the other way\")\n",
    "    \n",
    "    # Resetting to go the other way\n",
    "    visited_vertex=target_vtx\n",
    "\n",
    "    while found_vertex2!=edge[0] and found_vertex2!=edge[1]:\n",
    "        visiting_vertex=found_vertex2\n",
    "        neighbors=connection_indices(visiting_vertex,contour_edges)\n",
    "        for index in neighbors:\n",
    "            if index !=  visited_vertex:\n",
    "                set_2.add(index)\n",
    "                found_vertex2=index\n",
    "                #print(\"Found vertex:\",found_vertex2)     \n",
    "        visited_vertex=visiting_vertex\n",
    "    \n",
    "                \n",
    "                \n",
    "                \n",
    "  \n",
    "    return set_1,set_2\n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polygon_2_vtx(starting_vertex,edges_to_visit):\n",
    "    from  more_itertools import unique_everseen\n",
    "    \n",
    "    if not edges_to_visit:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    closed=False\n",
    "\n",
    "                \n",
    "    #print(\"Edges to visit:\",edges_to_visit)\n",
    "    subpolygon=[]\n",
    "    \n",
    "                \n",
    "    found_vertex=starting_vertex\n",
    "    \n",
    "    while not closed:\n",
    "        for index,edge in enumerate(edges_to_visit.copy()):\n",
    "            visiting_vertex=found_vertex\n",
    "            \n",
    "           \n",
    "            #if visiting_vertex not in set(edge) and index==len(edges_to_visit.copy()):\n",
    "               # Tracer()()\n",
    "                #print(\"Not found in list of edges\")\n",
    "                #closed=True\n",
    "                #break\n",
    "            if visiting_vertex not in set(edge):\n",
    "                continue\n",
    "            subpolygon.append(visiting_vertex)\n",
    "                \n",
    "                                \n",
    "            #print(\"Visiting vertex\",visiting_vertex)\n",
    "            \n",
    "            found_starting_vtx=False\n",
    "            subpolygon.append(found_vertex)\n",
    "            \n",
    "            \n",
    "            #print(visiting_vertex,\" in \", edge)\n",
    "                \n",
    "                \n",
    "            for index in set(edge):\n",
    "                if visiting_vertex!= index:\n",
    "                    found_vertex=index\n",
    "                    #print(\"Found vertex:\",found_vertex)\n",
    "                    subpolygon.append(found_vertex)\n",
    "                    \n",
    "                    \n",
    "            #print(\"Removing edge\",edge)\n",
    "            edges_to_visit.discard(edge)\n",
    "            #print(edges_to_visit)\n",
    "            if found_vertex==starting_vertex:\n",
    "                subpolygon=list(unique_everseen(subpolygon))\n",
    "                #print(\"Back to starting vertex\")    \n",
    "                closed=True\n",
    "                break\n",
    "                \n",
    "    if  len(subpolygon)<=3:\n",
    "        return \n",
    "    else:\n",
    "        return subpolygon\n",
    "     \n",
    "\n",
    "                       \n",
    "\n",
    "def check_for_sub_polygon(set_of_open_vertices,set_of_interior_edges,set_of_elements,polygon):\n",
    "\n",
    "    \n",
    "    \n",
    "    if not set_of_open_vertices or  len(set_of_open_vertices)<3:\n",
    "        return []\n",
    "    \n",
    "\n",
    "    sub_polygon_list=[]\n",
    "    modified_interior_edge_set=set_of_interior_edges.copy()\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    polygon_connectivity=[tuple(i) for i in get_contour_edges(polygon)]\n",
    "    \n",
    "    for edge in modified_interior_edge_set.copy():\n",
    "        if edge[0] not in set_of_open_vertices or edge[1] not in set_of_open_vertices:\n",
    "            modified_interior_edge_set.discard(edge)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # Taking care of vertices that are locked but the element is not seen\n",
    "    \n",
    "    set_of_unfound_locked_vertices=set()\n",
    "    continue_looking=True\n",
    "\n",
    "    \n",
    "    while continue_looking:\n",
    "        \n",
    "        if not set_of_open_vertices:\n",
    "            continue_looking=False\n",
    "            \n",
    "        for vtx in set_of_open_vertices.copy():\n",
    "                vtx1,vtx2 =connection_indices(vtx,get_contour_edges(polygon))\n",
    "                found_edges1,isclosed1=is_closed_ring(vtx,set_of_elements,vtx2,vtx1)\n",
    "                found_edges2,isclosed2=is_closed_ring(vtx,set_of_elements,vtx1,vtx2)\n",
    "                #print(\"Examining if vtx {} is locked\".format(vtx))\n",
    "                \n",
    "                if isclosed1 or isclosed2:\n",
    "                    #print(vtx,\"locked after all\")\n",
    "                    set_of_open_vertices.discard(vtx)\n",
    "                    for edge in modified_interior_edge_set.copy():\n",
    "                        if vtx in edge:\n",
    "                            modified_interior_edge_set.discard(edge)\n",
    "                    break\n",
    "                \n",
    "                for edge in found_edges1:\n",
    "                    if edge in polygon_connectivity or edge[::-1] in polygon_connectivity:\n",
    "                        found_edges1.remove(edge)\n",
    "                for edge in found_edges2:\n",
    "                    if edge in polygon_connectivity or edge[::-1] in polygon_connectivity:\n",
    "                        found_edges2.remove(edge)\n",
    "                between_edges=[]\n",
    "                for edge in found_edges1:\n",
    "                    for indices in edge:\n",
    "                        if indices==vtx:\n",
    "                            continue\n",
    "                    between_edges.append(indices)\n",
    "                for edge in found_edges2:\n",
    "                    for indices in edge:\n",
    "                        if indices==vtx:\n",
    "                            continue\n",
    "                    between_edges.append(indices)\n",
    "                for edge in set_of_interior_edges.copy():\n",
    "                    found_locked_vtx=False\n",
    "                    if set(between_edges)==set(edge):\n",
    "                       # print(vtx,\"locked after all\")\n",
    "                        found_locked_vtx=True\n",
    "                        set_of_unfound_locked_vertices.add(vtx)\n",
    "                        #Tracer()()\n",
    "                        if edge in set_of_interior_edges or edge[::-1] in set_of_interior_edges:                 \n",
    "                            #modified_interior_edge_set.discard(edge)\n",
    "                            #print(edge,\"removed\")               \n",
    "                            #modified_interior_edge_set.discard(edge[::-1])\n",
    "                            modified_interior_edge_set.discard((vtx,between_edges[0]))\n",
    "                            modified_interior_edge_set.discard((between_edges[0],vtx))\n",
    "                        \n",
    "    \n",
    "                            modified_interior_edge_set.discard((vtx,between_edges[1]))\n",
    "                            modified_interior_edge_set.discard((between_edges[1],vtx))\n",
    "                            element=(vtx,between_edges[0],between_edges[1])\n",
    "                            #print(\"Removed:\",(vtx),\"from set of open vertices\")\n",
    "    \n",
    "                            #print(\"Added new element:\",element)\n",
    "                            #print(\"Removed:\",(vtx,between_edges[0]),\"from set of edges\")\n",
    "                            #print(\"Removed:\",(vtx,between_edges[1]),\"from set of edges\")\n",
    "    \n",
    "                            set_of_elements.add(element) \n",
    "                            #print(\"New set of elements\",set_of_elements)\n",
    "                            set_of_open_vertices.discard(vtx)\n",
    "                            \n",
    "                    if found_locked_vtx:\n",
    "                        #Tracer()()\n",
    "                        continue_looking=True\n",
    "                        #print(\"Re-evaluting set of open vertices\")\n",
    "                        break\n",
    "                        \n",
    "                    else: continue_looking=False\n",
    "                        \n",
    "                        \n",
    "    #    for edge in modified_interior_edge_set.copy():\n",
    "    #        if set(edge).issubset(set_of_unfound_locked_vertices):\n",
    "    #            modified_interior_edge_set.discard(edge)\n",
    "#            modified_interior_edge_set.discard(edge[::-1])\n",
    "#            print(\"removed\",edge)\n",
    "            \n",
    "            #print(\"inbetween\",between_edges)\n",
    "                \n",
    "    #print(\"set of open vertices\",set_of_open_vertices)\n",
    "    \n",
    "    if not set_of_open_vertices or  len(set_of_open_vertices)<3:\n",
    "        return []\n",
    "    \n",
    "    # In the set of open vertices there may be vertices that are part of  of multiple polygon\n",
    "    found_common_vertex=False\n",
    "    set_of_common_vertices=set()\n",
    "    nb_of_polygon=0\n",
    "    for vertex in set_of_open_vertices:\n",
    "        count=0\n",
    "        for edge in modified_interior_edge_set.copy():\n",
    "            if vertex in set(edge):\n",
    "                count+=1\n",
    "        if count>=3:\n",
    "            nb_of_polygon=count-2\n",
    "            #print(\"Vertex {} is a common vtx of multiple polygons\".format(vertex))\n",
    "            set_of_common_vertices.add(vertex)\n",
    "        found_common_vertex=True\n",
    "        \n",
    "    # An edge could be part of more than one polygons. This means that the vertices of this edge\n",
    "    # are already in the set of common vertices and the edges is inside the set of the of modi\n",
    "    # fied interior edges\n",
    "    set_of_common_edges=set()\n",
    "    for vtx1 in  set_of_common_vertices:\n",
    "        for vtx2 in set_of_common_vertices:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # if the set found is les than 4 then now polygon is formed\n",
    "    if len(set_of_open_vertices)<4:\n",
    "        return []\n",
    "    \n",
    "    edges_to_visit=modified_interior_edge_set\n",
    "\n",
    "    \n",
    "    sub_polygon_list=[]\n",
    "    if len(edges_to_visit)<3:\n",
    "        return sub_polygon_list\n",
    "    if set_of_common_vertices:\n",
    "        while edges_to_visit:  \n",
    "            for vtx in set_of_common_vertices:\n",
    "                subpolygon=polygon_2_vtx(vtx,edges_to_visit)\n",
    "                if subpolygon is not None:\n",
    "                    sub_polygon_list.append(subpolygon)\n",
    "                    #print(sub_polygon_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Removing eges where one vertex is locked and the other is not:\n",
    "    for edge in edges_to_visit.copy():\n",
    "        if (edge[0] in set_of_open_vertices and edge[1] not in set_of_open_vertices) or (edge[1] in set_of_open_vertices and edge[0] not in set_of_open_vertices):\n",
    "            edges_to_visit.discard(edge)\n",
    "            #print(\"Removing\",edge,\"from edges to visit\")\n",
    "            #print(\"Edges to visit are now\",edges_to_visit)\n",
    "    \n",
    "    \n",
    "    while edges_to_visit:          \n",
    "        for vtx in set_of_open_vertices.copy():\n",
    "            #print(\"Starting with vertex\",vtx)\n",
    "            subpolygon=polygon_2_vtx(vtx,edges_to_visit)\n",
    "\n",
    "            if subpolygon is not None:\n",
    "                sub_polygon_list.append(subpolygon)\n",
    "                print(sub_polygon_list)\n",
    "        \n",
    "                                    \n",
    "    for sub_polygon in sub_polygon_list:\n",
    "        if len(sub_polygon)>3:\n",
    "            pass\n",
    "            #print(\"found polygon\",sub_polygon)\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"found element\",sub_polygon)\n",
    "    return sub_polygon_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_contour(filename,contour):\n",
    "    path=os.path.join('contour_cases',filename+'.txt')\n",
    "    file=open(path,'w')\n",
    "    for i in contour:\n",
    "        file.write(np.array2string(i)+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "def read_contour(filename):\n",
    "    path=os.path.join('contour_cases',filename+'.txt')\n",
    "    contour=[]\n",
    "    file=open(path,'r')\n",
    "    for line in file:\n",
    "        coord=np.fromstring(line.strip('[\\n]'), dtype=float, sep=' ')\n",
    "        contour.append(coord)\n",
    "    file.close()\n",
    "    return np.array(contour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(nb_of_contours,nb_of_points):\n",
    "    Polygons=np.empty([nb_of_contours,nb_of_points,2])\n",
    "    Labels_min=np.empty([nb_of_contours,nb_of_points,nb_of_points]) # Labels for polygons\n",
    "    Labels_mean=np.empty([nb_of_contours,nb_of_points,nb_of_points]) # Labels for polygons\n",
    "\n",
    "    count=0\n",
    "    for i in range(nb_of_contours):\n",
    "        Polygons[i] = apply_procrustes(generate_contour(nb_of_points,False),False)\n",
    "        Labels_min[i]=quality_matrix( Polygons[i],compute_minimum=True,mean=False)[0]\n",
    "        Labels_mean[i]=quality_matrix( Polygons[i],compute_minimum=True,mean=True)[0]\n",
    "\n",
    "        count+=1\n",
    "        print(count, \" out of \", nb_of_contours, \"calculated\",nb_of_points)\n",
    "    return Polygons,Labels_min,Labels_mean\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_dataset(filename,dataset):\n",
    "    path=os.path.join('datasets',filename)\n",
    "    \n",
    "    with open(path,'wb') as output:\n",
    "        pickle.dump(dataset,output)\n",
    "        \n",
    "def load_dataset(filename):\n",
    "    path=os.path.join('datasets',filename)\n",
    "\n",
    "    with open(path,'rb') as input:\n",
    "        dataset=pickle.load(input)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_network(filename,net):\n",
    "    net.eval()\n",
    "    path=os.path.join('networks',filename)\n",
    "\n",
    "    with open(path,'wb') as output:\n",
    "        pickle.dump(net,output)\n",
    "        \n",
    "def load_network(filename):\n",
    "    path=os.path.join('networks',filename)\n",
    "\n",
    "    with open(path,'rb') as input:\n",
    "        net=pickle.load(input)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "############### Create dataset ##################################\n",
    "nb_of_points=[7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "#nb_of_points=[6]\n",
    "nb_of_data=120000\n",
    "\n",
    "\n",
    "for i in nb_of_points[::-1]:\n",
    "    Polygons,qualities_min,qualities_mean= create_dataset(nb_of_data,i)\n",
    "    save_dataset(str(i)+'_polygons.pkl',Polygons)\n",
    "    save_dataset(str(i)+'_polygons_qualities_min.pkl',qualities_min)\n",
    "    save_dataset(str(i)+'_polygons_qualities_mean.pkl',qualities_mean)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "#Polygons,quality_matrices=load_dataset('nine_polygons.pkl'),load_dataset('9_polygons_quality.pkl')\n",
    "Polygons,quality_matrices=load_dataset('7_polygons.pkl'),load_dataset('7_polygons_qualities_min.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing data are usually 20 percent of the whole\n",
    "\n",
    "nb_of_points=7\n",
    "nb_of_contours=int(Polygons.shape[0])\n",
    "\n",
    "nb_test_data=int(0.2*Polygons.shape[0])\n",
    "nb_training_data=int(Polygons.shape[0])-nb_test_data\n",
    "Polygons.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Project set of polygons in 2d and 3d space #\n",
    "# Dimensionality reduction using Isomap, PCA, kernel PCA ... #\n",
    "\n",
    "\n",
    "Polygons_reshaped=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(nb_of_contours):\n",
    "    Polygons_reshaped.append(Polygons[i].reshape(2*nb_of_points))\n",
    "\n",
    "Polygons_reshaped=np.array(Polygons_reshaped) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        # Isomap #\n",
    "#nb_components=8\n",
    "#iso=manifold.Isomap(n_neighbors=8,n_components=nb_components,n_jobs=-1)\n",
    "#iso.fit(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "\n",
    "                        # PCA   #\n",
    "pca=PCA(.999)\n",
    "pca.fit(Polygons_reshaped)\n",
    "Polygons_projected=pca.transform(Polygons_reshaped)\n",
    "nb_components=int(pca.n_components_)\n",
    "print(nb_components)\n",
    "# Fitting into lesser dimension\n",
    "\n",
    "#Polygons_projected=iso.fit_transform(Polygons_reshaped)\n",
    "#Polygons_projected=iso.transform(Polygons_reshaped)\n",
    "\n",
    "#iso = KernelPCA(n_components=2,kernel=\"rbf\", fit_inverse_transform=True, gamma=1e-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58330921  0.0061483   0.52658235  0.88020902 -0.01295413  1.15799693\n",
      "  -0.74081581  0.16049292 -0.68821998 -0.37195432 -0.46971572 -1.01925784\n",
      "   0.80181409 -0.81363501]]\n",
      "[[ 0.58330921  0.0061483   0.52658235  0.88020902 -0.01295413  1.15799693\n",
      "  -0.74081581  0.16049292 -0.68821998 -0.37195432 -0.46971572 -1.01925784\n",
      "   0.80181409 -0.81363501]]\n"
     ]
    }
   ],
   "source": [
    "contour=apply_procrustes(generate_contour(nb_of_points,False),False)\n",
    "contour=contour.reshape(2*contour.shape[0]).reshape(1,-1)\n",
    "print(contour)\n",
    "contour_tranformed=pca.transform(contour)\n",
    "print(pca.inverse_transform(contour_tranformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Organizing data \n",
    "training_data = Polygons_projected[:nb_training_data]\n",
    "testing_data  = Polygons_projected[nb_training_data:]\n",
    "\n",
    "training_labels=quality_matrices[:nb_training_data]\n",
    "testing_labels=quality_matrices[nb_training_data:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert to pytorch tennsors\n",
    "\n",
    "x_tensor=torch.from_numpy(training_data).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(testing_data).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "y_tensor=torch.from_numpy(training_labels).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(testing_labels).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "# Convert to pytorch variables\n",
    "x_variable=Variable(x_tensor)\n",
    "x_variable_test=Variable(x_tensor_test)\n",
    "\n",
    "\n",
    "\n",
    "y_variable=Variable(y_tensor)\n",
    "y_variable=y_variable.resize(nb_training_data,nb_of_points*nb_of_points)\n",
    "\n",
    "\n",
    "y_variable_test=Variable(y_tensor_test)\n",
    "y_variable_test=y_variable_test.resize(nb_test_data,nb_of_points*nb_of_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B_INIT = -0.2 # use a bad bias constant initializer\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_features_dimension,out_features_dimension,nb_of_hidden_layers,nb_of_hidden_nodes,\n",
    "                 batch_normalization=False):\n",
    "        \n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.nb_hidden_layers=nb_of_hidden_layers\n",
    "        self.do_bn=batch_normalization\n",
    "        self.fcs=[]\n",
    "        self.bns=[]\n",
    "        self.bn_input=nn.BatchNorm1d(in_features_dimension,momentum=0.5) #for input data\n",
    "        \n",
    "        for i in range(nb_of_hidden_layers):                              # build hidden layers and BN layers\n",
    "            \n",
    "            input_size=in_features_dimension if i==0 else nb_of_hidden_nodes\n",
    "            fc=nn.Linear(input_size,nb_of_hidden_nodes)\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self._set_init(fc)                  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "            \n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(nb_of_hidden_nodes, momentum=0.5)\n",
    "                setattr(self, 'bn%i' % i, bn)                         # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "    \n",
    "            self.predict = nn.Linear(nb_of_hidden_nodes,out_features_dimension)         # output layer\n",
    "            self._set_init(self.predict)                                              # parameters initialization\n",
    "    \n",
    "    \n",
    "    def _set_init(self, layer):\n",
    "            init.normal(layer.weight, mean=0., std=.1)\n",
    "            init.constant(layer.bias, B_INIT)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ACTIVATION=F.relu\n",
    "        pre_activation = [x]\n",
    "        if self.do_bn: x = self.bn_input(x)     # input batch normalization\n",
    "        layer_input = [x]\n",
    "        for i in range(self.nb_hidden_layers):\n",
    "            x = self.fcs[i](x)\n",
    "            pre_activation.append(x)\n",
    "            if self.do_bn: x = self.bns[i](x)   # batch normalization\n",
    "            x = ACTIVATION(x)\n",
    "            layer_input.append(x)\n",
    "        out = self.predict(x)\n",
    "        return out\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "my_net=Net(nb_components,nb_of_points*nb_of_points,nb_of_hidden_layers=6,\n",
    "           nb_of_hidden_nodes=400,batch_normalization=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=.5)\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    my_net.cuda()\n",
    "    loss_func.cuda()\n",
    "    x_variable , y_variable = x_variable.cuda(), y_variable.cuda()\n",
    "    x_variable_test , y_variable_test = x_variable_test.cuda(), y_variable_test.cuda()\n",
    "    print(\"cuda activated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 134.39097222222222 Test Loss: 117.92444444444445\n",
      "Epoch: 1 Training Loss: 113.92357725694444 Test Loss: 106.54243055555555\n",
      "Epoch: 2 Training Loss: 96.52738454861111 Test Loss: 91.25663888888889\n",
      "Epoch: 3 Training Loss: 81.29320746527777 Test Loss: 77.20840277777778\n",
      "Epoch: 4 Training Loss: 67.76445399305555 Test Loss: 64.32471527777778\n",
      "Epoch: 5 Training Loss: 55.772403645833336 Test Loss: 52.786972222222225\n",
      "Epoch: 6 Training Loss: 45.39795138888889 Test Loss: 42.875930555555556\n",
      "Epoch: 7 Training Loss: 36.7926953125 Test Loss: 34.71607986111111\n",
      "Epoch: 8 Training Loss: 29.908186848958334 Test Loss: 28.2218125\n",
      "Epoch: 9 Training Loss: 24.517813368055556 Test Loss: 23.163758680555556\n",
      "Epoch: 10 Training Loss: 20.318305555555554 Test Loss: 19.253227430555555\n",
      "Epoch: 11 Training Loss: 17.070235460069444 Test Loss: 16.251543402777777\n",
      "Epoch: 12 Training Loss: 14.584787434895834 Test Loss: 13.959493923611111\n",
      "Epoch: 13 Training Loss: 12.67856325954861 Test Loss: 12.198003472222222\n",
      "Epoch: 14 Training Loss: 11.19414529079861 Test Loss: 10.820611111111111\n",
      "Epoch: 15 Training Loss: 10.018953450520833 Test Loss: 9.726637152777778\n",
      "Epoch: 16 Training Loss: 9.077099934895834 Test Loss: 8.847580729166667\n",
      "Epoch: 17 Training Loss: 8.315671549479166 Test Loss: 8.1347265625\n",
      "Epoch: 18 Training Loss: 7.692947374131944 Test Loss: 7.548394097222222\n",
      "Epoch: 19 Training Loss: 7.176517578125 Test Loss: 7.059322482638889\n",
      "Epoch: 20 Training Loss: 6.7419777560763885 Test Loss: 6.645739149305555\n",
      "Epoch: 21 Training Loss: 6.372213161892361 Test Loss: 6.292490017361111\n",
      "Epoch: 22 Training Loss: 6.054546712239583 Test Loss: 5.9880451388888885\n",
      "Epoch: 23 Training Loss: 5.779369466145833 Test Loss: 5.723634114583334\n",
      "Epoch: 24 Training Loss: 5.539279893663195 Test Loss: 5.492936197916666\n",
      "Epoch: 25 Training Loss: 5.328303927951389 Test Loss: 5.289506944444445\n",
      "Epoch: 26 Training Loss: 5.1414132486979165 Test Loss: 5.108913628472222\n",
      "Epoch: 27 Training Loss: 4.974849066840278 Test Loss: 4.947730034722222\n",
      "Epoch: 28 Training Loss: 4.825697102864583 Test Loss: 4.80335546875\n",
      "Epoch: 29 Training Loss: 4.691572808159722 Test Loss: 4.673339409722222\n",
      "Epoch: 30 Training Loss: 4.570464870876736 Test Loss: 4.555844618055556\n",
      "Epoch: 31 Training Loss: 4.460700900607639 Test Loss: 4.44925\n",
      "Epoch: 32 Training Loss: 4.360838514539931 Test Loss: 4.352178385416667\n",
      "Epoch: 33 Training Loss: 4.269645670572917 Test Loss: 4.263450954861111\n",
      "Epoch: 34 Training Loss: 4.186067437065972 Test Loss: 4.182049045138889\n",
      "Epoch: 35 Training Loss: 4.109232584635417 Test Loss: 4.107120225694445\n",
      "Epoch: 36 Training Loss: 4.038354302300347 Test Loss: 4.037956597222222\n",
      "Epoch: 37 Training Loss: 3.9727999403211807 Test Loss: 3.973927517361111\n",
      "Epoch: 38 Training Loss: 3.9120090874565974 Test Loss: 3.9144609375\n",
      "Epoch: 39 Training Loss: 3.855458740234375 Test Loss: 3.8590672743055556\n",
      "Epoch: 40 Training Loss: 3.8026955295138887 Test Loss: 3.8073272569444443\n",
      "Epoch: 41 Training Loss: 3.7532808702256943 Test Loss: 3.7588832465277777\n",
      "Epoch: 42 Training Loss: 3.7069816623263887 Test Loss: 3.713498263888889\n",
      "Epoch: 43 Training Loss: 3.663503716362847 Test Loss: 3.6707395833333334\n",
      "Epoch: 44 Training Loss: 3.6224993489583333 Test Loss: 3.6304014756944443\n",
      "Epoch: 45 Training Loss: 3.583742431640625 Test Loss: 3.5922408854166665\n",
      "Epoch: 46 Training Loss: 3.5470694986979168 Test Loss: 3.556120876736111\n",
      "Epoch: 47 Training Loss: 3.5123270941840277 Test Loss: 3.521884982638889\n",
      "Epoch: 48 Training Loss: 3.4793202039930557 Test Loss: 3.4892921006944446\n",
      "Epoch: 49 Training Loss: 3.4478385145399306 Test Loss: 3.4581729600694446\n",
      "Epoch: 50 Training Loss: 3.417722954644097 Test Loss: 3.428388454861111\n",
      "Epoch: 51 Training Loss: 3.388841037326389 Test Loss: 3.3997486979166665\n",
      "Epoch: 52 Training Loss: 3.361082953559028 Test Loss: 3.3722000868055555\n",
      "Epoch: 53 Training Loss: 3.3343502875434026 Test Loss: 3.345634982638889\n",
      "Epoch: 54 Training Loss: 3.3085455457899307 Test Loss: 3.3199696180555556\n",
      "Epoch: 55 Training Loss: 3.2835788302951388 Test Loss: 3.2951156684027776\n",
      "Epoch: 56 Training Loss: 3.2593695746527778 Test Loss: 3.271004123263889\n",
      "Epoch: 57 Training Loss: 3.2358427191840278 Test Loss: 3.247566189236111\n",
      "Epoch: 58 Training Loss: 3.212942355685764 Test Loss: 3.2247274305555558\n",
      "Epoch: 59 Training Loss: 3.1906080457899306 Test Loss: 3.2024401041666666\n",
      "Epoch: 60 Training Loss: 3.1687848036024304 Test Loss: 3.1806534288194444\n",
      "Epoch: 61 Training Loss: 3.1474349229600693 Test Loss: 3.1593120659722222\n",
      "Epoch: 62 Training Loss: 3.1265086534288193 Test Loss: 3.1383719618055554\n",
      "Epoch: 63 Training Loss: 3.105962483723958 Test Loss: 3.117806423611111\n",
      "Epoch: 64 Training Loss: 3.0857508680555554 Test Loss: 3.097561197916667\n",
      "Epoch: 65 Training Loss: 3.065849609375 Test Loss: 3.077615668402778\n",
      "Epoch: 66 Training Loss: 3.046233615451389 Test Loss: 3.0579654947916666\n",
      "Epoch: 67 Training Loss: 3.0268868272569445 Test Loss: 3.0385753038194445\n",
      "Epoch: 68 Training Loss: 3.007795383029514 Test Loss: 3.019421875\n",
      "Epoch: 69 Training Loss: 2.9889391276041666 Test Loss: 3.0004891493055554\n",
      "Epoch: 70 Training Loss: 2.9703089735243053 Test Loss: 2.9817836371527777\n",
      "Epoch: 71 Training Loss: 2.9518926866319446 Test Loss: 2.9632940538194443\n",
      "Epoch: 72 Training Loss: 2.933677788628472 Test Loss: 2.944986762152778\n",
      "Epoch: 73 Training Loss: 2.915656521267361 Test Loss: 2.926865017361111\n",
      "Epoch: 74 Training Loss: 2.897813069661458 Test Loss: 2.908927734375\n",
      "Epoch: 75 Training Loss: 2.8801397569444442 Test Loss: 2.8911636284722224\n",
      "Epoch: 76 Training Loss: 2.8626278754340277 Test Loss: 2.8735390625\n",
      "Epoch: 77 Training Loss: 2.845270697699653 Test Loss: 2.856076388888889\n",
      "Epoch: 78 Training Loss: 2.828067816840278 Test Loss: 2.8387667100694443\n",
      "Epoch: 79 Training Loss: 2.811019124348958 Test Loss: 2.8216050347222223\n",
      "Epoch: 80 Training Loss: 2.7941181098090278 Test Loss: 2.8046000434027776\n",
      "Epoch: 81 Training Loss: 2.777370524088542 Test Loss: 2.7877608506944442\n",
      "Epoch: 82 Training Loss: 2.7607770453559026 Test Loss: 2.771070529513889\n",
      "Epoch: 83 Training Loss: 2.7443446180555555 Test Loss: 2.754529296875\n",
      "Epoch: 84 Training Loss: 2.7280704210069446 Test Loss: 2.738168619791667\n",
      "Epoch: 85 Training Loss: 2.7119538031684027 Test Loss: 2.7219652777777776\n",
      "Epoch: 86 Training Loss: 2.6959923502604166 Test Loss: 2.7059188368055556\n",
      "Epoch: 87 Training Loss: 2.680191162109375 Test Loss: 2.6900286458333333\n",
      "Epoch: 88 Training Loss: 2.664555392795139 Test Loss: 2.674314453125\n",
      "Epoch: 89 Training Loss: 2.6490797526041665 Test Loss: 2.658765625\n",
      "Epoch: 90 Training Loss: 2.6337657606336804 Test Loss: 2.6433875868055554\n",
      "Epoch: 91 Training Loss: 2.6186214463975697 Test Loss: 2.628179904513889\n",
      "Epoch: 92 Training Loss: 2.6036439073350692 Test Loss: 2.6131395399305557\n",
      "Epoch: 93 Training Loss: 2.5888382703993056 Test Loss: 2.59826953125\n",
      "Epoch: 94 Training Loss: 2.574206570095486 Test Loss: 2.583586371527778\n",
      "Epoch: 95 Training Loss: 2.559755398220486 Test Loss: 2.569074435763889\n",
      "Epoch: 96 Training Loss: 2.5454851888020835 Test Loss: 2.5547504340277776\n",
      "Epoch: 97 Training Loss: 2.5314031575520834 Test Loss: 2.5406228298611113\n",
      "Epoch: 98 Training Loss: 2.5175045301649304 Test Loss: 2.526676215277778\n",
      "Epoch: 99 Training Loss: 2.503782877604167 Test Loss: 2.512912326388889\n",
      "Epoch: 100 Training Loss: 2.490242919921875 Test Loss: 2.4993309461805557\n",
      "Epoch: 101 Training Loss: 2.4768870442708333 Test Loss: 2.4859379340277776\n",
      "Epoch: 102 Training Loss: 2.4637162000868056 Test Loss: 2.4727473958333333\n",
      "Epoch: 103 Training Loss: 2.4507290852864583 Test Loss: 2.4597411024305558\n",
      "Epoch: 104 Training Loss: 2.4379310167100696 Test Loss: 2.4469368489583334\n",
      "Epoch: 105 Training Loss: 2.425320502387153 Test Loss: 2.4343203125\n",
      "Epoch: 106 Training Loss: 2.412904486762153 Test Loss: 2.421898654513889\n",
      "Epoch: 107 Training Loss: 2.400683783637153 Test Loss: 2.409681423611111\n",
      "Epoch: 108 Training Loss: 2.3886581217447915 Test Loss: 2.3976710069444445\n",
      "Epoch: 109 Training Loss: 2.376828097873264 Test Loss: 2.3858409288194444\n",
      "Epoch: 110 Training Loss: 2.3651963975694446 Test Loss: 2.3742278645833332\n",
      "Epoch: 111 Training Loss: 2.353765218098958 Test Loss: 2.3628129340277777\n",
      "Epoch: 112 Training Loss: 2.342528374565972 Test Loss: 2.351592013888889\n",
      "Epoch: 113 Training Loss: 2.3314810655381946 Test Loss: 2.3405655381944444\n",
      "Epoch: 114 Training Loss: 2.3206199273003474 Test Loss: 2.329726779513889\n",
      "Epoch: 115 Training Loss: 2.3099395345052085 Test Loss: 2.319078125\n",
      "Epoch: 116 Training Loss: 2.2994399685329863 Test Loss: 2.3085987413194444\n",
      "Epoch: 117 Training Loss: 2.2891192762586807 Test Loss: 2.2983016493055555\n",
      "Epoch: 118 Training Loss: 2.278977457682292 Test Loss: 2.288183159722222\n",
      "Epoch: 119 Training Loss: 2.269014404296875 Test Loss: 2.2782504340277776\n",
      "Epoch: 120 Training Loss: 2.2592261827256945 Test Loss: 2.2684969618055555\n",
      "Epoch: 121 Training Loss: 2.2496029188368056 Test Loss: 2.25891796875\n",
      "Epoch: 122 Training Loss: 2.2401452094184027 Test Loss: 2.2494926215277777\n",
      "Epoch: 123 Training Loss: 2.2308483344184027 Test Loss: 2.240235677083333\n",
      "Epoch: 124 Training Loss: 2.2217128092447918 Test Loss: 2.2311473524305554\n",
      "Epoch: 125 Training Loss: 2.2127331136067707 Test Loss: 2.2222228732638887\n",
      "Epoch: 126 Training Loss: 2.203902818467882 Test Loss: 2.213445746527778\n",
      "Epoch: 127 Training Loss: 2.195222208658854 Test Loss: 2.2048233506944444\n",
      "Epoch: 128 Training Loss: 2.186688462999132 Test Loss: 2.1963394097222224\n",
      "Epoch: 129 Training Loss: 2.1783001573350695 Test Loss: 2.188017361111111\n",
      "Epoch: 130 Training Loss: 2.1700565321180556 Test Loss: 2.1798246527777776\n",
      "Epoch: 131 Training Loss: 2.1619468858506945 Test Loss: 2.1717740885416665\n",
      "Epoch: 132 Training Loss: 2.1539722493489584 Test Loss: 2.163857421875\n",
      "Epoch: 133 Training Loss: 2.14613127983941 Test Loss: 2.1560703125\n",
      "Epoch: 134 Training Loss: 2.138419704861111 Test Loss: 2.148408203125\n",
      "Epoch: 135 Training Loss: 2.1308291829427084 Test Loss: 2.1408849826388887\n",
      "Epoch: 136 Training Loss: 2.123362318250868 Test Loss: 2.1334811197916665\n",
      "Epoch: 137 Training Loss: 2.1160179985894096 Test Loss: 2.1261940104166666\n",
      "Epoch: 138 Training Loss: 2.1087972140842015 Test Loss: 2.119025824652778\n",
      "Epoch: 139 Training Loss: 2.1016952039930556 Test Loss: 2.111978949652778\n",
      "Epoch: 140 Training Loss: 2.094706515842014 Test Loss: 2.1050414496527776\n",
      "Epoch: 141 Training Loss: 2.0878182373046874 Test Loss: 2.098206597222222\n",
      "Epoch: 142 Training Loss: 2.081034844292535 Test Loss: 2.091479166666667\n",
      "Epoch: 143 Training Loss: 2.0743572998046873 Test Loss: 2.0848689236111113\n",
      "Epoch: 144 Training Loss: 2.067776828342014 Test Loss: 2.078357204861111\n",
      "Epoch: 145 Training Loss: 2.0612938503689238 Test Loss: 2.071928385416667\n",
      "Epoch: 146 Training Loss: 2.054900892469618 Test Loss: 2.0655970052083332\n",
      "Epoch: 147 Training Loss: 2.048600043402778 Test Loss: 2.0593485243055554\n",
      "Epoch: 148 Training Loss: 2.042387654622396 Test Loss: 2.0531940104166666\n",
      "Epoch: 149 Training Loss: 2.0362624918619794 Test Loss: 2.047127170138889\n",
      "Epoch: 150 Training Loss: 2.030221177842882 Test Loss: 2.04114453125\n",
      "Epoch: 151 Training Loss: 2.0242615966796875 Test Loss: 2.0352408854166666\n",
      "Epoch: 152 Training Loss: 2.0183876003689236 Test Loss: 2.0294305555555554\n",
      "Epoch: 153 Training Loss: 2.0125864393446182 Test Loss: 2.023681857638889\n",
      "Epoch: 154 Training Loss: 2.006862982855903 Test Loss: 2.01801171875\n",
      "Epoch: 155 Training Loss: 2.0012091878255207 Test Loss: 2.012421440972222\n",
      "Epoch: 156 Training Loss: 1.9956285400390625 Test Loss: 2.006902126736111\n",
      "Epoch: 157 Training Loss: 1.990118882921007 Test Loss: 2.001452907986111\n",
      "Epoch: 158 Training Loss: 1.9846756863064237 Test Loss: 1.9960716145833333\n",
      "Epoch: 159 Training Loss: 1.9792906222873263 Test Loss: 1.9907309027777778\n",
      "Epoch: 160 Training Loss: 1.9739685736762154 Test Loss: 1.9854633246527778\n",
      "Epoch: 161 Training Loss: 1.9687063530815971 Test Loss: 1.9802606336805555\n",
      "Epoch: 162 Training Loss: 1.963505886501736 Test Loss: 1.9751260850694445\n",
      "Epoch: 163 Training Loss: 1.9583605414496528 Test Loss: 1.9700496961805556\n",
      "Epoch: 164 Training Loss: 1.9532738308376736 Test Loss: 1.9650290798611112\n",
      "Epoch: 165 Training Loss: 1.9482424045138889 Test Loss: 1.9600520833333333\n",
      "Epoch: 166 Training Loss: 1.943253160264757 Test Loss: 1.9551180555555556\n",
      "Epoch: 167 Training Loss: 1.93831494140625 Test Loss: 1.9502430555555557\n",
      "Epoch: 168 Training Loss: 1.9334217258029514 Test Loss: 1.9454110243055556\n",
      "Epoch: 169 Training Loss: 1.9285708821614584 Test Loss: 1.9406202256944445\n",
      "Epoch: 170 Training Loss: 1.9237663031684027 Test Loss: 1.9358780381944445\n",
      "Epoch: 171 Training Loss: 1.9190107828776042 Test Loss: 1.9311786024305555\n",
      "Epoch: 172 Training Loss: 1.9142962917751736 Test Loss: 1.92652734375\n",
      "Epoch: 173 Training Loss: 1.9096270616319444 Test Loss: 1.921925564236111\n",
      "Epoch: 174 Training Loss: 1.9050000813802084 Test Loss: 1.9173637152777778\n",
      "Epoch: 175 Training Loss: 1.9004124755859375 Test Loss: 1.91283984375\n",
      "Epoch: 176 Training Loss: 1.8958668077256944 Test Loss: 1.908349392361111\n",
      "Epoch: 177 Training Loss: 1.8913611518012152 Test Loss: 1.9039014756944443\n",
      "Epoch: 178 Training Loss: 1.8868939480251736 Test Loss: 1.899484592013889\n",
      "Epoch: 179 Training Loss: 1.8824660101996529 Test Loss: 1.8951223958333334\n",
      "Epoch: 180 Training Loss: 1.8780784912109374 Test Loss: 1.8907925347222223\n",
      "Epoch: 181 Training Loss: 1.8737256401909723 Test Loss: 1.8865095486111112\n",
      "Epoch: 182 Training Loss: 1.8694084743923611 Test Loss: 1.8822556423611112\n",
      "Epoch: 183 Training Loss: 1.8651248914930556 Test Loss: 1.8780438368055556\n",
      "Epoch: 184 Training Loss: 1.8608799235026041 Test Loss: 1.873884548611111\n",
      "Epoch: 185 Training Loss: 1.8566714680989584 Test Loss: 1.8697552083333333\n",
      "Epoch: 186 Training Loss: 1.8524895562065973 Test Loss: 1.8656473524305555\n",
      "Epoch: 187 Training Loss: 1.8483385416666667 Test Loss: 1.8615720486111111\n",
      "Epoch: 188 Training Loss: 1.8442173529730903 Test Loss: 1.857521701388889\n",
      "Epoch: 189 Training Loss: 1.840126736111111 Test Loss: 1.8534978298611111\n",
      "Epoch: 190 Training Loss: 1.836069322374132 Test Loss: 1.8495023871527778\n",
      "Epoch: 191 Training Loss: 1.8320426432291668 Test Loss: 1.8455381944444444\n",
      "Epoch: 192 Training Loss: 1.828042738172743 Test Loss: 1.841597873263889\n",
      "Epoch: 193 Training Loss: 1.8240742730034722 Test Loss: 1.8376905381944444\n",
      "Epoch: 194 Training Loss: 1.8201360948350696 Test Loss: 1.833812717013889\n",
      "Epoch: 195 Training Loss: 1.8162278917100694 Test Loss: 1.8299654947916666\n",
      "Epoch: 196 Training Loss: 1.8123503011067708 Test Loss: 1.8261473524305556\n",
      "Epoch: 197 Training Loss: 1.8084952392578124 Test Loss: 1.8223504774305554\n",
      "Epoch: 198 Training Loss: 1.804667005750868 Test Loss: 1.8185835503472223\n",
      "Epoch: 199 Training Loss: 1.800859415690104 Test Loss: 1.814845703125\n",
      "Epoch: 200 Training Loss: 1.797077650282118 Test Loss: 1.8111358506944444\n",
      "Epoch: 201 Training Loss: 1.7933205023871528 Test Loss: 1.807447265625\n",
      "Epoch: 202 Training Loss: 1.7895894911024306 Test Loss: 1.8037889539930556\n",
      "Epoch: 203 Training Loss: 1.7858822564019097 Test Loss: 1.8001511501736112\n",
      "Epoch: 204 Training Loss: 1.7821986626519097 Test Loss: 1.796550564236111\n",
      "Epoch: 205 Training Loss: 1.7785346001519098 Test Loss: 1.7929586588541666\n",
      "Epoch: 206 Training Loss: 1.774888956705729 Test Loss: 1.7893887803819444\n",
      "Epoch: 207 Training Loss: 1.7712659776475694 Test Loss: 1.7858446180555556\n",
      "Epoch: 208 Training Loss: 1.76766357421875 Test Loss: 1.7823180338541667\n",
      "Epoch: 209 Training Loss: 1.7640807562934029 Test Loss: 1.7788108723958334\n",
      "Epoch: 210 Training Loss: 1.7605177273220487 Test Loss: 1.7753254123263889\n",
      "Epoch: 211 Training Loss: 1.756974622938368 Test Loss: 1.7718500434027777\n",
      "Epoch: 212 Training Loss: 1.753449001736111 Test Loss: 1.7683967013888888\n",
      "Epoch: 213 Training Loss: 1.7499442138671875 Test Loss: 1.7649659288194444\n",
      "Epoch: 214 Training Loss: 1.746461683485243 Test Loss: 1.7615649956597221\n",
      "Epoch: 215 Training Loss: 1.7430018581814235 Test Loss: 1.758186740451389\n",
      "Epoch: 216 Training Loss: 1.739558851453993 Test Loss: 1.7548230251736112\n",
      "Epoch: 217 Training Loss: 1.7361306966145833 Test Loss: 1.7514722222222223\n",
      "Epoch: 218 Training Loss: 1.7327143283420139 Test Loss: 1.7481391059027778\n",
      "Epoch: 219 Training Loss: 1.7293179117838542 Test Loss: 1.744827907986111\n",
      "Epoch: 220 Training Loss: 1.7259378526475695 Test Loss: 1.741526150173611\n",
      "Epoch: 221 Training Loss: 1.7225767957899305 Test Loss: 1.738255642361111\n",
      "Epoch: 222 Training Loss: 1.7192320014105902 Test Loss: 1.7349976128472222\n",
      "Epoch: 223 Training Loss: 1.7158992648654514 Test Loss: 1.73175390625\n",
      "Epoch: 224 Training Loss: 1.7125753173828124 Test Loss: 1.728521267361111\n",
      "Epoch: 225 Training Loss: 1.7092661946614582 Test Loss: 1.7253068576388888\n",
      "Epoch: 226 Training Loss: 1.705970201280382 Test Loss: 1.7221078559027778\n",
      "Epoch: 227 Training Loss: 1.7026871744791667 Test Loss: 1.7189307725694445\n",
      "Epoch: 228 Training Loss: 1.6994191623263888 Test Loss: 1.71575390625\n",
      "Epoch: 229 Training Loss: 1.6961593831380208 Test Loss: 1.7125944010416667\n",
      "Epoch: 230 Training Loss: 1.692907212999132 Test Loss: 1.7094440104166666\n",
      "Epoch: 231 Training Loss: 1.6896651475694444 Test Loss: 1.7062998046875\n",
      "Epoch: 232 Training Loss: 1.6864311387803819 Test Loss: 1.7031627604166666\n",
      "Epoch: 233 Training Loss: 1.6832043728298611 Test Loss: 1.7000350477430555\n",
      "Epoch: 234 Training Loss: 1.6799871961805555 Test Loss: 1.6969214409722222\n",
      "Epoch: 235 Training Loss: 1.676780314127604 Test Loss: 1.6938246527777778\n",
      "Epoch: 236 Training Loss: 1.673589816623264 Test Loss: 1.6907495659722223\n",
      "Epoch: 237 Training Loss: 1.6704061957465277 Test Loss: 1.6876776258680555\n",
      "Epoch: 238 Training Loss: 1.6672257080078126 Test Loss: 1.6846041666666667\n",
      "Epoch: 239 Training Loss: 1.6640480550130208 Test Loss: 1.6815322265625\n",
      "Epoch: 240 Training Loss: 1.6608793402777777 Test Loss: 1.6784737413194444\n",
      "Epoch: 241 Training Loss: 1.6577146945529513 Test Loss: 1.675427734375\n",
      "Epoch: 242 Training Loss: 1.6545574408637154 Test Loss: 1.672384548611111\n",
      "Epoch: 243 Training Loss: 1.6514042561848958 Test Loss: 1.6693446180555556\n",
      "Epoch: 244 Training Loss: 1.6482515733506944 Test Loss: 1.666310329861111\n",
      "Epoch: 245 Training Loss: 1.6451046549479167 Test Loss: 1.6632788628472222\n",
      "Epoch: 246 Training Loss: 1.6419599609375 Test Loss: 1.6602465277777778\n",
      "Epoch: 247 Training Loss: 1.638818820529514 Test Loss: 1.6572113715277779\n",
      "Epoch: 248 Training Loss: 1.6356797010633681 Test Loss: 1.6541786024305556\n",
      "Epoch: 249 Training Loss: 1.6325435926649305 Test Loss: 1.6511494140625\n",
      "Epoch: 250 Training Loss: 1.6294099392361112 Test Loss: 1.6481282552083334\n",
      "Epoch: 251 Training Loss: 1.6262809380425347 Test Loss: 1.6451102430555555\n",
      "Epoch: 252 Training Loss: 1.6231524386935763 Test Loss: 1.6421069878472223\n",
      "Epoch: 253 Training Loss: 1.6200267469618055 Test Loss: 1.6391048177083334\n",
      "Epoch: 254 Training Loss: 1.616900173611111 Test Loss: 1.6361024305555556\n",
      "Epoch: 255 Training Loss: 1.613769259982639 Test Loss: 1.6331048177083334\n",
      "Epoch: 256 Training Loss: 1.6106397569444444 Test Loss: 1.6300989583333334\n",
      "Epoch: 257 Training Loss: 1.6075104437934027 Test Loss: 1.6270966796875\n",
      "Epoch: 258 Training Loss: 1.6043857150607639 Test Loss: 1.624093532986111\n",
      "Epoch: 259 Training Loss: 1.601261257595486 Test Loss: 1.6210835503472223\n",
      "Epoch: 260 Training Loss: 1.5981366102430556 Test Loss: 1.618080078125\n",
      "Epoch: 261 Training Loss: 1.5950121392144097 Test Loss: 1.6150824652777778\n",
      "Epoch: 262 Training Loss: 1.5918878580729168 Test Loss: 1.612086154513889\n",
      "Epoch: 263 Training Loss: 1.588764133029514 Test Loss: 1.6090908203125\n",
      "Epoch: 264 Training Loss: 1.5856376139322916 Test Loss: 1.6060933159722222\n",
      "Epoch: 265 Training Loss: 1.5825105929904515 Test Loss: 1.6030942925347222\n",
      "Epoch: 266 Training Loss: 1.5793844265407986 Test Loss: 1.6000954861111112\n",
      "Epoch: 267 Training Loss: 1.5762542317708332 Test Loss: 1.5970970052083333\n",
      "Epoch: 268 Training Loss: 1.5731197645399306 Test Loss: 1.5940844184027778\n",
      "Epoch: 269 Training Loss: 1.5699812418619792 Test Loss: 1.5910662977430556\n",
      "Epoch: 270 Training Loss: 1.5668412543402779 Test Loss: 1.5880481770833332\n",
      "Epoch: 271 Training Loss: 1.5637001274956597 Test Loss: 1.58503515625\n",
      "Epoch: 272 Training Loss: 1.5605584038628473 Test Loss: 1.5820149739583333\n",
      "Epoch: 273 Training Loss: 1.5574119194878473 Test Loss: 1.5790046657986112\n",
      "Epoch: 274 Training Loss: 1.5542684054904514 Test Loss: 1.5760021701388889\n",
      "Epoch: 275 Training Loss: 1.551123074001736 Test Loss: 1.5729940321180556\n",
      "Epoch: 276 Training Loss: 1.5479727918836805 Test Loss: 1.5699761284722222\n",
      "Epoch: 277 Training Loss: 1.544817640516493 Test Loss: 1.5669573567708333\n",
      "Epoch: 278 Training Loss: 1.5416617024739583 Test Loss: 1.563933810763889\n",
      "Epoch: 279 Training Loss: 1.5385060221354168 Test Loss: 1.5609111328125\n",
      "Epoch: 280 Training Loss: 1.535346923828125 Test Loss: 1.5578891059027777\n",
      "Epoch: 281 Training Loss: 1.5321861300998263 Test Loss: 1.5548607855902778\n",
      "Epoch: 282 Training Loss: 1.529024685329861 Test Loss: 1.5518327907986111\n",
      "Epoch: 283 Training Loss: 1.525863511827257 Test Loss: 1.5488075086805555\n",
      "Epoch: 284 Training Loss: 1.522707533094618 Test Loss: 1.5457893880208333\n",
      "Epoch: 285 Training Loss: 1.5195553521050347 Test Loss: 1.5427836371527779\n",
      "Epoch: 286 Training Loss: 1.5164076877170138 Test Loss: 1.539779296875\n",
      "Epoch: 287 Training Loss: 1.5132651774088541 Test Loss: 1.5367815755208334\n",
      "Epoch: 288 Training Loss: 1.5101266547309027 Test Loss: 1.5337928602430555\n",
      "Epoch: 289 Training Loss: 1.5069931369357639 Test Loss: 1.5308046875\n",
      "Epoch: 290 Training Loss: 1.5038688286675348 Test Loss: 1.5278223741319445\n",
      "Epoch: 291 Training Loss: 1.5007527804904515 Test Loss: 1.5248444010416666\n",
      "Epoch: 292 Training Loss: 1.4976448296440972 Test Loss: 1.5218857421875\n",
      "Epoch: 293 Training Loss: 1.4945462782118055 Test Loss: 1.5189361979166667\n",
      "Epoch: 294 Training Loss: 1.4914588216145834 Test Loss: 1.5159904513888889\n",
      "Epoch: 295 Training Loss: 1.4883855794270833 Test Loss: 1.5130673828125\n",
      "Epoch: 296 Training Loss: 1.485321044921875 Test Loss: 1.5101514756944445\n",
      "Epoch: 297 Training Loss: 1.482261257595486 Test Loss: 1.5072371961805555\n",
      "Epoch: 298 Training Loss: 1.4792127143012153 Test Loss: 1.5043313802083333\n",
      "Epoch: 299 Training Loss: 1.4761761745876736 Test Loss: 1.5014469401041666\n",
      "Epoch: 300 Training Loss: 1.4731498480902778 Test Loss: 1.4985729166666666\n",
      "Epoch: 301 Training Loss: 1.4701369222005207 Test Loss: 1.495713650173611\n",
      "Epoch: 302 Training Loss: 1.4671353217230902 Test Loss: 1.492856228298611\n",
      "Epoch: 303 Training Loss: 1.4641376274956597 Test Loss: 1.4900110677083334\n",
      "Epoch: 304 Training Loss: 1.4611497395833333 Test Loss: 1.487181640625\n",
      "Epoch: 305 Training Loss: 1.4581712646484375 Test Loss: 1.4843604600694444\n",
      "Epoch: 306 Training Loss: 1.4551990288628471 Test Loss: 1.4815415581597222\n",
      "Epoch: 307 Training Loss: 1.4522314046223959 Test Loss: 1.4787230902777777\n",
      "Epoch: 308 Training Loss: 1.4492723253038196 Test Loss: 1.4759171006944445\n",
      "Epoch: 309 Training Loss: 1.4463179253472223 Test Loss: 1.4731138237847223\n",
      "Epoch: 310 Training Loss: 1.443374742296007 Test Loss: 1.4703203125\n",
      "Epoch: 311 Training Loss: 1.4404390869140624 Test Loss: 1.4675450303819444\n",
      "Epoch: 312 Training Loss: 1.4375142550998263 Test Loss: 1.4647724609375\n",
      "Epoch: 313 Training Loss: 1.4345946451822917 Test Loss: 1.4620110677083333\n",
      "Epoch: 314 Training Loss: 1.4316752251519098 Test Loss: 1.4592482638888888\n",
      "Epoch: 315 Training Loss: 1.4287591145833334 Test Loss: 1.45650390625\n",
      "Epoch: 316 Training Loss: 1.4258503824869793 Test Loss: 1.4537587890625\n",
      "Epoch: 317 Training Loss: 1.4229476589626735 Test Loss: 1.4510259331597222\n",
      "Epoch: 318 Training Loss: 1.4200531277126736 Test Loss: 1.4483063151041666\n",
      "Epoch: 319 Training Loss: 1.4171632351345487 Test Loss: 1.4455834418402778\n",
      "Epoch: 320 Training Loss: 1.4142792697482638 Test Loss: 1.4428695746527778\n",
      "Epoch: 321 Training Loss: 1.4113994683159723 Test Loss: 1.4401527777777778\n",
      "Epoch: 322 Training Loss: 1.4085233696831598 Test Loss: 1.4374481336805556\n",
      "Epoch: 323 Training Loss: 1.4056482204861112 Test Loss: 1.4347379557291666\n",
      "Epoch: 324 Training Loss: 1.4027825249565973 Test Loss: 1.4320451388888888\n",
      "Epoch: 325 Training Loss: 1.3999263509114583 Test Loss: 1.4293530815972222\n",
      "Epoch: 326 Training Loss: 1.397075900607639 Test Loss: 1.426667751736111\n",
      "Epoch: 327 Training Loss: 1.3942343071831598 Test Loss: 1.4240015190972222\n",
      "Epoch: 328 Training Loss: 1.3913942464192708 Test Loss: 1.4213385416666666\n",
      "Epoch: 329 Training Loss: 1.3885614149305556 Test Loss: 1.4186787109375\n",
      "Epoch: 330 Training Loss: 1.3857298177083333 Test Loss: 1.4160244140625\n",
      "Epoch: 331 Training Loss: 1.3829010552300347 Test Loss: 1.4133682725694445\n",
      "Epoch: 332 Training Loss: 1.3800776774088541 Test Loss: 1.4107239583333333\n",
      "Epoch: 333 Training Loss: 1.3772609185112847 Test Loss: 1.4080868055555555\n",
      "Epoch: 334 Training Loss: 1.3744444444444444 Test Loss: 1.4054641927083333\n",
      "Epoch: 335 Training Loss: 1.3716310763888888 Test Loss: 1.4028272569444444\n",
      "Epoch: 336 Training Loss: 1.368820068359375 Test Loss: 1.400201388888889\n",
      "Epoch: 337 Training Loss: 1.366014119466146 Test Loss: 1.3975849609375\n",
      "Epoch: 338 Training Loss: 1.3632169325086805 Test Loss: 1.3949855685763888\n",
      "Epoch: 339 Training Loss: 1.3604197455512153 Test Loss: 1.3923827039930556\n",
      "Epoch: 340 Training Loss: 1.3576255018446182 Test Loss: 1.3897808159722222\n",
      "Epoch: 341 Training Loss: 1.3548347032335069 Test Loss: 1.3871946614583333\n",
      "Epoch: 342 Training Loss: 1.3520440266927083 Test Loss: 1.384592013888889\n",
      "Epoch: 343 Training Loss: 1.3492582600911458 Test Loss: 1.3819918619791667\n",
      "Epoch: 344 Training Loss: 1.346479763454861 Test Loss: 1.37940234375\n",
      "Epoch: 345 Training Loss: 1.343706800672743 Test Loss: 1.3768226996527777\n",
      "Epoch: 346 Training Loss: 1.3409355604383681 Test Loss: 1.374248046875\n",
      "Epoch: 347 Training Loss: 1.3381725124782986 Test Loss: 1.3716815321180555\n",
      "Epoch: 348 Training Loss: 1.3354141167534723 Test Loss: 1.3691189236111112\n",
      "Epoch: 349 Training Loss: 1.332660888671875 Test Loss: 1.3665546875\n",
      "Epoch: 350 Training Loss: 1.3299125298394097 Test Loss: 1.3640046657986111\n",
      "Epoch: 351 Training Loss: 1.3271710205078124 Test Loss: 1.3614592013888889\n",
      "Epoch: 352 Training Loss: 1.324436048719618 Test Loss: 1.358921875\n",
      "Epoch: 353 Training Loss: 1.3217030978732638 Test Loss: 1.356396484375\n",
      "Epoch: 354 Training Loss: 1.318972900390625 Test Loss: 1.3538763020833333\n",
      "Epoch: 355 Training Loss: 1.3162506103515625 Test Loss: 1.351357204861111\n",
      "Epoch: 356 Training Loss: 1.313534193250868 Test Loss: 1.3488474392361112\n",
      "Epoch: 357 Training Loss: 1.310821072048611 Test Loss: 1.3463322482638889\n",
      "Epoch: 358 Training Loss: 1.3081165093315972 Test Loss: 1.3438362630208334\n",
      "Epoch: 359 Training Loss: 1.3054203016493056 Test Loss: 1.3413372395833334\n",
      "Epoch: 360 Training Loss: 1.3027263997395833 Test Loss: 1.3388384331597223\n",
      "Epoch: 361 Training Loss: 1.3000292290581597 Test Loss: 1.3363352864583333\n",
      "Epoch: 362 Training Loss: 1.2973456488715278 Test Loss: 1.333853515625\n",
      "Epoch: 363 Training Loss: 1.2946677381727432 Test Loss: 1.3313773871527779\n",
      "Epoch: 364 Training Loss: 1.2919932318793403 Test Loss: 1.3289011501736112\n",
      "Epoch: 365 Training Loss: 1.289322265625 Test Loss: 1.326435546875\n",
      "Epoch: 366 Training Loss: 1.2866554633246527 Test Loss: 1.3239754774305557\n",
      "Epoch: 367 Training Loss: 1.2839908854166666 Test Loss: 1.321522677951389\n",
      "Epoch: 368 Training Loss: 1.2813294949001737 Test Loss: 1.3190768229166667\n",
      "Epoch: 369 Training Loss: 1.2786765679253471 Test Loss: 1.3166416015625\n",
      "Epoch: 370 Training Loss: 1.2760259602864583 Test Loss: 1.3142024739583333\n",
      "Epoch: 371 Training Loss: 1.2733827446831598 Test Loss: 1.3117719184027778\n",
      "Epoch: 372 Training Loss: 1.2707458902994793 Test Loss: 1.3093509114583333\n",
      "Epoch: 373 Training Loss: 1.2681113145616318 Test Loss: 1.3069342447916668\n",
      "Epoch: 374 Training Loss: 1.2654818793402778 Test Loss: 1.3045276692708334\n",
      "Epoch: 375 Training Loss: 1.2628606092664931 Test Loss: 1.3021197916666667\n",
      "Epoch: 376 Training Loss: 1.2602373996310763 Test Loss: 1.2997076822916667\n",
      "Epoch: 377 Training Loss: 1.2576189507378472 Test Loss: 1.2973060980902777\n",
      "Epoch: 378 Training Loss: 1.2550086941189236 Test Loss: 1.29490234375\n",
      "Epoch: 379 Training Loss: 1.2524031575520833 Test Loss: 1.2925081380208334\n",
      "Epoch: 380 Training Loss: 1.2498025580512153 Test Loss: 1.2901228298611112\n",
      "Epoch: 381 Training Loss: 1.2472088351779513 Test Loss: 1.2877358940972221\n",
      "Epoch: 382 Training Loss: 1.2446154920789931 Test Loss: 1.2853589409722221\n",
      "Epoch: 383 Training Loss: 1.2420262315538195 Test Loss: 1.2829832899305555\n",
      "Epoch: 384 Training Loss: 1.2394437120225694 Test Loss: 1.2806082899305555\n",
      "Epoch: 385 Training Loss: 1.2368692355685764 Test Loss: 1.2782513020833333\n",
      "Epoch: 386 Training Loss: 1.2343013102213543 Test Loss: 1.2758872612847223\n",
      "Epoch: 387 Training Loss: 1.2317354871961805 Test Loss: 1.2735380859375\n",
      "Epoch: 388 Training Loss: 1.2291759033203125 Test Loss: 1.2711848958333334\n",
      "Epoch: 389 Training Loss: 1.2266197916666666 Test Loss: 1.2688561197916666\n",
      "Epoch: 390 Training Loss: 1.2240652940538195 Test Loss: 1.266521267361111\n",
      "Epoch: 391 Training Loss: 1.2215122612847222 Test Loss: 1.2641870659722223\n",
      "Epoch: 392 Training Loss: 1.2189629448784722 Test Loss: 1.2618598090277777\n",
      "Epoch: 393 Training Loss: 1.2164189317491318 Test Loss: 1.2595462239583333\n",
      "Epoch: 394 Training Loss: 1.21387646484375 Test Loss: 1.25723046875\n",
      "Epoch: 395 Training Loss: 1.2113326958550348 Test Loss: 1.254900390625\n",
      "Epoch: 396 Training Loss: 1.2087928873697917 Test Loss: 1.2525846354166668\n",
      "Epoch: 397 Training Loss: 1.206261461046007 Test Loss: 1.2502696397569444\n",
      "Epoch: 398 Training Loss: 1.2037351752387153 Test Loss: 1.2479507378472223\n",
      "Epoch: 399 Training Loss: 1.2012133517795138 Test Loss: 1.2456392144097221\n",
      "Epoch: 400 Training Loss: 1.1986985405815973 Test Loss: 1.2433427734375\n",
      "Epoch: 401 Training Loss: 1.1961879204644097 Test Loss: 1.2410537109375\n",
      "Epoch: 402 Training Loss: 1.1936822916666667 Test Loss: 1.2387623697916668\n",
      "Epoch: 403 Training Loss: 1.1911788736979168 Test Loss: 1.2364759114583332\n",
      "Epoch: 404 Training Loss: 1.1886807861328126 Test Loss: 1.2341888020833334\n",
      "Epoch: 405 Training Loss: 1.1861895616319444 Test Loss: 1.2319128689236112\n",
      "Epoch: 406 Training Loss: 1.1837024875217015 Test Loss: 1.2296371527777779\n",
      "Epoch: 407 Training Loss: 1.1812214898003472 Test Loss: 1.2273682725694444\n",
      "Epoch: 408 Training Loss: 1.1787455376519098 Test Loss: 1.22510546875\n",
      "Epoch: 409 Training Loss: 1.1762714165581598 Test Loss: 1.2228490668402778\n",
      "Epoch: 410 Training Loss: 1.1738053249782987 Test Loss: 1.2205896267361112\n",
      "Epoch: 411 Training Loss: 1.171343967013889 Test Loss: 1.218339626736111\n",
      "Epoch: 412 Training Loss: 1.1688854302300347 Test Loss: 1.216094509548611\n",
      "Epoch: 413 Training Loss: 1.1664363742404513 Test Loss: 1.2138478732638889\n",
      "Epoch: 414 Training Loss: 1.163987291124132 Test Loss: 1.2116195746527778\n",
      "Epoch: 415 Training Loss: 1.16154296875 Test Loss: 1.2093803168402777\n",
      "Epoch: 416 Training Loss: 1.1591060248480902 Test Loss: 1.2071501736111112\n",
      "Epoch: 417 Training Loss: 1.1566746690538194 Test Loss: 1.204935546875\n",
      "Epoch: 418 Training Loss: 1.1542469211154514 Test Loss: 1.2027145182291668\n",
      "Epoch: 419 Training Loss: 1.1518230929904514 Test Loss: 1.2004937065972223\n",
      "Epoch: 420 Training Loss: 1.14941015625 Test Loss: 1.1982900390625\n",
      "Epoch: 421 Training Loss: 1.1469991183810764 Test Loss: 1.1960788845486112\n",
      "Epoch: 422 Training Loss: 1.1445947808159722 Test Loss: 1.1938719618055555\n",
      "Epoch: 423 Training Loss: 1.1421929796006944 Test Loss: 1.1916762152777778\n",
      "Epoch: 424 Training Loss: 1.139802747938368 Test Loss: 1.1894921875\n",
      "Epoch: 425 Training Loss: 1.1374165785047743 Test Loss: 1.1873135850694445\n",
      "Epoch: 426 Training Loss: 1.1350392184787326 Test Loss: 1.1851434461805554\n",
      "Epoch: 427 Training Loss: 1.1326659545898436 Test Loss: 1.1829721137152778\n",
      "Epoch: 428 Training Loss: 1.1302923380533854 Test Loss: 1.1808140190972223\n",
      "Epoch: 429 Training Loss: 1.1279287923177084 Test Loss: 1.1786655815972222\n",
      "Epoch: 430 Training Loss: 1.1255737169053819 Test Loss: 1.1765270182291667\n",
      "Epoch: 431 Training Loss: 1.1232208523220486 Test Loss: 1.1743799913194444\n",
      "Epoch: 432 Training Loss: 1.120872572157118 Test Loss: 1.1722443576388888\n",
      "Epoch: 433 Training Loss: 1.1185315958658855 Test Loss: 1.1701223958333333\n",
      "Epoch: 434 Training Loss: 1.1162008395724827 Test Loss: 1.1680180121527777\n",
      "Epoch: 435 Training Loss: 1.113871812608507 Test Loss: 1.1659069010416667\n",
      "Epoch: 436 Training Loss: 1.1115464070638021 Test Loss: 1.163797092013889\n",
      "Epoch: 437 Training Loss: 1.1092291124131946 Test Loss: 1.161697265625\n",
      "Epoch: 438 Training Loss: 1.1069224446614583 Test Loss: 1.1595993923611112\n",
      "Epoch: 439 Training Loss: 1.104621554904514 Test Loss: 1.1575004340277777\n",
      "Epoch: 440 Training Loss: 1.1023226250542535 Test Loss: 1.1554188368055556\n",
      "Epoch: 441 Training Loss: 1.100029073079427 Test Loss: 1.1533324652777779\n",
      "Epoch: 442 Training Loss: 1.097739495171441 Test Loss: 1.1512748480902777\n",
      "Epoch: 443 Training Loss: 1.0954466010199653 Test Loss: 1.1492010633680556\n",
      "Epoch: 444 Training Loss: 1.0931656358506945 Test Loss: 1.147140625\n",
      "Epoch: 445 Training Loss: 1.090890597873264 Test Loss: 1.1450814887152778\n",
      "Epoch: 446 Training Loss: 1.0886218939887153 Test Loss: 1.1430268012152778\n",
      "Epoch: 447 Training Loss: 1.0863572998046875 Test Loss: 1.1409911024305555\n",
      "Epoch: 448 Training Loss: 1.0841006401909723 Test Loss: 1.1389539930555554\n",
      "Epoch: 449 Training Loss: 1.0818487684461806 Test Loss: 1.136926540798611\n",
      "Epoch: 450 Training Loss: 1.079602098253038 Test Loss: 1.1349066840277777\n",
      "Epoch: 451 Training Loss: 1.0773602498372397 Test Loss: 1.1328891059027777\n",
      "Epoch: 452 Training Loss: 1.0751209920247395 Test Loss: 1.130883029513889\n",
      "Epoch: 453 Training Loss: 1.0728837483723959 Test Loss: 1.1288783637152777\n",
      "Epoch: 454 Training Loss: 1.0706537407769097 Test Loss: 1.126866427951389\n",
      "Epoch: 455 Training Loss: 1.0684391479492188 Test Loss: 1.1248704427083334\n",
      "Epoch: 456 Training Loss: 1.0662230495876737 Test Loss: 1.1228701171875\n",
      "Epoch: 457 Training Loss: 1.0640105455186633 Test Loss: 1.1208955078125\n",
      "Epoch: 458 Training Loss: 1.0618045925564237 Test Loss: 1.1189111328125\n",
      "Epoch: 459 Training Loss: 1.059602789984809 Test Loss: 1.1169317491319444\n",
      "Epoch: 460 Training Loss: 1.0574056057400174 Test Loss: 1.1149557291666667\n",
      "Epoch: 461 Training Loss: 1.0552122124565972 Test Loss: 1.1129850260416667\n",
      "Epoch: 462 Training Loss: 1.0530274115668403 Test Loss: 1.1110345052083332\n",
      "Epoch: 463 Training Loss: 1.0508482801649306 Test Loss: 1.1090807291666667\n",
      "Epoch: 464 Training Loss: 1.04867529296875 Test Loss: 1.1071213107638889\n",
      "Epoch: 465 Training Loss: 1.0464998508029515 Test Loss: 1.1051799045138888\n",
      "Epoch: 466 Training Loss: 1.0443366224500867 Test Loss: 1.1032290581597222\n",
      "Epoch: 467 Training Loss: 1.042173556857639 Test Loss: 1.1013100043402777\n",
      "Epoch: 468 Training Loss: 1.0400217013888888 Test Loss: 1.0993741319444446\n",
      "Epoch: 469 Training Loss: 1.0378726128472222 Test Loss: 1.0974803602430556\n",
      "Epoch: 470 Training Loss: 1.035733181423611 Test Loss: 1.095557400173611\n",
      "Epoch: 471 Training Loss: 1.0335933973524305 Test Loss: 1.093653537326389\n",
      "Epoch: 472 Training Loss: 1.031455105251736 Test Loss: 1.0917391493055555\n",
      "Epoch: 473 Training Loss: 1.0293305053710937 Test Loss: 1.0898407118055555\n",
      "Epoch: 474 Training Loss: 1.0272032945421008 Test Loss: 1.0879454210069444\n",
      "Epoch: 475 Training Loss: 1.0250775282118056 Test Loss: 1.0860469835069444\n",
      "Epoch: 476 Training Loss: 1.0229552069769965 Test Loss: 1.0841563585069445\n",
      "Epoch: 477 Training Loss: 1.020838860405816 Test Loss: 1.0822677951388888\n",
      "Epoch: 478 Training Loss: 1.0187325168185764 Test Loss: 1.0803799913194445\n",
      "Epoch: 479 Training Loss: 1.0166287706163195 Test Loss: 1.0785119357638888\n",
      "Epoch: 480 Training Loss: 1.0145267469618056 Test Loss: 1.0766295572916667\n",
      "Epoch: 481 Training Loss: 1.012424791124132 Test Loss: 1.0747682291666667\n",
      "Epoch: 482 Training Loss: 1.0103354966905382 Test Loss: 1.0728878038194445\n",
      "Epoch: 483 Training Loss: 1.0082550659179688 Test Loss: 1.0710516493055555\n",
      "Epoch: 484 Training Loss: 1.0061763780381945 Test Loss: 1.0691930338541666\n",
      "Epoch: 485 Training Loss: 1.0040985446506077 Test Loss: 1.0673454861111111\n",
      "Epoch: 486 Training Loss: 1.0020240207248263 Test Loss: 1.0654995659722222\n",
      "Epoch: 487 Training Loss: 0.9999542914496528 Test Loss: 1.0636548394097223\n",
      "Epoch: 488 Training Loss: 0.9978858574761285 Test Loss: 1.0618344184027777\n",
      "Epoch: 489 Training Loss: 0.9958274603949653 Test Loss: 1.060007052951389\n",
      "Epoch: 490 Training Loss: 0.9937717895507813 Test Loss: 1.0581899956597223\n",
      "Epoch: 491 Training Loss: 0.9917204454210069 Test Loss: 1.0563868272569445\n",
      "Epoch: 492 Training Loss: 0.9896742824978298 Test Loss: 1.0545674913194445\n",
      "Epoch: 493 Training Loss: 0.9876347384982639 Test Loss: 1.0527843967013888\n",
      "Epoch: 494 Training Loss: 0.9855984497070313 Test Loss: 1.0509860026041666\n",
      "Epoch: 495 Training Loss: 0.9835674302842882 Test Loss: 1.0491996527777778\n",
      "Epoch: 496 Training Loss: 0.9815400933159723 Test Loss: 1.0474122178819445\n",
      "Epoch: 497 Training Loss: 0.9795182698567708 Test Loss: 1.045643771701389\n",
      "Epoch: 498 Training Loss: 0.9774988674587674 Test Loss: 1.0438646918402779\n",
      "Epoch: 499 Training Loss: 0.975483405219184 Test Loss: 1.0420756293402778\n",
      "Epoch: 500 Training Loss: 0.9734709065755208 Test Loss: 1.040328125\n",
      "Epoch: 501 Training Loss: 0.9714640502929688 Test Loss: 1.03856640625\n",
      "Epoch: 502 Training Loss: 0.9694620090060764 Test Loss: 1.036800564236111\n",
      "Epoch: 503 Training Loss: 0.9674664510091145 Test Loss: 1.0350703125\n",
      "Epoch: 504 Training Loss: 0.9654744533962674 Test Loss: 1.0333346354166666\n",
      "Epoch: 505 Training Loss: 0.9634898003472222 Test Loss: 1.0315955946180555\n",
      "Epoch: 506 Training Loss: 0.9615091620551215 Test Loss: 1.0298784722222223\n",
      "Epoch: 507 Training Loss: 0.9595281236436632 Test Loss: 1.0281423611111111\n",
      "Epoch: 508 Training Loss: 0.9575598890516493 Test Loss: 1.0264324001736111\n",
      "Epoch: 509 Training Loss: 0.9555990058051216 Test Loss: 1.0247190755208333\n",
      "Epoch: 510 Training Loss: 0.9536353827582466 Test Loss: 1.0229867621527777\n",
      "Epoch: 511 Training Loss: 0.9516799519856771 Test Loss: 1.0212782118055554\n",
      "Epoch: 512 Training Loss: 0.9497275661892361 Test Loss: 1.0195711805555556\n",
      "Epoch: 513 Training Loss: 0.9477813110351563 Test Loss: 1.0178812934027779\n",
      "Epoch: 514 Training Loss: 0.9458384263780382 Test Loss: 1.016193359375\n",
      "Epoch: 515 Training Loss: 0.9439042019314237 Test Loss: 1.0144983723958334\n",
      "Epoch: 516 Training Loss: 0.9419659898546007 Test Loss: 1.0128054470486112\n",
      "Epoch: 517 Training Loss: 0.940038323296441 Test Loss: 1.0111028645833333\n",
      "Epoch: 518 Training Loss: 0.9381124538845486 Test Loss: 1.0094254557291666\n",
      "Epoch: 519 Training Loss: 0.9361878051757813 Test Loss: 1.0077344835069444\n",
      "Epoch: 520 Training Loss: 0.9342677883572048 Test Loss: 1.0060654296875\n",
      "Epoch: 521 Training Loss: 0.9323559231228299 Test Loss: 1.0043743489583334\n",
      "Epoch: 522 Training Loss: 0.9304466484917535 Test Loss: 1.0027046440972223\n",
      "Epoch: 523 Training Loss: 0.9285457288953993 Test Loss: 1.0010443793402777\n",
      "Epoch: 524 Training Loss: 0.926650139702691 Test Loss: 0.9993790147569445\n",
      "Epoch: 525 Training Loss: 0.9247579888237847 Test Loss: 0.9977390407986111\n",
      "Epoch: 526 Training Loss: 0.9228712361653646 Test Loss: 0.9960890842013889\n",
      "Epoch: 527 Training Loss: 0.9209823947482639 Test Loss: 0.9944299045138889\n",
      "Epoch: 528 Training Loss: 0.9191003011067709 Test Loss: 0.9927799479166667\n",
      "Epoch: 529 Training Loss: 0.9172246771918403 Test Loss: 0.9911575520833333\n",
      "Epoch: 530 Training Loss: 0.9153496500651042 Test Loss: 0.9895088975694445\n",
      "Epoch: 531 Training Loss: 0.9134796278211805 Test Loss: 0.9879063585069444\n",
      "Epoch: 532 Training Loss: 0.9116187676323785 Test Loss: 0.9862686631944444\n",
      "Epoch: 533 Training Loss: 0.9097662828233507 Test Loss: 0.9846737196180556\n",
      "Epoch: 534 Training Loss: 0.907915045844184 Test Loss: 0.9830763888888889\n",
      "Epoch: 535 Training Loss: 0.9060713840060763 Test Loss: 0.9814494357638889\n",
      "Epoch: 536 Training Loss: 0.9042310248480903 Test Loss: 0.9798770616319444\n",
      "Epoch: 537 Training Loss: 0.9023931070963541 Test Loss: 0.9782351345486111\n",
      "Epoch: 538 Training Loss: 0.9005631713867187 Test Loss: 0.9766489800347222\n",
      "Epoch: 539 Training Loss: 0.8987382948133681 Test Loss: 0.9750485026041666\n",
      "Epoch: 540 Training Loss: 0.896914048936632 Test Loss: 0.9734607204861111\n",
      "Epoch: 541 Training Loss: 0.8951009250217014 Test Loss: 0.971875\n",
      "Epoch: 542 Training Loss: 0.893288825141059 Test Loss: 0.9703230251736111\n",
      "Epoch: 543 Training Loss: 0.8914808485243055 Test Loss: 0.9687204861111111\n",
      "Epoch: 544 Training Loss: 0.8896771714952257 Test Loss: 0.9671948784722222\n",
      "Epoch: 545 Training Loss: 0.8878802151150174 Test Loss: 0.9656216362847222\n",
      "Epoch: 546 Training Loss: 0.8860884535047743 Test Loss: 0.9640567491319444\n",
      "Epoch: 547 Training Loss: 0.8843006659613716 Test Loss: 0.9624825303819444\n",
      "Epoch: 548 Training Loss: 0.8825166829427084 Test Loss: 0.9609306640625\n",
      "Epoch: 549 Training Loss: 0.8807370130750868 Test Loss: 0.9593875868055556\n",
      "Epoch: 550 Training Loss: 0.8789617377387153 Test Loss: 0.9578736979166667\n",
      "Epoch: 551 Training Loss: 0.877192647298177 Test Loss: 0.9563187934027778\n",
      "Epoch: 552 Training Loss: 0.8754285142686632 Test Loss: 0.9547786458333334\n",
      "Epoch: 553 Training Loss: 0.8736711086697049 Test Loss: 0.9532570529513889\n",
      "Epoch: 554 Training Loss: 0.8719166598849827 Test Loss: 0.9517482638888889\n",
      "Epoch: 555 Training Loss: 0.8701687961154514 Test Loss: 0.9502411024305556\n",
      "Epoch: 556 Training Loss: 0.8684241807725694 Test Loss: 0.9487509765625\n",
      "Epoch: 557 Training Loss: 0.8666832885742187 Test Loss: 0.9472341579861111\n",
      "Epoch: 558 Training Loss: 0.8649449327256944 Test Loss: 0.9457473958333333\n",
      "Epoch: 559 Training Loss: 0.8632109714084202 Test Loss: 0.9442300347222222\n",
      "Epoch: 560 Training Loss: 0.8614827134874132 Test Loss: 0.9427284071180555\n",
      "Epoch: 561 Training Loss: 0.8597618001302083 Test Loss: 0.9412735460069445\n",
      "Epoch: 562 Training Loss: 0.8580413479275174 Test Loss: 0.9397918836805556\n",
      "Epoch: 563 Training Loss: 0.8563299560546875 Test Loss: 0.9383422309027778\n",
      "Epoch: 564 Training Loss: 0.8546216159396701 Test Loss: 0.9368751085069444\n",
      "Epoch: 565 Training Loss: 0.8529162394205729 Test Loss: 0.9354070095486111\n",
      "Epoch: 566 Training Loss: 0.8512191501193577 Test Loss: 0.9339750434027778\n",
      "Epoch: 567 Training Loss: 0.8495286865234375 Test Loss: 0.9325008680555555\n",
      "Epoch: 568 Training Loss: 0.8478429701063368 Test Loss: 0.9310626085069444\n",
      "Epoch: 569 Training Loss: 0.8461652967664931 Test Loss: 0.9296159939236112\n",
      "Epoch: 570 Training Loss: 0.8444833170572916 Test Loss: 0.9281676432291667\n",
      "Epoch: 571 Training Loss: 0.8428102688259549 Test Loss: 0.9267470703125\n",
      "Epoch: 572 Training Loss: 0.8411358981662327 Test Loss: 0.9253519965277778\n",
      "Epoch: 573 Training Loss: 0.8394768405490451 Test Loss: 0.9239471571180555\n",
      "Epoch: 574 Training Loss: 0.8378133273654514 Test Loss: 0.9225208333333333\n",
      "Epoch: 575 Training Loss: 0.8361600613064236 Test Loss: 0.9211187065972222\n",
      "Epoch: 576 Training Loss: 0.8345135091145833 Test Loss: 0.9197248263888889\n",
      "Epoch: 577 Training Loss: 0.8328675604926216 Test Loss: 0.9183346354166667\n",
      "Epoch: 578 Training Loss: 0.8312294311523437 Test Loss: 0.9169215494791667\n",
      "Epoch: 579 Training Loss: 0.829593499077691 Test Loss: 0.9155286458333334\n",
      "Epoch: 580 Training Loss: 0.8279629787868924 Test Loss: 0.9141207682291667\n",
      "Epoch: 581 Training Loss: 0.8263336724175347 Test Loss: 0.912736328125\n",
      "Epoch: 582 Training Loss: 0.8247128295898437 Test Loss: 0.9113464626736111\n",
      "Epoch: 583 Training Loss: 0.8231009589301215 Test Loss: 0.9099943033854166\n",
      "Epoch: 584 Training Loss: 0.8214954630533854 Test Loss: 0.9086297743055556\n",
      "Epoch: 585 Training Loss: 0.8198902113172744 Test Loss: 0.9072553168402778\n",
      "Epoch: 586 Training Loss: 0.8182931925455729 Test Loss: 0.9058962673611111\n",
      "Epoch: 587 Training Loss: 0.8167013142903646 Test Loss: 0.9045783420138889\n",
      "Epoch: 588 Training Loss: 0.8151153021918403 Test Loss: 0.90321240234375\n",
      "Epoch: 589 Training Loss: 0.8135316094292535 Test Loss: 0.9018806423611111\n",
      "Epoch: 590 Training Loss: 0.811953864203559 Test Loss: 0.9005404730902777\n",
      "Epoch: 591 Training Loss: 0.8103825344509549 Test Loss: 0.8992341037326389\n",
      "Epoch: 592 Training Loss: 0.8088134358723958 Test Loss: 0.8978924696180556\n",
      "Epoch: 593 Training Loss: 0.8072507052951389 Test Loss: 0.8965855034722222\n",
      "Epoch: 594 Training Loss: 0.805691887749566 Test Loss: 0.8952666015625\n",
      "Epoch: 595 Training Loss: 0.8041399400499132 Test Loss: 0.8939688585069444\n",
      "Epoch: 596 Training Loss: 0.8025877685546875 Test Loss: 0.8926504448784722\n",
      "Epoch: 597 Training Loss: 0.8010409071180555 Test Loss: 0.8913473849826389\n",
      "Epoch: 598 Training Loss: 0.7994993353949653 Test Loss: 0.8900480143229167\n",
      "Epoch: 599 Training Loss: 0.7979587673611112 Test Loss: 0.8887669270833334\n",
      "Epoch: 600 Training Loss: 0.7964315863715278 Test Loss: 0.8874653862847223\n",
      "Epoch: 601 Training Loss: 0.794907233344184 Test Loss: 0.8861878255208333\n",
      "Epoch: 602 Training Loss: 0.7933894382052952 Test Loss: 0.8849074435763888\n",
      "Epoch: 603 Training Loss: 0.7918746473524305 Test Loss: 0.8836239691840277\n",
      "Epoch: 604 Training Loss: 0.7903662516276042 Test Loss: 0.8823528645833333\n",
      "Epoch: 605 Training Loss: 0.7888601345486111 Test Loss: 0.8810660807291667\n",
      "Epoch: 606 Training Loss: 0.7873590766059028 Test Loss: 0.8798200954861111\n",
      "Epoch: 607 Training Loss: 0.7858650716145833 Test Loss: 0.8785936414930555\n",
      "Epoch: 608 Training Loss: 0.7843746541341146 Test Loss: 0.8773589409722222\n",
      "Epoch: 609 Training Loss: 0.7828927612304688 Test Loss: 0.8760762803819444\n",
      "Epoch: 610 Training Loss: 0.7814073147243924 Test Loss: 0.8748149956597222\n",
      "Epoch: 611 Training Loss: 0.7799300944010417 Test Loss: 0.8735828993055555\n",
      "Epoch: 612 Training Loss: 0.7784566243489583 Test Loss: 0.8723595920138889\n",
      "Epoch: 613 Training Loss: 0.7769861314561632 Test Loss: 0.8711434461805555\n",
      "Epoch: 614 Training Loss: 0.775522691514757 Test Loss: 0.8699070095486111\n",
      "Epoch: 615 Training Loss: 0.7740642225477431 Test Loss: 0.8686871744791667\n",
      "Epoch: 616 Training Loss: 0.7726095241970486 Test Loss: 0.8675032009548611\n",
      "Epoch: 617 Training Loss: 0.7711642116970486 Test Loss: 0.8663250868055555\n",
      "Epoch: 618 Training Loss: 0.7697239990234375 Test Loss: 0.8651069878472222\n",
      "Epoch: 619 Training Loss: 0.7682839558919271 Test Loss: 0.8639098849826389\n",
      "Epoch: 620 Training Loss: 0.7668502400716146 Test Loss: 0.8627032335069444\n",
      "Epoch: 621 Training Loss: 0.7654232584635416 Test Loss: 0.8615441623263889\n",
      "Epoch: 622 Training Loss: 0.7639978162977431 Test Loss: 0.8603874240451389\n",
      "Epoch: 623 Training Loss: 0.7625772026909722 Test Loss: 0.8592240125868056\n",
      "Epoch: 624 Training Loss: 0.7611635131835938 Test Loss: 0.8580627170138889\n",
      "Epoch: 625 Training Loss: 0.7597570326063368 Test Loss: 0.8569120008680555\n",
      "Epoch: 626 Training Loss: 0.7583567640516493 Test Loss: 0.85576171875\n",
      "Epoch: 627 Training Loss: 0.7569584621853298 Test Loss: 0.8546325412326389\n",
      "Epoch: 628 Training Loss: 0.7555676066080729 Test Loss: 0.8534774848090277\n",
      "Epoch: 629 Training Loss: 0.7541775105794271 Test Loss: 0.8523140190972223\n",
      "Epoch: 630 Training Loss: 0.75279345703125 Test Loss: 0.8511661783854166\n",
      "Epoch: 631 Training Loss: 0.7514165310329861 Test Loss: 0.8500495876736112\n",
      "Epoch: 632 Training Loss: 0.7500469495985244 Test Loss: 0.8489303385416667\n",
      "Epoch: 633 Training Loss: 0.7486778971354167 Test Loss: 0.8478056640625\n",
      "Epoch: 634 Training Loss: 0.7473138292100694 Test Loss: 0.8466655815972223\n",
      "Epoch: 635 Training Loss: 0.7459501139322917 Test Loss: 0.8455397135416667\n",
      "Epoch: 636 Training Loss: 0.7445912679036458 Test Loss: 0.8444331597222222\n",
      "Epoch: 637 Training Loss: 0.7432396918402778 Test Loss: 0.8433195529513889\n",
      "Epoch: 638 Training Loss: 0.7418966539171007 Test Loss: 0.8422429470486111\n",
      "Epoch: 639 Training Loss: 0.7405608995225694 Test Loss: 0.8411578776041667\n",
      "Epoch: 640 Training Loss: 0.7392254570855035 Test Loss: 0.8400838216145833\n",
      "Epoch: 641 Training Loss: 0.7378932223849827 Test Loss: 0.8389587673611111\n",
      "Epoch: 642 Training Loss: 0.7365624050564236 Test Loss: 0.83783935546875\n",
      "Epoch: 643 Training Loss: 0.7352416178385417 Test Loss: 0.8367779405381944\n",
      "Epoch: 644 Training Loss: 0.7339251505533854 Test Loss: 0.8357271050347223\n",
      "Epoch: 645 Training Loss: 0.7326133083767361 Test Loss: 0.8346605360243056\n",
      "Epoch: 646 Training Loss: 0.7313046603732639 Test Loss: 0.8335890842013889\n",
      "Epoch: 647 Training Loss: 0.7300028347439236 Test Loss: 0.8325294053819444\n",
      "Epoch: 648 Training Loss: 0.728704094780816 Test Loss: 0.8314799262152778\n",
      "Epoch: 649 Training Loss: 0.7274142252604167 Test Loss: 0.8304963107638889\n",
      "Epoch: 650 Training Loss: 0.7261318427191841 Test Loss: 0.8294713541666666\n",
      "Epoch: 651 Training Loss: 0.7248553059895834 Test Loss: 0.8284233940972222\n",
      "Epoch: 652 Training Loss: 0.7235765855577257 Test Loss: 0.8273379991319444\n",
      "Epoch: 653 Training Loss: 0.7223005642361111 Test Loss: 0.8262805989583333\n",
      "Epoch: 654 Training Loss: 0.7210277642144097 Test Loss: 0.8252813585069444\n",
      "Epoch: 655 Training Loss: 0.7197646891276042 Test Loss: 0.8242588433159722\n",
      "Epoch: 656 Training Loss: 0.718505628797743 Test Loss: 0.8232688259548611\n",
      "Epoch: 657 Training Loss: 0.7172558186848959 Test Loss: 0.82230322265625\n",
      "Epoch: 658 Training Loss: 0.7160064493815104 Test Loss: 0.8212859157986111\n",
      "Epoch: 659 Training Loss: 0.7147570597330729 Test Loss: 0.82026513671875\n",
      "Epoch: 660 Training Loss: 0.7135111897786458 Test Loss: 0.8192092556423611\n",
      "Epoch: 661 Training Loss: 0.712268805609809 Test Loss: 0.8181877170138889\n",
      "Epoch: 662 Training Loss: 0.7110355224609375 Test Loss: 0.8171830512152778\n",
      "Epoch: 663 Training Loss: 0.7098041585286459 Test Loss: 0.8162010633680555\n",
      "Epoch: 664 Training Loss: 0.7085812174479167 Test Loss: 0.8152124565972222\n",
      "Epoch: 665 Training Loss: 0.7073667399088541 Test Loss: 0.8143001844618055\n",
      "Epoch: 666 Training Loss: 0.7061559787326389 Test Loss: 0.8133790690104167\n",
      "Epoch: 667 Training Loss: 0.7049450954861111 Test Loss: 0.8124290364583333\n",
      "Epoch: 668 Training Loss: 0.7037382202148438 Test Loss: 0.8114463975694445\n",
      "Epoch: 669 Training Loss: 0.7025311279296875 Test Loss: 0.8104693467881945\n",
      "Epoch: 670 Training Loss: 0.7013322347005209 Test Loss: 0.8094782986111111\n",
      "Epoch: 671 Training Loss: 0.7001365085177952 Test Loss: 0.8084973958333334\n",
      "Epoch: 672 Training Loss: 0.698946553548177 Test Loss: 0.8075512152777777\n",
      "Epoch: 673 Training Loss: 0.6977627360026042 Test Loss: 0.8066131727430556\n",
      "Epoch: 674 Training Loss: 0.6965810885959202 Test Loss: 0.8056824001736111\n",
      "Epoch: 675 Training Loss: 0.6954053615993924 Test Loss: 0.80477490234375\n",
      "Epoch: 676 Training Loss: 0.6942322930230035 Test Loss: 0.8038519965277777\n",
      "Epoch: 677 Training Loss: 0.6930623779296875 Test Loss: 0.8029045138888888\n",
      "Epoch: 678 Training Loss: 0.6918970269097222 Test Loss: 0.8019429253472222\n",
      "Epoch: 679 Training Loss: 0.6907328152126736 Test Loss: 0.8009970703125\n",
      "Epoch: 680 Training Loss: 0.6895719129774306 Test Loss: 0.8000246310763889\n",
      "Epoch: 681 Training Loss: 0.6884100477430556 Test Loss: 0.7990613606770833\n",
      "Epoch: 682 Training Loss: 0.687250745985243 Test Loss: 0.7981298828125\n",
      "Epoch: 683 Training Loss: 0.6860947943793403 Test Loss: 0.7972246636284722\n",
      "Epoch: 684 Training Loss: 0.6849498494466146 Test Loss: 0.7962848849826389\n",
      "Epoch: 685 Training Loss: 0.6838069322374132 Test Loss: 0.7953657769097222\n",
      "Epoch: 686 Training Loss: 0.6826645643446181 Test Loss: 0.7943999565972222\n",
      "Epoch: 687 Training Loss: 0.6815247734917534 Test Loss: 0.7934913736979167\n",
      "Epoch: 688 Training Loss: 0.6803960639105903 Test Loss: 0.79258056640625\n",
      "Epoch: 689 Training Loss: 0.6792661743164062 Test Loss: 0.7916880425347222\n",
      "Epoch: 690 Training Loss: 0.6781349758572048 Test Loss: 0.7907757703993056\n",
      "Epoch: 691 Training Loss: 0.6770079142252604 Test Loss: 0.7898234049479167\n",
      "Epoch: 692 Training Loss: 0.6758793809678819 Test Loss: 0.7888890516493056\n",
      "Epoch: 693 Training Loss: 0.6747588161892362 Test Loss: 0.7879922960069444\n",
      "Epoch: 694 Training Loss: 0.6736481323242187 Test Loss: 0.7871142578125\n",
      "Epoch: 695 Training Loss: 0.6725450778537326 Test Loss: 0.7862611762152778\n",
      "Epoch: 696 Training Loss: 0.6714466620551215 Test Loss: 0.7854154730902778\n",
      "Epoch: 697 Training Loss: 0.6703461439344618 Test Loss: 0.7845646701388889\n",
      "Epoch: 698 Training Loss: 0.6692497829861112 Test Loss: 0.7836656358506945\n",
      "Epoch: 699 Training Loss: 0.6681566569010416 Test Loss: 0.7828051215277778\n",
      "Epoch: 700 Training Loss: 0.6670599297417534 Test Loss: 0.7818939887152778\n",
      "Epoch: 701 Training Loss: 0.6659690416124132 Test Loss: 0.7810286458333333\n",
      "Epoch: 702 Training Loss: 0.6648845147026909 Test Loss: 0.7801395399305555\n",
      "Epoch: 703 Training Loss: 0.6637959933810764 Test Loss: 0.7792738172743056\n",
      "Epoch: 704 Training Loss: 0.6627185940212673 Test Loss: 0.7784166124131945\n",
      "Epoch: 705 Training Loss: 0.6616402384440104 Test Loss: 0.7775595703125\n",
      "Epoch: 706 Training Loss: 0.6605663655598958 Test Loss: 0.7767086046006945\n",
      "Epoch: 707 Training Loss: 0.6594991251627604 Test Loss: 0.7758406032986112\n",
      "Epoch: 708 Training Loss: 0.6584281005859375 Test Loss: 0.7749883355034722\n",
      "Epoch: 709 Training Loss: 0.6573662855360243 Test Loss: 0.7741393229166667\n",
      "Epoch: 710 Training Loss: 0.6563087904188368 Test Loss: 0.7733461371527778\n",
      "Epoch: 711 Training Loss: 0.655254889594184 Test Loss: 0.7725044487847222\n",
      "Epoch: 712 Training Loss: 0.6541978691948784 Test Loss: 0.7716528862847222\n",
      "Epoch: 713 Training Loss: 0.6531412421332465 Test Loss: 0.7707964409722222\n",
      "Epoch: 714 Training Loss: 0.6520906914605035 Test Loss: 0.7699448784722223\n",
      "Epoch: 715 Training Loss: 0.651042256673177 Test Loss: 0.7691231011284723\n",
      "Epoch: 716 Training Loss: 0.6500020887586806 Test Loss: 0.7683070746527778\n",
      "Epoch: 717 Training Loss: 0.6489616834852431 Test Loss: 0.7674932183159722\n",
      "Epoch: 718 Training Loss: 0.6479285142686632 Test Loss: 0.7666822374131944\n",
      "Epoch: 719 Training Loss: 0.646895514594184 Test Loss: 0.7658837890625\n",
      "Epoch: 720 Training Loss: 0.6458643256293403 Test Loss: 0.7650801866319444\n",
      "Epoch: 721 Training Loss: 0.6448356662326389 Test Loss: 0.7642881944444444\n",
      "Epoch: 722 Training Loss: 0.6438135375976562 Test Loss: 0.7635095486111111\n",
      "Epoch: 723 Training Loss: 0.6427925211588542 Test Loss: 0.7627434895833334\n",
      "Epoch: 724 Training Loss: 0.6417803480360244 Test Loss: 0.7619921332465278\n",
      "Epoch: 725 Training Loss: 0.6407743259006077 Test Loss: 0.7611935763888888\n",
      "Epoch: 726 Training Loss: 0.6397610880533854 Test Loss: 0.7604423828125\n",
      "Epoch: 727 Training Loss: 0.6387527465820313 Test Loss: 0.7596319986979166\n",
      "Epoch: 728 Training Loss: 0.6377403293185764 Test Loss: 0.7588302408854166\n",
      "Epoch: 729 Training Loss: 0.6367331814236111 Test Loss: 0.7580118272569445\n",
      "Epoch: 730 Training Loss: 0.6357330661349826 Test Loss: 0.7572707790798611\n",
      "Epoch: 731 Training Loss: 0.634739495171441 Test Loss: 0.7564921332465278\n",
      "Epoch: 732 Training Loss: 0.6337451443142361 Test Loss: 0.75573779296875\n",
      "Epoch: 733 Training Loss: 0.6327585856119792 Test Loss: 0.7550129123263889\n",
      "Epoch: 734 Training Loss: 0.6317729017469618 Test Loss: 0.7542620985243056\n",
      "Epoch: 735 Training Loss: 0.6307846950954861 Test Loss: 0.7534739583333333\n",
      "Epoch: 736 Training Loss: 0.6298002658420139 Test Loss: 0.7527564019097223\n",
      "Epoch: 737 Training Loss: 0.628823479546441 Test Loss: 0.7520144314236111\n",
      "Epoch: 738 Training Loss: 0.6278473985460069 Test Loss: 0.7512569444444445\n",
      "Epoch: 739 Training Loss: 0.6268765936957466 Test Loss: 0.7505144856770833\n",
      "Epoch: 740 Training Loss: 0.6259132893880208 Test Loss: 0.7497790256076389\n",
      "Epoch: 741 Training Loss: 0.6249480387369791 Test Loss: 0.7490286458333333\n",
      "Epoch: 742 Training Loss: 0.6239774780273437 Test Loss: 0.7482681206597223\n",
      "Epoch: 743 Training Loss: 0.623017069498698 Test Loss: 0.7475660807291666\n",
      "Epoch: 744 Training Loss: 0.6220646497938368 Test Loss: 0.7468455403645833\n",
      "Epoch: 745 Training Loss: 0.6211130099826389 Test Loss: 0.7461256510416666\n",
      "Epoch: 746 Training Loss: 0.620161634657118 Test Loss: 0.7453625217013888\n",
      "Epoch: 747 Training Loss: 0.6192098388671875 Test Loss: 0.7446158311631944\n",
      "Epoch: 748 Training Loss: 0.618262471516927 Test Loss: 0.7438926866319444\n",
      "Epoch: 749 Training Loss: 0.617319329155816 Test Loss: 0.74316552734375\n",
      "Epoch: 750 Training Loss: 0.616380120171441 Test Loss: 0.7424479166666667\n",
      "Epoch: 751 Training Loss: 0.6154439561631945 Test Loss: 0.7417254231770833\n",
      "Epoch: 752 Training Loss: 0.6145055202907986 Test Loss: 0.7409963107638889\n",
      "Epoch: 753 Training Loss: 0.6135671861436632 Test Loss: 0.7402932942708333\n",
      "Epoch: 754 Training Loss: 0.612635762532552 Test Loss: 0.7395281032986111\n",
      "Epoch: 755 Training Loss: 0.6117088012695312 Test Loss: 0.7388419596354167\n",
      "Epoch: 756 Training Loss: 0.6107888793945313 Test Loss: 0.7381560329861111\n",
      "Epoch: 757 Training Loss: 0.6098724568684896 Test Loss: 0.7374445529513889\n",
      "Epoch: 758 Training Loss: 0.6089485609266493 Test Loss: 0.7367322048611111\n",
      "Epoch: 759 Training Loss: 0.6080258992513021 Test Loss: 0.7359998914930556\n",
      "Epoch: 760 Training Loss: 0.6071056722005208 Test Loss: 0.7353232964409723\n",
      "Epoch: 761 Training Loss: 0.606188985188802 Test Loss: 0.7345587022569444\n",
      "Epoch: 762 Training Loss: 0.6052793240017361 Test Loss: 0.7338595377604167\n",
      "Epoch: 763 Training Loss: 0.6043773328993055 Test Loss: 0.7331321614583334\n",
      "Epoch: 764 Training Loss: 0.6034673868815105 Test Loss: 0.7324241536458334\n",
      "Epoch: 765 Training Loss: 0.6025652737087673 Test Loss: 0.7317224392361111\n",
      "Epoch: 766 Training Loss: 0.6016623196072048 Test Loss: 0.7310650499131944\n",
      "Epoch: 767 Training Loss: 0.6007645263671875 Test Loss: 0.7303662651909723\n",
      "Epoch: 768 Training Loss: 0.5998644680447048 Test Loss: 0.7296402994791666\n",
      "Epoch: 769 Training Loss: 0.5989658949110243 Test Loss: 0.7289522569444444\n",
      "Epoch: 770 Training Loss: 0.5980783148871528 Test Loss: 0.7282742513020833\n",
      "Epoch: 771 Training Loss: 0.597193122016059 Test Loss: 0.727615234375\n",
      "Epoch: 772 Training Loss: 0.5963095024956597 Test Loss: 0.7269435221354167\n",
      "Epoch: 773 Training Loss: 0.5954323323567708 Test Loss: 0.726283203125\n",
      "Epoch: 774 Training Loss: 0.5945519544813368 Test Loss: 0.7255845269097222\n",
      "Epoch: 775 Training Loss: 0.5936697998046875 Test Loss: 0.7248991970486112\n",
      "Epoch: 776 Training Loss: 0.5927884657118055 Test Loss: 0.7242176649305555\n",
      "Epoch: 777 Training Loss: 0.5919140014648437 Test Loss: 0.7235329861111112\n",
      "Epoch: 778 Training Loss: 0.5910466986762153 Test Loss: 0.7228701171875\n",
      "Epoch: 779 Training Loss: 0.590175272623698 Test Loss: 0.7221624891493056\n",
      "Epoch: 780 Training Loss: 0.589302001953125 Test Loss: 0.7214621310763889\n",
      "Epoch: 781 Training Loss: 0.5884358927408854 Test Loss: 0.7207671440972222\n",
      "Epoch: 782 Training Loss: 0.5875694986979166 Test Loss: 0.7201162651909723\n",
      "Epoch: 783 Training Loss: 0.5867108018663194 Test Loss: 0.7194566514756945\n",
      "Epoch: 784 Training Loss: 0.5858589952256944 Test Loss: 0.7188069118923611\n",
      "Epoch: 785 Training Loss: 0.5850013224283854 Test Loss: 0.7180876193576389\n",
      "Epoch: 786 Training Loss: 0.5841463148328994 Test Loss: 0.7174175347222222\n",
      "Epoch: 787 Training Loss: 0.5832902221679688 Test Loss: 0.71674951171875\n",
      "Epoch: 788 Training Loss: 0.5824387749565972 Test Loss: 0.7160857204861111\n",
      "Epoch: 789 Training Loss: 0.5815909695095486 Test Loss: 0.7154267578125\n",
      "Epoch: 790 Training Loss: 0.5807449883355035 Test Loss: 0.7147667100694445\n",
      "Epoch: 791 Training Loss: 0.5799041883680556 Test Loss: 0.7141075846354167\n",
      "Epoch: 792 Training Loss: 0.5790577731662326 Test Loss: 0.7134534505208333\n",
      "Epoch: 793 Training Loss: 0.5782261216905382 Test Loss: 0.7128287760416666\n",
      "Epoch: 794 Training Loss: 0.5773890991210937 Test Loss: 0.7121453450520834\n",
      "Epoch: 795 Training Loss: 0.5765517171223958 Test Loss: 0.7115326605902778\n",
      "Epoch: 796 Training Loss: 0.5757205335828993 Test Loss: 0.7108985460069445\n",
      "Epoch: 797 Training Loss: 0.5748892279730903 Test Loss: 0.71026708984375\n",
      "Epoch: 798 Training Loss: 0.574053236219618 Test Loss: 0.7095888671875\n",
      "Epoch: 799 Training Loss: 0.5732267591688368 Test Loss: 0.70896533203125\n",
      "Epoch: 800 Training Loss: 0.5724020385742188 Test Loss: 0.7083247612847222\n",
      "Epoch: 801 Training Loss: 0.5715832248263889 Test Loss: 0.7077205403645833\n",
      "Epoch: 802 Training Loss: 0.5707616305881077 Test Loss: 0.7070450303819444\n",
      "Epoch: 803 Training Loss: 0.5699351704915364 Test Loss: 0.706421875\n",
      "Epoch: 804 Training Loss: 0.5691172180175781 Test Loss: 0.7057890625\n",
      "Epoch: 805 Training Loss: 0.5683029344346788 Test Loss: 0.7051548394097222\n",
      "Epoch: 806 Training Loss: 0.567490475124783 Test Loss: 0.7045302734375\n",
      "Epoch: 807 Training Loss: 0.5666770629882812 Test Loss: 0.7038914930555555\n",
      "Epoch: 808 Training Loss: 0.5658720397949218 Test Loss: 0.7032543402777778\n",
      "Epoch: 809 Training Loss: 0.5650639275444879 Test Loss: 0.7026615668402778\n",
      "Epoch: 810 Training Loss: 0.5642615831163195 Test Loss: 0.7020271267361111\n",
      "Epoch: 811 Training Loss: 0.5634640129937066 Test Loss: 0.7014407552083334\n",
      "Epoch: 812 Training Loss: 0.5626615600585938 Test Loss: 0.7008288302951389\n",
      "Epoch: 813 Training Loss: 0.5618652547200521 Test Loss: 0.7002302517361111\n",
      "Epoch: 814 Training Loss: 0.5610646599663629 Test Loss: 0.6996220703125\n",
      "Epoch: 815 Training Loss: 0.5602757093641493 Test Loss: 0.6990249565972222\n",
      "Epoch: 816 Training Loss: 0.5594883863661024 Test Loss: 0.6984130316840278\n",
      "Epoch: 817 Training Loss: 0.558699215359158 Test Loss: 0.6978304578993055\n",
      "Epoch: 818 Training Loss: 0.557913333468967 Test Loss: 0.6972458224826389\n",
      "Epoch: 819 Training Loss: 0.5571247829861111 Test Loss: 0.6966573350694445\n",
      "Epoch: 820 Training Loss: 0.5563441297743056 Test Loss: 0.6960562608506945\n",
      "Epoch: 821 Training Loss: 0.5555635308159722 Test Loss: 0.6954631076388889\n",
      "Epoch: 822 Training Loss: 0.5547841423882378 Test Loss: 0.69487109375\n",
      "Epoch: 823 Training Loss: 0.5540012613932291 Test Loss: 0.6942819010416666\n",
      "Epoch: 824 Training Loss: 0.5532268405490451 Test Loss: 0.6937130533854167\n",
      "Epoch: 825 Training Loss: 0.5524516906738282 Test Loss: 0.6931329210069445\n",
      "Epoch: 826 Training Loss: 0.5516866726345486 Test Loss: 0.6925459526909722\n",
      "Epoch: 827 Training Loss: 0.5509148763020834 Test Loss: 0.6920047743055555\n",
      "Epoch: 828 Training Loss: 0.55015478515625 Test Loss: 0.6914282769097222\n",
      "Epoch: 829 Training Loss: 0.5493897637261285 Test Loss: 0.6908771701388889\n",
      "Epoch: 830 Training Loss: 0.5486331346299913 Test Loss: 0.6902775065104166\n",
      "Epoch: 831 Training Loss: 0.5478715549045139 Test Loss: 0.6897433810763889\n",
      "Epoch: 832 Training Loss: 0.5471133219401042 Test Loss: 0.6891606987847222\n",
      "Epoch: 833 Training Loss: 0.546355960422092 Test Loss: 0.6885933159722222\n",
      "Epoch: 834 Training Loss: 0.545597910563151 Test Loss: 0.6880563151041666\n",
      "Epoch: 835 Training Loss: 0.5448509012858073 Test Loss: 0.6875110677083334\n",
      "Epoch: 836 Training Loss: 0.5441005418565538 Test Loss: 0.6869930555555556\n",
      "Epoch: 837 Training Loss: 0.5433520745171441 Test Loss: 0.6864292534722222\n",
      "Epoch: 838 Training Loss: 0.5426082831488716 Test Loss: 0.685890625\n",
      "Epoch: 839 Training Loss: 0.5418620096842448 Test Loss: 0.6853576388888889\n",
      "Epoch: 840 Training Loss: 0.5411192050509983 Test Loss: 0.6848002387152777\n",
      "Epoch: 841 Training Loss: 0.5403806830512152 Test Loss: 0.684275390625\n",
      "Epoch: 842 Training Loss: 0.5396437784830729 Test Loss: 0.6837635633680555\n",
      "Epoch: 843 Training Loss: 0.5389099561903212 Test Loss: 0.6832158203125\n",
      "Epoch: 844 Training Loss: 0.5381792127821181 Test Loss: 0.6827082790798611\n",
      "Epoch: 845 Training Loss: 0.5374468960232205 Test Loss: 0.6821813693576388\n",
      "Epoch: 846 Training Loss: 0.5367151048448351 Test Loss: 0.6816524522569445\n",
      "Epoch: 847 Training Loss: 0.535991943359375 Test Loss: 0.6811097005208333\n",
      "Epoch: 848 Training Loss: 0.5352625461154514 Test Loss: 0.6805879448784722\n",
      "Epoch: 849 Training Loss: 0.53453466796875 Test Loss: 0.6800868055555556\n",
      "Epoch: 850 Training Loss: 0.5338129611545139 Test Loss: 0.6795598415798612\n",
      "Epoch: 851 Training Loss: 0.533091566297743 Test Loss: 0.6790649956597222\n",
      "Epoch: 852 Training Loss: 0.5323722466362847 Test Loss: 0.6785381944444444\n",
      "Epoch: 853 Training Loss: 0.5316584777832031 Test Loss: 0.6780305447048611\n",
      "Epoch: 854 Training Loss: 0.5309421793619792 Test Loss: 0.6775611979166667\n",
      "Epoch: 855 Training Loss: 0.530223402235243 Test Loss: 0.6770662434895833\n",
      "Epoch: 856 Training Loss: 0.5295146009657118 Test Loss: 0.6765577799479167\n",
      "Epoch: 857 Training Loss: 0.5288025105794271 Test Loss: 0.6760404188368055\n",
      "Epoch: 858 Training Loss: 0.5280921664767795 Test Loss: 0.6755959201388889\n",
      "Epoch: 859 Training Loss: 0.527384256998698 Test Loss: 0.6750705295138889\n",
      "Epoch: 860 Training Loss: 0.5266779174804688 Test Loss: 0.6745782877604166\n",
      "Epoch: 861 Training Loss: 0.5259762573242187 Test Loss: 0.6740731879340278\n",
      "Epoch: 862 Training Loss: 0.5252736070421007 Test Loss: 0.6735872395833333\n",
      "Epoch: 863 Training Loss: 0.524565663655599 Test Loss: 0.67315625\n",
      "Epoch: 864 Training Loss: 0.523860361735026 Test Loss: 0.6726781141493056\n",
      "Epoch: 865 Training Loss: 0.5231641099717882 Test Loss: 0.6721553276909722\n",
      "Epoch: 866 Training Loss: 0.5224791836208768 Test Loss: 0.6716673719618056\n",
      "Epoch: 867 Training Loss: 0.5217885369194879 Test Loss: 0.6711931423611112\n",
      "Epoch: 868 Training Loss: 0.5210926886664496 Test Loss: 0.6707801649305556\n",
      "Epoch: 869 Training Loss: 0.5203927103678385 Test Loss: 0.6703522677951389\n",
      "Epoch: 870 Training Loss: 0.5196993781195747 Test Loss: 0.6698482530381944\n",
      "Epoch: 871 Training Loss: 0.5190175509982639 Test Loss: 0.6693474934895833\n",
      "Epoch: 872 Training Loss: 0.5183443467881944 Test Loss: 0.6688614366319444\n",
      "Epoch: 873 Training Loss: 0.5176746215820313 Test Loss: 0.6684072265625\n",
      "Epoch: 874 Training Loss: 0.5169864773220486 Test Loss: 0.6679918619791667\n",
      "Epoch: 875 Training Loss: 0.5162925347222223 Test Loss: 0.6675930989583333\n",
      "Epoch: 876 Training Loss: 0.5156084696451823 Test Loss: 0.6671112196180555\n",
      "Epoch: 877 Training Loss: 0.5149360826280382 Test Loss: 0.6666064453125\n",
      "Epoch: 878 Training Loss: 0.5142765265570747 Test Loss: 0.6661381293402778\n",
      "Epoch: 879 Training Loss: 0.5136149156358507 Test Loss: 0.6656786024305555\n",
      "Epoch: 880 Training Loss: 0.5129448208279079 Test Loss: 0.6652759874131945\n",
      "Epoch: 881 Training Loss: 0.5122690802680121 Test Loss: 0.6648532443576389\n",
      "Epoch: 882 Training Loss: 0.5115896979437934 Test Loss: 0.6644707573784723\n",
      "Epoch: 883 Training Loss: 0.5109163682725695 Test Loss: 0.6640037434895834\n",
      "Epoch: 884 Training Loss: 0.5102529059516059 Test Loss: 0.6635164930555556\n",
      "Epoch: 885 Training Loss: 0.5095960591634114 Test Loss: 0.6630374348958333\n",
      "Epoch: 886 Training Loss: 0.5089492797851562 Test Loss: 0.6626176215277778\n",
      "Epoch: 887 Training Loss: 0.5082953186035156 Test Loss: 0.6621878255208333\n",
      "Epoch: 888 Training Loss: 0.507630106608073 Test Loss: 0.6617560763888889\n",
      "Epoch: 889 Training Loss: 0.5069557800292969 Test Loss: 0.6613961588541667\n",
      "Epoch: 890 Training Loss: 0.5062783270941841 Test Loss: 0.6610728081597222\n",
      "Epoch: 891 Training Loss: 0.5056089138454861 Test Loss: 0.6606254340277777\n",
      "Epoch: 892 Training Loss: 0.5049495713975695 Test Loss: 0.6601325954861111\n",
      "Epoch: 893 Training Loss: 0.5043062676323785 Test Loss: 0.6596349826388889\n",
      "Epoch: 894 Training Loss: 0.5036731736924913 Test Loss: 0.6592111545138889\n",
      "Epoch: 895 Training Loss: 0.503049329969618 Test Loss: 0.6588513454861111\n",
      "Epoch: 896 Training Loss: 0.5024352925618489 Test Loss: 0.6584992404513889\n",
      "Epoch: 897 Training Loss: 0.5018226453993055 Test Loss: 0.6580642361111111\n",
      "Epoch: 898 Training Loss: 0.5011973266601563 Test Loss: 0.6575493706597222\n",
      "Epoch: 899 Training Loss: 0.5005445353190104 Test Loss: 0.6571437717013889\n",
      "Epoch: 900 Training Loss: 0.4998662618001302 Test Loss: 0.6569503580729167\n",
      "Epoch: 901 Training Loss: 0.49919373236762155 Test Loss: 0.6567160915798611\n",
      "Epoch: 902 Training Loss: 0.4985350070529514 Test Loss: 0.6563649088541667\n",
      "Epoch: 903 Training Loss: 0.4978925984700521 Test Loss: 0.6557713758680556\n",
      "Epoch: 904 Training Loss: 0.49726333957248264 Test Loss: 0.6551659071180556\n",
      "Epoch: 905 Training Loss: 0.4966447448730469 Test Loss: 0.6547021484375\n",
      "Epoch: 906 Training Loss: 0.4960446573893229 Test Loss: 0.65439306640625\n",
      "Epoch: 907 Training Loss: 0.49546367730034724 Test Loss: 0.6541303168402778\n",
      "Epoch: 908 Training Loss: 0.4948931477864583 Test Loss: 0.6538586154513889\n",
      "Epoch: 909 Training Loss: 0.49430814276801216 Test Loss: 0.6533013237847223\n",
      "Epoch: 910 Training Loss: 0.49369766235351564 Test Loss: 0.6526591796875\n",
      "Epoch: 911 Training Loss: 0.4930489603678385 Test Loss: 0.6523273654513889\n",
      "Epoch: 912 Training Loss: 0.4923846740722656 Test Loss: 0.6524002821180556\n",
      "Epoch: 913 Training Loss: 0.4917367791069878 Test Loss: 0.6523562282986111\n",
      "Epoch: 914 Training Loss: 0.4911037156846788 Test Loss: 0.6519053276909722\n",
      "Epoch: 915 Training Loss: 0.4904834696451823 Test Loss: 0.6511083984375\n",
      "Epoch: 916 Training Loss: 0.4898712565104167 Test Loss: 0.6503416883680555\n",
      "Epoch: 917 Training Loss: 0.48927095540364585 Test Loss: 0.6498419053819444\n",
      "Epoch: 918 Training Loss: 0.4886915486653646 Test Loss: 0.6495723741319445\n",
      "Epoch: 919 Training Loss: 0.48811787923177086 Test Loss: 0.6493776584201388\n",
      "Epoch: 920 Training Loss: 0.48754479302300346 Test Loss: 0.6490282118055556\n",
      "Epoch: 921 Training Loss: 0.4869714898003472 Test Loss: 0.6485\n",
      "Epoch: 922 Training Loss: 0.4863725789388021 Test Loss: 0.6479131944444444\n",
      "Epoch: 923 Training Loss: 0.4857402615017361 Test Loss: 0.6476459418402778\n",
      "Epoch: 924 Training Loss: 0.48509591674804686 Test Loss: 0.6477345377604167\n",
      "Epoch: 925 Training Loss: 0.48448399861653646 Test Loss: 0.6476427951388889\n",
      "Epoch: 926 Training Loss: 0.4838810526529948 Test Loss: 0.6471529405381945\n",
      "Epoch: 927 Training Loss: 0.483279551188151 Test Loss: 0.6463368055555555\n",
      "Epoch: 928 Training Loss: 0.4826937052408854 Test Loss: 0.6456384548611112\n",
      "Epoch: 929 Training Loss: 0.4821195271809896 Test Loss: 0.6452189670138889\n",
      "Epoch: 930 Training Loss: 0.48155753241644966 Test Loss: 0.6450262044270834\n",
      "Epoch: 931 Training Loss: 0.4810100775824653 Test Loss: 0.6448828125\n",
      "Epoch: 932 Training Loss: 0.4804692857530382 Test Loss: 0.6445598415798611\n",
      "Epoch: 933 Training Loss: 0.47990947469075523 Test Loss: 0.6439416775173611\n",
      "Epoch: 934 Training Loss: 0.4793236558702257 Test Loss: 0.6433438585069444\n",
      "Epoch: 935 Training Loss: 0.47871351453993055 Test Loss: 0.6432090928819445\n",
      "Epoch: 936 Training Loss: 0.47809412299262155 Test Loss: 0.6433674045138889\n",
      "Epoch: 937 Training Loss: 0.47750091213650175 Test Loss: 0.6432893337673611\n",
      "Epoch: 938 Training Loss: 0.4769164089626736 Test Loss: 0.6426483832465277\n",
      "Epoch: 939 Training Loss: 0.47633878919813366 Test Loss: 0.6418646918402777\n",
      "Epoch: 940 Training Loss: 0.4757654825846354 Test Loss: 0.6411898871527778\n",
      "Epoch: 941 Training Loss: 0.47520958116319445 Test Loss: 0.6407873263888889\n",
      "Epoch: 942 Training Loss: 0.4746629808213976 Test Loss: 0.6406104600694444\n",
      "Epoch: 943 Training Loss: 0.47413956705729166 Test Loss: 0.6404951171875\n",
      "Epoch: 944 Training Loss: 0.4736182454427083 Test Loss: 0.64012255859375\n",
      "Epoch: 945 Training Loss: 0.47308031548394097 Test Loss: 0.6394765082465278\n",
      "Epoch: 946 Training Loss: 0.4725060797797309 Test Loss: 0.6390172526041666\n",
      "Epoch: 947 Training Loss: 0.47192964680989585 Test Loss: 0.6389961480034723\n",
      "Epoch: 948 Training Loss: 0.47134506564670137 Test Loss: 0.6391510959201389\n",
      "Epoch: 949 Training Loss: 0.47077979532877606 Test Loss: 0.6389544270833334\n",
      "Epoch: 950 Training Loss: 0.4702241448296441 Test Loss: 0.6383077256944445\n",
      "Epoch: 951 Training Loss: 0.4696720309787326 Test Loss: 0.6375623914930556\n",
      "Epoch: 952 Training Loss: 0.4691254102918837 Test Loss: 0.6369048394097222\n",
      "Epoch: 953 Training Loss: 0.468583740234375 Test Loss: 0.6365830078125\n",
      "Epoch: 954 Training Loss: 0.4680721435546875 Test Loss: 0.636533203125\n",
      "Epoch: 955 Training Loss: 0.4675734354654948 Test Loss: 0.6362993706597222\n",
      "Epoch: 956 Training Loss: 0.46706844753689236 Test Loss: 0.6358153211805555\n",
      "Epoch: 957 Training Loss: 0.4665466105143229 Test Loss: 0.6352072482638889\n",
      "Epoch: 958 Training Loss: 0.46600135294596357 Test Loss: 0.6349010416666667\n",
      "Epoch: 959 Training Loss: 0.4654346143934462 Test Loss: 0.6349478081597222\n",
      "Epoch: 960 Training Loss: 0.46489536878797744 Test Loss: 0.6350053168402777\n",
      "Epoch: 961 Training Loss: 0.46438949584960937 Test Loss: 0.6346662326388889\n",
      "Epoch: 962 Training Loss: 0.46388385009765626 Test Loss: 0.6339371744791666\n",
      "Epoch: 963 Training Loss: 0.4633803982204861 Test Loss: 0.6332354600694444\n",
      "Epoch: 964 Training Loss: 0.46287777370876737 Test Loss: 0.6329518229166666\n",
      "Epoch: 965 Training Loss: 0.4624064670138889 Test Loss: 0.6330125868055556\n",
      "Epoch: 966 Training Loss: 0.46197066582573787 Test Loss: 0.63297900390625\n",
      "Epoch: 967 Training Loss: 0.46155013020833335 Test Loss: 0.6328438042534722\n",
      "Epoch: 968 Training Loss: 0.46114528062608506 Test Loss: 0.6329915364583333\n",
      "Epoch: 969 Training Loss: 0.46075309922960067 Test Loss: 0.6334037543402777\n",
      "Epoch: 970 Training Loss: 0.4604103698730469 Test Loss: 0.6330188259548611\n",
      "Epoch: 971 Training Loss: 0.46009156629774306 Test Loss: 0.631466796875\n",
      "Epoch: 972 Training Loss: 0.45972114732530384 Test Loss: 0.6298837890625\n",
      "Epoch: 973 Training Loss: 0.4592028266059028 Test Loss: 0.6294324001736111\n",
      "Epoch: 974 Training Loss: 0.45855963134765626 Test Loss: 0.6297961154513889\n",
      "Epoch: 975 Training Loss: 0.45791285196940107 Test Loss: 0.6295268012152778\n",
      "Epoch: 976 Training Loss: 0.4572054273817274 Test Loss: 0.6287148980034722\n",
      "Epoch: 977 Training Loss: 0.45645513237847224 Test Loss: 0.6283786892361111\n",
      "Epoch: 978 Training Loss: 0.4558043484157986 Test Loss: 0.6283714192708333\n",
      "Epoch: 979 Training Loss: 0.45526463148328994 Test Loss: 0.6281239149305555\n",
      "Epoch: 980 Training Loss: 0.4547850036621094 Test Loss: 0.62768896484375\n",
      "Epoch: 981 Training Loss: 0.4543253377278646 Test Loss: 0.6272747938368055\n",
      "Epoch: 982 Training Loss: 0.45385479736328127 Test Loss: 0.6269243706597222\n",
      "Epoch: 983 Training Loss: 0.4533811374240451 Test Loss: 0.6265927191840278\n",
      "Epoch: 984 Training Loss: 0.45290403917100697 Test Loss: 0.6262881401909722\n",
      "Epoch: 985 Training Loss: 0.4524314439561632 Test Loss: 0.6260231119791667\n",
      "Epoch: 986 Training Loss: 0.4519573940700955 Test Loss: 0.6257965494791666\n",
      "Epoch: 987 Training Loss: 0.45148248291015625 Test Loss: 0.6255207790798611\n",
      "Epoch: 988 Training Loss: 0.45101475355360243 Test Loss: 0.6252599826388889\n",
      "Epoch: 989 Training Loss: 0.4505553724500868 Test Loss: 0.6249512261284722\n",
      "Epoch: 990 Training Loss: 0.4501065741644965 Test Loss: 0.6245573459201389\n",
      "Epoch: 991 Training Loss: 0.4496671854654948 Test Loss: 0.6241319986979167\n",
      "Epoch: 992 Training Loss: 0.449234859890408 Test Loss: 0.6237331814236111\n",
      "Epoch: 993 Training Loss: 0.4488040228949653 Test Loss: 0.6234429253472222\n",
      "Epoch: 994 Training Loss: 0.4483882548014323 Test Loss: 0.6231720920138889\n",
      "Epoch: 995 Training Loss: 0.44798318481445315 Test Loss: 0.6229468858506945\n",
      "Epoch: 996 Training Loss: 0.44758731418185765 Test Loss: 0.6226688368055555\n",
      "Epoch: 997 Training Loss: 0.4471915995279948 Test Loss: 0.6223251953125\n",
      "Epoch: 998 Training Loss: 0.44680118476019964 Test Loss: 0.6218648003472222\n",
      "Epoch: 999 Training Loss: 0.44640758599175345 Test Loss: 0.6213190104166667\n",
      "Epoch: 1000 Training Loss: 0.4459897901746962 Test Loss: 0.6209946831597222\n",
      "Epoch: 1001 Training Loss: 0.4455696478949653 Test Loss: 0.6212115885416667\n",
      "Epoch: 1002 Training Loss: 0.4451745130750868 Test Loss: 0.6216073676215278\n",
      "Epoch: 1003 Training Loss: 0.4448003200954861 Test Loss: 0.6213480902777778\n",
      "Epoch: 1004 Training Loss: 0.44442118326822916 Test Loss: 0.6202570529513889\n",
      "Epoch: 1005 Training Loss: 0.4440174221462674 Test Loss: 0.6192868923611111\n",
      "Epoch: 1006 Training Loss: 0.4436377665201823 Test Loss: 0.6189497612847222\n",
      "Epoch: 1007 Training Loss: 0.44327858140733506 Test Loss: 0.6189106987847223\n",
      "Epoch: 1008 Training Loss: 0.4429256591796875 Test Loss: 0.6186235894097222\n",
      "Epoch: 1009 Training Loss: 0.44257001071506075 Test Loss: 0.6182010633680556\n",
      "Epoch: 1010 Training Loss: 0.4422089335123698 Test Loss: 0.6180473090277778\n",
      "Epoch: 1011 Training Loss: 0.4418475307888455 Test Loss: 0.6182179904513889\n",
      "Epoch: 1012 Training Loss: 0.4415126681857639 Test Loss: 0.6183235677083333\n",
      "Epoch: 1013 Training Loss: 0.4411881103515625 Test Loss: 0.6181248914930556\n",
      "Epoch: 1014 Training Loss: 0.44086761474609376 Test Loss: 0.61783740234375\n",
      "Epoch: 1015 Training Loss: 0.44055087280273436 Test Loss: 0.6177459852430556\n",
      "Epoch: 1016 Training Loss: 0.4402474839952257 Test Loss: 0.6177896592881944\n",
      "Epoch: 1017 Training Loss: 0.4399452243381076 Test Loss: 0.6177529296875\n",
      "Epoch: 1018 Training Loss: 0.4396295166015625 Test Loss: 0.6176158854166667\n",
      "Epoch: 1019 Training Loss: 0.4392915276421441 Test Loss: 0.6175141059027778\n",
      "Epoch: 1020 Training Loss: 0.43893433295355905 Test Loss: 0.6173952365451388\n",
      "Epoch: 1021 Training Loss: 0.4385875040690104 Test Loss: 0.6171417100694444\n",
      "Epoch: 1022 Training Loss: 0.4382286580403646 Test Loss: 0.6168299696180556\n",
      "Epoch: 1023 Training Loss: 0.43786500718858506 Test Loss: 0.6167263454861112\n",
      "Epoch: 1024 Training Loss: 0.43748173014322916 Test Loss: 0.6168404947916667\n",
      "Epoch: 1025 Training Loss: 0.4371055230034722 Test Loss: 0.6170064019097222\n",
      "Epoch: 1026 Training Loss: 0.43672756618923614 Test Loss: 0.6171908637152778\n",
      "Epoch: 1027 Training Loss: 0.43635911729600696 Test Loss: 0.6173548177083333\n",
      "Epoch: 1028 Training Loss: 0.4360019259982639 Test Loss: 0.6174436848958333\n",
      "Epoch: 1029 Training Loss: 0.43566886393229165 Test Loss: 0.61748876953125\n",
      "Epoch: 1030 Training Loss: 0.43535904947916665 Test Loss: 0.6173997395833334\n",
      "Epoch: 1031 Training Loss: 0.43505828518337675 Test Loss: 0.6170743272569444\n",
      "Epoch: 1032 Training Loss: 0.43477142673068575 Test Loss: 0.6165279947916666\n",
      "Epoch: 1033 Training Loss: 0.4344716322157118 Test Loss: 0.6157722981770833\n",
      "Epoch: 1034 Training Loss: 0.4341283976236979 Test Loss: 0.614958984375\n",
      "Epoch: 1035 Training Loss: 0.4337272440592448 Test Loss: 0.6140926649305556\n",
      "Epoch: 1036 Training Loss: 0.43327860175238714 Test Loss: 0.6132081163194445\n",
      "Epoch: 1037 Training Loss: 0.4327903069390191 Test Loss: 0.6124962022569445\n",
      "Epoch: 1038 Training Loss: 0.43228623792860243 Test Loss: 0.6119292534722223\n",
      "Epoch: 1039 Training Loss: 0.4317847188313802 Test Loss: 0.6114546983506944\n",
      "Epoch: 1040 Training Loss: 0.4312725118001302 Test Loss: 0.6111233181423611\n",
      "Epoch: 1041 Training Loss: 0.43078368462456595 Test Loss: 0.6107870008680556\n",
      "Epoch: 1042 Training Loss: 0.43031404961480035 Test Loss: 0.6105115017361111\n",
      "Epoch: 1043 Training Loss: 0.429861575656467 Test Loss: 0.6102594943576389\n",
      "Epoch: 1044 Training Loss: 0.42943545193142363 Test Loss: 0.6100080295138889\n",
      "Epoch: 1045 Training Loss: 0.42903472900390627 Test Loss: 0.6097988823784722\n",
      "Epoch: 1046 Training Loss: 0.42867645602756077 Test Loss: 0.6096151258680556\n",
      "Epoch: 1047 Training Loss: 0.4283498094346788 Test Loss: 0.6094390190972222\n",
      "Epoch: 1048 Training Loss: 0.4280487196180556 Test Loss: 0.6090422634548611\n",
      "Epoch: 1049 Training Loss: 0.4277696533203125 Test Loss: 0.6086070421006945\n",
      "Epoch: 1050 Training Loss: 0.42751643880208334 Test Loss: 0.6081316189236111\n",
      "Epoch: 1051 Training Loss: 0.4272747497558594 Test Loss: 0.6075516493055556\n",
      "Epoch: 1052 Training Loss: 0.4270394558376736 Test Loss: 0.6068409288194444\n",
      "Epoch: 1053 Training Loss: 0.42680831231011285 Test Loss: 0.60598193359375\n",
      "Epoch: 1054 Training Loss: 0.4265733337402344 Test Loss: 0.6051589626736111\n",
      "Epoch: 1055 Training Loss: 0.42633286878797744 Test Loss: 0.6044850260416667\n",
      "Epoch: 1056 Training Loss: 0.42607059393988717 Test Loss: 0.6039273003472222\n",
      "Epoch: 1057 Training Loss: 0.42576871744791667 Test Loss: 0.6035281032986111\n",
      "Epoch: 1058 Training Loss: 0.4254231601291233 Test Loss: 0.6031956380208333\n",
      "Epoch: 1059 Training Loss: 0.42503255208333335 Test Loss: 0.6031093207465278\n",
      "Epoch: 1060 Training Loss: 0.42460671997070315 Test Loss: 0.6032550998263889\n",
      "Epoch: 1061 Training Loss: 0.4241397942437066 Test Loss: 0.6035039605034722\n",
      "Epoch: 1062 Training Loss: 0.4236409166124132 Test Loss: 0.6037431640625\n",
      "Epoch: 1063 Training Loss: 0.4231389906141493 Test Loss: 0.6038495008680556\n",
      "Epoch: 1064 Training Loss: 0.42262809244791666 Test Loss: 0.6039817708333334\n",
      "Epoch: 1065 Training Loss: 0.42211607191297745 Test Loss: 0.6039343532986111\n",
      "Epoch: 1066 Training Loss: 0.4216077880859375 Test Loss: 0.6038059353298612\n",
      "Epoch: 1067 Training Loss: 0.4211181369357639 Test Loss: 0.6036503363715278\n",
      "Epoch: 1068 Training Loss: 0.4206410488552517 Test Loss: 0.6034290364583333\n",
      "Epoch: 1069 Training Loss: 0.4201724277072483 Test Loss: 0.6030837673611111\n",
      "Epoch: 1070 Training Loss: 0.41971890258789063 Test Loss: 0.6027277018229167\n",
      "Epoch: 1071 Training Loss: 0.41929698011610245 Test Loss: 0.6023484157986111\n",
      "Epoch: 1072 Training Loss: 0.41889913601345485 Test Loss: 0.6020922309027777\n",
      "Epoch: 1073 Training Loss: 0.4185105692545573 Test Loss: 0.6018452690972222\n",
      "Epoch: 1074 Training Loss: 0.4181387939453125 Test Loss: 0.6015612521701389\n",
      "Epoch: 1075 Training Loss: 0.41778373209635417 Test Loss: 0.6012754448784722\n",
      "Epoch: 1076 Training Loss: 0.4174389139811198 Test Loss: 0.6009940321180556\n",
      "Epoch: 1077 Training Loss: 0.41710346137152776 Test Loss: 0.6007230360243055\n",
      "Epoch: 1078 Training Loss: 0.4167705824110243 Test Loss: 0.6003844401041667\n",
      "Epoch: 1079 Training Loss: 0.4164555901421441 Test Loss: 0.6001242404513889\n",
      "Epoch: 1080 Training Loss: 0.41615044487847225 Test Loss: 0.5998499348958334\n",
      "Epoch: 1081 Training Loss: 0.41585567220052083 Test Loss: 0.5996046006944444\n",
      "Epoch: 1082 Training Loss: 0.415568593343099 Test Loss: 0.5993619791666667\n",
      "Epoch: 1083 Training Loss: 0.4152867397732205 Test Loss: 0.5991419270833334\n",
      "Epoch: 1084 Training Loss: 0.41501095920138886 Test Loss: 0.5989034288194445\n",
      "Epoch: 1085 Training Loss: 0.41474380493164065 Test Loss: 0.5987204861111111\n",
      "Epoch: 1086 Training Loss: 0.41447789171006943 Test Loss: 0.5985754123263889\n",
      "Epoch: 1087 Training Loss: 0.41422199164496526 Test Loss: 0.5984332682291666\n",
      "Epoch: 1088 Training Loss: 0.41396914672851565 Test Loss: 0.5983360460069445\n",
      "Epoch: 1089 Training Loss: 0.4137189059787326 Test Loss: 0.5982357313368055\n",
      "Epoch: 1090 Training Loss: 0.4134669630262587 Test Loss: 0.5981663411458333\n",
      "Epoch: 1091 Training Loss: 0.4132201571994358 Test Loss: 0.59810986328125\n",
      "Epoch: 1092 Training Loss: 0.4129785427517361 Test Loss: 0.5980677083333333\n",
      "Epoch: 1093 Training Loss: 0.41273881022135417 Test Loss: 0.5980256618923611\n",
      "Epoch: 1094 Training Loss: 0.41250791761610245 Test Loss: 0.5979708116319444\n",
      "Epoch: 1095 Training Loss: 0.4122797580295139 Test Loss: 0.5979808485243056\n",
      "Epoch: 1096 Training Loss: 0.41205371432834204 Test Loss: 0.5980088975694444\n",
      "Epoch: 1097 Training Loss: 0.41181840345594617 Test Loss: 0.5980010850694445\n",
      "Epoch: 1098 Training Loss: 0.41158831787109373 Test Loss: 0.59789453125\n",
      "Epoch: 1099 Training Loss: 0.4113633321126302 Test Loss: 0.59778662109375\n",
      "Epoch: 1100 Training Loss: 0.41113382975260415 Test Loss: 0.5976663411458333\n",
      "Epoch: 1101 Training Loss: 0.41089864773220486 Test Loss: 0.5976083984375\n",
      "Epoch: 1102 Training Loss: 0.41065512084960937 Test Loss: 0.597498046875\n",
      "Epoch: 1103 Training Loss: 0.41041975911458334 Test Loss: 0.5974245334201389\n",
      "Epoch: 1104 Training Loss: 0.4101781277126736 Test Loss: 0.5974047309027778\n",
      "Epoch: 1105 Training Loss: 0.40992205132378473 Test Loss: 0.5973615993923611\n",
      "Epoch: 1106 Training Loss: 0.40965152316623266 Test Loss: 0.5973039279513889\n",
      "Epoch: 1107 Training Loss: 0.4093803066677517 Test Loss: 0.5972941080729167\n",
      "Epoch: 1108 Training Loss: 0.40910595364040797 Test Loss: 0.5973253580729166\n",
      "Epoch: 1109 Training Loss: 0.4088287150065104 Test Loss: 0.5972532009548611\n",
      "Epoch: 1110 Training Loss: 0.4085547688802083 Test Loss: 0.5972193467881944\n",
      "Epoch: 1111 Training Loss: 0.40827997165256075 Test Loss: 0.5971049262152778\n",
      "Epoch: 1112 Training Loss: 0.40800885348849825 Test Loss: 0.5968825412326388\n",
      "Epoch: 1113 Training Loss: 0.407755852593316 Test Loss: 0.5966777886284722\n",
      "Epoch: 1114 Training Loss: 0.4075014173719618 Test Loss: 0.5963587782118056\n",
      "Epoch: 1115 Training Loss: 0.40725731743706595 Test Loss: 0.5959585503472222\n",
      "Epoch: 1116 Training Loss: 0.4070216573079427 Test Loss: 0.5955518663194445\n",
      "Epoch: 1117 Training Loss: 0.4068038601345486 Test Loss: 0.5951507161458334\n",
      "Epoch: 1118 Training Loss: 0.4065860460069444 Test Loss: 0.5946771918402778\n",
      "Epoch: 1119 Training Loss: 0.4063867390950521 Test Loss: 0.5941608615451389\n",
      "Epoch: 1120 Training Loss: 0.40618761528862846 Test Loss: 0.5935912543402778\n",
      "Epoch: 1121 Training Loss: 0.40597078789605034 Test Loss: 0.5929930555555556\n",
      "Epoch: 1122 Training Loss: 0.4057764146592882 Test Loss: 0.59245751953125\n",
      "Epoch: 1123 Training Loss: 0.40557308281792537 Test Loss: 0.5920086263020833\n",
      "Epoch: 1124 Training Loss: 0.4053514268663194 Test Loss: 0.5914976128472222\n",
      "Epoch: 1125 Training Loss: 0.4051518859863281 Test Loss: 0.5910686848958333\n",
      "Epoch: 1126 Training Loss: 0.40492005072699655 Test Loss: 0.5906997612847222\n",
      "Epoch: 1127 Training Loss: 0.404693362765842 Test Loss: 0.59041796875\n",
      "Epoch: 1128 Training Loss: 0.4044664950900608 Test Loss: 0.5901907552083333\n",
      "Epoch: 1129 Training Loss: 0.4042106153700087 Test Loss: 0.5900286458333334\n",
      "Epoch: 1130 Training Loss: 0.4039676310221354 Test Loss: 0.5898287760416666\n",
      "Epoch: 1131 Training Loss: 0.403717288547092 Test Loss: 0.58960986328125\n",
      "Epoch: 1132 Training Loss: 0.4034830084906684 Test Loss: 0.5893921983506945\n",
      "Epoch: 1133 Training Loss: 0.4032498508029514 Test Loss: 0.5891083984375\n",
      "Epoch: 1134 Training Loss: 0.40304240247938367 Test Loss: 0.58876708984375\n",
      "Epoch: 1135 Training Loss: 0.40285359361436635 Test Loss: 0.5885172526041667\n",
      "Epoch: 1136 Training Loss: 0.4027646213107639 Test Loss: 0.5884223090277778\n",
      "Epoch: 1137 Training Loss: 0.40295528835720484 Test Loss: 0.5893662109375\n",
      "Epoch: 1138 Training Loss: 0.40357444932725695 Test Loss: 0.5945157877604167\n",
      "Epoch: 1139 Training Loss: 0.40490760294596356 Test Loss: 0.6164063585069445\n",
      "Epoch: 1140 Training Loss: 0.40806681315104165 Test Loss: 0.6264517144097222\n",
      "Epoch: 1141 Training Loss: 0.4092614271375868 Test Loss: 0.5863933376736111\n",
      "Epoch: 1142 Training Loss: 0.4035384996202257 Test Loss: 0.5886706814236111\n",
      "Epoch: 1143 Training Loss: 0.4003969930013021 Test Loss: 0.5856936848958333\n",
      "Epoch: 1144 Training Loss: 0.40094473944769965 Test Loss: 0.5899445529513889\n",
      "Epoch: 1145 Training Loss: 0.4019355943467882 Test Loss: 0.5845066189236111\n",
      "Epoch: 1146 Training Loss: 0.4009312235514323 Test Loss: 0.5835220811631945\n",
      "Epoch: 1147 Training Loss: 0.39945569186740454 Test Loss: 0.5834606119791667\n",
      "Epoch: 1148 Training Loss: 0.39886212158203127 Test Loss: 0.5843161892361111\n",
      "Epoch: 1149 Training Loss: 0.3988542039659288 Test Loss: 0.5846734483506945\n",
      "Epoch: 1150 Training Loss: 0.39875830078125 Test Loss: 0.5840858832465278\n",
      "Epoch: 1151 Training Loss: 0.3983072747124566 Test Loss: 0.5838648003472222\n",
      "Epoch: 1152 Training Loss: 0.3977171664767795 Test Loss: 0.5840276692708334\n",
      "Epoch: 1153 Training Loss: 0.3972100321451823 Test Loss: 0.5843695746527777\n",
      "Epoch: 1154 Training Loss: 0.39688357204861113 Test Loss: 0.5848312717013889\n",
      "Epoch: 1155 Training Loss: 0.3966161126030816 Test Loss: 0.5849518229166667\n",
      "Epoch: 1156 Training Loss: 0.3963147447374132 Test Loss: 0.5848629557291667\n",
      "Epoch: 1157 Training Loss: 0.3959461263020833 Test Loss: 0.5847298719618056\n",
      "Epoch: 1158 Training Loss: 0.39554074435763886 Test Loss: 0.5846172960069445\n",
      "Epoch: 1159 Training Loss: 0.3951307949490017 Test Loss: 0.5846051432291667\n",
      "Epoch: 1160 Training Loss: 0.394743401421441 Test Loss: 0.5845912000868055\n",
      "Epoch: 1161 Training Loss: 0.39437147352430557 Test Loss: 0.58450634765625\n",
      "Epoch: 1162 Training Loss: 0.39402026706271703 Test Loss: 0.5843116319444445\n",
      "Epoch: 1163 Training Loss: 0.3936659376356337 Test Loss: 0.5840461154513888\n",
      "Epoch: 1164 Training Loss: 0.39330015733506946 Test Loss: 0.5837777777777777\n",
      "Epoch: 1165 Training Loss: 0.39292462158203123 Test Loss: 0.5835151909722223\n",
      "Epoch: 1166 Training Loss: 0.39254908921983506 Test Loss: 0.5833004557291667\n",
      "Epoch: 1167 Training Loss: 0.39218162706163195 Test Loss: 0.5830602213541667\n",
      "Epoch: 1168 Training Loss: 0.39183084445529515 Test Loss: 0.5828220486111111\n",
      "Epoch: 1169 Training Loss: 0.3914973449707031 Test Loss: 0.5825650499131945\n",
      "Epoch: 1170 Training Loss: 0.39114302571614584 Test Loss: 0.5823412000868056\n",
      "Epoch: 1171 Training Loss: 0.39080676947699655 Test Loss: 0.5821228841145833\n",
      "Epoch: 1172 Training Loss: 0.3904761216905382 Test Loss: 0.5818948025173611\n",
      "Epoch: 1173 Training Loss: 0.39014796617296005 Test Loss: 0.5816453993055556\n",
      "Epoch: 1174 Training Loss: 0.38981607394748263 Test Loss: 0.5813737521701389\n",
      "Epoch: 1175 Training Loss: 0.38949023098415797 Test Loss: 0.5811241319444445\n",
      "Epoch: 1176 Training Loss: 0.389168216281467 Test Loss: 0.58092578125\n",
      "Epoch: 1177 Training Loss: 0.3888516337076823 Test Loss: 0.5807684461805556\n",
      "Epoch: 1178 Training Loss: 0.3885385267469618 Test Loss: 0.5806373155381944\n",
      "Epoch: 1179 Training Loss: 0.38822409057617185 Test Loss: 0.5805076497395834\n",
      "Epoch: 1180 Training Loss: 0.3879211629231771 Test Loss: 0.5803900282118055\n",
      "Epoch: 1181 Training Loss: 0.38763177151150174 Test Loss: 0.5802865668402778\n",
      "Epoch: 1182 Training Loss: 0.38734693400065107 Test Loss: 0.5801740993923611\n",
      "Epoch: 1183 Training Loss: 0.3870643378363715 Test Loss: 0.5800239800347222\n",
      "Epoch: 1184 Training Loss: 0.38678846571180553 Test Loss: 0.5798785264756945\n",
      "Epoch: 1185 Training Loss: 0.3865169372558594 Test Loss: 0.5797188585069445\n",
      "Epoch: 1186 Training Loss: 0.38624132622612845 Test Loss: 0.5795772026909722\n",
      "Epoch: 1187 Training Loss: 0.38597270033094616 Test Loss: 0.5794157986111111\n",
      "Epoch: 1188 Training Loss: 0.38569329155815973 Test Loss: 0.5792746310763889\n",
      "Epoch: 1189 Training Loss: 0.38541714138454863 Test Loss: 0.5791620551215277\n",
      "Epoch: 1190 Training Loss: 0.3851410759819878 Test Loss: 0.5789883897569444\n",
      "Epoch: 1191 Training Loss: 0.38487485080295136 Test Loss: 0.5788741319444445\n",
      "Epoch: 1192 Training Loss: 0.38460149468315974 Test Loss: 0.5787228732638889\n",
      "Epoch: 1193 Training Loss: 0.3843352864583333 Test Loss: 0.5786068793402778\n",
      "Epoch: 1194 Training Loss: 0.3840752902560764 Test Loss: 0.5784710828993056\n",
      "Epoch: 1195 Training Loss: 0.3838154093424479 Test Loss: 0.5783529188368055\n",
      "Epoch: 1196 Training Loss: 0.38354914008246527 Test Loss: 0.5782086588541666\n",
      "Epoch: 1197 Training Loss: 0.3832875332302517 Test Loss: 0.5781212565104167\n",
      "Epoch: 1198 Training Loss: 0.3830263841417101 Test Loss: 0.5780001085069445\n",
      "Epoch: 1199 Training Loss: 0.38275754801432293 Test Loss: 0.5778739149305555\n",
      "Epoch: 1200 Training Loss: 0.3824978739420573 Test Loss: 0.5778206380208334\n",
      "Epoch: 1201 Training Loss: 0.38223310343424477 Test Loss: 0.57770703125\n",
      "Epoch: 1202 Training Loss: 0.38197135755750866 Test Loss: 0.5776807725694444\n",
      "Epoch: 1203 Training Loss: 0.38171087985568575 Test Loss: 0.5776292860243055\n",
      "Epoch: 1204 Training Loss: 0.38145062594943574 Test Loss: 0.5775619574652778\n",
      "Epoch: 1205 Training Loss: 0.3811963161892361 Test Loss: 0.5774990776909722\n",
      "Epoch: 1206 Training Loss: 0.38094993760850693 Test Loss: 0.5774583333333333\n",
      "Epoch: 1207 Training Loss: 0.38070877753363713 Test Loss: 0.5774214409722223\n",
      "Epoch: 1208 Training Loss: 0.38047137112087676 Test Loss: 0.5773844401041667\n",
      "Epoch: 1209 Training Loss: 0.3802311740451389 Test Loss: 0.5773092447916667\n",
      "Epoch: 1210 Training Loss: 0.37999699232313366 Test Loss: 0.5771768663194444\n",
      "Epoch: 1211 Training Loss: 0.37976917182074654 Test Loss: 0.5771337348090277\n",
      "Epoch: 1212 Training Loss: 0.3795329996744792 Test Loss: 0.5770018446180556\n",
      "Epoch: 1213 Training Loss: 0.3792989569769965 Test Loss: 0.5769292534722222\n",
      "Epoch: 1214 Training Loss: 0.37906734551323784 Test Loss: 0.5768413628472222\n",
      "Epoch: 1215 Training Loss: 0.3788380635579427 Test Loss: 0.57665087890625\n",
      "Epoch: 1216 Training Loss: 0.37861202324761284 Test Loss: 0.5765399305555555\n",
      "Epoch: 1217 Training Loss: 0.37838267347547744 Test Loss: 0.5763578559027778\n",
      "Epoch: 1218 Training Loss: 0.37815614149305554 Test Loss: 0.5761652560763889\n",
      "Epoch: 1219 Training Loss: 0.37792416381835936 Test Loss: 0.5759378797743056\n",
      "Epoch: 1220 Training Loss: 0.3776999003092448 Test Loss: 0.5756990017361111\n",
      "Epoch: 1221 Training Loss: 0.3774732428656684 Test Loss: 0.57545263671875\n",
      "Epoch: 1222 Training Loss: 0.37724202473958335 Test Loss: 0.5752218967013889\n",
      "Epoch: 1223 Training Loss: 0.37701038614908855 Test Loss: 0.5749404296875\n",
      "Epoch: 1224 Training Loss: 0.3767719217936198 Test Loss: 0.57462744140625\n",
      "Epoch: 1225 Training Loss: 0.3765289001464844 Test Loss: 0.5743657769097222\n",
      "Epoch: 1226 Training Loss: 0.3762888658311632 Test Loss: 0.5741460503472222\n",
      "Epoch: 1227 Training Loss: 0.3760442097981771 Test Loss: 0.5738905164930556\n",
      "Epoch: 1228 Training Loss: 0.3757933044433594 Test Loss: 0.5737054578993056\n",
      "Epoch: 1229 Training Loss: 0.3755308091905382 Test Loss: 0.5735164388020834\n",
      "Epoch: 1230 Training Loss: 0.3752516513400608 Test Loss: 0.5733651801215278\n",
      "Epoch: 1231 Training Loss: 0.3749689636230469 Test Loss: 0.5731962890625\n",
      "Epoch: 1232 Training Loss: 0.37468175591362846 Test Loss: 0.5730575086805556\n",
      "Epoch: 1233 Training Loss: 0.3744071282280816 Test Loss: 0.5729277886284723\n",
      "Epoch: 1234 Training Loss: 0.3741351657443576 Test Loss: 0.57280419921875\n",
      "Epoch: 1235 Training Loss: 0.3738677741156684 Test Loss: 0.5726794162326388\n",
      "Epoch: 1236 Training Loss: 0.3736004367404514 Test Loss: 0.5725613064236111\n",
      "Epoch: 1237 Training Loss: 0.3733310309516059 Test Loss: 0.5724641927083334\n",
      "Epoch: 1238 Training Loss: 0.3730680575900608 Test Loss: 0.5723438585069445\n",
      "Epoch: 1239 Training Loss: 0.37280438232421875 Test Loss: 0.5722467447916667\n",
      "Epoch: 1240 Training Loss: 0.37253521728515626 Test Loss: 0.57218017578125\n",
      "Epoch: 1241 Training Loss: 0.372271487765842 Test Loss: 0.5720759006076389\n",
      "Epoch: 1242 Training Loss: 0.37200940958658857 Test Loss: 0.5719793294270833\n",
      "Epoch: 1243 Training Loss: 0.37174828084309897 Test Loss: 0.5719152560763889\n",
      "Epoch: 1244 Training Loss: 0.3714929402669271 Test Loss: 0.5718395182291667\n",
      "Epoch: 1245 Training Loss: 0.3712400817871094 Test Loss: 0.5716998697916666\n",
      "Epoch: 1246 Training Loss: 0.37098665025499133 Test Loss: 0.5716413845486111\n",
      "Epoch: 1247 Training Loss: 0.3707403394911024 Test Loss: 0.5715884874131945\n",
      "Epoch: 1248 Training Loss: 0.37048906792534725 Test Loss: 0.5714898003472222\n",
      "Epoch: 1249 Training Loss: 0.3702437845865885 Test Loss: 0.5714114583333333\n",
      "Epoch: 1250 Training Loss: 0.3699994642469618 Test Loss: 0.5713130425347223\n",
      "Epoch: 1251 Training Loss: 0.3697587212456597 Test Loss: 0.5712356228298611\n",
      "Epoch: 1252 Training Loss: 0.3695178426106771 Test Loss: 0.5711019965277778\n",
      "Epoch: 1253 Training Loss: 0.3692793511284722 Test Loss: 0.5710608181423611\n",
      "Epoch: 1254 Training Loss: 0.3690391404893663 Test Loss: 0.5709542100694445\n",
      "Epoch: 1255 Training Loss: 0.3688031785753038 Test Loss: 0.5709408094618056\n",
      "Epoch: 1256 Training Loss: 0.3685645955403646 Test Loss: 0.5708514539930556\n",
      "Epoch: 1257 Training Loss: 0.3683254631890191 Test Loss: 0.5706963433159722\n",
      "Epoch: 1258 Training Loss: 0.36808643934461804 Test Loss: 0.5706967230902777\n",
      "Epoch: 1259 Training Loss: 0.3678508809407552 Test Loss: 0.5706198459201389\n",
      "Epoch: 1260 Training Loss: 0.3676111687554253 Test Loss: 0.5705168728298611\n",
      "Epoch: 1261 Training Loss: 0.3673742947048611 Test Loss: 0.5704649522569445\n",
      "Epoch: 1262 Training Loss: 0.3671409776475694 Test Loss: 0.5703278537326388\n",
      "Epoch: 1263 Training Loss: 0.3669032253689236 Test Loss: 0.5702250434027778\n",
      "Epoch: 1264 Training Loss: 0.3666657375759549 Test Loss: 0.5700305447048611\n",
      "Epoch: 1265 Training Loss: 0.3664308403862847 Test Loss: 0.5699319118923611\n",
      "Epoch: 1266 Training Loss: 0.3661998494466146 Test Loss: 0.5697804904513889\n",
      "Epoch: 1267 Training Loss: 0.36596537272135415 Test Loss: 0.5696860894097222\n",
      "Epoch: 1268 Training Loss: 0.36572950236002605 Test Loss: 0.5695396050347222\n",
      "Epoch: 1269 Training Loss: 0.3654961920844184 Test Loss: 0.56954443359375\n",
      "Epoch: 1270 Training Loss: 0.3652621358235677 Test Loss: 0.5694520941840278\n",
      "Epoch: 1271 Training Loss: 0.36502657063802085 Test Loss: 0.5694166124131944\n",
      "Epoch: 1272 Training Loss: 0.36478960842556424 Test Loss: 0.5693678385416666\n",
      "Epoch: 1273 Training Loss: 0.3645575866699219 Test Loss: 0.5692976345486112\n",
      "Epoch: 1274 Training Loss: 0.3643198988172743 Test Loss: 0.5692818467881945\n",
      "Epoch: 1275 Training Loss: 0.3640888129340278 Test Loss: 0.5692608506944444\n",
      "Epoch: 1276 Training Loss: 0.3638552483452691 Test Loss: 0.5691819661458334\n",
      "Epoch: 1277 Training Loss: 0.36362019517686633 Test Loss: 0.5690730251736111\n",
      "Epoch: 1278 Training Loss: 0.36339124213324653 Test Loss: 0.5689643012152777\n",
      "Epoch: 1279 Training Loss: 0.3631570197211372 Test Loss: 0.5687473958333333\n",
      "Epoch: 1280 Training Loss: 0.3629228244357639 Test Loss: 0.56856982421875\n",
      "Epoch: 1281 Training Loss: 0.3626917487250434 Test Loss: 0.5684246961805556\n",
      "Epoch: 1282 Training Loss: 0.36246444023980035 Test Loss: 0.5681306966145834\n",
      "Epoch: 1283 Training Loss: 0.3622354498969184 Test Loss: 0.5679165581597222\n",
      "Epoch: 1284 Training Loss: 0.36200298733181424 Test Loss: 0.567802734375\n",
      "Epoch: 1285 Training Loss: 0.36178099907769096 Test Loss: 0.567560546875\n",
      "Epoch: 1286 Training Loss: 0.3615577494303385 Test Loss: 0.5673723415798612\n",
      "Epoch: 1287 Training Loss: 0.3613313225640191 Test Loss: 0.5672645941840277\n",
      "Epoch: 1288 Training Loss: 0.36111395602756075 Test Loss: 0.5671387803819444\n",
      "Epoch: 1289 Training Loss: 0.36089107259114583 Test Loss: 0.5670900607638889\n",
      "Epoch: 1290 Training Loss: 0.36066962348090276 Test Loss: 0.5669669053819445\n",
      "Epoch: 1291 Training Loss: 0.3604517279730903 Test Loss: 0.5670098741319445\n",
      "Epoch: 1292 Training Loss: 0.3602396308051215 Test Loss: 0.5669810655381945\n",
      "Epoch: 1293 Training Loss: 0.3600247802734375 Test Loss: 0.5670733506944444\n",
      "Epoch: 1294 Training Loss: 0.3598171149359809 Test Loss: 0.5673601888020834\n",
      "Epoch: 1295 Training Loss: 0.35961777072482637 Test Loss: 0.5677254231770833\n",
      "Epoch: 1296 Training Loss: 0.35942529635959203 Test Loss: 0.5681345486111111\n",
      "Epoch: 1297 Training Loss: 0.3592424553765191 Test Loss: 0.5685933159722222\n",
      "Epoch: 1298 Training Loss: 0.35907691446940104 Test Loss: 0.5689853515625\n",
      "Epoch: 1299 Training Loss: 0.35892838880750866 Test Loss: 0.5688713107638889\n",
      "Epoch: 1300 Training Loss: 0.35879420640733506 Test Loss: 0.5684231228298611\n",
      "Epoch: 1301 Training Loss: 0.3586506856282552 Test Loss: 0.5674777018229167\n",
      "Epoch: 1302 Training Loss: 0.35848885769314237 Test Loss: 0.5662077365451389\n",
      "Epoch: 1303 Training Loss: 0.3582678019205729 Test Loss: 0.5652574869791667\n",
      "Epoch: 1304 Training Loss: 0.3580145094129774 Test Loss: 0.5646749131944444\n",
      "Epoch: 1305 Training Loss: 0.3577759738498264 Test Loss: 0.5644830186631944\n",
      "Epoch: 1306 Training Loss: 0.35757137722439236 Test Loss: 0.5643099500868055\n",
      "Epoch: 1307 Training Loss: 0.3573513420952691 Test Loss: 0.5639307725694445\n",
      "Epoch: 1308 Training Loss: 0.35703625827365454 Test Loss: 0.5641796875\n",
      "Epoch: 1309 Training Loss: 0.35665701972113717 Test Loss: 0.5651099717881944\n",
      "Epoch: 1310 Training Loss: 0.35631959364149307 Test Loss: 0.5661124674479167\n",
      "Epoch: 1311 Training Loss: 0.356043697781033 Test Loss: 0.5668126085069445\n",
      "Epoch: 1312 Training Loss: 0.3558049858940972 Test Loss: 0.5665386284722222\n",
      "Epoch: 1313 Training Loss: 0.35555438571506076 Test Loss: 0.5656844618055555\n",
      "Epoch: 1314 Training Loss: 0.3552818298339844 Test Loss: 0.5647913411458333\n",
      "Epoch: 1315 Training Loss: 0.35500330268012154 Test Loss: 0.5641043836805556\n",
      "Epoch: 1316 Training Loss: 0.35473831854926213 Test Loss: 0.5635847439236111\n",
      "Epoch: 1317 Training Loss: 0.35449745008680555 Test Loss: 0.5633821614583333\n",
      "Epoch: 1318 Training Loss: 0.3542653130425347 Test Loss: 0.5633675672743056\n",
      "Epoch: 1319 Training Loss: 0.3540416022406684 Test Loss: 0.5633666449652778\n",
      "Epoch: 1320 Training Loss: 0.3538104451497396 Test Loss: 0.5635770399305555\n",
      "Epoch: 1321 Training Loss: 0.3535732150607639 Test Loss: 0.5639395073784722\n",
      "Epoch: 1322 Training Loss: 0.3533403625488281 Test Loss: 0.5643938802083334\n",
      "Epoch: 1323 Training Loss: 0.35312125651041665 Test Loss: 0.5647938910590278\n",
      "Epoch: 1324 Training Loss: 0.35290671793619793 Test Loss: 0.5649331597222222\n",
      "Epoch: 1325 Training Loss: 0.3526970689561632 Test Loss: 0.5647109917534722\n",
      "Epoch: 1326 Training Loss: 0.352485107421875 Test Loss: 0.5642728407118055\n",
      "Epoch: 1327 Training Loss: 0.35226110161675345 Test Loss: 0.5638264973958333\n",
      "Epoch: 1328 Training Loss: 0.3520284661187066 Test Loss: 0.56342578125\n",
      "Epoch: 1329 Training Loss: 0.35179705471462674 Test Loss: 0.5629987521701388\n",
      "Epoch: 1330 Training Loss: 0.351571038140191 Test Loss: 0.5625656467013889\n",
      "Epoch: 1331 Training Loss: 0.3513567403157552 Test Loss: 0.5621053602430556\n",
      "Epoch: 1332 Training Loss: 0.35115125189887153 Test Loss: 0.5618336588541667\n",
      "Epoch: 1333 Training Loss: 0.35095452202690974 Test Loss: 0.5617207573784723\n",
      "Epoch: 1334 Training Loss: 0.350756351047092 Test Loss: 0.5617860243055556\n",
      "Epoch: 1335 Training Loss: 0.35055833943684894 Test Loss: 0.5619547526041667\n",
      "Epoch: 1336 Training Loss: 0.3503567911783854 Test Loss: 0.5622608506944444\n",
      "Epoch: 1337 Training Loss: 0.35015613132052953 Test Loss: 0.5625785047743056\n",
      "Epoch: 1338 Training Loss: 0.3499584791395399 Test Loss: 0.5628601345486111\n",
      "Epoch: 1339 Training Loss: 0.3497542724609375 Test Loss: 0.5632353515625\n",
      "Epoch: 1340 Training Loss: 0.34956250678168405 Test Loss: 0.5637633463541667\n",
      "Epoch: 1341 Training Loss: 0.3493923848470052 Test Loss: 0.5640818142361111\n",
      "Epoch: 1342 Training Loss: 0.34922669813368057 Test Loss: 0.5641222330729166\n",
      "Epoch: 1343 Training Loss: 0.3490509745279948 Test Loss: 0.5635152994791667\n",
      "Epoch: 1344 Training Loss: 0.3488505113389757 Test Loss: 0.5625611436631944\n",
      "Epoch: 1345 Training Loss: 0.34861791314019097 Test Loss: 0.5614140625\n",
      "Epoch: 1346 Training Loss: 0.34837284003363717 Test Loss: 0.5604825846354167\n",
      "Epoch: 1347 Training Loss: 0.34813742404513887 Test Loss: 0.5599997287326389\n",
      "Epoch: 1348 Training Loss: 0.34793799845377604 Test Loss: 0.5596946614583334\n",
      "Epoch: 1349 Training Loss: 0.3477434556749132 Test Loss: 0.5595476888020834\n",
      "Epoch: 1350 Training Loss: 0.3475340576171875 Test Loss: 0.5594820963541667\n",
      "Epoch: 1351 Training Loss: 0.3472879503038194 Test Loss: 0.5600027126736111\n",
      "Epoch: 1352 Training Loss: 0.34703385416666666 Test Loss: 0.5613244900173611\n",
      "Epoch: 1353 Training Loss: 0.3468077901204427 Test Loss: 0.5627956814236111\n",
      "Epoch: 1354 Training Loss: 0.3466041395399306 Test Loss: 0.5634076063368055\n",
      "Epoch: 1355 Training Loss: 0.3464123738606771 Test Loss: 0.5625504557291666\n",
      "Epoch: 1356 Training Loss: 0.3461995442708333 Test Loss: 0.5614964192708334\n",
      "Epoch: 1357 Training Loss: 0.34598082139756947 Test Loss: 0.5603972981770833\n",
      "Epoch: 1358 Training Loss: 0.34571471150716143 Test Loss: 0.5594460720486111\n",
      "Epoch: 1359 Training Loss: 0.34542459445529516 Test Loss: 0.5591451822916667\n",
      "Epoch: 1360 Training Loss: 0.3451477525499132 Test Loss: 0.55938232421875\n",
      "Epoch: 1361 Training Loss: 0.34491973537868925 Test Loss: 0.5597554253472222\n",
      "Epoch: 1362 Training Loss: 0.34472395833333336 Test Loss: 0.5598780924479166\n",
      "Epoch: 1363 Training Loss: 0.3445422634548611 Test Loss: 0.5596514756944444\n",
      "Epoch: 1364 Training Loss: 0.3443341742621528 Test Loss: 0.5590410698784722\n",
      "Epoch: 1365 Training Loss: 0.34408775160047744 Test Loss: 0.5589734157986112\n",
      "Epoch: 1366 Training Loss: 0.34383822631835936 Test Loss: 0.5595074869791666\n",
      "Epoch: 1367 Training Loss: 0.3436326633029514 Test Loss: 0.5599439019097222\n",
      "Epoch: 1368 Training Loss: 0.34344163343641493 Test Loss: 0.5600614149305555\n",
      "Epoch: 1369 Training Loss: 0.34325599500868054 Test Loss: 0.5603816189236112\n",
      "Epoch: 1370 Training Loss: 0.34305224270290796 Test Loss: 0.5609129774305556\n",
      "Epoch: 1371 Training Loss: 0.3428393893771701 Test Loss: 0.5610484483506945\n",
      "Epoch: 1372 Training Loss: 0.34260733710394964 Test Loss: 0.5601589626736111\n",
      "Epoch: 1373 Training Loss: 0.34237649875217013 Test Loss: 0.5587231987847222\n",
      "Epoch: 1374 Training Loss: 0.342178961859809 Test Loss: 0.5574590928819444\n",
      "Epoch: 1375 Training Loss: 0.342004631890191 Test Loss: 0.5569297417534722\n",
      "Epoch: 1376 Training Loss: 0.34180071682400176 Test Loss: 0.5568476019965278\n",
      "Epoch: 1377 Training Loss: 0.34158540513780383 Test Loss: 0.5566735568576389\n",
      "Epoch: 1378 Training Loss: 0.34138144938151044 Test Loss: 0.5568911675347222\n",
      "Epoch: 1379 Training Loss: 0.3411720241970486 Test Loss: 0.5580003255208333\n",
      "Epoch: 1380 Training Loss: 0.34095768568250867 Test Loss: 0.559951171875\n",
      "Epoch: 1381 Training Loss: 0.3407779100206163 Test Loss: 0.5615028754340278\n",
      "Epoch: 1382 Training Loss: 0.3406330871582031 Test Loss: 0.5610010850694445\n",
      "Epoch: 1383 Training Loss: 0.34043939548068575 Test Loss: 0.5594318033854166\n",
      "Epoch: 1384 Training Loss: 0.3402476840549045 Test Loss: 0.5584025607638889\n",
      "Epoch: 1385 Training Loss: 0.34003951687282985 Test Loss: 0.5576360134548611\n",
      "Epoch: 1386 Training Loss: 0.3398035854763455 Test Loss: 0.5569461805555556\n",
      "Epoch: 1387 Training Loss: 0.3395424635145399 Test Loss: 0.5569173177083333\n",
      "Epoch: 1388 Training Loss: 0.33930835639105905 Test Loss: 0.5573619791666666\n",
      "Epoch: 1389 Training Loss: 0.3390952487521701 Test Loss: 0.5577355143229167\n",
      "Epoch: 1390 Training Loss: 0.3389402940538194 Test Loss: 0.5573231336805555\n",
      "Epoch: 1391 Training Loss: 0.33880537584092885 Test Loss: 0.5562992621527778\n",
      "Epoch: 1392 Training Loss: 0.3386544935438368 Test Loss: 0.5554375\n",
      "Epoch: 1393 Training Loss: 0.33842528957790796 Test Loss: 0.5553131510416667\n",
      "Epoch: 1394 Training Loss: 0.33819092475043405 Test Loss: 0.5559628363715278\n",
      "Epoch: 1395 Training Loss: 0.33797376166449655 Test Loss: 0.5569823676215278\n",
      "Epoch: 1396 Training Loss: 0.33778074476453995 Test Loss: 0.5580770399305556\n",
      "Epoch: 1397 Training Loss: 0.33761707899305554 Test Loss: 0.5597650824652778\n",
      "Epoch: 1398 Training Loss: 0.3374676615397135 Test Loss: 0.5606854383680555\n",
      "Epoch: 1399 Training Loss: 0.3373093939887153 Test Loss: 0.5597232530381945\n",
      "Epoch: 1400 Training Loss: 0.33709921264648435 Test Loss: 0.5575185004340277\n",
      "Epoch: 1401 Training Loss: 0.33684737141927085 Test Loss: 0.5554619140625\n",
      "Epoch: 1402 Training Loss: 0.3366141594780816 Test Loss: 0.5545191514756944\n",
      "Epoch: 1403 Training Loss: 0.3363829854329427 Test Loss: 0.5541664496527777\n",
      "Epoch: 1404 Training Loss: 0.33619290500217014 Test Loss: 0.5539160698784722\n",
      "Epoch: 1405 Training Loss: 0.33601035902235243 Test Loss: 0.5540018988715277\n",
      "Epoch: 1406 Training Loss: 0.3357964782714844 Test Loss: 0.5550505099826389\n",
      "Epoch: 1407 Training Loss: 0.3355744764539931 Test Loss: 0.5569463975694444\n",
      "Epoch: 1408 Training Loss: 0.3353815646701389 Test Loss: 0.5584399956597222\n",
      "Epoch: 1409 Training Loss: 0.33522227986653647 Test Loss: 0.5581568467881944\n",
      "Epoch: 1410 Training Loss: 0.33501644558376736 Test Loss: 0.5571594509548611\n",
      "Epoch: 1411 Training Loss: 0.33481925794813366 Test Loss: 0.5564343532986111\n",
      "Epoch: 1412 Training Loss: 0.3346132303873698 Test Loss: 0.5555350477430555\n",
      "Epoch: 1413 Training Loss: 0.33437742445203994 Test Loss: 0.5548669162326388\n",
      "Epoch: 1414 Training Loss: 0.33410857476128475 Test Loss: 0.5546378038194445\n",
      "Epoch: 1415 Training Loss: 0.3338536851671007 Test Loss: 0.5551171875\n",
      "Epoch: 1416 Training Loss: 0.3336485087076823 Test Loss: 0.5555634223090278\n",
      "Epoch: 1417 Training Loss: 0.33350070529513887 Test Loss: 0.5553224826388888\n",
      "Epoch: 1418 Training Loss: 0.3333525797526042 Test Loss: 0.5543999565972222\n",
      "Epoch: 1419 Training Loss: 0.3331482645670573 Test Loss: 0.5539188368055555\n",
      "Epoch: 1420 Training Loss: 0.3329169481065538 Test Loss: 0.5541161566840278\n",
      "Epoch: 1421 Training Loss: 0.3326964077419705 Test Loss: 0.5546573893229166\n",
      "Epoch: 1422 Training Loss: 0.33250422837999133 Test Loss: 0.5551831597222222\n",
      "Epoch: 1423 Training Loss: 0.3323370157877604 Test Loss: 0.5558259548611111\n",
      "Epoch: 1424 Training Loss: 0.33216007147894966 Test Loss: 0.55630859375\n",
      "Epoch: 1425 Training Loss: 0.3319751756456163 Test Loss: 0.5569734157986111\n",
      "Epoch: 1426 Training Loss: 0.33177014838324653 Test Loss: 0.55711181640625\n",
      "Epoch: 1427 Training Loss: 0.3315687730577257 Test Loss: 0.5561398654513889\n",
      "Epoch: 1428 Training Loss: 0.33139764404296873 Test Loss: 0.5542546657986112\n",
      "Epoch: 1429 Training Loss: 0.33124378458658854 Test Loss: 0.5526929253472223\n",
      "Epoch: 1430 Training Loss: 0.33108543904622395 Test Loss: 0.5518045789930556\n",
      "Epoch: 1431 Training Loss: 0.3308799845377604 Test Loss: 0.5516965603298611\n",
      "Epoch: 1432 Training Loss: 0.3306635064019097 Test Loss: 0.5519097222222222\n",
      "Epoch: 1433 Training Loss: 0.330447998046875 Test Loss: 0.5531706814236111\n",
      "Epoch: 1434 Training Loss: 0.3302482469346788 Test Loss: 0.5555380316840278\n",
      "Epoch: 1435 Training Loss: 0.33009246826171873 Test Loss: 0.5573658854166667\n",
      "Epoch: 1436 Training Loss: 0.3299654303656684 Test Loss: 0.5569769422743056\n",
      "Epoch: 1437 Training Loss: 0.3298057623969184 Test Loss: 0.5551715494791667\n",
      "Epoch: 1438 Training Loss: 0.3296171196831597 Test Loss: 0.5537250434027777\n",
      "Epoch: 1439 Training Loss: 0.32941526624891493 Test Loss: 0.5528152126736111\n",
      "Epoch: 1440 Training Loss: 0.3291781921386719 Test Loss: 0.552271484375\n",
      "Epoch: 1441 Training Loss: 0.32893780856662325 Test Loss: 0.5524711371527777\n",
      "Epoch: 1442 Training Loss: 0.32873672146267363 Test Loss: 0.5531021050347222\n",
      "Epoch: 1443 Training Loss: 0.3285735778808594 Test Loss: 0.5534242621527777\n",
      "Epoch: 1444 Training Loss: 0.32844753011067707 Test Loss: 0.5529398328993056\n",
      "Epoch: 1445 Training Loss: 0.32829798041449654 Test Loss: 0.5522100151909722\n",
      "Epoch: 1446 Training Loss: 0.3281044650607639 Test Loss: 0.5518097330729167\n",
      "Epoch: 1447 Training Loss: 0.3278888685438368 Test Loss: 0.5522808702256945\n",
      "Epoch: 1448 Training Loss: 0.3276851399739583 Test Loss: 0.5532290581597222\n",
      "Epoch: 1449 Training Loss: 0.3275179612901476 Test Loss: 0.5541144748263889\n",
      "Epoch: 1450 Training Loss: 0.32737615627712674 Test Loss: 0.5549228515625\n",
      "Epoch: 1451 Training Loss: 0.3272403632269965 Test Loss: 0.5554069552951388\n",
      "Epoch: 1452 Training Loss: 0.3270606248643663 Test Loss: 0.5551446940104167\n",
      "Epoch: 1453 Training Loss: 0.3268525763617621 Test Loss: 0.5540187717013889\n",
      "Epoch: 1454 Training Loss: 0.32666618177625867 Test Loss: 0.5521810438368056\n",
      "Epoch: 1455 Training Loss: 0.3265022989908854 Test Loss: 0.5507664388020833\n",
      "Epoch: 1456 Training Loss: 0.32634747992621527 Test Loss: 0.5498800455729167\n",
      "Epoch: 1457 Training Loss: 0.32616012912326386 Test Loss: 0.5493894856770833\n",
      "Epoch: 1458 Training Loss: 0.32595120578342013 Test Loss: 0.5497287326388889\n",
      "Epoch: 1459 Training Loss: 0.32573959350585935 Test Loss: 0.5515905490451389\n",
      "Epoch: 1460 Training Loss: 0.32555774603949655 Test Loss: 0.5541623263888888\n",
      "Epoch: 1461 Training Loss: 0.32542049153645836 Test Loss: 0.5554122721354167\n",
      "Epoch: 1462 Training Loss: 0.3252977464463976 Test Loss: 0.5546942274305555\n",
      "Epoch: 1463 Training Loss: 0.3251341654459635 Test Loss: 0.5528481987847222\n",
      "Epoch: 1464 Training Loss: 0.32494028049045137 Test Loss: 0.551525390625\n",
      "Epoch: 1465 Training Loss: 0.3247259453667535 Test Loss: 0.5508257378472222\n",
      "Epoch: 1466 Training Loss: 0.3244691704644097 Test Loss: 0.5510394965277777\n",
      "Epoch: 1467 Training Loss: 0.3242544657389323 Test Loss: 0.5514950086805556\n",
      "Epoch: 1468 Training Loss: 0.32407586669921873 Test Loss: 0.5519847005208334\n",
      "Epoch: 1469 Training Loss: 0.32392822943793403 Test Loss: 0.5516564670138889\n",
      "Epoch: 1470 Training Loss: 0.32378042602539064 Test Loss: 0.5510673828125\n",
      "Epoch: 1471 Training Loss: 0.32361028713650175 Test Loss: 0.5504083116319445\n",
      "Epoch: 1472 Training Loss: 0.32339590454101563 Test Loss: 0.5505174696180556\n",
      "Epoch: 1473 Training Loss: 0.3231903076171875 Test Loss: 0.5509966362847222\n",
      "Epoch: 1474 Training Loss: 0.3230270046657986 Test Loss: 0.5518532443576389\n",
      "Epoch: 1475 Training Loss: 0.3228867458767361 Test Loss: 0.5528922526041666\n",
      "Epoch: 1476 Training Loss: 0.32274998982747394 Test Loss: 0.5539031575520833\n",
      "Epoch: 1477 Training Loss: 0.3225985785590278 Test Loss: 0.5540326063368055\n",
      "Epoch: 1478 Training Loss: 0.3224344923231337 Test Loss: 0.5530665147569445\n",
      "Epoch: 1479 Training Loss: 0.3222751091851129 Test Loss: 0.5514812282986111\n",
      "Epoch: 1480 Training Loss: 0.3221321716308594 Test Loss: 0.5497489691840278\n",
      "Epoch: 1481 Training Loss: 0.3219914143880208 Test Loss: 0.5485831705729166\n",
      "Epoch: 1482 Training Loss: 0.3218185221354167 Test Loss: 0.5482027994791666\n",
      "Epoch: 1483 Training Loss: 0.32162232801649304 Test Loss: 0.5486382921006945\n",
      "Epoch: 1484 Training Loss: 0.3214460279676649 Test Loss: 0.5504710286458333\n",
      "Epoch: 1485 Training Loss: 0.3212986178927951 Test Loss: 0.5524512803819445\n",
      "Epoch: 1486 Training Loss: 0.32120118543836806 Test Loss: 0.5534851888020833\n",
      "Epoch: 1487 Training Loss: 0.32111325073242186 Test Loss: 0.5527913411458333\n",
      "Epoch: 1488 Training Loss: 0.3210194600423177 Test Loss: 0.5513416341145834\n",
      "Epoch: 1489 Training Loss: 0.3208824937608507 Test Loss: 0.5510744357638889\n",
      "Epoch: 1490 Training Loss: 0.3207170172797309 Test Loss: 0.5512321506076389\n",
      "Epoch: 1491 Training Loss: 0.32053775702582465 Test Loss: 0.5516876627604167\n",
      "Epoch: 1492 Training Loss: 0.32035608588324654 Test Loss: 0.5518283420138889\n",
      "Epoch: 1493 Training Loss: 0.3202053392198351 Test Loss: 0.5514240451388889\n",
      "Epoch: 1494 Training Loss: 0.32010196601019963 Test Loss: 0.5502819010416666\n",
      "Epoch: 1495 Training Loss: 0.32000441148546005 Test Loss: 0.5488440212673611\n",
      "Epoch: 1496 Training Loss: 0.3198942192925347 Test Loss: 0.5481552734375\n",
      "Epoch: 1497 Training Loss: 0.31974766710069447 Test Loss: 0.5483500434027778\n",
      "Epoch: 1498 Training Loss: 0.31960164388020834 Test Loss: 0.5496247287326389\n",
      "Epoch: 1499 Training Loss: 0.31949595133463543 Test Loss: 0.5512478841145834\n",
      "Epoch: 1500 Training Loss: 0.3194532029893663 Test Loss: 0.5524264865451389\n",
      "Epoch: 1501 Training Loss: 0.3194382019042969 Test Loss: 0.5523992513020833\n",
      "Epoch: 1502 Training Loss: 0.3194069959852431 Test Loss: 0.5510616861979166\n",
      "Epoch: 1503 Training Loss: 0.31934716796875 Test Loss: 0.5499927300347223\n",
      "Epoch: 1504 Training Loss: 0.3192681443956163 Test Loss: 0.5495096028645833\n",
      "Epoch: 1505 Training Loss: 0.31918759155273435 Test Loss: 0.5494097764756944\n",
      "Epoch: 1506 Training Loss: 0.31918172200520833 Test Loss: 0.5491146375868056\n",
      "Epoch: 1507 Training Loss: 0.3192282138400608 Test Loss: 0.5479592013888889\n",
      "Epoch: 1508 Training Loss: 0.3192885233561198 Test Loss: 0.5473225368923611\n",
      "Epoch: 1509 Training Loss: 0.3193667738172743 Test Loss: 0.5479559461805555\n",
      "Epoch: 1510 Training Loss: 0.31951456705729164 Test Loss: 0.54986328125\n",
      "Epoch: 1511 Training Loss: 0.31979587131076387 Test Loss: 0.5513006727430556\n",
      "Epoch: 1512 Training Loss: 0.32018387518988717 Test Loss: 0.5514832356770833\n",
      "Epoch: 1513 Training Loss: 0.32060291883680553 Test Loss: 0.5518398980034722\n",
      "Epoch: 1514 Training Loss: 0.3210739881727431 Test Loss: 0.5525402018229166\n",
      "Epoch: 1515 Training Loss: 0.3216868184407552 Test Loss: 0.5533059895833333\n",
      "Epoch: 1516 Training Loss: 0.32242692057291666 Test Loss: 0.5546443142361112\n",
      "Epoch: 1517 Training Loss: 0.3233115471733941 Test Loss: 0.5563420138888889\n",
      "Epoch: 1518 Training Loss: 0.32427379353841146 Test Loss: 0.5564990776909722\n",
      "Epoch: 1519 Training Loss: 0.3252490743001302 Test Loss: 0.5547607964409722\n",
      "Epoch: 1520 Training Loss: 0.3259775899251302 Test Loss: 0.5516615668402778\n",
      "Epoch: 1521 Training Loss: 0.325968020968967 Test Loss: 0.5514410264756945\n",
      "Epoch: 1522 Training Loss: 0.3251099446614583 Test Loss: 0.5547975260416667\n",
      "Epoch: 1523 Training Loss: 0.32361121622721356 Test Loss: 0.5609734157986112\n",
      "Epoch: 1524 Training Loss: 0.32221702406141495 Test Loss: 0.56655078125\n",
      "Epoch: 1525 Training Loss: 0.3212266303168403 Test Loss: 0.5680251736111112\n",
      "Epoch: 1526 Training Loss: 0.3208294406467014 Test Loss: 0.5692970920138889\n",
      "Epoch: 1527 Training Loss: 0.320830322265625 Test Loss: 0.5747278645833334\n",
      "Epoch: 1528 Training Loss: 0.32159354315863714 Test Loss: 0.5878582899305556\n",
      "Epoch: 1529 Training Loss: 0.3238696051703559 Test Loss: 0.5909433051215278\n",
      "Epoch: 1530 Training Loss: 0.32811642456054685 Test Loss: 0.54715234375\n",
      "Epoch: 1531 Training Loss: 0.32705301920572916 Test Loss: 0.5565504014756945\n",
      "Epoch: 1532 Training Loss: 0.3223546142578125 Test Loss: 0.5434716796875\n",
      "Epoch: 1533 Training Loss: 0.3195514865451389 Test Loss: 0.5424592556423611\n",
      "Epoch: 1534 Training Loss: 0.32090486992730033 Test Loss: 0.5412696397569444\n",
      "Epoch: 1535 Training Loss: 0.32179276529947914 Test Loss: 0.5439065755208333\n",
      "Epoch: 1536 Training Loss: 0.3209884474012587 Test Loss: 0.5444143880208333\n",
      "Epoch: 1537 Training Loss: 0.3202010091145833 Test Loss: 0.5419021809895833\n",
      "Epoch: 1538 Training Loss: 0.32052904256184894 Test Loss: 0.5418264973958333\n",
      "Epoch: 1539 Training Loss: 0.3212656724717882 Test Loss: 0.5422161458333333\n",
      "Epoch: 1540 Training Loss: 0.3215741678873698 Test Loss: 0.5434002278645833\n",
      "Epoch: 1541 Training Loss: 0.3215997823079427 Test Loss: 0.5439781901041667\n",
      "Epoch: 1542 Training Loss: 0.32168283759223093 Test Loss: 0.5448460828993056\n",
      "Epoch: 1543 Training Loss: 0.32200655788845484 Test Loss: 0.5465286458333334\n",
      "Epoch: 1544 Training Loss: 0.322341796875 Test Loss: 0.5479555121527778\n",
      "Epoch: 1545 Training Loss: 0.322574211968316 Test Loss: 0.5483315972222222\n",
      "Epoch: 1546 Training Loss: 0.3226334431966146 Test Loss: 0.5470169813368055\n",
      "Epoch: 1547 Training Loss: 0.32258458116319444 Test Loss: 0.5440656467013889\n",
      "Epoch: 1548 Training Loss: 0.3225070054796007 Test Loss: 0.5407369791666666\n",
      "Epoch: 1549 Training Loss: 0.32245824178059895 Test Loss: 0.5378002387152778\n",
      "Epoch: 1550 Training Loss: 0.32242357042100694 Test Loss: 0.5361620551215278\n",
      "Epoch: 1551 Training Loss: 0.32239498562282987 Test Loss: 0.5362183159722222\n",
      "Epoch: 1552 Training Loss: 0.32241322496202257 Test Loss: 0.5376089409722222\n",
      "Epoch: 1553 Training Loss: 0.3225037061903212 Test Loss: 0.5400618489583333\n",
      "Epoch: 1554 Training Loss: 0.32270057169596356 Test Loss: 0.5429359809027777\n",
      "Epoch: 1555 Training Loss: 0.32296832614474824 Test Loss: 0.5461357421875\n",
      "Epoch: 1556 Training Loss: 0.32325598483615453 Test Loss: 0.54953955078125\n",
      "Epoch: 1557 Training Loss: 0.3235659891764323 Test Loss: 0.5523916015625\n",
      "Epoch: 1558 Training Loss: 0.32381675550672745 Test Loss: 0.5543401150173611\n",
      "Epoch: 1559 Training Loss: 0.3239714830186632 Test Loss: 0.5543670789930556\n",
      "Epoch: 1560 Training Loss: 0.3239009941948785 Test Loss: 0.5523990885416666\n",
      "Epoch: 1561 Training Loss: 0.32350953504774305 Test Loss: 0.5489395073784722\n",
      "Epoch: 1562 Training Loss: 0.3227232767740885 Test Loss: 0.5440410698784722\n",
      "Epoch: 1563 Training Loss: 0.3215357157389323 Test Loss: 0.5391925998263889\n",
      "Epoch: 1564 Training Loss: 0.3200580206976997 Test Loss: 0.5356418728298611\n",
      "Epoch: 1565 Training Loss: 0.31851500447591147 Test Loss: 0.5335188802083334\n",
      "Epoch: 1566 Training Loss: 0.3171055263943142 Test Loss: 0.5321666124131944\n",
      "Epoch: 1567 Training Loss: 0.3159302978515625 Test Loss: 0.5315981987847223\n",
      "Epoch: 1568 Training Loss: 0.3150105692545573 Test Loss: 0.5314108072916667\n",
      "Epoch: 1569 Training Loss: 0.31429915364583333 Test Loss: 0.5317542317708334\n",
      "Epoch: 1570 Training Loss: 0.3137482401529948 Test Loss: 0.53209765625\n",
      "Epoch: 1571 Training Loss: 0.3133241441514757 Test Loss: 0.5323484700520833\n",
      "Epoch: 1572 Training Loss: 0.3129984910753038 Test Loss: 0.5325641276041667\n",
      "Epoch: 1573 Training Loss: 0.3127492438422309 Test Loss: 0.5328106553819445\n",
      "Epoch: 1574 Training Loss: 0.31254630194769967 Test Loss: 0.533134765625\n",
      "Epoch: 1575 Training Loss: 0.31237393527560764 Test Loss: 0.5335118272569445\n",
      "Epoch: 1576 Training Loss: 0.3122149251302083 Test Loss: 0.5339220377604167\n",
      "Epoch: 1577 Training Loss: 0.3120647989908854 Test Loss: 0.5342431098090278\n",
      "Epoch: 1578 Training Loss: 0.31193000962999134 Test Loss: 0.5345037434895833\n",
      "Epoch: 1579 Training Loss: 0.3118064473470052 Test Loss: 0.5347924262152778\n",
      "Epoch: 1580 Training Loss: 0.31169362046983506 Test Loss: 0.5349606119791667\n",
      "Epoch: 1581 Training Loss: 0.31159692721896703 Test Loss: 0.5351993272569444\n",
      "Epoch: 1582 Training Loss: 0.31152267116970483 Test Loss: 0.5354860568576388\n",
      "Epoch: 1583 Training Loss: 0.31145946248372397 Test Loss: 0.5358791232638889\n",
      "Epoch: 1584 Training Loss: 0.3113844909667969 Test Loss: 0.5363551974826389\n",
      "Epoch: 1585 Training Loss: 0.3113068135579427 Test Loss: 0.5368649631076389\n",
      "Epoch: 1586 Training Loss: 0.31123235405815974 Test Loss: 0.5372394748263889\n",
      "Epoch: 1587 Training Loss: 0.31116758897569446 Test Loss: 0.5376445855034723\n",
      "Epoch: 1588 Training Loss: 0.31110326131184896 Test Loss: 0.5380324978298611\n",
      "Epoch: 1589 Training Loss: 0.31105995008680554 Test Loss: 0.5383493381076389\n",
      "Epoch: 1590 Training Loss: 0.3110143534342448 Test Loss: 0.5386283637152778\n",
      "Epoch: 1591 Training Loss: 0.3109925774468316 Test Loss: 0.53913671875\n",
      "Epoch: 1592 Training Loss: 0.31099521891276044 Test Loss: 0.5395439995659722\n",
      "Epoch: 1593 Training Loss: 0.31101675415039065 Test Loss: 0.5398948567708334\n",
      "Epoch: 1594 Training Loss: 0.3110609198676215 Test Loss: 0.5405179036458333\n",
      "Epoch: 1595 Training Loss: 0.31113201226128473 Test Loss: 0.5411837565104166\n",
      "Epoch: 1596 Training Loss: 0.3112268744574653 Test Loss: 0.5418025716145833\n",
      "Epoch: 1597 Training Loss: 0.31132894219292534 Test Loss: 0.5424845920138889\n",
      "Epoch: 1598 Training Loss: 0.3114454311794705 Test Loss: 0.5429723307291666\n",
      "Epoch: 1599 Training Loss: 0.311583001030816 Test Loss: 0.5434286566840277\n",
      "Epoch: 1600 Training Loss: 0.31173716227213544 Test Loss: 0.5436296657986112\n",
      "Epoch: 1601 Training Loss: 0.31189856296115454 Test Loss: 0.5436380750868055\n",
      "Epoch: 1602 Training Loss: 0.31205601331922744 Test Loss: 0.5433203667534722\n",
      "Epoch: 1603 Training Loss: 0.312221930609809 Test Loss: 0.5426561414930555\n",
      "Epoch: 1604 Training Loss: 0.3123669026692708 Test Loss: 0.5417369249131945\n",
      "Epoch: 1605 Training Loss: 0.31247781711154515 Test Loss: 0.5405575629340278\n",
      "Epoch: 1606 Training Loss: 0.312538811577691 Test Loss: 0.5393755425347222\n",
      "Epoch: 1607 Training Loss: 0.31254369778103297 Test Loss: 0.5382155490451389\n",
      "Epoch: 1608 Training Loss: 0.3125228576660156 Test Loss: 0.537423828125\n",
      "Epoch: 1609 Training Loss: 0.31246941460503475 Test Loss: 0.5366348741319444\n",
      "Epoch: 1610 Training Loss: 0.3124021267361111 Test Loss: 0.5361076388888889\n",
      "Epoch: 1611 Training Loss: 0.31236901177300347 Test Loss: 0.5361572265625\n",
      "Epoch: 1612 Training Loss: 0.3124079827202691 Test Loss: 0.5365066189236111\n",
      "Epoch: 1613 Training Loss: 0.3125481262207031 Test Loss: 0.536970703125\n",
      "Epoch: 1614 Training Loss: 0.3127970140245226 Test Loss: 0.5375445963541666\n",
      "Epoch: 1615 Training Loss: 0.3131904839409722 Test Loss: 0.5379376085069445\n",
      "Epoch: 1616 Training Loss: 0.3137766655815972 Test Loss: 0.5379045138888889\n",
      "Epoch: 1617 Training Loss: 0.31457481553819444 Test Loss: 0.5368994140625\n",
      "Epoch: 1618 Training Loss: 0.31562723795572917 Test Loss: 0.5343585069444444\n",
      "Epoch: 1619 Training Loss: 0.31685977172851565 Test Loss: 0.5308498263888889\n",
      "Epoch: 1620 Training Loss: 0.3179904276529948 Test Loss: 0.5270570746527777\n",
      "Epoch: 1621 Training Loss: 0.318579830593533 Test Loss: 0.5255028754340277\n",
      "Epoch: 1622 Training Loss: 0.31833204820421007 Test Loss: 0.5266446397569444\n",
      "Epoch: 1623 Training Loss: 0.31723731825086804 Test Loss: 0.5292167426215277\n",
      "Epoch: 1624 Training Loss: 0.31567013210720485 Test Loss: 0.5305192599826389\n",
      "Epoch: 1625 Training Loss: 0.3140070631239149 Test Loss: 0.5302533637152778\n",
      "Epoch: 1626 Training Loss: 0.3125281202528212 Test Loss: 0.5292347005208333\n",
      "Epoch: 1627 Training Loss: 0.3113687303331163 Test Loss: 0.5282370334201388\n",
      "Epoch: 1628 Training Loss: 0.31055335489908853 Test Loss: 0.5273586697048611\n",
      "Epoch: 1629 Training Loss: 0.3099932861328125 Test Loss: 0.5266073676215278\n",
      "Epoch: 1630 Training Loss: 0.30961790296766495 Test Loss: 0.5261833224826389\n",
      "Epoch: 1631 Training Loss: 0.3093660617404514 Test Loss: 0.5258818359375\n",
      "Epoch: 1632 Training Loss: 0.3091657579210069 Test Loss: 0.5255647786458333\n",
      "Epoch: 1633 Training Loss: 0.3089916008843316 Test Loss: 0.5252386610243056\n",
      "Epoch: 1634 Training Loss: 0.3088207261827257 Test Loss: 0.5250129665798611\n",
      "Epoch: 1635 Training Loss: 0.3086524692111545 Test Loss: 0.5247988823784723\n",
      "Epoch: 1636 Training Loss: 0.3084682142469618 Test Loss: 0.5246123046875\n",
      "Epoch: 1637 Training Loss: 0.30827449883355035 Test Loss: 0.5244614800347223\n",
      "Epoch: 1638 Training Loss: 0.3080809088812934 Test Loss: 0.5242481553819445\n",
      "Epoch: 1639 Training Loss: 0.30787991672092013 Test Loss: 0.5240878363715278\n",
      "Epoch: 1640 Training Loss: 0.3076859198676215 Test Loss: 0.5240240342881944\n",
      "Epoch: 1641 Training Loss: 0.3075099690755208 Test Loss: 0.5238758680555555\n",
      "Epoch: 1642 Training Loss: 0.3073454386393229 Test Loss: 0.5237184244791667\n",
      "Epoch: 1643 Training Loss: 0.3071843736436632 Test Loss: 0.5237048068576389\n",
      "Epoch: 1644 Training Loss: 0.30703651258680553 Test Loss: 0.5236626519097223\n",
      "Epoch: 1645 Training Loss: 0.30691455078125 Test Loss: 0.5237227647569445\n",
      "Epoch: 1646 Training Loss: 0.3067998758951823 Test Loss: 0.5238104383680555\n",
      "Epoch: 1647 Training Loss: 0.30670521375868054 Test Loss: 0.523693359375\n",
      "Epoch: 1648 Training Loss: 0.3066372850206163 Test Loss: 0.5236202256944444\n",
      "Epoch: 1649 Training Loss: 0.3065803968641493 Test Loss: 0.5234967447916666\n",
      "Epoch: 1650 Training Loss: 0.3065360649956597 Test Loss: 0.5234505750868056\n",
      "Epoch: 1651 Training Loss: 0.30650682915581595 Test Loss: 0.5234227430555556\n",
      "Epoch: 1652 Training Loss: 0.30650865342881944 Test Loss: 0.5232916124131944\n",
      "Epoch: 1653 Training Loss: 0.3065347256130642 Test Loss: 0.5231394856770833\n",
      "Epoch: 1654 Training Loss: 0.30658401489257814 Test Loss: 0.5230703667534722\n",
      "Epoch: 1655 Training Loss: 0.30667189534505207 Test Loss: 0.5229393446180556\n",
      "Epoch: 1656 Training Loss: 0.3068184543185764 Test Loss: 0.5228399522569445\n",
      "Epoch: 1657 Training Loss: 0.30702510579427084 Test Loss: 0.5228303493923611\n",
      "Epoch: 1658 Training Loss: 0.3073103366427951 Test Loss: 0.5227881944444445\n",
      "Epoch: 1659 Training Loss: 0.3076973164876302 Test Loss: 0.5227681749131945\n",
      "Epoch: 1660 Training Loss: 0.30815526326497394 Test Loss: 0.5229627278645833\n",
      "Epoch: 1661 Training Loss: 0.3087665506998698 Test Loss: 0.5233719618055556\n",
      "Epoch: 1662 Training Loss: 0.30956241861979167 Test Loss: 0.5244619140625\n",
      "Epoch: 1663 Training Loss: 0.31058431667751735 Test Loss: 0.5259695638020834\n",
      "Epoch: 1664 Training Loss: 0.3118221672905816 Test Loss: 0.5281805555555555\n",
      "Epoch: 1665 Training Loss: 0.31338101535373264 Test Loss: 0.5323570421006945\n",
      "Epoch: 1666 Training Loss: 0.31520624118381074 Test Loss: 0.538169921875\n",
      "Epoch: 1667 Training Loss: 0.3172637261284722 Test Loss: 0.5453572048611111\n",
      "Epoch: 1668 Training Loss: 0.31945531548394096 Test Loss: 0.55240380859375\n",
      "Epoch: 1669 Training Loss: 0.321496819390191 Test Loss: 0.5541982964409722\n",
      "Epoch: 1670 Training Loss: 0.32270529174804685 Test Loss: 0.5499913736979166\n",
      "Epoch: 1671 Training Loss: 0.32252874077690974 Test Loss: 0.5398778754340278\n",
      "Epoch: 1672 Training Loss: 0.32035101996527776 Test Loss: 0.5304586046006945\n",
      "Epoch: 1673 Training Loss: 0.31683462863498263 Test Loss: 0.5255667860243055\n",
      "Epoch: 1674 Training Loss: 0.3133518846299913 Test Loss: 0.5233875868055555\n",
      "Epoch: 1675 Training Loss: 0.3107436252170139 Test Loss: 0.5236205512152777\n",
      "Epoch: 1676 Training Loss: 0.3090631205240885 Test Loss: 0.5247849392361111\n",
      "Epoch: 1677 Training Loss: 0.30803851657443576 Test Loss: 0.5260887044270833\n",
      "Epoch: 1678 Training Loss: 0.3074237365722656 Test Loss: 0.5270581597222223\n",
      "Epoch: 1679 Training Loss: 0.3070214063856337 Test Loss: 0.5277527126736111\n",
      "Epoch: 1680 Training Loss: 0.3067296108669705 Test Loss: 0.5281905381944444\n",
      "Epoch: 1681 Training Loss: 0.3064819573296441 Test Loss: 0.5284999457465278\n",
      "Epoch: 1682 Training Loss: 0.30623416476779514 Test Loss: 0.5285780164930556\n",
      "Epoch: 1683 Training Loss: 0.305983157687717 Test Loss: 0.5285177408854167\n",
      "Epoch: 1684 Training Loss: 0.30572197808159723 Test Loss: 0.5284716254340278\n",
      "Epoch: 1685 Training Loss: 0.3054659254286024 Test Loss: 0.5284840494791667\n",
      "Epoch: 1686 Training Loss: 0.3052024366590712 Test Loss: 0.5282691514756944\n",
      "Epoch: 1687 Training Loss: 0.30491156344943576 Test Loss: 0.5281673177083334\n",
      "Epoch: 1688 Training Loss: 0.3046055603027344 Test Loss: 0.5279676106770833\n",
      "Epoch: 1689 Training Loss: 0.30430903795030384 Test Loss: 0.5277967122395834\n",
      "Epoch: 1690 Training Loss: 0.3040223320855035 Test Loss: 0.52768896484375\n",
      "Epoch: 1691 Training Loss: 0.3037464633517795 Test Loss: 0.5276587999131944\n",
      "Epoch: 1692 Training Loss: 0.3034797668457031 Test Loss: 0.52774609375\n",
      "Epoch: 1693 Training Loss: 0.3032248060438368 Test Loss: 0.5276932508680555\n",
      "Epoch: 1694 Training Loss: 0.30297931247287324 Test Loss: 0.5276107855902777\n",
      "Epoch: 1695 Training Loss: 0.30274854871961804 Test Loss: 0.5275363498263889\n",
      "Epoch: 1696 Training Loss: 0.3025192633734809 Test Loss: 0.5274866536458334\n",
      "Epoch: 1697 Training Loss: 0.302304439968533 Test Loss: 0.5274742838541666\n",
      "Epoch: 1698 Training Loss: 0.30209327528211805 Test Loss: 0.5274033745659722\n",
      "Epoch: 1699 Training Loss: 0.3018852301703559 Test Loss: 0.5273701171875\n",
      "Epoch: 1700 Training Loss: 0.30168441433376736 Test Loss: 0.5272400716145833\n",
      "Epoch: 1701 Training Loss: 0.30147576904296874 Test Loss: 0.5271004231770834\n",
      "Epoch: 1702 Training Loss: 0.3012790154351129 Test Loss: 0.5270170355902778\n",
      "Epoch: 1703 Training Loss: 0.3010802442762587 Test Loss: 0.5269420572916667\n",
      "Epoch: 1704 Training Loss: 0.3008816731770833 Test Loss: 0.5268970269097222\n",
      "Epoch: 1705 Training Loss: 0.3006885545518663 Test Loss: 0.5267468532986112\n",
      "Epoch: 1706 Training Loss: 0.3004834221733941 Test Loss: 0.5266681857638889\n",
      "Epoch: 1707 Training Loss: 0.30028170776367186 Test Loss: 0.52665576171875\n",
      "Epoch: 1708 Training Loss: 0.30008544921875 Test Loss: 0.5266385091145833\n",
      "Epoch: 1709 Training Loss: 0.29988388061523436 Test Loss: 0.5266439887152777\n",
      "Epoch: 1710 Training Loss: 0.2996786838107639 Test Loss: 0.5266680772569444\n",
      "Epoch: 1711 Training Loss: 0.29946395195855036 Test Loss: 0.5266178927951389\n",
      "Epoch: 1712 Training Loss: 0.29925709025065106 Test Loss: 0.5265809461805555\n",
      "Epoch: 1713 Training Loss: 0.29904547119140623 Test Loss: 0.5264586588541667\n",
      "Epoch: 1714 Training Loss: 0.2988323025173611 Test Loss: 0.5264782986111111\n",
      "Epoch: 1715 Training Loss: 0.2986243794759115 Test Loss: 0.5264735243055556\n",
      "Epoch: 1716 Training Loss: 0.29841590033637155 Test Loss: 0.5263587782118055\n",
      "Epoch: 1717 Training Loss: 0.29821488104926214 Test Loss: 0.5262975260416667\n",
      "Epoch: 1718 Training Loss: 0.2980276048448351 Test Loss: 0.5261927083333333\n",
      "Epoch: 1719 Training Loss: 0.2978523627387153 Test Loss: 0.5261866861979166\n",
      "Epoch: 1720 Training Loss: 0.29767886691623263 Test Loss: 0.5260757921006944\n",
      "Epoch: 1721 Training Loss: 0.2975057101779514 Test Loss: 0.5260083550347222\n",
      "Epoch: 1722 Training Loss: 0.29733463880750866 Test Loss: 0.5258695746527777\n",
      "Epoch: 1723 Training Loss: 0.29716979302300345 Test Loss: 0.5257987196180556\n",
      "Epoch: 1724 Training Loss: 0.2970016140407986 Test Loss: 0.5257144097222223\n",
      "Epoch: 1725 Training Loss: 0.2968367648654514 Test Loss: 0.5256276584201389\n",
      "Epoch: 1726 Training Loss: 0.2966744147406684 Test Loss: 0.5255864800347222\n",
      "Epoch: 1727 Training Loss: 0.2965110270182292 Test Loss: 0.5255531684027778\n",
      "Epoch: 1728 Training Loss: 0.29634866333007814 Test Loss: 0.5255036892361111\n",
      "Epoch: 1729 Training Loss: 0.2961938680013021 Test Loss: 0.5254909396701389\n",
      "Epoch: 1730 Training Loss: 0.29604546440972224 Test Loss: 0.52549267578125\n",
      "Epoch: 1731 Training Loss: 0.29590200466579863 Test Loss: 0.5253829752604167\n",
      "Epoch: 1732 Training Loss: 0.29576412285698783 Test Loss: 0.5254328342013889\n",
      "Epoch: 1733 Training Loss: 0.2956294826931424 Test Loss: 0.5254307725694445\n",
      "Epoch: 1734 Training Loss: 0.29549735514322917 Test Loss: 0.5254079861111111\n",
      "Epoch: 1735 Training Loss: 0.2953650309244792 Test Loss: 0.5253877495659722\n",
      "Epoch: 1736 Training Loss: 0.29523342895507815 Test Loss: 0.5253929578993055\n",
      "Epoch: 1737 Training Loss: 0.2951002638075087 Test Loss: 0.5253968098958334\n",
      "Epoch: 1738 Training Loss: 0.29496429443359373 Test Loss: 0.5253285047743056\n",
      "Epoch: 1739 Training Loss: 0.29483140733506946 Test Loss: 0.52522021484375\n",
      "Epoch: 1740 Training Loss: 0.2946960686577691 Test Loss: 0.5251558159722223\n",
      "Epoch: 1741 Training Loss: 0.2945611775716146 Test Loss: 0.5251238064236111\n",
      "Epoch: 1742 Training Loss: 0.2944214138454861 Test Loss: 0.5251892361111111\n",
      "Epoch: 1743 Training Loss: 0.29428127712673613 Test Loss: 0.5251567925347222\n",
      "Epoch: 1744 Training Loss: 0.2941423102484809 Test Loss: 0.5251026475694445\n",
      "Epoch: 1745 Training Loss: 0.29400061713324654 Test Loss: 0.5250304361979167\n",
      "Epoch: 1746 Training Loss: 0.29386610582139755 Test Loss: 0.5249406467013888\n",
      "Epoch: 1747 Training Loss: 0.29373126898871527 Test Loss: 0.5250223524305555\n",
      "Epoch: 1748 Training Loss: 0.2935939263237847 Test Loss: 0.5250068359375\n",
      "Epoch: 1749 Training Loss: 0.29346522013346354 Test Loss: 0.5249623480902778\n",
      "Epoch: 1750 Training Loss: 0.2933363783094618 Test Loss: 0.5248895399305555\n",
      "Epoch: 1751 Training Loss: 0.293210205078125 Test Loss: 0.5249212782118056\n",
      "Epoch: 1752 Training Loss: 0.29308440483940973 Test Loss: 0.5250181206597222\n",
      "Epoch: 1753 Training Loss: 0.2929544203016493 Test Loss: 0.5249448784722223\n",
      "Epoch: 1754 Training Loss: 0.29282962375217014 Test Loss: 0.5249005533854166\n",
      "Epoch: 1755 Training Loss: 0.29270174492730033 Test Loss: 0.5249036458333334\n",
      "Epoch: 1756 Training Loss: 0.29257892184787326 Test Loss: 0.52484619140625\n",
      "Epoch: 1757 Training Loss: 0.29245222981770835 Test Loss: 0.5248746744791667\n",
      "Epoch: 1758 Training Loss: 0.2923349100748698 Test Loss: 0.5248297526041666\n",
      "Epoch: 1759 Training Loss: 0.2922113003200955 Test Loss: 0.5248499348958333\n",
      "Epoch: 1760 Training Loss: 0.29209276326497396 Test Loss: 0.5248212890625\n",
      "Epoch: 1761 Training Loss: 0.2919694349500868 Test Loss: 0.5247947591145833\n",
      "Epoch: 1762 Training Loss: 0.2918413696289063 Test Loss: 0.5247443033854167\n",
      "Epoch: 1763 Training Loss: 0.29172444661458335 Test Loss: 0.5247462022569445\n",
      "Epoch: 1764 Training Loss: 0.29160077582465277 Test Loss: 0.5247502170138889\n",
      "Epoch: 1765 Training Loss: 0.2914792921278212 Test Loss: 0.5247077907986111\n",
      "Epoch: 1766 Training Loss: 0.29136189100477433 Test Loss: 0.5247254774305555\n",
      "Epoch: 1767 Training Loss: 0.29123748779296876 Test Loss: 0.524685546875\n",
      "Epoch: 1768 Training Loss: 0.2911205003526476 Test Loss: 0.5246497395833334\n",
      "Epoch: 1769 Training Loss: 0.29100326538085936 Test Loss: 0.5246229383680555\n",
      "Epoch: 1770 Training Loss: 0.2908829854329427 Test Loss: 0.5246662326388889\n",
      "Epoch: 1771 Training Loss: 0.2907666592068142 Test Loss: 0.5245398220486112\n",
      "Epoch: 1772 Training Loss: 0.29064830525716145 Test Loss: 0.5245581597222222\n",
      "Epoch: 1773 Training Loss: 0.29053559027777776 Test Loss: 0.5245504014756944\n",
      "Epoch: 1774 Training Loss: 0.2904151373969184 Test Loss: 0.5245486653645833\n",
      "Epoch: 1775 Training Loss: 0.29030033365885416 Test Loss: 0.5244946831597223\n",
      "Epoch: 1776 Training Loss: 0.29018619791666667 Test Loss: 0.5245142144097222\n",
      "Epoch: 1777 Training Loss: 0.29006524658203126 Test Loss: 0.5245372178819444\n",
      "Epoch: 1778 Training Loss: 0.2899513278537326 Test Loss: 0.5245393880208333\n",
      "Epoch: 1779 Training Loss: 0.28983777872721356 Test Loss: 0.5244679904513889\n",
      "Epoch: 1780 Training Loss: 0.28971880764431424 Test Loss: 0.5243656684027778\n",
      "Epoch: 1781 Training Loss: 0.2896013827853733 Test Loss: 0.5244193250868056\n",
      "Epoch: 1782 Training Loss: 0.28949176364474827 Test Loss: 0.52430029296875\n",
      "Epoch: 1783 Training Loss: 0.2893775838216146 Test Loss: 0.5243126627604167\n",
      "Epoch: 1784 Training Loss: 0.2892578362358941 Test Loss: 0.5242881944444444\n",
      "Epoch: 1785 Training Loss: 0.28914377848307293 Test Loss: 0.5242181532118055\n",
      "Epoch: 1786 Training Loss: 0.28903024291992185 Test Loss: 0.5241829427083333\n",
      "Epoch: 1787 Training Loss: 0.2889141337076823 Test Loss: 0.5241558159722223\n",
      "Epoch: 1788 Training Loss: 0.2887965426974826 Test Loss: 0.5241828884548612\n",
      "Epoch: 1789 Training Loss: 0.2886847669813368 Test Loss: 0.5240851236979167\n",
      "Epoch: 1790 Training Loss: 0.28857057359483507 Test Loss: 0.5240766059027778\n",
      "Epoch: 1791 Training Loss: 0.2884572957356771 Test Loss: 0.5241126302083333\n",
      "Epoch: 1792 Training Loss: 0.28834303114149307 Test Loss: 0.5239714626736112\n",
      "Epoch: 1793 Training Loss: 0.28823121812608504 Test Loss: 0.5239790581597222\n",
      "Epoch: 1794 Training Loss: 0.28811672634548613 Test Loss: 0.5239220920138888\n",
      "Epoch: 1795 Training Loss: 0.2880006340874566 Test Loss: 0.5238940972222222\n",
      "Epoch: 1796 Training Loss: 0.2878888990614149 Test Loss: 0.5238317057291667\n",
      "Epoch: 1797 Training Loss: 0.28777875773111977 Test Loss: 0.5238172743055556\n",
      "Epoch: 1798 Training Loss: 0.2876664598253038 Test Loss: 0.5237140842013889\n",
      "Epoch: 1799 Training Loss: 0.28754707845052085 Test Loss: 0.5237594401041666\n",
      "Epoch: 1800 Training Loss: 0.28743682861328124 Test Loss: 0.5236939019097222\n",
      "Epoch: 1801 Training Loss: 0.2873260769314236 Test Loss: 0.5234994574652778\n",
      "Epoch: 1802 Training Loss: 0.28721475897894966 Test Loss: 0.5235570203993055\n",
      "Epoch: 1803 Training Loss: 0.28710230509440104 Test Loss: 0.5236508246527778\n",
      "Epoch: 1804 Training Loss: 0.2869882337782118 Test Loss: 0.5236266276041667\n",
      "Epoch: 1805 Training Loss: 0.28687547810872394 Test Loss: 0.5233959418402778\n",
      "Epoch: 1806 Training Loss: 0.2867733391655816 Test Loss: 0.5233668619791667\n",
      "Epoch: 1807 Training Loss: 0.28665782674153645 Test Loss: 0.5234428168402778\n",
      "Epoch: 1808 Training Loss: 0.2865417683919271 Test Loss: 0.5234127604166666\n",
      "Epoch: 1809 Training Loss: 0.2864279310438368 Test Loss: 0.5234015842013889\n",
      "Epoch: 1810 Training Loss: 0.28631866115993926 Test Loss: 0.5232075737847223\n",
      "Epoch: 1811 Training Loss: 0.286214606391059 Test Loss: 0.5231650933159723\n",
      "Epoch: 1812 Training Loss: 0.28610055202907986 Test Loss: 0.5231373697916667\n",
      "Epoch: 1813 Training Loss: 0.28598534138997395 Test Loss: 0.5233285047743056\n",
      "Epoch: 1814 Training Loss: 0.285866689046224 Test Loss: 0.5233637152777778\n",
      "Epoch: 1815 Training Loss: 0.285761220296224 Test Loss: 0.5230310872395834\n",
      "Epoch: 1816 Training Loss: 0.28565482584635415 Test Loss: 0.5229747178819445\n",
      "Epoch: 1817 Training Loss: 0.2855464070638021 Test Loss: 0.5229953884548612\n",
      "Epoch: 1818 Training Loss: 0.2854297349717882 Test Loss: 0.5230967881944445\n",
      "Epoch: 1819 Training Loss: 0.28531410386827255 Test Loss: 0.5231402994791666\n",
      "Epoch: 1820 Training Loss: 0.2852037353515625 Test Loss: 0.5230228949652778\n",
      "Epoch: 1821 Training Loss: 0.2850968560112847 Test Loss: 0.5227886827256945\n",
      "Epoch: 1822 Training Loss: 0.2849957580566406 Test Loss: 0.5227308485243055\n",
      "Epoch: 1823 Training Loss: 0.28488051181369356 Test Loss: 0.5227769097222222\n",
      "Epoch: 1824 Training Loss: 0.28476635064019096 Test Loss: 0.5227974175347222\n",
      "Epoch: 1825 Training Loss: 0.28464946662055124 Test Loss: 0.5229369574652778\n",
      "Epoch: 1826 Training Loss: 0.28454378594292534 Test Loss: 0.5230273980034722\n",
      "Epoch: 1827 Training Loss: 0.28443532477484806 Test Loss: 0.5227978515625\n",
      "Epoch: 1828 Training Loss: 0.2843260226779514 Test Loss: 0.52258251953125\n",
      "Epoch: 1829 Training Loss: 0.2842278086344401 Test Loss: 0.5225284830729167\n",
      "Epoch: 1830 Training Loss: 0.28412178717719183 Test Loss: 0.5223895399305556\n",
      "Epoch: 1831 Training Loss: 0.2840042588975694 Test Loss: 0.522560546875\n",
      "Epoch: 1832 Training Loss: 0.283891847398546 Test Loss: 0.5229220920138888\n",
      "Epoch: 1833 Training Loss: 0.28378274536132814 Test Loss: 0.5229280598958334\n",
      "Epoch: 1834 Training Loss: 0.28367432996961806 Test Loss: 0.5226143663194445\n",
      "Epoch: 1835 Training Loss: 0.28357144843207466 Test Loss: 0.5221617296006944\n",
      "Epoch: 1836 Training Loss: 0.28347261725531686 Test Loss: 0.5222520616319445\n",
      "Epoch: 1837 Training Loss: 0.2833745914035373 Test Loss: 0.5222411024305555\n",
      "Epoch: 1838 Training Loss: 0.2832491370307075 Test Loss: 0.5222111002604166\n",
      "Epoch: 1839 Training Loss: 0.2831373307969835 Test Loss: 0.5226243489583333\n",
      "Epoch: 1840 Training Loss: 0.28302823045518666 Test Loss: 0.52293017578125\n",
      "Epoch: 1841 Training Loss: 0.282927239312066 Test Loss: 0.5228050130208334\n",
      "Epoch: 1842 Training Loss: 0.28282369316948786 Test Loss: 0.5222224934895834\n",
      "Epoch: 1843 Training Loss: 0.2827263014051649 Test Loss: 0.5219040256076389\n",
      "Epoch: 1844 Training Loss: 0.2826412980821398 Test Loss: 0.5220100911458333\n",
      "Epoch: 1845 Training Loss: 0.28253069220648874 Test Loss: 0.5219339735243056\n",
      "Epoch: 1846 Training Loss: 0.28240646531846786 Test Loss: 0.5221038411458333\n",
      "Epoch: 1847 Training Loss: 0.28228645663791235 Test Loss: 0.5226604817708334\n",
      "Epoch: 1848 Training Loss: 0.28218147616916234 Test Loss: 0.5230956488715278\n",
      "Epoch: 1849 Training Loss: 0.28209033881293405 Test Loss: 0.5227230360243056\n",
      "Epoch: 1850 Training Loss: 0.28198551093207463 Test Loss: 0.5218924696180556\n",
      "Epoch: 1851 Training Loss: 0.2818819732666016 Test Loss: 0.5216263563368055\n",
      "Epoch: 1852 Training Loss: 0.2818033430311415 Test Loss: 0.5219603949652778\n",
      "Epoch: 1853 Training Loss: 0.2816848822699653 Test Loss: 0.5216801215277778\n",
      "Epoch: 1854 Training Loss: 0.2815578036838108 Test Loss: 0.5217754448784723\n",
      "Epoch: 1855 Training Loss: 0.2814349704318576 Test Loss: 0.5225993381076389\n",
      "Epoch: 1856 Training Loss: 0.28133458116319443 Test Loss: 0.5232165256076389\n",
      "Epoch: 1857 Training Loss: 0.28125096638997393 Test Loss: 0.5227779405381945\n",
      "Epoch: 1858 Training Loss: 0.2811421373155382 Test Loss: 0.5214180772569444\n",
      "Epoch: 1859 Training Loss: 0.281037360297309 Test Loss: 0.5213782552083334\n",
      "Epoch: 1860 Training Loss: 0.2809563259548611 Test Loss: 0.5217380099826389\n",
      "Epoch: 1861 Training Loss: 0.2808265804714627 Test Loss: 0.5215278862847222\n",
      "Epoch: 1862 Training Loss: 0.2806895446777344 Test Loss: 0.5217501085069445\n",
      "Epoch: 1863 Training Loss: 0.280567623562283 Test Loss: 0.5225653211805555\n",
      "Epoch: 1864 Training Loss: 0.28047000630696617 Test Loss: 0.5232301432291667\n",
      "Epoch: 1865 Training Loss: 0.28039029947916666 Test Loss: 0.5226471354166666\n",
      "Epoch: 1866 Training Loss: 0.28027816602918837 Test Loss: 0.5213594835069445\n",
      "Epoch: 1867 Training Loss: 0.28017510986328126 Test Loss: 0.5212466362847222\n",
      "Epoch: 1868 Training Loss: 0.28008573744032117 Test Loss: 0.5215169270833333\n",
      "Epoch: 1869 Training Loss: 0.2799534098307292 Test Loss: 0.5212535264756945\n",
      "Epoch: 1870 Training Loss: 0.27981580098470055 Test Loss: 0.5215871853298611\n",
      "Epoch: 1871 Training Loss: 0.2796959923638238 Test Loss: 0.5222950303819445\n",
      "Epoch: 1872 Training Loss: 0.27960422939724394 Test Loss: 0.5232237413194445\n",
      "Epoch: 1873 Training Loss: 0.2795320485432943 Test Loss: 0.5226703559027778\n",
      "Epoch: 1874 Training Loss: 0.2794273885091146 Test Loss: 0.5211394314236111\n",
      "Epoch: 1875 Training Loss: 0.27931046040852864 Test Loss: 0.5210426974826389\n",
      "Epoch: 1876 Training Loss: 0.2792216322157118 Test Loss: 0.5213106553819444\n",
      "Epoch: 1877 Training Loss: 0.27910787624782984 Test Loss: 0.5211302625868055\n",
      "Epoch: 1878 Training Loss: 0.27895967610677086 Test Loss: 0.5213309461805555\n",
      "Epoch: 1879 Training Loss: 0.2788301239013672 Test Loss: 0.5220508355034722\n",
      "Epoch: 1880 Training Loss: 0.2787363908555773 Test Loss: 0.5231532118055555\n",
      "Epoch: 1881 Training Loss: 0.2786719360351563 Test Loss: 0.5229738498263888\n",
      "Epoch: 1882 Training Loss: 0.27857899136013453 Test Loss: 0.5212708333333333\n",
      "Epoch: 1883 Training Loss: 0.2784585791693793 Test Loss: 0.5206537543402778\n",
      "Epoch: 1884 Training Loss: 0.27837672424316406 Test Loss: 0.5210201822916667\n",
      "Epoch: 1885 Training Loss: 0.2782702331542969 Test Loss: 0.5210118272569444\n",
      "Epoch: 1886 Training Loss: 0.27811921861436634 Test Loss: 0.5210940212673612\n",
      "Epoch: 1887 Training Loss: 0.27798543633355033 Test Loss: 0.5219734157986111\n",
      "Epoch: 1888 Training Loss: 0.27788342624240453 Test Loss: 0.5230244140625\n",
      "Epoch: 1889 Training Loss: 0.2778179863823785 Test Loss: 0.5229472113715278\n",
      "Epoch: 1890 Training Loss: 0.2777331814236111 Test Loss: 0.5210461697048611\n",
      "Epoch: 1891 Training Loss: 0.27760568406846786 Test Loss: 0.52047998046875\n",
      "Epoch: 1892 Training Loss: 0.2775200958251953 Test Loss: 0.5207699652777777\n",
      "Epoch: 1893 Training Loss: 0.27740902201334633 Test Loss: 0.5206944986979166\n",
      "Epoch: 1894 Training Loss: 0.27726919894748264 Test Loss: 0.5210213758680555\n",
      "Epoch: 1895 Training Loss: 0.2771313086615668 Test Loss: 0.5218891059027778\n",
      "Epoch: 1896 Training Loss: 0.2770304480658637 Test Loss: 0.5230090603298612\n",
      "Epoch: 1897 Training Loss: 0.2769730478922526 Test Loss: 0.5226686197916667\n",
      "Epoch: 1898 Training Loss: 0.2768817630343967 Test Loss: 0.52073974609375\n",
      "Epoch: 1899 Training Loss: 0.2767572309705946 Test Loss: 0.5200781792534722\n",
      "Epoch: 1900 Training Loss: 0.2766669684516059 Test Loss: 0.5204305555555555\n",
      "Epoch: 1901 Training Loss: 0.2765644955105252 Test Loss: 0.5203683810763889\n",
      "Epoch: 1902 Training Loss: 0.2764157901340061 Test Loss: 0.5210717230902778\n",
      "Epoch: 1903 Training Loss: 0.276285637749566 Test Loss: 0.5223008897569444\n",
      "Epoch: 1904 Training Loss: 0.27618516540527344 Test Loss: 0.5232040473090278\n",
      "Epoch: 1905 Training Loss: 0.27612183464898005 Test Loss: 0.5221291775173611\n",
      "Epoch: 1906 Training Loss: 0.27603294711642795 Test Loss: 0.5202898220486111\n",
      "Epoch: 1907 Training Loss: 0.2759229244656033 Test Loss: 0.51984765625\n",
      "Epoch: 1908 Training Loss: 0.2758408779568142 Test Loss: 0.5200536566840278\n",
      "Epoch: 1909 Training Loss: 0.2757265794542101 Test Loss: 0.5200814887152778\n",
      "Epoch: 1910 Training Loss: 0.27558130221896704 Test Loss: 0.5211468641493056\n",
      "Epoch: 1911 Training Loss: 0.27544704861111113 Test Loss: 0.5226964518229167\n",
      "Epoch: 1912 Training Loss: 0.27535302734375 Test Loss: 0.5232549370659723\n",
      "Epoch: 1913 Training Loss: 0.27526979234483506 Test Loss: 0.5215606011284722\n",
      "Epoch: 1914 Training Loss: 0.2751960991753472 Test Loss: 0.5200105794270833\n",
      "Epoch: 1915 Training Loss: 0.27510097079806856 Test Loss: 0.5198109266493055\n",
      "Epoch: 1916 Training Loss: 0.27500094265407987 Test Loss: 0.5196225043402778\n",
      "Epoch: 1917 Training Loss: 0.27488191392686634 Test Loss: 0.5198400607638889\n",
      "Epoch: 1918 Training Loss: 0.2747479688856337 Test Loss: 0.5212897135416666\n",
      "Epoch: 1919 Training Loss: 0.2746431138780382 Test Loss: 0.5226685112847222\n",
      "Epoch: 1920 Training Loss: 0.2745513203938802 Test Loss: 0.521896484375\n",
      "Epoch: 1921 Training Loss: 0.2744783257378472 Test Loss: 0.5199324001736111\n",
      "Epoch: 1922 Training Loss: 0.2743795623779297 Test Loss: 0.5194232855902777\n",
      "Epoch: 1923 Training Loss: 0.2742457021077474 Test Loss: 0.5194188368055556\n",
      "Epoch: 1924 Training Loss: 0.2741195237901476 Test Loss: 0.5197322591145833\n",
      "Epoch: 1925 Training Loss: 0.27400094434950084 Test Loss: 0.5210965711805555\n",
      "Epoch: 1926 Training Loss: 0.27389415486653645 Test Loss: 0.5225822482638889\n",
      "Epoch: 1927 Training Loss: 0.2738238864474826 Test Loss: 0.5225900607638889\n",
      "Epoch: 1928 Training Loss: 0.273773188273112 Test Loss: 0.5208135308159723\n",
      "Epoch: 1929 Training Loss: 0.2737245313856337 Test Loss: 0.5191208224826389\n",
      "Epoch: 1930 Training Loss: 0.27360733540852866 Test Loss: 0.5192152777777778\n",
      "Epoch: 1931 Training Loss: 0.273473637898763 Test Loss: 0.51890185546875\n",
      "Epoch: 1932 Training Loss: 0.2733576999240451 Test Loss: 0.5197339409722223\n",
      "Epoch: 1933 Training Loss: 0.2732574734157986 Test Loss: 0.52117919921875\n",
      "Epoch: 1934 Training Loss: 0.27320941162109375 Test Loss: 0.5216663411458333\n",
      "Epoch: 1935 Training Loss: 0.2731930474175347 Test Loss: 0.5196999782986111\n",
      "Epoch: 1936 Training Loss: 0.2731222720675998 Test Loss: 0.5185892469618055\n",
      "Epoch: 1937 Training Loss: 0.27300733778211805 Test Loss: 0.5186376953125\n",
      "Epoch: 1938 Training Loss: 0.2729189690483941 Test Loss: 0.5191437174479167\n",
      "Epoch: 1939 Training Loss: 0.27287245856391057 Test Loss: 0.5211946614583334\n",
      "Epoch: 1940 Training Loss: 0.27284595913357207 Test Loss: 0.5227886284722222\n",
      "Epoch: 1941 Training Loss: 0.27284194098578557 Test Loss: 0.5222484809027778\n",
      "Epoch: 1942 Training Loss: 0.2728355712890625 Test Loss: 0.5207471788194444\n",
      "Epoch: 1943 Training Loss: 0.27279193284776476 Test Loss: 0.5204759114583334\n",
      "Epoch: 1944 Training Loss: 0.27278209262424047 Test Loss: 0.5210572916666667\n",
      "Epoch: 1945 Training Loss: 0.2728448282877604 Test Loss: 0.5219914279513889\n",
      "Epoch: 1946 Training Loss: 0.273006352742513 Test Loss: 0.522171875\n",
      "Epoch: 1947 Training Loss: 0.27323509894476994 Test Loss: 0.5204873589409722\n",
      "Epoch: 1948 Training Loss: 0.273371587117513 Test Loss: 0.5188635525173612\n",
      "Epoch: 1949 Training Loss: 0.27326661173502603 Test Loss: 0.5188875868055556\n",
      "Epoch: 1950 Training Loss: 0.27310553487141925 Test Loss: 0.5191872829861112\n",
      "Epoch: 1951 Training Loss: 0.2730622846815321 Test Loss: 0.5199265407986111\n",
      "Epoch: 1952 Training Loss: 0.27295916917588975 Test Loss: 0.5206394856770833\n",
      "Epoch: 1953 Training Loss: 0.2725998959011502 Test Loss: 0.5212887369791667\n",
      "Epoch: 1954 Training Loss: 0.2721265004475911 Test Loss: 0.5210512152777778\n",
      "Epoch: 1955 Training Loss: 0.2717735205756293 Test Loss: 0.5209315321180555\n",
      "Epoch: 1956 Training Loss: 0.27157354566786024 Test Loss: 0.5202766927083333\n",
      "Epoch: 1957 Training Loss: 0.2714125501844618 Test Loss: 0.5193853081597222\n",
      "Epoch: 1958 Training Loss: 0.2712038794623481 Test Loss: 0.5186524522569445\n",
      "Epoch: 1959 Training Loss: 0.2710255330403646 Test Loss: 0.5187678493923611\n",
      "Epoch: 1960 Training Loss: 0.27090655517578127 Test Loss: 0.5190970052083334\n",
      "Epoch: 1961 Training Loss: 0.27078840976291235 Test Loss: 0.5191394314236111\n",
      "Epoch: 1962 Training Loss: 0.27062699042426214 Test Loss: 0.5195438368055556\n",
      "Epoch: 1963 Training Loss: 0.27042387220594616 Test Loss: 0.5204425455729167\n",
      "Epoch: 1964 Training Loss: 0.27025646464029945 Test Loss: 0.5210406358506945\n",
      "Epoch: 1965 Training Loss: 0.27017213779025606 Test Loss: 0.5207848849826389\n",
      "Epoch: 1966 Training Loss: 0.2701102803548177 Test Loss: 0.5196750217013889\n",
      "Epoch: 1967 Training Loss: 0.2700158420138889 Test Loss: 0.5187747395833333\n",
      "Epoch: 1968 Training Loss: 0.2698894822862413 Test Loss: 0.5189827473958334\n",
      "Epoch: 1969 Training Loss: 0.26979256693522136 Test Loss: 0.5194949544270834\n",
      "Epoch: 1970 Training Loss: 0.2697130449083116 Test Loss: 0.5196745876736111\n",
      "Epoch: 1971 Training Loss: 0.269645026312934 Test Loss: 0.5194775933159722\n",
      "Epoch: 1972 Training Loss: 0.26958889600965713 Test Loss: 0.519248046875\n",
      "Epoch: 1973 Training Loss: 0.26950499640570746 Test Loss: 0.5193731011284722\n",
      "Epoch: 1974 Training Loss: 0.2694083218044705 Test Loss: 0.5202732204861111\n",
      "Epoch: 1975 Training Loss: 0.26931347995334204 Test Loss: 0.5211758355034722\n",
      "Epoch: 1976 Training Loss: 0.2692415042453342 Test Loss: 0.5216205512152777\n",
      "Epoch: 1977 Training Loss: 0.26917388407389325 Test Loss: 0.5216852213541666\n",
      "Epoch: 1978 Training Loss: 0.26908394368489585 Test Loss: 0.5212403428819444\n",
      "Epoch: 1979 Training Loss: 0.26899092780219186 Test Loss: 0.5202854817708333\n",
      "Epoch: 1980 Training Loss: 0.2689090016682943 Test Loss: 0.5194293077256944\n",
      "Epoch: 1981 Training Loss: 0.2688378380669488 Test Loss: 0.5190148654513889\n",
      "Epoch: 1982 Training Loss: 0.2687722863091363 Test Loss: 0.5188449435763889\n",
      "Epoch: 1983 Training Loss: 0.26870836724175345 Test Loss: 0.5195321180555555\n",
      "Epoch: 1984 Training Loss: 0.2686285349527995 Test Loss: 0.521220703125\n",
      "Epoch: 1985 Training Loss: 0.2685362531873915 Test Loss: 0.5229270833333334\n",
      "Epoch: 1986 Training Loss: 0.26844827779134117 Test Loss: 0.5231924370659722\n",
      "Epoch: 1987 Training Loss: 0.26838735283745657 Test Loss: 0.5222182074652778\n",
      "Epoch: 1988 Training Loss: 0.2682895134819879 Test Loss: 0.5212061631944445\n",
      "Epoch: 1989 Training Loss: 0.26817545403374565 Test Loss: 0.5205272352430556\n",
      "Epoch: 1990 Training Loss: 0.26806439378526475 Test Loss: 0.5207445203993055\n",
      "Epoch: 1991 Training Loss: 0.26799854193793404 Test Loss: 0.52110693359375\n",
      "Epoch: 1992 Training Loss: 0.26798279317220053 Test Loss: 0.5200232204861112\n",
      "Epoch: 1993 Training Loss: 0.2680141042073568 Test Loss: 0.5191391059027778\n",
      "Epoch: 1994 Training Loss: 0.2679970974392361 Test Loss: 0.5194584418402778\n",
      "Epoch: 1995 Training Loss: 0.2679106021457248 Test Loss: 0.5214254014756945\n",
      "Epoch: 1996 Training Loss: 0.2678111402723524 Test Loss: 0.5240345052083333\n",
      "Epoch: 1997 Training Loss: 0.2677054731580946 Test Loss: 0.5249996744791666\n",
      "Epoch: 1998 Training Loss: 0.2676226569281684 Test Loss: 0.5235287000868055\n",
      "Epoch: 1999 Training Loss: 0.26754590691460506 Test Loss: 0.5220113932291667\n",
      "Epoch: 2000 Training Loss: 0.26744224887424045 Test Loss: 0.5215374891493055\n",
      "Epoch: 2001 Training Loss: 0.2673634609646267 Test Loss: 0.5214617513020834\n",
      "Epoch: 2002 Training Loss: 0.2673448791503906 Test Loss: 0.5213977864583333\n",
      "Epoch: 2003 Training Loss: 0.2673567538791233 Test Loss: 0.520857421875\n",
      "Epoch: 2004 Training Loss: 0.2673542039659288 Test Loss: 0.5209660373263889\n",
      "Epoch: 2005 Training Loss: 0.26734820726182723 Test Loss: 0.5217618272569444\n",
      "Epoch: 2006 Training Loss: 0.267381837632921 Test Loss: 0.5230166015625\n",
      "Epoch: 2007 Training Loss: 0.26747371249728735 Test Loss: 0.5237170138888889\n",
      "Epoch: 2008 Training Loss: 0.26757137722439234 Test Loss: 0.5246665581597222\n",
      "Epoch: 2009 Training Loss: 0.2677442915174696 Test Loss: 0.5261495768229166\n",
      "Epoch: 2010 Training Loss: 0.2679412655300564 Test Loss: 0.5252827690972223\n",
      "Epoch: 2011 Training Loss: 0.26813097466362845 Test Loss: 0.5238229709201389\n",
      "Epoch: 2012 Training Loss: 0.2682828657362196 Test Loss: 0.5232898763020833\n",
      "Epoch: 2013 Training Loss: 0.26833496941460505 Test Loss: 0.5253429904513889\n",
      "Epoch: 2014 Training Loss: 0.26851153733995226 Test Loss: 0.5272727864583333\n",
      "Epoch: 2015 Training Loss: 0.26878739251030814 Test Loss: 0.53063427734375\n",
      "Epoch: 2016 Training Loss: 0.26918811882866756 Test Loss: 0.5352439778645833\n",
      "Epoch: 2017 Training Loss: 0.269688235812717 Test Loss: 0.5403679470486111\n",
      "Epoch: 2018 Training Loss: 0.2702013176812066 Test Loss: 0.5438649631076389\n",
      "Epoch: 2019 Training Loss: 0.2708079223632813 Test Loss: 0.5465665690104167\n",
      "Epoch: 2020 Training Loss: 0.2716019507514106 Test Loss: 0.5503134223090278\n",
      "Epoch: 2021 Training Loss: 0.27295970492892796 Test Loss: 0.5561967230902778\n",
      "Epoch: 2022 Training Loss: 0.27538807678222654 Test Loss: 0.5537821180555556\n",
      "Epoch: 2023 Training Loss: 0.2792499270968967 Test Loss: 0.5429014214409722\n",
      "Epoch: 2024 Training Loss: 0.2832896948920356 Test Loss: 0.5332421332465278\n",
      "Epoch: 2025 Training Loss: 0.28318635559082034 Test Loss: 0.5313553059895834\n",
      "Epoch: 2026 Training Loss: 0.275855460272895 Test Loss: 0.5195582139756945\n",
      "Epoch: 2027 Training Loss: 0.2711601053873698 Test Loss: 0.5196462131076389\n",
      "Epoch: 2028 Training Loss: 0.2712122311062283 Test Loss: 0.5213060438368056\n",
      "Epoch: 2029 Training Loss: 0.2712980109320747 Test Loss: 0.5235695529513889\n",
      "Epoch: 2030 Training Loss: 0.27031325107150606 Test Loss: 0.5251332465277778\n",
      "Epoch: 2031 Training Loss: 0.268904781765408 Test Loss: 0.5241971571180556\n",
      "Epoch: 2032 Training Loss: 0.2679349178738064 Test Loss: 0.5229444444444444\n",
      "Epoch: 2033 Training Loss: 0.26755853610568575 Test Loss: 0.5225553927951389\n",
      "Epoch: 2034 Training Loss: 0.267490229288737 Test Loss: 0.5226994357638889\n",
      "Epoch: 2035 Training Loss: 0.26745969645182294 Test Loss: 0.5228219401041667\n",
      "Epoch: 2036 Training Loss: 0.2673595564100477 Test Loss: 0.5230323350694445\n",
      "Epoch: 2037 Training Loss: 0.2672210744222005 Test Loss: 0.5233279079861111\n",
      "Epoch: 2038 Training Loss: 0.26703028530544703 Test Loss: 0.5234933268229167\n",
      "Epoch: 2039 Training Loss: 0.26684165106879343 Test Loss: 0.5237158745659722\n",
      "Epoch: 2040 Training Loss: 0.2666559566921658 Test Loss: 0.52385791015625\n",
      "Epoch: 2041 Training Loss: 0.2664994930691189 Test Loss: 0.5241767578125\n",
      "Epoch: 2042 Training Loss: 0.26638592698838975 Test Loss: 0.5245345052083333\n",
      "Epoch: 2043 Training Loss: 0.26629587978786895 Test Loss: 0.5246987847222222\n",
      "Epoch: 2044 Training Loss: 0.2662046169704861 Test Loss: 0.5243880208333334\n",
      "Epoch: 2045 Training Loss: 0.2661049092610677 Test Loss: 0.5242723524305556\n",
      "Epoch: 2046 Training Loss: 0.2660270300971137 Test Loss: 0.5240041232638889\n",
      "Epoch: 2047 Training Loss: 0.2659557308620877 Test Loss: 0.5239252387152777\n",
      "Epoch: 2048 Training Loss: 0.26591635301378036 Test Loss: 0.5245726453993056\n",
      "Epoch: 2049 Training Loss: 0.2659007347954644 Test Loss: 0.524880859375\n",
      "Epoch: 2050 Training Loss: 0.26588638983832463 Test Loss: 0.5253938802083333\n",
      "Epoch: 2051 Training Loss: 0.265865481906467 Test Loss: 0.5254261067708333\n",
      "Epoch: 2052 Training Loss: 0.2658469458685981 Test Loss: 0.5255155707465278\n",
      "Epoch: 2053 Training Loss: 0.2658237169053819 Test Loss: 0.5252439236111112\n",
      "Epoch: 2054 Training Loss: 0.2657957187228733 Test Loss: 0.5248017578125\n",
      "Epoch: 2055 Training Loss: 0.26574286905924477 Test Loss: 0.5240449761284722\n",
      "Epoch: 2056 Training Loss: 0.2657004191080729 Test Loss: 0.5233417426215278\n",
      "Epoch: 2057 Training Loss: 0.265695792304145 Test Loss: 0.5218391927083333\n",
      "Epoch: 2058 Training Loss: 0.26573030090332034 Test Loss: 0.52071875\n",
      "Epoch: 2059 Training Loss: 0.26583793301052516 Test Loss: 0.5193282335069445\n",
      "Epoch: 2060 Training Loss: 0.26599186367458766 Test Loss: 0.5184128689236112\n",
      "Epoch: 2061 Training Loss: 0.2662054392496745 Test Loss: 0.51711767578125\n",
      "Epoch: 2062 Training Loss: 0.2664754384358724 Test Loss: 0.5162316623263888\n",
      "Epoch: 2063 Training Loss: 0.2668075375027127 Test Loss: 0.5149026692708333\n",
      "Epoch: 2064 Training Loss: 0.2671970960828993 Test Loss: 0.5141944444444444\n",
      "Epoch: 2065 Training Loss: 0.26761664496527776 Test Loss: 0.5133992513020833\n",
      "Epoch: 2066 Training Loss: 0.2680418955485026 Test Loss: 0.5131416015625\n",
      "Epoch: 2067 Training Loss: 0.2684495629204644 Test Loss: 0.5130633138020834\n",
      "Epoch: 2068 Training Loss: 0.2687833794487847 Test Loss: 0.51469482421875\n",
      "Epoch: 2069 Training Loss: 0.2690170118543837 Test Loss: 0.5166663953993056\n",
      "Epoch: 2070 Training Loss: 0.26919760131835935 Test Loss: 0.5191589626736111\n",
      "Epoch: 2071 Training Loss: 0.26927960883246527 Test Loss: 0.5206193576388889\n",
      "Epoch: 2072 Training Loss: 0.26928783671061196 Test Loss: 0.5209054904513889\n",
      "Epoch: 2073 Training Loss: 0.26917684597439234 Test Loss: 0.5206538628472223\n",
      "Epoch: 2074 Training Loss: 0.26884984503851994 Test Loss: 0.5198875868055556\n",
      "Epoch: 2075 Training Loss: 0.2683458726671007 Test Loss: 0.5190785047743055\n",
      "Epoch: 2076 Training Loss: 0.26772495693630644 Test Loss: 0.5185197482638889\n",
      "Epoch: 2077 Training Loss: 0.2671106889512804 Test Loss: 0.5181570095486111\n",
      "Epoch: 2078 Training Loss: 0.26659036424424915 Test Loss: 0.5180110134548611\n",
      "Epoch: 2079 Training Loss: 0.2661888292100694 Test Loss: 0.5180840386284722\n",
      "Epoch: 2080 Training Loss: 0.2659253200954861 Test Loss: 0.5179276801215278\n",
      "Epoch: 2081 Training Loss: 0.26577454969618053 Test Loss: 0.5178190104166667\n",
      "Epoch: 2082 Training Loss: 0.2657199435763889 Test Loss: 0.5174831814236112\n",
      "Epoch: 2083 Training Loss: 0.26575865681966143 Test Loss: 0.5170013563368056\n",
      "Epoch: 2084 Training Loss: 0.2658751983642578 Test Loss: 0.5166962890625\n",
      "Epoch: 2085 Training Loss: 0.26604042561848956 Test Loss: 0.5165979817708334\n",
      "Epoch: 2086 Training Loss: 0.2662383541531033 Test Loss: 0.5171576063368055\n",
      "Epoch: 2087 Training Loss: 0.26644945271809894 Test Loss: 0.5175827365451389\n",
      "Epoch: 2088 Training Loss: 0.26667925855848523 Test Loss: 0.5188224826388889\n",
      "Epoch: 2089 Training Loss: 0.26684066433376735 Test Loss: 0.5197819010416667\n",
      "Epoch: 2090 Training Loss: 0.26694981045193145 Test Loss: 0.5199889865451389\n",
      "Epoch: 2091 Training Loss: 0.2670454830593533 Test Loss: 0.5194084201388889\n",
      "Epoch: 2092 Training Loss: 0.2671933135986328 Test Loss: 0.5188843315972222\n",
      "Epoch: 2093 Training Loss: 0.26742851087782116 Test Loss: 0.5185949435763889\n",
      "Epoch: 2094 Training Loss: 0.2677294481065538 Test Loss: 0.5174071723090278\n",
      "Epoch: 2095 Training Loss: 0.26804293484157987 Test Loss: 0.5161072048611111\n",
      "Epoch: 2096 Training Loss: 0.26830033196343317 Test Loss: 0.5147298719618055\n",
      "Epoch: 2097 Training Loss: 0.26846546596950954 Test Loss: 0.5137149522569444\n",
      "Epoch: 2098 Training Loss: 0.26850435384114585 Test Loss: 0.5137030707465278\n",
      "Epoch: 2099 Training Loss: 0.2684495222303602 Test Loss: 0.5137645399305556\n",
      "Epoch: 2100 Training Loss: 0.2683261956108941 Test Loss: 0.5136325412326389\n",
      "Epoch: 2101 Training Loss: 0.26812119717068145 Test Loss: 0.5126623806423611\n",
      "Epoch: 2102 Training Loss: 0.26781915791829425 Test Loss: 0.5108386501736111\n",
      "Epoch: 2103 Training Loss: 0.2673500264485677 Test Loss: 0.5085380859375\n",
      "Epoch: 2104 Training Loss: 0.26670311482747394 Test Loss: 0.5066723090277778\n",
      "Epoch: 2105 Training Loss: 0.2659454566107856 Test Loss: 0.5056185438368056\n",
      "Epoch: 2106 Training Loss: 0.2651196560329861 Test Loss: 0.5044845920138888\n",
      "Epoch: 2107 Training Loss: 0.26432632615831164 Test Loss: 0.5035705837673611\n",
      "Epoch: 2108 Training Loss: 0.26360037909613715 Test Loss: 0.5031610243055555\n",
      "Epoch: 2109 Training Loss: 0.26297943623860676 Test Loss: 0.5027689344618056\n",
      "Epoch: 2110 Training Loss: 0.2624739023844401 Test Loss: 0.5027322591145833\n",
      "Epoch: 2111 Training Loss: 0.26208155653211807 Test Loss: 0.5028097873263889\n",
      "Epoch: 2112 Training Loss: 0.26178492058648 Test Loss: 0.5030029839409722\n",
      "Epoch: 2113 Training Loss: 0.26157369656032986 Test Loss: 0.5030321723090277\n",
      "Epoch: 2114 Training Loss: 0.2614342719184028 Test Loss: 0.5033160807291667\n",
      "Epoch: 2115 Training Loss: 0.2613589816623264 Test Loss: 0.5037630208333334\n",
      "Epoch: 2116 Training Loss: 0.2613316158718533 Test Loss: 0.5040263129340278\n",
      "Epoch: 2117 Training Loss: 0.2613592664930556 Test Loss: 0.5044887152777778\n",
      "Epoch: 2118 Training Loss: 0.2614151323106554 Test Loss: 0.5048742947048611\n",
      "Epoch: 2119 Training Loss: 0.2615061289469401 Test Loss: 0.5053075629340278\n",
      "Epoch: 2120 Training Loss: 0.2616127454969618 Test Loss: 0.506181640625\n",
      "Epoch: 2121 Training Loss: 0.26174485948350695 Test Loss: 0.5078524305555555\n",
      "Epoch: 2122 Training Loss: 0.26187823316786024 Test Loss: 0.5099887152777778\n",
      "Epoch: 2123 Training Loss: 0.2620244734022352 Test Loss: 0.5125108506944445\n",
      "Epoch: 2124 Training Loss: 0.26216630215115017 Test Loss: 0.5146257052951388\n",
      "Epoch: 2125 Training Loss: 0.2622940114339193 Test Loss: 0.5158245442708334\n",
      "Epoch: 2126 Training Loss: 0.26241266377766925 Test Loss: 0.5158866102430556\n",
      "Epoch: 2127 Training Loss: 0.2625141805013021 Test Loss: 0.5150007595486111\n",
      "Epoch: 2128 Training Loss: 0.2626250372992622 Test Loss: 0.5137932942708333\n",
      "Epoch: 2129 Training Loss: 0.2627513190375434 Test Loss: 0.5125817599826389\n",
      "Epoch: 2130 Training Loss: 0.26290926106770834 Test Loss: 0.5113099500868056\n",
      "Epoch: 2131 Training Loss: 0.26309545389811195 Test Loss: 0.5097017144097222\n",
      "Epoch: 2132 Training Loss: 0.263309083726671 Test Loss: 0.5082941080729166\n",
      "Epoch: 2133 Training Loss: 0.26349711269802517 Test Loss: 0.5067422960069444\n",
      "Epoch: 2134 Training Loss: 0.2636458214653863 Test Loss: 0.5053533528645834\n",
      "Epoch: 2135 Training Loss: 0.2637312757703993 Test Loss: 0.5041871202256945\n",
      "Epoch: 2136 Training Loss: 0.2637275153266059 Test Loss: 0.5033972439236111\n",
      "Epoch: 2137 Training Loss: 0.263656504313151 Test Loss: 0.503140625\n",
      "Epoch: 2138 Training Loss: 0.2635108913845486 Test Loss: 0.5032462022569445\n",
      "Epoch: 2139 Training Loss: 0.2632952151828342 Test Loss: 0.5035871310763889\n",
      "Epoch: 2140 Training Loss: 0.26302508375379774 Test Loss: 0.5039930555555555\n",
      "Epoch: 2141 Training Loss: 0.26271206665039065 Test Loss: 0.5044072265625\n",
      "Epoch: 2142 Training Loss: 0.2623960266113281 Test Loss: 0.5047098524305556\n",
      "Epoch: 2143 Training Loss: 0.262074464586046 Test Loss: 0.50483056640625\n",
      "Epoch: 2144 Training Loss: 0.2617616441514757 Test Loss: 0.5049328342013889\n",
      "Epoch: 2145 Training Loss: 0.2614899054633247 Test Loss: 0.5049431966145833\n",
      "Epoch: 2146 Training Loss: 0.261253914727105 Test Loss: 0.5049856228298611\n",
      "Epoch: 2147 Training Loss: 0.26107796563042535 Test Loss: 0.5051565212673611\n",
      "Epoch: 2148 Training Loss: 0.2609397311740451 Test Loss: 0.5053252495659722\n",
      "Epoch: 2149 Training Loss: 0.2608596428765191 Test Loss: 0.50570751953125\n",
      "Epoch: 2150 Training Loss: 0.2608198733859592 Test Loss: 0.5060088433159722\n",
      "Epoch: 2151 Training Loss: 0.2608246002197266 Test Loss: 0.5064148763020834\n",
      "Epoch: 2152 Training Loss: 0.2608939700656467 Test Loss: 0.5068100043402778\n",
      "Epoch: 2153 Training Loss: 0.2610136939154731 Test Loss: 0.5072579752604167\n",
      "Epoch: 2154 Training Loss: 0.26119010416666666 Test Loss: 0.5076143120659722\n",
      "Epoch: 2155 Training Loss: 0.26142100016276043 Test Loss: 0.5076658528645833\n",
      "Epoch: 2156 Training Loss: 0.2617106441921658 Test Loss: 0.5075035807291667\n",
      "Epoch: 2157 Training Loss: 0.2620636783175998 Test Loss: 0.5072462022569444\n",
      "Epoch: 2158 Training Loss: 0.2624845191107856 Test Loss: 0.5068333875868055\n",
      "Epoch: 2159 Training Loss: 0.2629963667127821 Test Loss: 0.5063932834201389\n",
      "Epoch: 2160 Training Loss: 0.2635960862901476 Test Loss: 0.5061452907986111\n",
      "Epoch: 2161 Training Loss: 0.26428260294596356 Test Loss: 0.5062578667534722\n",
      "Epoch: 2162 Training Loss: 0.26502266438802086 Test Loss: 0.5068638237847222\n",
      "Epoch: 2163 Training Loss: 0.26585807461208766 Test Loss: 0.5081986762152778\n",
      "Epoch: 2164 Training Loss: 0.26673821682400173 Test Loss: 0.5093123914930555\n",
      "Epoch: 2165 Training Loss: 0.2676660919189453 Test Loss: 0.5104544813368056\n",
      "Epoch: 2166 Training Loss: 0.2685953606499566 Test Loss: 0.5118395724826389\n",
      "Epoch: 2167 Training Loss: 0.2694343736436632 Test Loss: 0.5128662109375\n",
      "Epoch: 2168 Training Loss: 0.2701313629150391 Test Loss: 0.5133461371527778\n",
      "Epoch: 2169 Training Loss: 0.27055205790201825 Test Loss: 0.51263330078125\n",
      "Epoch: 2170 Training Loss: 0.2705762769911024 Test Loss: 0.5104108072916667\n",
      "Epoch: 2171 Training Loss: 0.27015284559461805 Test Loss: 0.5069172092013889\n",
      "Epoch: 2172 Training Loss: 0.2691399180094401 Test Loss: 0.5030384657118055\n",
      "Epoch: 2173 Training Loss: 0.2676117146809896 Test Loss: 0.4997695855034722\n",
      "Epoch: 2174 Training Loss: 0.26585877143012154 Test Loss: 0.49780360243055555\n",
      "Epoch: 2175 Training Loss: 0.26417349751790364 Test Loss: 0.4971189236111111\n",
      "Epoch: 2176 Training Loss: 0.2627250705295139 Test Loss: 0.4969662543402778\n",
      "Epoch: 2177 Training Loss: 0.26156715223524307 Test Loss: 0.49719384765625\n",
      "Epoch: 2178 Training Loss: 0.2606850195990668 Test Loss: 0.49750287543402777\n",
      "Epoch: 2179 Training Loss: 0.26002488708496097 Test Loss: 0.4976251627604167\n",
      "Epoch: 2180 Training Loss: 0.2595559692382812 Test Loss: 0.49778211805555556\n",
      "Epoch: 2181 Training Loss: 0.25923269314236114 Test Loss: 0.49766053602430554\n",
      "Epoch: 2182 Training Loss: 0.25902501763237845 Test Loss: 0.4975284288194444\n",
      "Epoch: 2183 Training Loss: 0.25891224331325957 Test Loss: 0.49724576822916666\n",
      "Epoch: 2184 Training Loss: 0.2588814900716146 Test Loss: 0.49686029730902775\n",
      "Epoch: 2185 Training Loss: 0.2589156104193793 Test Loss: 0.49640174696180556\n",
      "Epoch: 2186 Training Loss: 0.2590093485514323 Test Loss: 0.495732421875\n",
      "Epoch: 2187 Training Loss: 0.25915127902560764 Test Loss: 0.49510791015625\n",
      "Epoch: 2188 Training Loss: 0.2593301018608941 Test Loss: 0.49436653645833334\n",
      "Epoch: 2189 Training Loss: 0.2595347629123264 Test Loss: 0.49366444227430556\n",
      "Epoch: 2190 Training Loss: 0.25976358879937067 Test Loss: 0.49309922960069447\n",
      "Epoch: 2191 Training Loss: 0.2600196261935764 Test Loss: 0.4928601888020833\n",
      "Epoch: 2192 Training Loss: 0.2602695990668403 Test Loss: 0.4932614474826389\n",
      "Epoch: 2193 Training Loss: 0.2604695688883464 Test Loss: 0.49406635199652776\n",
      "Epoch: 2194 Training Loss: 0.2605867852105035 Test Loss: 0.49545893012152775\n",
      "Epoch: 2195 Training Loss: 0.26061574978298613 Test Loss: 0.4972916666666667\n",
      "Epoch: 2196 Training Loss: 0.2605198211669922 Test Loss: 0.4989638671875\n",
      "Epoch: 2197 Training Loss: 0.2602915581597222 Test Loss: 0.5000175238715278\n",
      "Epoch: 2198 Training Loss: 0.259952389187283 Test Loss: 0.5001980251736111\n",
      "Epoch: 2199 Training Loss: 0.259512691921658 Test Loss: 0.49981846788194445\n",
      "Epoch: 2200 Training Loss: 0.2589614969889323 Test Loss: 0.4990044487847222\n",
      "Epoch: 2201 Training Loss: 0.25833978610568575 Test Loss: 0.49797059461805554\n",
      "Epoch: 2202 Training Loss: 0.2576835361056858 Test Loss: 0.49719379340277775\n",
      "Epoch: 2203 Training Loss: 0.25705441284179686 Test Loss: 0.49670062934027776\n",
      "Epoch: 2204 Training Loss: 0.2564729275173611 Test Loss: 0.49650927734375\n",
      "Epoch: 2205 Training Loss: 0.25597272067599824 Test Loss: 0.49647195095486113\n",
      "Epoch: 2206 Training Loss: 0.2555545722113715 Test Loss: 0.4965495876736111\n",
      "Epoch: 2207 Training Loss: 0.25521922641330297 Test Loss: 0.4968974609375\n",
      "Epoch: 2208 Training Loss: 0.2549637485080295 Test Loss: 0.49715006510416665\n",
      "Epoch: 2209 Training Loss: 0.2547704298231337 Test Loss: 0.49753857421875\n",
      "Epoch: 2210 Training Loss: 0.2546281958685981 Test Loss: 0.4978773328993056\n",
      "Epoch: 2211 Training Loss: 0.25452125549316407 Test Loss: 0.4982995876736111\n",
      "Epoch: 2212 Training Loss: 0.25445427958170574 Test Loss: 0.49872287326388887\n",
      "Epoch: 2213 Training Loss: 0.25442330084906684 Test Loss: 0.4991929796006944\n",
      "Epoch: 2214 Training Loss: 0.2544275563557943 Test Loss: 0.4997648654513889\n",
      "Epoch: 2215 Training Loss: 0.2544516109890408 Test Loss: 0.5001940646701389\n",
      "Epoch: 2216 Training Loss: 0.254495357937283 Test Loss: 0.5006650390625\n",
      "Epoch: 2217 Training Loss: 0.25457972378200955 Test Loss: 0.5012784288194444\n",
      "Epoch: 2218 Training Loss: 0.25469598727756076 Test Loss: 0.5018233506944444\n",
      "Epoch: 2219 Training Loss: 0.25484288703070745 Test Loss: 0.5025339626736111\n",
      "Epoch: 2220 Training Loss: 0.2550275336371528 Test Loss: 0.5032302517361111\n",
      "Epoch: 2221 Training Loss: 0.25525809393988713 Test Loss: 0.5039259982638888\n",
      "Epoch: 2222 Training Loss: 0.25555339728461374 Test Loss: 0.50470458984375\n",
      "Epoch: 2223 Training Loss: 0.2559192420111762 Test Loss: 0.5053806423611111\n",
      "Epoch: 2224 Training Loss: 0.25637689208984377 Test Loss: 0.5060983072916667\n",
      "Epoch: 2225 Training Loss: 0.2569378882514106 Test Loss: 0.50676611328125\n",
      "Epoch: 2226 Training Loss: 0.25762855360243053 Test Loss: 0.5071982964409723\n",
      "Epoch: 2227 Training Loss: 0.2584962649875217 Test Loss: 0.5070401475694445\n",
      "Epoch: 2228 Training Loss: 0.25955623881022133 Test Loss: 0.5062100151909722\n",
      "Epoch: 2229 Training Loss: 0.2608091176350911 Test Loss: 0.5044191080729167\n",
      "Epoch: 2230 Training Loss: 0.26215873209635415 Test Loss: 0.5016810980902778\n",
      "Epoch: 2231 Training Loss: 0.26341714138454864 Test Loss: 0.4990270182291667\n",
      "Epoch: 2232 Training Loss: 0.2642501135932075 Test Loss: 0.4977058376736111\n",
      "Epoch: 2233 Training Loss: 0.26429249233669705 Test Loss: 0.49785731336805555\n",
      "Epoch: 2234 Training Loss: 0.26343802388509113 Test Loss: 0.49798594835069443\n",
      "Epoch: 2235 Training Loss: 0.26194897291395397 Test Loss: 0.49801692708333334\n",
      "Epoch: 2236 Training Loss: 0.2602035420735677 Test Loss: 0.49749430338541667\n",
      "Epoch: 2237 Training Loss: 0.2584567599826389 Test Loss: 0.49674766710069446\n",
      "Epoch: 2238 Training Loss: 0.2568839060465495 Test Loss: 0.49606439887152776\n",
      "Epoch: 2239 Training Loss: 0.2555740458170573 Test Loss: 0.49552815755208335\n",
      "Epoch: 2240 Training Loss: 0.25454671393500433 Test Loss: 0.4949912109375\n",
      "Epoch: 2241 Training Loss: 0.25376654222276473 Test Loss: 0.4947657877604167\n",
      "Epoch: 2242 Training Loss: 0.2531725429958767 Test Loss: 0.4947478298611111\n",
      "Epoch: 2243 Training Loss: 0.25271329243977864 Test Loss: 0.4947478841145833\n",
      "Epoch: 2244 Training Loss: 0.25235242886013454 Test Loss: 0.49476128472222225\n",
      "Epoch: 2245 Training Loss: 0.2520598670111762 Test Loss: 0.4948218858506944\n",
      "Epoch: 2246 Training Loss: 0.2518193817138672 Test Loss: 0.49494428168402776\n",
      "Epoch: 2247 Training Loss: 0.2516114281548394 Test Loss: 0.49505479600694446\n",
      "Epoch: 2248 Training Loss: 0.25142567613389755 Test Loss: 0.4952151692708333\n",
      "Epoch: 2249 Training Loss: 0.25125833977593315 Test Loss: 0.4953152126736111\n",
      "Epoch: 2250 Training Loss: 0.25110416327582463 Test Loss: 0.4953816731770833\n",
      "Epoch: 2251 Training Loss: 0.2509601253933377 Test Loss: 0.4955006510416667\n",
      "Epoch: 2252 Training Loss: 0.2508302544487847 Test Loss: 0.49560948350694445\n",
      "Epoch: 2253 Training Loss: 0.2507055274115668 Test Loss: 0.49570458984375\n",
      "Epoch: 2254 Training Loss: 0.25058751763237846 Test Loss: 0.49578857421875\n",
      "Epoch: 2255 Training Loss: 0.25047332763671876 Test Loss: 0.4958271484375\n",
      "Epoch: 2256 Training Loss: 0.2503627132839627 Test Loss: 0.49595735677083336\n",
      "Epoch: 2257 Training Loss: 0.25025877210828995 Test Loss: 0.4960597330729167\n",
      "Epoch: 2258 Training Loss: 0.25015430535210503 Test Loss: 0.4960993923611111\n",
      "Epoch: 2259 Training Loss: 0.250062013414171 Test Loss: 0.4961462131076389\n",
      "Epoch: 2260 Training Loss: 0.24996517096625434 Test Loss: 0.49631640625\n",
      "Epoch: 2261 Training Loss: 0.24986481051974826 Test Loss: 0.49638004557291665\n",
      "Epoch: 2262 Training Loss: 0.24977691650390624 Test Loss: 0.4964403754340278\n",
      "Epoch: 2263 Training Loss: 0.2496925048828125 Test Loss: 0.4964296332465278\n",
      "Epoch: 2264 Training Loss: 0.24959762403700086 Test Loss: 0.4966583658854167\n",
      "Epoch: 2265 Training Loss: 0.24950689697265624 Test Loss: 0.4966279296875\n",
      "Epoch: 2266 Training Loss: 0.2494282430013021 Test Loss: 0.4966232096354167\n",
      "Epoch: 2267 Training Loss: 0.24933812120225696 Test Loss: 0.49680740017361114\n",
      "Epoch: 2268 Training Loss: 0.2492491726345486 Test Loss: 0.49688557942708333\n",
      "Epoch: 2269 Training Loss: 0.24917149013943143 Test Loss: 0.49672276475694443\n",
      "Epoch: 2270 Training Loss: 0.24909512159559463 Test Loss: 0.49678162977430557\n",
      "Epoch: 2271 Training Loss: 0.24900130208333332 Test Loss: 0.4970370551215278\n",
      "Epoch: 2272 Training Loss: 0.24891687181260852 Test Loss: 0.4970377061631944\n",
      "Epoch: 2273 Training Loss: 0.24884403652615017 Test Loss: 0.49695057508680557\n",
      "Epoch: 2274 Training Loss: 0.24877097574869791 Test Loss: 0.4969782986111111\n",
      "Epoch: 2275 Training Loss: 0.24868631998697915 Test Loss: 0.4972096354166667\n",
      "Epoch: 2276 Training Loss: 0.24859797668457032 Test Loss: 0.49731846788194445\n",
      "Epoch: 2277 Training Loss: 0.24852808464898005 Test Loss: 0.49718543836805557\n",
      "Epoch: 2278 Training Loss: 0.24846265157063802 Test Loss: 0.4971650390625\n",
      "Epoch: 2279 Training Loss: 0.24838489956325954 Test Loss: 0.49733642578125\n",
      "Epoch: 2280 Training Loss: 0.24828744506835937 Test Loss: 0.49753065321180556\n",
      "Epoch: 2281 Training Loss: 0.24821577623155383 Test Loss: 0.4974574110243056\n",
      "Epoch: 2282 Training Loss: 0.24814829847547742 Test Loss: 0.49732996961805553\n",
      "Epoch: 2283 Training Loss: 0.24808316718207465 Test Loss: 0.49735118272569445\n",
      "Epoch: 2284 Training Loss: 0.2480031297471788 Test Loss: 0.4974679904513889\n",
      "Epoch: 2285 Training Loss: 0.24791307406955296 Test Loss: 0.4977621527777778\n",
      "Epoch: 2286 Training Loss: 0.24783973185221353 Test Loss: 0.4977682834201389\n",
      "Epoch: 2287 Training Loss: 0.24777825927734376 Test Loss: 0.4974958767361111\n",
      "Epoch: 2288 Training Loss: 0.2477177480061849 Test Loss: 0.4974242621527778\n",
      "Epoch: 2289 Training Loss: 0.24764596388075086 Test Loss: 0.4974489474826389\n",
      "Epoch: 2290 Training Loss: 0.2475521748860677 Test Loss: 0.49798014322916667\n",
      "Epoch: 2291 Training Loss: 0.24746972825792102 Test Loss: 0.4980710177951389\n",
      "Epoch: 2292 Training Loss: 0.24740980529785156 Test Loss: 0.49783409288194447\n",
      "Epoch: 2293 Training Loss: 0.24735325961642796 Test Loss: 0.4975209418402778\n",
      "Epoch: 2294 Training Loss: 0.24729124111599393 Test Loss: 0.49752983940972223\n",
      "Epoch: 2295 Training Loss: 0.24721381462944877 Test Loss: 0.49775358072916664\n",
      "Epoch: 2296 Training Loss: 0.2471179419623481 Test Loss: 0.4982842881944444\n",
      "Epoch: 2297 Training Loss: 0.24704383511013456 Test Loss: 0.49839252387152777\n",
      "Epoch: 2298 Training Loss: 0.2469955834282769 Test Loss: 0.4979268120659722\n",
      "Epoch: 2299 Training Loss: 0.24693637084960937 Test Loss: 0.49758604600694445\n",
      "Epoch: 2300 Training Loss: 0.24687528483072918 Test Loss: 0.4976349826388889\n",
      "Epoch: 2301 Training Loss: 0.2468026411268446 Test Loss: 0.49776361762152777\n",
      "Epoch: 2302 Training Loss: 0.24670020039876303 Test Loss: 0.4983401150173611\n",
      "Epoch: 2303 Training Loss: 0.2466227298312717 Test Loss: 0.4986203342013889\n",
      "Epoch: 2304 Training Loss: 0.24656682501898872 Test Loss: 0.49855528428819446\n",
      "Epoch: 2305 Training Loss: 0.24651841057671442 Test Loss: 0.4978142361111111\n",
      "Epoch: 2306 Training Loss: 0.24645812140570747 Test Loss: 0.49765087890625\n",
      "Epoch: 2307 Training Loss: 0.2464024454752604 Test Loss: 0.49766362847222223\n",
      "Epoch: 2308 Training Loss: 0.24632581753200955 Test Loss: 0.4978033854166667\n",
      "Epoch: 2309 Training Loss: 0.2462315707736545 Test Loss: 0.49856624348958334\n",
      "Epoch: 2310 Training Loss: 0.24615074835883247 Test Loss: 0.4990895724826389\n",
      "Epoch: 2311 Training Loss: 0.2460985565185547 Test Loss: 0.4990862630208333\n",
      "Epoch: 2312 Training Loss: 0.2460559777153863 Test Loss: 0.49818473307291666\n",
      "Epoch: 2313 Training Loss: 0.2459980214436849 Test Loss: 0.49770149739583336\n",
      "Epoch: 2314 Training Loss: 0.24594686381022135 Test Loss: 0.49771321614583336\n",
      "Epoch: 2315 Training Loss: 0.24587730238172742 Test Loss: 0.4979585503472222\n",
      "Epoch: 2316 Training Loss: 0.2457800513373481 Test Loss: 0.4982606879340278\n",
      "Epoch: 2317 Training Loss: 0.24568734741210937 Test Loss: 0.4991609157986111\n",
      "Epoch: 2318 Training Loss: 0.2456189405653212 Test Loss: 0.49959396701388886\n",
      "Epoch: 2319 Training Loss: 0.24557609388563367 Test Loss: 0.4996215277777778\n",
      "Epoch: 2320 Training Loss: 0.24555259704589844 Test Loss: 0.49832145182291665\n",
      "Epoch: 2321 Training Loss: 0.2454966345893012 Test Loss: 0.49772802734375\n",
      "Epoch: 2322 Training Loss: 0.24544470893012152 Test Loss: 0.49782769097222224\n",
      "Epoch: 2323 Training Loss: 0.2453666483561198 Test Loss: 0.4980259874131944\n",
      "Epoch: 2324 Training Loss: 0.24526769341362847 Test Loss: 0.4984169921875\n",
      "Epoch: 2325 Training Loss: 0.2451679467095269 Test Loss: 0.4994739040798611\n",
      "Epoch: 2326 Training Loss: 0.24509637790256075 Test Loss: 0.5001516927083334\n",
      "Epoch: 2327 Training Loss: 0.24505138821072048 Test Loss: 0.5005930447048611\n",
      "Epoch: 2328 Training Loss: 0.2450446302625868 Test Loss: 0.4990726996527778\n",
      "Epoch: 2329 Training Loss: 0.24499128384060329 Test Loss: 0.4979585503472222\n",
      "Epoch: 2330 Training Loss: 0.24494741990831162 Test Loss: 0.49790152994791664\n",
      "Epoch: 2331 Training Loss: 0.2448891160753038 Test Loss: 0.49808192274305557\n",
      "Epoch: 2332 Training Loss: 0.2447856682671441 Test Loss: 0.4985474175347222\n",
      "Epoch: 2333 Training Loss: 0.24467904493543838 Test Loss: 0.49930246310763887\n",
      "Epoch: 2334 Training Loss: 0.24457983567979602 Test Loss: 0.5005425347222222\n",
      "Epoch: 2335 Training Loss: 0.24452863226996527 Test Loss: 0.5013078884548611\n",
      "Epoch: 2336 Training Loss: 0.2445209757486979 Test Loss: 0.5004326171875\n",
      "Epoch: 2337 Training Loss: 0.24449983893500435 Test Loss: 0.49836518012152775\n",
      "Epoch: 2338 Training Loss: 0.2444424285888672 Test Loss: 0.4979990234375\n",
      "Epoch: 2339 Training Loss: 0.24441063266330296 Test Loss: 0.49815695529513887\n",
      "Epoch: 2340 Training Loss: 0.2443273637559679 Test Loss: 0.49868798828125\n",
      "Epoch: 2341 Training Loss: 0.24420874532063802 Test Loss: 0.4994384765625\n",
      "Epoch: 2342 Training Loss: 0.24409071011013456 Test Loss: 0.5007235243055556\n",
      "Epoch: 2343 Training Loss: 0.2440142856174045 Test Loss: 0.5022681749131944\n",
      "Epoch: 2344 Training Loss: 0.24399534606933593 Test Loss: 0.5016328667534722\n",
      "Epoch: 2345 Training Loss: 0.2439750230577257 Test Loss: 0.4990712890625\n",
      "Epoch: 2346 Training Loss: 0.24392109849717883 Test Loss: 0.49799403211805554\n",
      "Epoch: 2347 Training Loss: 0.24388782925075955 Test Loss: 0.49811187065972223\n",
      "Epoch: 2348 Training Loss: 0.24381933932834202 Test Loss: 0.49850705295138886\n",
      "Epoch: 2349 Training Loss: 0.24370514763726128 Test Loss: 0.4995606011284722\n",
      "Epoch: 2350 Training Loss: 0.24358314005533854 Test Loss: 0.5009527452256944\n",
      "Epoch: 2351 Training Loss: 0.24347926330566405 Test Loss: 0.5024791666666667\n",
      "Epoch: 2352 Training Loss: 0.24342822943793402 Test Loss: 0.5025694444444444\n",
      "Epoch: 2353 Training Loss: 0.2434048784044054 Test Loss: 0.5006535915798611\n",
      "Epoch: 2354 Training Loss: 0.24335608757866753 Test Loss: 0.4987151692708333\n",
      "Epoch: 2355 Training Loss: 0.24332493421766493 Test Loss: 0.4982509223090278\n",
      "Epoch: 2356 Training Loss: 0.24330411953396267 Test Loss: 0.4983414713541667\n",
      "Epoch: 2357 Training Loss: 0.24323525831434462 Test Loss: 0.4988008897569444\n",
      "Epoch: 2358 Training Loss: 0.24312505425347222 Test Loss: 0.5000568576388889\n",
      "Epoch: 2359 Training Loss: 0.24301425340440538 Test Loss: 0.5019484049479167\n",
      "Epoch: 2360 Training Loss: 0.24293768649631076 Test Loss: 0.5040628797743055\n",
      "Epoch: 2361 Training Loss: 0.24292144097222224 Test Loss: 0.5029684787326388\n",
      "Epoch: 2362 Training Loss: 0.24289225260416666 Test Loss: 0.5000250108506944\n",
      "Epoch: 2363 Training Loss: 0.2428278588189019 Test Loss: 0.4990055338541667\n",
      "Epoch: 2364 Training Loss: 0.24281946818033853 Test Loss: 0.49909016927083333\n",
      "Epoch: 2365 Training Loss: 0.24273169623480903 Test Loss: 0.49919634331597224\n",
      "Epoch: 2366 Training Loss: 0.2426087883843316 Test Loss: 0.5007213541666666\n",
      "Epoch: 2367 Training Loss: 0.242498779296875 Test Loss: 0.5023658854166667\n",
      "Epoch: 2368 Training Loss: 0.2424200981987847 Test Loss: 0.5042471788194445\n",
      "Epoch: 2369 Training Loss: 0.24240291341145834 Test Loss: 0.5025098741319445\n",
      "Epoch: 2370 Training Loss: 0.242368891398112 Test Loss: 0.4996931423611111\n",
      "Epoch: 2371 Training Loss: 0.2423169691297743 Test Loss: 0.4989862738715278\n",
      "Epoch: 2372 Training Loss: 0.2423043721516927 Test Loss: 0.49893174913194444\n",
      "Epoch: 2373 Training Loss: 0.24221087137858072 Test Loss: 0.4997705078125\n",
      "Epoch: 2374 Training Loss: 0.2420925004747179 Test Loss: 0.5016423068576389\n",
      "Epoch: 2375 Training Loss: 0.24199443223741318 Test Loss: 0.5037342664930555\n",
      "Epoch: 2376 Training Loss: 0.24192574055989582 Test Loss: 0.5039157986111111\n",
      "Epoch: 2377 Training Loss: 0.24189100646972655 Test Loss: 0.5017247178819444\n",
      "Epoch: 2378 Training Loss: 0.24183532206217448 Test Loss: 0.5000576171875\n",
      "Epoch: 2379 Training Loss: 0.2418121320936415 Test Loss: 0.49985671657986114\n",
      "Epoch: 2380 Training Loss: 0.24179099867078993 Test Loss: 0.4999463975694444\n",
      "Epoch: 2381 Training Loss: 0.24168851386176216 Test Loss: 0.5011961263020833\n",
      "Epoch: 2382 Training Loss: 0.24156949700249566 Test Loss: 0.5033404947916666\n",
      "Epoch: 2383 Training Loss: 0.24147589619954427 Test Loss: 0.5051677517361111\n",
      "Epoch: 2384 Training Loss: 0.241433839586046 Test Loss: 0.504345703125\n",
      "Epoch: 2385 Training Loss: 0.24139337327745225 Test Loss: 0.501919921875\n",
      "Epoch: 2386 Training Loss: 0.24134008110894098 Test Loss: 0.5003611653645833\n",
      "Epoch: 2387 Training Loss: 0.24132220967610676 Test Loss: 0.5000467664930556\n",
      "Epoch: 2388 Training Loss: 0.24127838304307725 Test Loss: 0.49977332899305554\n",
      "Epoch: 2389 Training Loss: 0.24117274136013456 Test Loss: 0.5011787109375\n",
      "Epoch: 2390 Training Loss: 0.24109012858072917 Test Loss: 0.5034731987847222\n",
      "Epoch: 2391 Training Loss: 0.24103511725531684 Test Loss: 0.50455078125\n",
      "Epoch: 2392 Training Loss: 0.24101793585883247 Test Loss: 0.50229248046875\n",
      "Epoch: 2393 Training Loss: 0.24097108968098957 Test Loss: 0.49954340277777776\n",
      "Epoch: 2394 Training Loss: 0.24095128377278646 Test Loss: 0.49917041015625\n",
      "Epoch: 2395 Training Loss: 0.2409614952935113 Test Loss: 0.49903233506944444\n",
      "Epoch: 2396 Training Loss: 0.24090487331814237 Test Loss: 0.5004426540798611\n",
      "Epoch: 2397 Training Loss: 0.24082269287109376 Test Loss: 0.5028522677951389\n",
      "Epoch: 2398 Training Loss: 0.24076441955566405 Test Loss: 0.5037295464409722\n",
      "Epoch: 2399 Training Loss: 0.240740236070421 Test Loss: 0.5022570529513889\n",
      "Epoch: 2400 Training Loss: 0.24071597798665365 Test Loss: 0.5006392144097223\n",
      "Epoch: 2401 Training Loss: 0.24077428181966146 Test Loss: 0.5006941731770833\n",
      "Epoch: 2402 Training Loss: 0.2408329823811849 Test Loss: 0.5017510308159723\n",
      "Epoch: 2403 Training Loss: 0.24078839111328126 Test Loss: 0.5043856879340278\n",
      "Epoch: 2404 Training Loss: 0.24076257663302952 Test Loss: 0.5072817925347223\n",
      "Epoch: 2405 Training Loss: 0.2407567392985026 Test Loss: 0.5081974826388889\n",
      "Epoch: 2406 Training Loss: 0.24077456834581162 Test Loss: 0.5074089626736111\n",
      "Epoch: 2407 Training Loss: 0.2408095228407118 Test Loss: 0.5070025499131945\n",
      "Epoch: 2408 Training Loss: 0.24086104668511285 Test Loss: 0.5069806857638889\n",
      "Epoch: 2409 Training Loss: 0.24095247395833333 Test Loss: 0.5073056098090277\n",
      "Epoch: 2410 Training Loss: 0.24109342108832466 Test Loss: 0.5065784505208333\n",
      "Epoch: 2411 Training Loss: 0.241257320827908 Test Loss: 0.50275732421875\n",
      "Epoch: 2412 Training Loss: 0.24131837293836805 Test Loss: 0.4991826171875\n",
      "Epoch: 2413 Training Loss: 0.24126695590549044 Test Loss: 0.4982392578125\n",
      "Epoch: 2414 Training Loss: 0.2411418253580729 Test Loss: 0.49860074869791665\n",
      "Epoch: 2415 Training Loss: 0.24105654568142362 Test Loss: 0.49909646267361113\n",
      "Epoch: 2416 Training Loss: 0.24102703348795573 Test Loss: 0.5002242838541666\n",
      "Epoch: 2417 Training Loss: 0.24094991387261286 Test Loss: 0.5026477322048611\n",
      "Epoch: 2418 Training Loss: 0.24076707797580296 Test Loss: 0.5062546115451388\n",
      "Epoch: 2419 Training Loss: 0.24055559794108072 Test Loss: 0.508494140625\n",
      "Epoch: 2420 Training Loss: 0.24037877061631943 Test Loss: 0.5079986979166666\n",
      "Epoch: 2421 Training Loss: 0.24023751831054688 Test Loss: 0.5052756076388889\n",
      "Epoch: 2422 Training Loss: 0.24013535563151042 Test Loss: 0.5028230794270834\n",
      "Epoch: 2423 Training Loss: 0.2400458289252387 Test Loss: 0.5014380425347222\n",
      "Epoch: 2424 Training Loss: 0.2399518330891927 Test Loss: 0.5006552191840278\n",
      "Epoch: 2425 Training Loss: 0.23984848022460936 Test Loss: 0.5000053168402778\n",
      "Epoch: 2426 Training Loss: 0.23978468492296007 Test Loss: 0.4993444552951389\n",
      "Epoch: 2427 Training Loss: 0.23974826219346787 Test Loss: 0.4993102756076389\n",
      "Epoch: 2428 Training Loss: 0.23974210781521268 Test Loss: 0.49995952690972223\n",
      "Epoch: 2429 Training Loss: 0.23972447543674044 Test Loss: 0.50142431640625\n",
      "Epoch: 2430 Training Loss: 0.2397275390625 Test Loss: 0.5032135959201389\n",
      "Epoch: 2431 Training Loss: 0.23973225572374132 Test Loss: 0.5047979058159722\n",
      "Epoch: 2432 Training Loss: 0.23971771579318576 Test Loss: 0.5053177083333333\n",
      "Epoch: 2433 Training Loss: 0.23971448262532552 Test Loss: 0.5048780381944444\n",
      "Epoch: 2434 Training Loss: 0.23971970791286892 Test Loss: 0.5037790256076389\n",
      "Epoch: 2435 Training Loss: 0.23973194546169704 Test Loss: 0.5029295247395833\n",
      "Epoch: 2436 Training Loss: 0.23977165222167968 Test Loss: 0.5023940972222222\n",
      "Epoch: 2437 Training Loss: 0.239822024875217 Test Loss: 0.5020687934027778\n",
      "Epoch: 2438 Training Loss: 0.23989754062228733 Test Loss: 0.5018146701388889\n",
      "Epoch: 2439 Training Loss: 0.24000986735026042 Test Loss: 0.5017687717013889\n",
      "Epoch: 2440 Training Loss: 0.24015601603190104 Test Loss: 0.5016637912326389\n",
      "Epoch: 2441 Training Loss: 0.2403229285346137 Test Loss: 0.5020011935763888\n",
      "Epoch: 2442 Training Loss: 0.2404897206624349 Test Loss: 0.5027090928819444\n",
      "Epoch: 2443 Training Loss: 0.24068337673611112 Test Loss: 0.5045744357638889\n",
      "Epoch: 2444 Training Loss: 0.24087651740180122 Test Loss: 0.5073107638888888\n",
      "Epoch: 2445 Training Loss: 0.24103028530544704 Test Loss: 0.5081959635416666\n",
      "Epoch: 2446 Training Loss: 0.2412149692111545 Test Loss: 0.5071878255208333\n",
      "Epoch: 2447 Training Loss: 0.2413901112874349 Test Loss: 0.5070319552951389\n",
      "Epoch: 2448 Training Loss: 0.2415519324408637 Test Loss: 0.5073973524305555\n",
      "Epoch: 2449 Training Loss: 0.2417397206624349 Test Loss: 0.50845654296875\n",
      "Epoch: 2450 Training Loss: 0.24202350192599825 Test Loss: 0.50911083984375\n",
      "Epoch: 2451 Training Loss: 0.24243748982747396 Test Loss: 0.5101450737847222\n",
      "Epoch: 2452 Training Loss: 0.24297058953179254 Test Loss: 0.5120144856770833\n",
      "Epoch: 2453 Training Loss: 0.2436119876437717 Test Loss: 0.5146575520833333\n",
      "Epoch: 2454 Training Loss: 0.2443873748779297 Test Loss: 0.5167906901041667\n",
      "Epoch: 2455 Training Loss: 0.24536964586046006 Test Loss: 0.5193675672743056\n",
      "Epoch: 2456 Training Loss: 0.24650888909233942 Test Loss: 0.5220909288194444\n",
      "Epoch: 2457 Training Loss: 0.24789597913953992 Test Loss: 0.5246755099826389\n",
      "Epoch: 2458 Training Loss: 0.24963720024956598 Test Loss: 0.5270925564236111\n",
      "Epoch: 2459 Training Loss: 0.25179847208658857 Test Loss: 0.5290583767361111\n",
      "Epoch: 2460 Training Loss: 0.2543558298746745 Test Loss: 0.5308313802083333\n",
      "Epoch: 2461 Training Loss: 0.2571447330050998 Test Loss: 0.5323319769965278\n",
      "Epoch: 2462 Training Loss: 0.2598823445638021 Test Loss: 0.5326569010416666\n",
      "Epoch: 2463 Training Loss: 0.2620195600721571 Test Loss: 0.5308859049479167\n",
      "Epoch: 2464 Training Loss: 0.2626711290147569 Test Loss: 0.5224932183159722\n",
      "Epoch: 2465 Training Loss: 0.2611265089246962 Test Loss: 0.5127332899305556\n",
      "Epoch: 2466 Training Loss: 0.25838002861870657 Test Loss: 0.5074142795138888\n",
      "Epoch: 2467 Training Loss: 0.25555270894368487 Test Loss: 0.5037518446180556\n",
      "Epoch: 2468 Training Loss: 0.25316731770833334 Test Loss: 0.5019794379340278\n",
      "Epoch: 2469 Training Loss: 0.25136309983995225 Test Loss: 0.498326171875\n",
      "Epoch: 2470 Training Loss: 0.2501256527370877 Test Loss: 0.4980967881944444\n",
      "Epoch: 2471 Training Loss: 0.24940956963433158 Test Loss: 0.49786349826388887\n",
      "Epoch: 2472 Training Loss: 0.24908447265625 Test Loss: 0.4973693033854167\n",
      "Epoch: 2473 Training Loss: 0.24885677761501737 Test Loss: 0.49637245008680553\n",
      "Epoch: 2474 Training Loss: 0.24870042080349392 Test Loss: 0.49355995008680553\n",
      "Epoch: 2475 Training Loss: 0.24865740627712674 Test Loss: 0.49254524739583333\n",
      "Epoch: 2476 Training Loss: 0.2491878407796224 Test Loss: 0.4909538302951389\n",
      "Epoch: 2477 Training Loss: 0.250051517062717 Test Loss: 0.5022870551215278\n",
      "Epoch: 2478 Training Loss: 0.25048865932888453 Test Loss: 0.5260018446180555\n",
      "Epoch: 2479 Training Loss: 0.24943470255533853 Test Loss: 0.5152302517361111\n",
      "Epoch: 2480 Training Loss: 0.2460848609076606 Test Loss: 0.4992405056423611\n",
      "Epoch: 2481 Training Loss: 0.24371879577636718 Test Loss: 0.49786149088541665\n",
      "Epoch: 2482 Training Loss: 0.24313804626464844 Test Loss: 0.5008294270833333\n",
      "Epoch: 2483 Training Loss: 0.243169921875 Test Loss: 0.50458154296875\n",
      "Epoch: 2484 Training Loss: 0.2431873982747396 Test Loss: 0.5069870876736111\n",
      "Epoch: 2485 Training Loss: 0.2430109168158637 Test Loss: 0.5074812825520834\n",
      "Epoch: 2486 Training Loss: 0.24269485643174912 Test Loss: 0.5068674045138889\n",
      "Epoch: 2487 Training Loss: 0.24235045369466146 Test Loss: 0.5059104275173611\n",
      "Epoch: 2488 Training Loss: 0.2419897986518012 Test Loss: 0.5044212782118056\n",
      "Epoch: 2489 Training Loss: 0.24164137268066407 Test Loss: 0.5029679361979167\n",
      "Epoch: 2490 Training Loss: 0.24139895799424912 Test Loss: 0.5018996310763889\n",
      "Epoch: 2491 Training Loss: 0.24124549357096353 Test Loss: 0.5011213107638889\n",
      "Epoch: 2492 Training Loss: 0.24114822218153212 Test Loss: 0.5006893988715277\n",
      "Epoch: 2493 Training Loss: 0.24110147433810764 Test Loss: 0.5007970920138889\n",
      "Epoch: 2494 Training Loss: 0.24108018493652345 Test Loss: 0.5011776801215277\n",
      "Epoch: 2495 Training Loss: 0.24107922193739148 Test Loss: 0.5015979275173611\n",
      "Epoch: 2496 Training Loss: 0.24107903205023873 Test Loss: 0.502228515625\n",
      "Epoch: 2497 Training Loss: 0.24109042527940538 Test Loss: 0.5030955946180555\n",
      "Epoch: 2498 Training Loss: 0.24111421034071182 Test Loss: 0.5045352647569444\n",
      "Epoch: 2499 Training Loss: 0.24117415364583333 Test Loss: 0.5058862847222222\n",
      "Epoch: 2500 Training Loss: 0.24125300089518228 Test Loss: 0.5069380967881945\n",
      "Epoch: 2501 Training Loss: 0.24132707044813367 Test Loss: 0.5076503363715278\n",
      "Epoch: 2502 Training Loss: 0.24140364244249132 Test Loss: 0.5073353407118055\n",
      "Epoch: 2503 Training Loss: 0.2415056423611111 Test Loss: 0.5068966471354167\n",
      "Epoch: 2504 Training Loss: 0.2416236334906684 Test Loss: 0.5062559136284722\n",
      "Epoch: 2505 Training Loss: 0.24177428012424046 Test Loss: 0.5051255425347222\n",
      "Epoch: 2506 Training Loss: 0.24194765896267362 Test Loss: 0.5035411783854167\n",
      "Epoch: 2507 Training Loss: 0.24217523701985677 Test Loss: 0.5014993489583334\n",
      "Epoch: 2508 Training Loss: 0.24245655822753906 Test Loss: 0.49951182725694443\n",
      "Epoch: 2509 Training Loss: 0.24279798719618056 Test Loss: 0.4972848307291667\n",
      "Epoch: 2510 Training Loss: 0.24318145073784722 Test Loss: 0.49501746961805554\n",
      "Epoch: 2511 Training Loss: 0.24352904934353298 Test Loss: 0.49323898654513887\n",
      "Epoch: 2512 Training Loss: 0.24384441290961373 Test Loss: 0.4917890625\n",
      "Epoch: 2513 Training Loss: 0.2440868411593967 Test Loss: 0.4913515082465278\n",
      "Epoch: 2514 Training Loss: 0.24421461486816406 Test Loss: 0.49160069444444443\n",
      "Epoch: 2515 Training Loss: 0.24421163601345486 Test Loss: 0.49193565538194445\n",
      "Epoch: 2516 Training Loss: 0.2440575883653429 Test Loss: 0.49246728515625\n",
      "Epoch: 2517 Training Loss: 0.24378109232584635 Test Loss: 0.49262098524305553\n",
      "Epoch: 2518 Training Loss: 0.24344183688693577 Test Loss: 0.4922353515625\n",
      "Epoch: 2519 Training Loss: 0.24310338168674045 Test Loss: 0.4913195529513889\n",
      "Epoch: 2520 Training Loss: 0.24276551988389758 Test Loss: 0.4900352105034722\n",
      "Epoch: 2521 Training Loss: 0.24245089043511284 Test Loss: 0.48887049696180557\n",
      "Epoch: 2522 Training Loss: 0.2421891869439019 Test Loss: 0.4881494683159722\n",
      "Epoch: 2523 Training Loss: 0.24200885518391926 Test Loss: 0.4877974175347222\n",
      "Epoch: 2524 Training Loss: 0.24190193854437933 Test Loss: 0.48821435546875\n",
      "Epoch: 2525 Training Loss: 0.24190601094563802 Test Loss: 0.4894235568576389\n",
      "Epoch: 2526 Training Loss: 0.2419791310628255 Test Loss: 0.49104850260416666\n",
      "Epoch: 2527 Training Loss: 0.24209177822536893 Test Loss: 0.49267567274305557\n",
      "Epoch: 2528 Training Loss: 0.24220161777072483 Test Loss: 0.49392263454861113\n",
      "Epoch: 2529 Training Loss: 0.2423226318359375 Test Loss: 0.4947810329861111\n",
      "Epoch: 2530 Training Loss: 0.24244554477267796 Test Loss: 0.4951183810763889\n",
      "Epoch: 2531 Training Loss: 0.24253765530056423 Test Loss: 0.49501014539930555\n",
      "Epoch: 2532 Training Loss: 0.2425528089735243 Test Loss: 0.4944789496527778\n",
      "Epoch: 2533 Training Loss: 0.24247647942437067 Test Loss: 0.49362190755208335\n",
      "Epoch: 2534 Training Loss: 0.24233442857530382 Test Loss: 0.4927561306423611\n",
      "Epoch: 2535 Training Loss: 0.24215221489800348 Test Loss: 0.4918946940104167\n",
      "Epoch: 2536 Training Loss: 0.24194933064778645 Test Loss: 0.49119932725694443\n",
      "Epoch: 2537 Training Loss: 0.2417302754720052 Test Loss: 0.49088519965277777\n",
      "Epoch: 2538 Training Loss: 0.24148785061306424 Test Loss: 0.4909878472222222\n",
      "Epoch: 2539 Training Loss: 0.2412339647081163 Test Loss: 0.4914754231770833\n",
      "Epoch: 2540 Training Loss: 0.2410595889621311 Test Loss: 0.49227414279513887\n",
      "Epoch: 2541 Training Loss: 0.24091864013671874 Test Loss: 0.4930543619791667\n",
      "Epoch: 2542 Training Loss: 0.24082740953233506 Test Loss: 0.4940236545138889\n",
      "Epoch: 2543 Training Loss: 0.24076383124457465 Test Loss: 0.4951357964409722\n",
      "Epoch: 2544 Training Loss: 0.24073432244194878 Test Loss: 0.49590608723958335\n",
      "Epoch: 2545 Training Loss: 0.24071504889594184 Test Loss: 0.4964729817708333\n",
      "Epoch: 2546 Training Loss: 0.24071155802408853 Test Loss: 0.496677734375\n",
      "Epoch: 2547 Training Loss: 0.24070802307128905 Test Loss: 0.4968718532986111\n",
      "Epoch: 2548 Training Loss: 0.24069681125217013 Test Loss: 0.4963779296875\n",
      "Epoch: 2549 Training Loss: 0.24066197374131945 Test Loss: 0.4953630642361111\n",
      "Epoch: 2550 Training Loss: 0.24057386101616754 Test Loss: 0.4940252821180556\n",
      "Epoch: 2551 Training Loss: 0.24041544426812067 Test Loss: 0.49235492621527777\n",
      "Epoch: 2552 Training Loss: 0.2401944766574436 Test Loss: 0.4904592556423611\n",
      "Epoch: 2553 Training Loss: 0.23992906019422744 Test Loss: 0.48861697048611114\n",
      "Epoch: 2554 Training Loss: 0.2396348588731554 Test Loss: 0.48700439453125\n",
      "Epoch: 2555 Training Loss: 0.23933340793185764 Test Loss: 0.4855236545138889\n",
      "Epoch: 2556 Training Loss: 0.2390242648654514 Test Loss: 0.4844445529513889\n",
      "Epoch: 2557 Training Loss: 0.2387220458984375 Test Loss: 0.48342664930555557\n",
      "Epoch: 2558 Training Loss: 0.23843477376302083 Test Loss: 0.4829022894965278\n",
      "Epoch: 2559 Training Loss: 0.2381558091905382 Test Loss: 0.4826056857638889\n",
      "Epoch: 2560 Training Loss: 0.23787366909450955 Test Loss: 0.48268174913194445\n",
      "Epoch: 2561 Training Loss: 0.2376085662841797 Test Loss: 0.48287467447916665\n",
      "Epoch: 2562 Training Loss: 0.23736549038357205 Test Loss: 0.4833947482638889\n",
      "Epoch: 2563 Training Loss: 0.23714019097222222 Test Loss: 0.4841171332465278\n",
      "Epoch: 2564 Training Loss: 0.2369313761393229 Test Loss: 0.48477685546875\n",
      "Epoch: 2565 Training Loss: 0.23672776624891492 Test Loss: 0.48564854600694446\n",
      "Epoch: 2566 Training Loss: 0.23654591200086805 Test Loss: 0.48650390625\n",
      "Epoch: 2567 Training Loss: 0.23638197157118054 Test Loss: 0.48726405164930553\n",
      "Epoch: 2568 Training Loss: 0.2362186991373698 Test Loss: 0.48804600694444444\n",
      "Epoch: 2569 Training Loss: 0.2360643310546875 Test Loss: 0.4888218315972222\n",
      "Epoch: 2570 Training Loss: 0.23591328938802084 Test Loss: 0.4895280490451389\n",
      "Epoch: 2571 Training Loss: 0.2357686750623915 Test Loss: 0.49019129774305553\n",
      "Epoch: 2572 Training Loss: 0.23562940470377605 Test Loss: 0.4906949869791667\n",
      "Epoch: 2573 Training Loss: 0.23548585849338108 Test Loss: 0.4911181640625\n",
      "Epoch: 2574 Training Loss: 0.23534301079644096 Test Loss: 0.4912961154513889\n",
      "Epoch: 2575 Training Loss: 0.23517611694335938 Test Loss: 0.49133534071180557\n",
      "Epoch: 2576 Training Loss: 0.2350116000705295 Test Loss: 0.4912810329861111\n",
      "Epoch: 2577 Training Loss: 0.2348709208170573 Test Loss: 0.4911931966145833\n",
      "Epoch: 2578 Training Loss: 0.2347279086642795 Test Loss: 0.49086219618055554\n",
      "Epoch: 2579 Training Loss: 0.23459332275390626 Test Loss: 0.4905335286458333\n",
      "Epoch: 2580 Training Loss: 0.23445000881618924 Test Loss: 0.4896103515625\n",
      "Epoch: 2581 Training Loss: 0.23427813890245225 Test Loss: 0.488568359375\n",
      "Epoch: 2582 Training Loss: 0.23411089070638022 Test Loss: 0.48754991319444446\n",
      "Epoch: 2583 Training Loss: 0.23395291646321614 Test Loss: 0.48666449652777777\n",
      "Epoch: 2584 Training Loss: 0.23377897304958767 Test Loss: 0.4859069010416667\n",
      "Epoch: 2585 Training Loss: 0.2336112314860026 Test Loss: 0.4851936306423611\n",
      "Epoch: 2586 Training Loss: 0.23344748942057292 Test Loss: 0.48481429036458334\n",
      "Epoch: 2587 Training Loss: 0.2332992384168837 Test Loss: 0.4845260416666667\n",
      "Epoch: 2588 Training Loss: 0.23316710238986546 Test Loss: 0.4843566623263889\n",
      "Epoch: 2589 Training Loss: 0.23304086303710939 Test Loss: 0.48431743706597224\n",
      "Epoch: 2590 Training Loss: 0.23292424689398872 Test Loss: 0.48419178602430557\n",
      "Epoch: 2591 Training Loss: 0.23282683308919272 Test Loss: 0.4841985134548611\n",
      "Epoch: 2592 Training Loss: 0.23273599073621962 Test Loss: 0.4842394205729167\n",
      "Epoch: 2593 Training Loss: 0.23264816114637588 Test Loss: 0.4844553493923611\n",
      "Epoch: 2594 Training Loss: 0.23258856201171876 Test Loss: 0.48452001953125\n",
      "Epoch: 2595 Training Loss: 0.23252806091308595 Test Loss: 0.4846240776909722\n",
      "Epoch: 2596 Training Loss: 0.23247550455729166 Test Loss: 0.48474582248263887\n",
      "Epoch: 2597 Training Loss: 0.23243380059136284 Test Loss: 0.48491422526041666\n",
      "Epoch: 2598 Training Loss: 0.2323984171549479 Test Loss: 0.48504356553819444\n",
      "Epoch: 2599 Training Loss: 0.23236437140570745 Test Loss: 0.4852879774305556\n",
      "Epoch: 2600 Training Loss: 0.23233814832899305 Test Loss: 0.4856031901041667\n",
      "Epoch: 2601 Training Loss: 0.23230838690863714 Test Loss: 0.4858224826388889\n",
      "Epoch: 2602 Training Loss: 0.23229743109809028 Test Loss: 0.48611263020833334\n",
      "Epoch: 2603 Training Loss: 0.23229108005099827 Test Loss: 0.48621240234375\n",
      "Epoch: 2604 Training Loss: 0.2322791273328993 Test Loss: 0.48645594618055554\n",
      "Epoch: 2605 Training Loss: 0.23227266269259983 Test Loss: 0.48678472222222224\n",
      "Epoch: 2606 Training Loss: 0.2322731713189019 Test Loss: 0.48706022135416666\n",
      "Epoch: 2607 Training Loss: 0.23227611287434896 Test Loss: 0.487318359375\n",
      "Epoch: 2608 Training Loss: 0.23228197564019099 Test Loss: 0.4875498046875\n",
      "Epoch: 2609 Training Loss: 0.23230077785915798 Test Loss: 0.48786832682291664\n",
      "Epoch: 2610 Training Loss: 0.2323134240044488 Test Loss: 0.4881486002604167\n",
      "Epoch: 2611 Training Loss: 0.2323305392795139 Test Loss: 0.48839876302083335\n",
      "Epoch: 2612 Training Loss: 0.23236225212944878 Test Loss: 0.4886664496527778\n",
      "Epoch: 2613 Training Loss: 0.23239636569552952 Test Loss: 0.48898621961805555\n",
      "Epoch: 2614 Training Loss: 0.23244073825412326 Test Loss: 0.48921983506944444\n",
      "Epoch: 2615 Training Loss: 0.23250081549750434 Test Loss: 0.4895883246527778\n",
      "Epoch: 2616 Training Loss: 0.23257427300347222 Test Loss: 0.48988997395833334\n",
      "Epoch: 2617 Training Loss: 0.23265688069661458 Test Loss: 0.4901764865451389\n",
      "Epoch: 2618 Training Loss: 0.2327476111518012 Test Loss: 0.4903872612847222\n",
      "Epoch: 2619 Training Loss: 0.2328491007486979 Test Loss: 0.49068044704861113\n",
      "Epoch: 2620 Training Loss: 0.23296336873372395 Test Loss: 0.49090863715277777\n",
      "Epoch: 2621 Training Loss: 0.23309238009982639 Test Loss: 0.4910156792534722\n",
      "Epoch: 2622 Training Loss: 0.23325778706868489 Test Loss: 0.49112255859375\n",
      "Epoch: 2623 Training Loss: 0.23344441223144533 Test Loss: 0.49115440538194444\n",
      "Epoch: 2624 Training Loss: 0.2336536373562283 Test Loss: 0.4910668402777778\n",
      "Epoch: 2625 Training Loss: 0.23389744567871093 Test Loss: 0.4909327799479167\n",
      "Epoch: 2626 Training Loss: 0.23418017069498698 Test Loss: 0.49073036024305555\n",
      "Epoch: 2627 Training Loss: 0.23453023613823784 Test Loss: 0.49042220052083335\n",
      "Epoch: 2628 Training Loss: 0.23494818115234375 Test Loss: 0.49018717447916665\n",
      "Epoch: 2629 Training Loss: 0.23543250698513454 Test Loss: 0.4896677517361111\n",
      "Epoch: 2630 Training Loss: 0.2359642520480686 Test Loss: 0.48905967881944445\n",
      "Epoch: 2631 Training Loss: 0.23656204901801214 Test Loss: 0.48837152777777776\n",
      "Epoch: 2632 Training Loss: 0.23714723375108507 Test Loss: 0.48702701822916666\n",
      "Epoch: 2633 Training Loss: 0.2378286641438802 Test Loss: 0.48506070963541664\n",
      "Epoch: 2634 Training Loss: 0.238552980211046 Test Loss: 0.48541162109375\n",
      "Epoch: 2635 Training Loss: 0.23924628533257378 Test Loss: 0.48891107855902777\n",
      "Epoch: 2636 Training Loss: 0.23979959954155816 Test Loss: 0.4914274631076389\n",
      "Epoch: 2637 Training Loss: 0.24000635782877605 Test Loss: 0.4899672309027778\n",
      "Epoch: 2638 Training Loss: 0.23976330227322049 Test Loss: 0.48747303602430553\n",
      "Epoch: 2639 Training Loss: 0.23937894694010417 Test Loss: 0.4861720920138889\n",
      "Epoch: 2640 Training Loss: 0.23899966769748263 Test Loss: 0.486267578125\n",
      "Epoch: 2641 Training Loss: 0.23871180046929252 Test Loss: 0.4860302734375\n",
      "Epoch: 2642 Training Loss: 0.2384529334174262 Test Loss: 0.48527962239583333\n",
      "Epoch: 2643 Training Loss: 0.23818951755099826 Test Loss: 0.48428206380208333\n",
      "Epoch: 2644 Training Loss: 0.23791595289442274 Test Loss: 0.4830039605034722\n",
      "Epoch: 2645 Training Loss: 0.2376093037923177 Test Loss: 0.48196533203125\n",
      "Epoch: 2646 Training Loss: 0.23729490661621094 Test Loss: 0.48125455729166666\n",
      "Epoch: 2647 Training Loss: 0.23697559611002605 Test Loss: 0.48072992621527777\n",
      "Epoch: 2648 Training Loss: 0.2366568400065104 Test Loss: 0.4803967013888889\n",
      "Epoch: 2649 Training Loss: 0.2363702867296007 Test Loss: 0.48037044270833335\n",
      "Epoch: 2650 Training Loss: 0.23608753289116755 Test Loss: 0.48045426432291666\n",
      "Epoch: 2651 Training Loss: 0.23580508592393662 Test Loss: 0.4804011501736111\n",
      "Epoch: 2652 Training Loss: 0.23554804992675782 Test Loss: 0.4804306098090278\n",
      "Epoch: 2653 Training Loss: 0.2353019019232856 Test Loss: 0.48030322265625\n",
      "Epoch: 2654 Training Loss: 0.23506886121961806 Test Loss: 0.48006461588541666\n",
      "Epoch: 2655 Training Loss: 0.2348734876844618 Test Loss: 0.47968229166666665\n",
      "Epoch: 2656 Training Loss: 0.23470584784613716 Test Loss: 0.4793816189236111\n",
      "Epoch: 2657 Training Loss: 0.23456056891547308 Test Loss: 0.47912261284722224\n",
      "Epoch: 2658 Training Loss: 0.23445340474446616 Test Loss: 0.47904177517361113\n",
      "Epoch: 2659 Training Loss: 0.23438593377007377 Test Loss: 0.4788578016493056\n",
      "Epoch: 2660 Training Loss: 0.23438216484917535 Test Loss: 0.47898063151041664\n",
      "Epoch: 2661 Training Loss: 0.2344403839111328 Test Loss: 0.4791926540798611\n",
      "Epoch: 2662 Training Loss: 0.234578618367513 Test Loss: 0.47985606553819443\n",
      "Epoch: 2663 Training Loss: 0.23478913370768228 Test Loss: 0.48078483072916667\n",
      "Epoch: 2664 Training Loss: 0.23509341769748263 Test Loss: 0.4813178168402778\n",
      "Epoch: 2665 Training Loss: 0.23546692233615452 Test Loss: 0.4821597222222222\n",
      "Epoch: 2666 Training Loss: 0.23594617886013455 Test Loss: 0.48334505208333334\n",
      "Epoch: 2667 Training Loss: 0.2364872589111328 Test Loss: 0.48495985243055556\n",
      "Epoch: 2668 Training Loss: 0.23711728922526043 Test Loss: 0.48627332899305553\n",
      "Epoch: 2669 Training Loss: 0.23785321044921875 Test Loss: 0.4877226019965278\n",
      "Epoch: 2670 Training Loss: 0.23866256883409287 Test Loss: 0.48950303819444446\n",
      "Epoch: 2671 Training Loss: 0.2395386284722222 Test Loss: 0.4914106987847222\n",
      "Epoch: 2672 Training Loss: 0.24046053738064235 Test Loss: 0.49339675564236113\n",
      "Epoch: 2673 Training Loss: 0.24143471272786457 Test Loss: 0.4954674479166667\n",
      "Epoch: 2674 Training Loss: 0.24245454745822484 Test Loss: 0.4975383029513889\n",
      "Epoch: 2675 Training Loss: 0.24349931165907118 Test Loss: 0.4997282443576389\n",
      "Epoch: 2676 Training Loss: 0.24451119147406683 Test Loss: 0.5011730685763889\n",
      "Epoch: 2677 Training Loss: 0.2454600355360243 Test Loss: 0.5022488606770833\n",
      "Epoch: 2678 Training Loss: 0.24631261528862847 Test Loss: 0.5038228624131944\n",
      "Epoch: 2679 Training Loss: 0.24701326666937934 Test Loss: 0.5066918402777778\n",
      "Epoch: 2680 Training Loss: 0.2475676506890191 Test Loss: 0.5078712022569445\n",
      "Epoch: 2681 Training Loss: 0.24811078050401475 Test Loss: 0.5105453559027778\n",
      "Epoch: 2682 Training Loss: 0.248722410413954 Test Loss: 0.5132817925347222\n",
      "Epoch: 2683 Training Loss: 0.2493889651828342 Test Loss: 0.5132916666666667\n",
      "Epoch: 2684 Training Loss: 0.2502112223307292 Test Loss: 0.5105862630208333\n",
      "Epoch: 2685 Training Loss: 0.2511283230251736 Test Loss: 0.5056784939236111\n",
      "Epoch: 2686 Training Loss: 0.25158430480957034 Test Loss: 0.4974746636284722\n",
      "Epoch: 2687 Training Loss: 0.2517432861328125 Test Loss: 0.48900656467013887\n",
      "Epoch: 2688 Training Loss: 0.2514490712483724 Test Loss: 0.48041617838541667\n",
      "Epoch: 2689 Training Loss: 0.24959869893391928 Test Loss: 0.47467952473958336\n",
      "Epoch: 2690 Training Loss: 0.24669127400716145 Test Loss: 0.47208322482638887\n",
      "Epoch: 2691 Training Loss: 0.24403526475694445 Test Loss: 0.4700087890625\n",
      "Epoch: 2692 Training Loss: 0.24143738301595052 Test Loss: 0.46977723524305554\n",
      "Epoch: 2693 Training Loss: 0.23894198438856337 Test Loss: 0.46934895833333334\n",
      "Epoch: 2694 Training Loss: 0.23718751864963108 Test Loss: 0.4688319227430556\n",
      "Epoch: 2695 Training Loss: 0.23605474514431424 Test Loss: 0.4687434353298611\n",
      "Epoch: 2696 Training Loss: 0.23507455105251737 Test Loss: 0.46936170789930554\n",
      "Epoch: 2697 Training Loss: 0.2343079817030165 Test Loss: 0.46978369140625\n",
      "Epoch: 2698 Training Loss: 0.2338099619547526 Test Loss: 0.4697763129340278\n",
      "Epoch: 2699 Training Loss: 0.2333629896375868 Test Loss: 0.4698882378472222\n",
      "Epoch: 2700 Training Loss: 0.2329580569797092 Test Loss: 0.4703035481770833\n",
      "Epoch: 2701 Training Loss: 0.23263001166449654 Test Loss: 0.4707024197048611\n",
      "Epoch: 2702 Training Loss: 0.23231768629286023 Test Loss: 0.4709508463541667\n",
      "Epoch: 2703 Training Loss: 0.2320218505859375 Test Loss: 0.4710638020833333\n",
      "Epoch: 2704 Training Loss: 0.23174139912923178 Test Loss: 0.47106022135416664\n",
      "Epoch: 2705 Training Loss: 0.23147769334581164 Test Loss: 0.47131689453125\n",
      "Epoch: 2706 Training Loss: 0.23123184712727865 Test Loss: 0.4713879123263889\n",
      "Epoch: 2707 Training Loss: 0.23100047471788193 Test Loss: 0.47154286024305553\n",
      "Epoch: 2708 Training Loss: 0.23077554490831162 Test Loss: 0.4717103949652778\n",
      "Epoch: 2709 Training Loss: 0.23055728827582464 Test Loss: 0.4718108181423611\n",
      "Epoch: 2710 Training Loss: 0.23034681023491754 Test Loss: 0.47191758897569447\n",
      "Epoch: 2711 Training Loss: 0.2301348843044705 Test Loss: 0.47199267578125\n",
      "Epoch: 2712 Training Loss: 0.2299320746527778 Test Loss: 0.4720558268229167\n",
      "Epoch: 2713 Training Loss: 0.2297387491861979 Test Loss: 0.4720729709201389\n",
      "Epoch: 2714 Training Loss: 0.2295530480278863 Test Loss: 0.47207394748263887\n",
      "Epoch: 2715 Training Loss: 0.2293784434000651 Test Loss: 0.4721802842881944\n",
      "Epoch: 2716 Training Loss: 0.22920620557996962 Test Loss: 0.4722530381944444\n",
      "Epoch: 2717 Training Loss: 0.22903905910915798 Test Loss: 0.47232194010416667\n",
      "Epoch: 2718 Training Loss: 0.22886758931477866 Test Loss: 0.47248958333333335\n",
      "Epoch: 2719 Training Loss: 0.22870155164930556 Test Loss: 0.47261539713541667\n",
      "Epoch: 2720 Training Loss: 0.2285358378092448 Test Loss: 0.4727080078125\n",
      "Epoch: 2721 Training Loss: 0.22837232462565105 Test Loss: 0.4727377387152778\n",
      "Epoch: 2722 Training Loss: 0.22821331956651475 Test Loss: 0.47289892578125\n",
      "Epoch: 2723 Training Loss: 0.2280589124891493 Test Loss: 0.4729152560763889\n",
      "Epoch: 2724 Training Loss: 0.2279146440294054 Test Loss: 0.47295496961805555\n",
      "Epoch: 2725 Training Loss: 0.22777328660753038 Test Loss: 0.47306727430555556\n",
      "Epoch: 2726 Training Loss: 0.22763065761990017 Test Loss: 0.4731144748263889\n",
      "Epoch: 2727 Training Loss: 0.2274988047281901 Test Loss: 0.47318223741319443\n",
      "Epoch: 2728 Training Loss: 0.22736972893608942 Test Loss: 0.4732275390625\n",
      "Epoch: 2729 Training Loss: 0.2272422332763672 Test Loss: 0.4732822265625\n",
      "Epoch: 2730 Training Loss: 0.22712320793999566 Test Loss: 0.4733486870659722\n",
      "Epoch: 2731 Training Loss: 0.22700457933213974 Test Loss: 0.4734125434027778\n",
      "Epoch: 2732 Training Loss: 0.22688878207736546 Test Loss: 0.4734990776909722\n",
      "Epoch: 2733 Training Loss: 0.2267730763753255 Test Loss: 0.47355186631944446\n",
      "Epoch: 2734 Training Loss: 0.22665938313802084 Test Loss: 0.4736204427083333\n",
      "Epoch: 2735 Training Loss: 0.22654403008355034 Test Loss: 0.47372129991319445\n",
      "Epoch: 2736 Training Loss: 0.2264335208468967 Test Loss: 0.47387679036458336\n",
      "Epoch: 2737 Training Loss: 0.2263238084581163 Test Loss: 0.47390190972222224\n",
      "Epoch: 2738 Training Loss: 0.22622490776909723 Test Loss: 0.47396299913194445\n",
      "Epoch: 2739 Training Loss: 0.2261222669813368 Test Loss: 0.47408452690972225\n",
      "Epoch: 2740 Training Loss: 0.22601849873860677 Test Loss: 0.4741038411458333\n",
      "Epoch: 2741 Training Loss: 0.22591776529947916 Test Loss: 0.4742697482638889\n",
      "Epoch: 2742 Training Loss: 0.22581576368543838 Test Loss: 0.4743997395833333\n",
      "Epoch: 2743 Training Loss: 0.2257127210828993 Test Loss: 0.4744403754340278\n",
      "Epoch: 2744 Training Loss: 0.2256133812798394 Test Loss: 0.4745187717013889\n",
      "Epoch: 2745 Training Loss: 0.2255128462049696 Test Loss: 0.4746149088541667\n",
      "Epoch: 2746 Training Loss: 0.22541435919867622 Test Loss: 0.4746765407986111\n",
      "Epoch: 2747 Training Loss: 0.22531835428873698 Test Loss: 0.4747376302083333\n",
      "Epoch: 2748 Training Loss: 0.2252254163953993 Test Loss: 0.47482161458333333\n",
      "Epoch: 2749 Training Loss: 0.22512985399034288 Test Loss: 0.47490283203125\n",
      "Epoch: 2750 Training Loss: 0.22504069519042968 Test Loss: 0.47502712673611114\n",
      "Epoch: 2751 Training Loss: 0.2249468282063802 Test Loss: 0.47512706163194446\n",
      "Epoch: 2752 Training Loss: 0.2248564232720269 Test Loss: 0.47512071397569444\n",
      "Epoch: 2753 Training Loss: 0.22477207607693142 Test Loss: 0.4751599934895833\n",
      "Epoch: 2754 Training Loss: 0.2246844770643446 Test Loss: 0.4752591145833333\n",
      "Epoch: 2755 Training Loss: 0.22459912957085504 Test Loss: 0.47530875651041665\n",
      "Epoch: 2756 Training Loss: 0.22451188490125867 Test Loss: 0.47535020616319446\n",
      "Epoch: 2757 Training Loss: 0.22442796834309897 Test Loss: 0.47542599826388887\n",
      "Epoch: 2758 Training Loss: 0.22435323757595485 Test Loss: 0.47549669053819443\n",
      "Epoch: 2759 Training Loss: 0.22426708984375 Test Loss: 0.47555485026041666\n",
      "Epoch: 2760 Training Loss: 0.2241935560438368 Test Loss: 0.4755888671875\n",
      "Epoch: 2761 Training Loss: 0.22410977681477864 Test Loss: 0.4756711697048611\n",
      "Epoch: 2762 Training Loss: 0.22403456624348958 Test Loss: 0.4757098524305556\n",
      "Epoch: 2763 Training Loss: 0.22395960150824654 Test Loss: 0.4757703450520833\n",
      "Epoch: 2764 Training Loss: 0.22388567606608073 Test Loss: 0.47581119791666665\n",
      "Epoch: 2765 Training Loss: 0.2238062201605903 Test Loss: 0.47578955078125\n",
      "Epoch: 2766 Training Loss: 0.22373516506618923 Test Loss: 0.47588161892361114\n",
      "Epoch: 2767 Training Loss: 0.22366324191623263 Test Loss: 0.4759575737847222\n",
      "Epoch: 2768 Training Loss: 0.2235931922064887 Test Loss: 0.47606407335069445\n",
      "Epoch: 2769 Training Loss: 0.22352338155110676 Test Loss: 0.47611892361111113\n",
      "Epoch: 2770 Training Loss: 0.223451902601454 Test Loss: 0.47613427734375\n",
      "Epoch: 2771 Training Loss: 0.22338465203179253 Test Loss: 0.47616389973958334\n",
      "Epoch: 2772 Training Loss: 0.22331430053710938 Test Loss: 0.4762147352430556\n",
      "Epoch: 2773 Training Loss: 0.22324783494737413 Test Loss: 0.47630772569444446\n",
      "Epoch: 2774 Training Loss: 0.22318528578016494 Test Loss: 0.4763430447048611\n",
      "Epoch: 2775 Training Loss: 0.22310981920030382 Test Loss: 0.47638899739583335\n",
      "Epoch: 2776 Training Loss: 0.22304591369628907 Test Loss: 0.47648627387152775\n",
      "Epoch: 2777 Training Loss: 0.22298040771484376 Test Loss: 0.4765532769097222\n",
      "Epoch: 2778 Training Loss: 0.22291578504774306 Test Loss: 0.47661149088541666\n",
      "Epoch: 2779 Training Loss: 0.22284901089138454 Test Loss: 0.4766166449652778\n",
      "Epoch: 2780 Training Loss: 0.22278443739149306 Test Loss: 0.47669835069444444\n",
      "Epoch: 2781 Training Loss: 0.22271849907769098 Test Loss: 0.4767641059027778\n",
      "Epoch: 2782 Training Loss: 0.2226560295952691 Test Loss: 0.47677229817708333\n",
      "Epoch: 2783 Training Loss: 0.2225894758436415 Test Loss: 0.47676812065972224\n",
      "Epoch: 2784 Training Loss: 0.2225275370279948 Test Loss: 0.47685243055555554\n",
      "Epoch: 2785 Training Loss: 0.22246912807888455 Test Loss: 0.4770031467013889\n",
      "Epoch: 2786 Training Loss: 0.22240631273057726 Test Loss: 0.47697900390625\n",
      "Epoch: 2787 Training Loss: 0.2223406982421875 Test Loss: 0.4769515516493056\n",
      "Epoch: 2788 Training Loss: 0.2222743411593967 Test Loss: 0.4770988498263889\n",
      "Epoch: 2789 Training Loss: 0.2222179463704427 Test Loss: 0.47719520399305554\n",
      "Epoch: 2790 Training Loss: 0.22216189066569011 Test Loss: 0.4772017144097222\n",
      "Epoch: 2791 Training Loss: 0.22209618462456598 Test Loss: 0.4772262912326389\n",
      "Epoch: 2792 Training Loss: 0.22203139241536457 Test Loss: 0.4773095703125\n",
      "Epoch: 2793 Training Loss: 0.22197512647840711 Test Loss: 0.4773598090277778\n",
      "Epoch: 2794 Training Loss: 0.2219203898111979 Test Loss: 0.4774558376736111\n",
      "Epoch: 2795 Training Loss: 0.22185692342122396 Test Loss: 0.4774741753472222\n",
      "Epoch: 2796 Training Loss: 0.2217969207763672 Test Loss: 0.4775804036458333\n",
      "Epoch: 2797 Training Loss: 0.2217398003472222 Test Loss: 0.4776462131076389\n",
      "Epoch: 2798 Training Loss: 0.22168331231011285 Test Loss: 0.47765277777777776\n",
      "Epoch: 2799 Training Loss: 0.2216278754340278 Test Loss: 0.4776843532986111\n",
      "Epoch: 2800 Training Loss: 0.22156719801161023 Test Loss: 0.4778436414930556\n",
      "Epoch: 2801 Training Loss: 0.22151334126790365 Test Loss: 0.47783973524305556\n",
      "Epoch: 2802 Training Loss: 0.22145382859971788 Test Loss: 0.47798253038194444\n",
      "Epoch: 2803 Training Loss: 0.2213993631998698 Test Loss: 0.47793467881944446\n",
      "Epoch: 2804 Training Loss: 0.2213468051486545 Test Loss: 0.4779583333333333\n",
      "Epoch: 2805 Training Loss: 0.22129222954644098 Test Loss: 0.4781427951388889\n",
      "Epoch: 2806 Training Loss: 0.2212321319580078 Test Loss: 0.47823719618055555\n",
      "Epoch: 2807 Training Loss: 0.22117902289496527 Test Loss: 0.4782616102430556\n",
      "Epoch: 2808 Training Loss: 0.22112832472059463 Test Loss: 0.4782733289930556\n",
      "Epoch: 2809 Training Loss: 0.22107506815592448 Test Loss: 0.47827435980902777\n",
      "Epoch: 2810 Training Loss: 0.22102630954318575 Test Loss: 0.47827115885416666\n",
      "Epoch: 2811 Training Loss: 0.22096747673882378 Test Loss: 0.4785045030381944\n",
      "Epoch: 2812 Training Loss: 0.22090335252549914 Test Loss: 0.47877918836805555\n",
      "Epoch: 2813 Training Loss: 0.22085009256998697 Test Loss: 0.47856825086805554\n",
      "Epoch: 2814 Training Loss: 0.22079991997612847 Test Loss: 0.47850054253472224\n",
      "Epoch: 2815 Training Loss: 0.2207522922092014 Test Loss: 0.47873057725694446\n",
      "Epoch: 2816 Training Loss: 0.22070304022894965 Test Loss: 0.478650390625\n",
      "Epoch: 2817 Training Loss: 0.22065086703830294 Test Loss: 0.47862516276041667\n",
      "Epoch: 2818 Training Loss: 0.22059343126085068 Test Loss: 0.4790564236111111\n",
      "Epoch: 2819 Training Loss: 0.22053468153211805 Test Loss: 0.4793036566840278\n",
      "Epoch: 2820 Training Loss: 0.2204921129014757 Test Loss: 0.4789443359375\n",
      "Epoch: 2821 Training Loss: 0.22045456441243488 Test Loss: 0.47893256293402775\n",
      "Epoch: 2822 Training Loss: 0.22040730624728733 Test Loss: 0.4792090386284722\n",
      "Epoch: 2823 Training Loss: 0.22036610751681857 Test Loss: 0.4790143229166667\n",
      "Epoch: 2824 Training Loss: 0.22031107584635418 Test Loss: 0.4790394965277778\n",
      "Epoch: 2825 Training Loss: 0.2202405276828342 Test Loss: 0.4795623914930556\n",
      "Epoch: 2826 Training Loss: 0.22017477247450087 Test Loss: 0.4798869900173611\n",
      "Epoch: 2827 Training Loss: 0.22013878038194445 Test Loss: 0.4794448784722222\n",
      "Epoch: 2828 Training Loss: 0.2201014912923177 Test Loss: 0.4792746853298611\n",
      "Epoch: 2829 Training Loss: 0.2200599365234375 Test Loss: 0.47966276041666667\n",
      "Epoch: 2830 Training Loss: 0.22002129618326824 Test Loss: 0.47928043619791666\n",
      "Epoch: 2831 Training Loss: 0.21996764289008247 Test Loss: 0.4792585720486111\n",
      "Epoch: 2832 Training Loss: 0.21989581298828126 Test Loss: 0.4801458333333333\n",
      "Epoch: 2833 Training Loss: 0.21982640414767796 Test Loss: 0.48046397569444443\n",
      "Epoch: 2834 Training Loss: 0.21979226006401908 Test Loss: 0.47978260633680553\n",
      "Epoch: 2835 Training Loss: 0.2197526092529297 Test Loss: 0.4795811089409722\n",
      "Epoch: 2836 Training Loss: 0.21970651075575087 Test Loss: 0.48009619140625\n",
      "Epoch: 2837 Training Loss: 0.21966995239257814 Test Loss: 0.4796592881944444\n",
      "Epoch: 2838 Training Loss: 0.2196152801513672 Test Loss: 0.4796796332465278\n",
      "Epoch: 2839 Training Loss: 0.21954605780707465 Test Loss: 0.4806845160590278\n",
      "Epoch: 2840 Training Loss: 0.219479002210829 Test Loss: 0.4810007595486111\n",
      "Epoch: 2841 Training Loss: 0.21945540703667535 Test Loss: 0.48011892361111114\n",
      "Epoch: 2842 Training Loss: 0.21940958150227866 Test Loss: 0.4799873589409722\n",
      "Epoch: 2843 Training Loss: 0.21936332872178818 Test Loss: 0.48037858072916667\n",
      "Epoch: 2844 Training Loss: 0.21932358805338542 Test Loss: 0.4798033854166667\n",
      "Epoch: 2845 Training Loss: 0.21926291571723092 Test Loss: 0.48026573350694446\n",
      "Epoch: 2846 Training Loss: 0.21918369547526043 Test Loss: 0.4813779296875\n",
      "Epoch: 2847 Training Loss: 0.2191209513346354 Test Loss: 0.48132763671875\n",
      "Epoch: 2848 Training Loss: 0.21909765964084202 Test Loss: 0.4803053927951389\n",
      "Epoch: 2849 Training Loss: 0.2190465121799045 Test Loss: 0.4803541666666667\n",
      "Epoch: 2850 Training Loss: 0.2190073496500651 Test Loss: 0.4805833875868056\n",
      "Epoch: 2851 Training Loss: 0.21896670193142362 Test Loss: 0.4800686848958333\n",
      "Epoch: 2852 Training Loss: 0.21889917331271702 Test Loss: 0.480953125\n",
      "Epoch: 2853 Training Loss: 0.21881016370985243 Test Loss: 0.48195675998263887\n",
      "Epoch: 2854 Training Loss: 0.21875998942057293 Test Loss: 0.4814255642361111\n",
      "Epoch: 2855 Training Loss: 0.21873040432400173 Test Loss: 0.4806042751736111\n",
      "Epoch: 2856 Training Loss: 0.21867026435004341 Test Loss: 0.48092393663194444\n",
      "Epoch: 2857 Training Loss: 0.21863986714680989 Test Loss: 0.48069411892361114\n",
      "Epoch: 2858 Training Loss: 0.21859314982096353 Test Loss: 0.4803820529513889\n",
      "Epoch: 2859 Training Loss: 0.21853070407443576 Test Loss: 0.48157269965277777\n",
      "Epoch: 2860 Training Loss: 0.21845504591200088 Test Loss: 0.48258973524305554\n",
      "Epoch: 2861 Training Loss: 0.21843208312988283 Test Loss: 0.48153016493055556\n",
      "Epoch: 2862 Training Loss: 0.2183923848470052 Test Loss: 0.48092035590277776\n",
      "Epoch: 2863 Training Loss: 0.21833666483561198 Test Loss: 0.48130333116319446\n",
      "Epoch: 2864 Training Loss: 0.2183174828423394 Test Loss: 0.48077625868055557\n",
      "Epoch: 2865 Training Loss: 0.2182617916531033 Test Loss: 0.4809880099826389\n",
      "Epoch: 2866 Training Loss: 0.2181928473578559 Test Loss: 0.48259966362847223\n",
      "Epoch: 2867 Training Loss: 0.21812083774142796 Test Loss: 0.4828391384548611\n",
      "Epoch: 2868 Training Loss: 0.2180963321261936 Test Loss: 0.4815241970486111\n",
      "Epoch: 2869 Training Loss: 0.21803241305881077 Test Loss: 0.48126139322916667\n",
      "Epoch: 2870 Training Loss: 0.2179880845811632 Test Loss: 0.4815600043402778\n",
      "Epoch: 2871 Training Loss: 0.21796252102322047 Test Loss: 0.48089583333333336\n",
      "Epoch: 2872 Training Loss: 0.2179037356906467 Test Loss: 0.4820563151041667\n",
      "Epoch: 2873 Training Loss: 0.21782452053493923 Test Loss: 0.48365223524305556\n",
      "Epoch: 2874 Training Loss: 0.21779195319281683 Test Loss: 0.48265201822916665\n",
      "Epoch: 2875 Training Loss: 0.21775091552734374 Test Loss: 0.48160096571180555\n",
      "Epoch: 2876 Training Loss: 0.21767709181043837 Test Loss: 0.4818811848958333\n",
      "Epoch: 2877 Training Loss: 0.2176472863091363 Test Loss: 0.48148681640625\n",
      "Epoch: 2878 Training Loss: 0.2175908440483941 Test Loss: 0.4817043728298611\n",
      "Epoch: 2879 Training Loss: 0.2175247802734375 Test Loss: 0.4833318142361111\n",
      "Epoch: 2880 Training Loss: 0.2174670901828342 Test Loss: 0.4838249240451389\n",
      "Epoch: 2881 Training Loss: 0.2174451124403212 Test Loss: 0.48236241319444445\n",
      "Epoch: 2882 Training Loss: 0.21737293497721355 Test Loss: 0.48198388671875\n",
      "Epoch: 2883 Training Loss: 0.21732638380262587 Test Loss: 0.48212369791666665\n",
      "Epoch: 2884 Training Loss: 0.21729204813639322 Test Loss: 0.48166710069444446\n",
      "Epoch: 2885 Training Loss: 0.2172377692328559 Test Loss: 0.4826662326388889\n",
      "Epoch: 2886 Training Loss: 0.21716993204752605 Test Loss: 0.48480251736111113\n",
      "Epoch: 2887 Training Loss: 0.21715656873914932 Test Loss: 0.4837854275173611\n",
      "Epoch: 2888 Training Loss: 0.2171286163330078 Test Loss: 0.48240863715277776\n",
      "Epoch: 2889 Training Loss: 0.21704300435384113 Test Loss: 0.48261436631944443\n",
      "Epoch: 2890 Training Loss: 0.21701251390245227 Test Loss: 0.4821937934027778\n",
      "Epoch: 2891 Training Loss: 0.21693756951226129 Test Loss: 0.48235503472222224\n",
      "Epoch: 2892 Training Loss: 0.2168692118326823 Test Loss: 0.48438151041666666\n",
      "Epoch: 2893 Training Loss: 0.216819580078125 Test Loss: 0.48552501085069444\n",
      "Epoch: 2894 Training Loss: 0.21683062744140624 Test Loss: 0.48358778211805553\n",
      "Epoch: 2895 Training Loss: 0.2167607184516059 Test Loss: 0.4828282335069444\n",
      "Epoch: 2896 Training Loss: 0.21669765896267362 Test Loss: 0.482943359375\n",
      "Epoch: 2897 Training Loss: 0.21665067206488714 Test Loss: 0.4824527452256944\n",
      "Epoch: 2898 Training Loss: 0.21657327779134114 Test Loss: 0.48353363715277775\n",
      "Epoch: 2899 Training Loss: 0.2164905276828342 Test Loss: 0.4857067599826389\n",
      "Epoch: 2900 Training Loss: 0.2164737548828125 Test Loss: 0.48568001302083336\n",
      "Epoch: 2901 Training Loss: 0.21647811211480034 Test Loss: 0.48378016493055553\n",
      "Epoch: 2902 Training Loss: 0.21638748677571615 Test Loss: 0.4835139431423611\n",
      "Epoch: 2903 Training Loss: 0.21635805426703558 Test Loss: 0.4832392035590278\n",
      "Epoch: 2904 Training Loss: 0.2162988535563151 Test Loss: 0.4831408420138889\n",
      "Epoch: 2905 Training Loss: 0.21622694566514758 Test Loss: 0.4848474392361111\n",
      "Epoch: 2906 Training Loss: 0.2161691674126519 Test Loss: 0.48715885416666665\n",
      "Epoch: 2907 Training Loss: 0.2161952395968967 Test Loss: 0.4859143337673611\n",
      "Epoch: 2908 Training Loss: 0.21615542772081164 Test Loss: 0.48415570746527775\n",
      "Epoch: 2909 Training Loss: 0.21606934102376302 Test Loss: 0.48414105902777776\n",
      "Epoch: 2910 Training Loss: 0.21604384358723958 Test Loss: 0.4836563042534722\n",
      "Epoch: 2911 Training Loss: 0.2159727528889974 Test Loss: 0.4844220920138889\n",
      "Epoch: 2912 Training Loss: 0.21589307318793402 Test Loss: 0.4868754340277778\n",
      "Epoch: 2913 Training Loss: 0.21589588080512154 Test Loss: 0.48764827473958333\n",
      "Epoch: 2914 Training Loss: 0.21593126254611544 Test Loss: 0.4854064670138889\n",
      "Epoch: 2915 Training Loss: 0.21582913716634114 Test Loss: 0.4847127821180556\n",
      "Epoch: 2916 Training Loss: 0.21579636806911892 Test Loss: 0.4840724826388889\n",
      "Epoch: 2917 Training Loss: 0.21575998942057292 Test Loss: 0.48357687717013886\n",
      "Epoch: 2918 Training Loss: 0.2156858350965712 Test Loss: 0.4863152669270833\n",
      "Epoch: 2919 Training Loss: 0.2156334906684028 Test Loss: 0.48852061631944443\n",
      "Epoch: 2920 Training Loss: 0.21571223788791233 Test Loss: 0.4864161783854167\n",
      "Epoch: 2921 Training Loss: 0.21566043429904513 Test Loss: 0.48573475477430555\n",
      "Epoch: 2922 Training Loss: 0.21566931491427951 Test Loss: 0.4850316840277778\n",
      "Epoch: 2923 Training Loss: 0.21561952379014757 Test Loss: 0.48359044053819444\n",
      "Epoch: 2924 Training Loss: 0.21554316880967883 Test Loss: 0.4845032552083333\n",
      "Epoch: 2925 Training Loss: 0.21551632012261285 Test Loss: 0.4853435329861111\n",
      "Epoch: 2926 Training Loss: 0.2155662841796875 Test Loss: 0.4841484375\n",
      "Epoch: 2927 Training Loss: 0.21555752393934463 Test Loss: 0.48356770833333335\n",
      "Epoch: 2928 Training Loss: 0.2156590118408203 Test Loss: 0.4826203884548611\n",
      "Epoch: 2929 Training Loss: 0.21565683831108942 Test Loss: 0.48313802083333335\n",
      "Epoch: 2930 Training Loss: 0.21556227620442708 Test Loss: 0.48615966796875\n",
      "Epoch: 2931 Training Loss: 0.21569918145073785 Test Loss: 0.48610026041666665\n",
      "Epoch: 2932 Training Loss: 0.2159325917561849 Test Loss: 0.48359120008680556\n",
      "Epoch: 2933 Training Loss: 0.21600221421983506 Test Loss: 0.48237288411458334\n",
      "Epoch: 2934 Training Loss: 0.215808107164171 Test Loss: 0.48223828125\n",
      "Epoch: 2935 Training Loss: 0.21564176771375868 Test Loss: 0.48328304036458336\n",
      "Epoch: 2936 Training Loss: 0.2155105438232422 Test Loss: 0.4829402669270833\n",
      "Epoch: 2937 Training Loss: 0.21528420172797308 Test Loss: 0.4820027669270833\n",
      "Epoch: 2938 Training Loss: 0.21511451382107205 Test Loss: 0.48308865017361113\n",
      "Epoch: 2939 Training Loss: 0.2151751403808594 Test Loss: 0.48197471788194446\n",
      "Epoch: 2940 Training Loss: 0.21508339436848958 Test Loss: 0.4812883029513889\n",
      "Epoch: 2941 Training Loss: 0.2148877648247613 Test Loss: 0.48301204427083333\n",
      "Epoch: 2942 Training Loss: 0.21480005730523002 Test Loss: 0.4832453342013889\n",
      "Epoch: 2943 Training Loss: 0.2146745859781901 Test Loss: 0.48218305121527777\n",
      "Epoch: 2944 Training Loss: 0.2144746805826823 Test Loss: 0.48285107421875\n",
      "Epoch: 2945 Training Loss: 0.21442289055718317 Test Loss: 0.48263878038194447\n",
      "Epoch: 2946 Training Loss: 0.21440440199110242 Test Loss: 0.4818110894097222\n",
      "Epoch: 2947 Training Loss: 0.21433750406901042 Test Loss: 0.48200428602430556\n",
      "Epoch: 2948 Training Loss: 0.21428913879394532 Test Loss: 0.48299300130208334\n",
      "Epoch: 2949 Training Loss: 0.21428811475965712 Test Loss: 0.4824576822916667\n",
      "Epoch: 2950 Training Loss: 0.21421264817979602 Test Loss: 0.482244140625\n",
      "Epoch: 2951 Training Loss: 0.21417560323079426 Test Loss: 0.4831061197916667\n",
      "Epoch: 2952 Training Loss: 0.2142599826388889 Test Loss: 0.4828994140625\n",
      "Epoch: 2953 Training Loss: 0.21432901000976562 Test Loss: 0.4821276584201389\n",
      "Epoch: 2954 Training Loss: 0.2143481648763021 Test Loss: 0.48365321180555554\n",
      "Epoch: 2955 Training Loss: 0.21439161512586805 Test Loss: 0.4852713758680556\n",
      "Epoch: 2956 Training Loss: 0.21452884419759113 Test Loss: 0.48398372395833333\n",
      "Epoch: 2957 Training Loss: 0.21458888075086804 Test Loss: 0.4841195746527778\n",
      "Epoch: 2958 Training Loss: 0.2147617458767361 Test Loss: 0.48548209635416667\n",
      "Epoch: 2959 Training Loss: 0.2150266859266493 Test Loss: 0.48519840494791666\n",
      "Epoch: 2960 Training Loss: 0.21525006612141928 Test Loss: 0.48616162109375\n",
      "Epoch: 2961 Training Loss: 0.21550327385796442 Test Loss: 0.4863866102430556\n",
      "Epoch: 2962 Training Loss: 0.2157144317626953 Test Loss: 0.48470882161458334\n",
      "Epoch: 2963 Training Loss: 0.21586973402235243 Test Loss: 0.48558626302083335\n",
      "Epoch: 2964 Training Loss: 0.21609697808159722 Test Loss: 0.4854199761284722\n",
      "Epoch: 2965 Training Loss: 0.21622958543565537 Test Loss: 0.4852497829861111\n",
      "Epoch: 2966 Training Loss: 0.2159933556450738 Test Loss: 0.48376068793402777\n",
      "Epoch: 2967 Training Loss: 0.21546736145019532 Test Loss: 0.48348155381944447\n",
      "Epoch: 2968 Training Loss: 0.21525558641221787 Test Loss: 0.4854680989583333\n",
      "Epoch: 2969 Training Loss: 0.21519176398383247 Test Loss: 0.4839729817708333\n",
      "Epoch: 2970 Training Loss: 0.21485552639431424 Test Loss: 0.48223106553819445\n",
      "Epoch: 2971 Training Loss: 0.2145092502170139 Test Loss: 0.4841183810763889\n",
      "Epoch: 2972 Training Loss: 0.21438622029622395 Test Loss: 0.48461002604166664\n",
      "Epoch: 2973 Training Loss: 0.21420854187011718 Test Loss: 0.4832892252604167\n",
      "Epoch: 2974 Training Loss: 0.21389501953125 Test Loss: 0.4829821506076389\n",
      "Epoch: 2975 Training Loss: 0.21375644768608942 Test Loss: 0.48367003038194445\n",
      "Epoch: 2976 Training Loss: 0.21382268948025174 Test Loss: 0.4837424587673611\n",
      "Epoch: 2977 Training Loss: 0.21385018751356336 Test Loss: 0.4838156467013889\n",
      "Epoch: 2978 Training Loss: 0.21383033752441405 Test Loss: 0.484810546875\n",
      "Epoch: 2979 Training Loss: 0.21384455023871526 Test Loss: 0.485591796875\n",
      "Epoch: 2980 Training Loss: 0.21379427422417535 Test Loss: 0.48518853081597224\n",
      "Epoch: 2981 Training Loss: 0.2136996392144097 Test Loss: 0.48442762586805554\n",
      "Epoch: 2982 Training Loss: 0.2136458774142795 Test Loss: 0.4842853732638889\n",
      "Epoch: 2983 Training Loss: 0.2137198265923394 Test Loss: 0.48453016493055556\n",
      "Epoch: 2984 Training Loss: 0.2138453589545356 Test Loss: 0.48492458767361113\n",
      "Epoch: 2985 Training Loss: 0.21394351535373263 Test Loss: 0.48536631944444447\n",
      "Epoch: 2986 Training Loss: 0.21403607008192274 Test Loss: 0.4861984592013889\n",
      "Epoch: 2987 Training Loss: 0.21411596001519098 Test Loss: 0.48638411458333336\n",
      "Epoch: 2988 Training Loss: 0.21420452372233073 Test Loss: 0.4850372178819444\n",
      "Epoch: 2989 Training Loss: 0.21424557664659288 Test Loss: 0.48389312065972223\n",
      "Epoch: 2990 Training Loss: 0.21430951436360676 Test Loss: 0.4834014214409722\n",
      "Epoch: 2991 Training Loss: 0.214438478257921 Test Loss: 0.48359559461805557\n",
      "Epoch: 2992 Training Loss: 0.21463519626193575 Test Loss: 0.4838251410590278\n",
      "Epoch: 2993 Training Loss: 0.2148586171468099 Test Loss: 0.48409044053819444\n",
      "Epoch: 2994 Training Loss: 0.21511651441786026 Test Loss: 0.48366861979166664\n",
      "Epoch: 2995 Training Loss: 0.21541579013400608 Test Loss: 0.48259977213541666\n",
      "Epoch: 2996 Training Loss: 0.2157147233751085 Test Loss: 0.48154844835069444\n",
      "Epoch: 2997 Training Loss: 0.21596453857421874 Test Loss: 0.48101155598958334\n",
      "Epoch: 2998 Training Loss: 0.21625215318467883 Test Loss: 0.48177842881944444\n",
      "Epoch: 2999 Training Loss: 0.21659437900119358 Test Loss: 0.4825519748263889\n",
      "Epoch: 3000 Training Loss: 0.216939448038737 Test Loss: 0.48323350694444445\n",
      "Epoch: 3001 Training Loss: 0.21719418165418836 Test Loss: 0.482314453125\n",
      "Epoch: 3002 Training Loss: 0.21729899597167968 Test Loss: 0.4808652886284722\n",
      "Epoch: 3003 Training Loss: 0.21719439866807724 Test Loss: 0.4797861870659722\n",
      "Epoch: 3004 Training Loss: 0.2168802032470703 Test Loss: 0.48065950520833334\n",
      "Epoch: 3005 Training Loss: 0.2164530046251085 Test Loss: 0.48213069661458335\n",
      "Epoch: 3006 Training Loss: 0.2160077379014757 Test Loss: 0.4832369249131944\n",
      "Epoch: 3007 Training Loss: 0.21555567253960503 Test Loss: 0.48333344184027777\n",
      "Epoch: 3008 Training Loss: 0.2150671657986111 Test Loss: 0.4827640516493056\n",
      "Epoch: 3009 Training Loss: 0.21455521647135417 Test Loss: 0.4825476888020833\n",
      "Epoch: 3010 Training Loss: 0.21402315775553385 Test Loss: 0.4823874782986111\n",
      "Epoch: 3011 Training Loss: 0.2135439961751302 Test Loss: 0.4823299696180556\n",
      "Epoch: 3012 Training Loss: 0.21316036648220485 Test Loss: 0.48263850911458334\n",
      "Epoch: 3013 Training Loss: 0.21286624484592015 Test Loss: 0.4831228298611111\n",
      "Epoch: 3014 Training Loss: 0.21262954711914062 Test Loss: 0.4835142144097222\n",
      "Epoch: 3015 Training Loss: 0.21243485514322916 Test Loss: 0.4837719184027778\n",
      "Epoch: 3016 Training Loss: 0.21227910529242622 Test Loss: 0.48368977864583335\n",
      "Epoch: 3017 Training Loss: 0.21215304565429688 Test Loss: 0.48349175347222223\n",
      "Epoch: 3018 Training Loss: 0.21205551147460938 Test Loss: 0.48323773871527775\n",
      "Epoch: 3019 Training Loss: 0.2119766150580512 Test Loss: 0.48319694010416664\n",
      "Epoch: 3020 Training Loss: 0.21191241115993922 Test Loss: 0.4830389539930556\n",
      "Epoch: 3021 Training Loss: 0.21186083814832898 Test Loss: 0.4831456705729167\n",
      "Epoch: 3022 Training Loss: 0.21182877773708766 Test Loss: 0.4831542426215278\n",
      "Epoch: 3023 Training Loss: 0.2118081817626953 Test Loss: 0.4831259765625\n",
      "Epoch: 3024 Training Loss: 0.2117963629828559 Test Loss: 0.4832417534722222\n",
      "Epoch: 3025 Training Loss: 0.21179522874620227 Test Loss: 0.48329134114583333\n",
      "Epoch: 3026 Training Loss: 0.2117953559027778 Test Loss: 0.48337706163194444\n",
      "Epoch: 3027 Training Loss: 0.21179907565646702 Test Loss: 0.48346885850694443\n",
      "Epoch: 3028 Training Loss: 0.21180469597710502 Test Loss: 0.48365901692708335\n",
      "Epoch: 3029 Training Loss: 0.21180844794379342 Test Loss: 0.4839421657986111\n",
      "Epoch: 3030 Training Loss: 0.21182046847873265 Test Loss: 0.4840807834201389\n",
      "Epoch: 3031 Training Loss: 0.2118408677842882 Test Loss: 0.48412982855902775\n",
      "Epoch: 3032 Training Loss: 0.21186908298068577 Test Loss: 0.48393592664930557\n",
      "Epoch: 3033 Training Loss: 0.21190215555826822 Test Loss: 0.48397216796875\n",
      "Epoch: 3034 Training Loss: 0.21194454277886285 Test Loss: 0.4842135416666667\n",
      "Epoch: 3035 Training Loss: 0.21199786546495225 Test Loss: 0.48426220703125\n",
      "Epoch: 3036 Training Loss: 0.212055421617296 Test Loss: 0.4848404947916667\n",
      "Epoch: 3037 Training Loss: 0.21211224704318576 Test Loss: 0.4855921223958333\n",
      "Epoch: 3038 Training Loss: 0.2121922590467665 Test Loss: 0.48599131944444446\n",
      "Epoch: 3039 Training Loss: 0.21226182556152343 Test Loss: 0.48546956380208334\n",
      "Epoch: 3040 Training Loss: 0.21234194946289062 Test Loss: 0.48562369791666665\n",
      "Epoch: 3041 Training Loss: 0.21248922220865885 Test Loss: 0.48534011501736113\n",
      "Epoch: 3042 Training Loss: 0.2126017812093099 Test Loss: 0.48455946180555554\n",
      "Epoch: 3043 Training Loss: 0.21265088568793403 Test Loss: 0.4844356011284722\n",
      "Epoch: 3044 Training Loss: 0.21266168551974826 Test Loss: 0.48341276041666664\n",
      "Epoch: 3045 Training Loss: 0.21272398376464843 Test Loss: 0.4831916775173611\n",
      "Epoch: 3046 Training Loss: 0.21289999050564237 Test Loss: 0.4830519748263889\n",
      "Epoch: 3047 Training Loss: 0.21309882269965277 Test Loss: 0.4822899305555556\n",
      "Epoch: 3048 Training Loss: 0.2132476603190104 Test Loss: 0.48224343532986114\n",
      "Epoch: 3049 Training Loss: 0.2134479709201389 Test Loss: 0.48199137369791667\n",
      "Epoch: 3050 Training Loss: 0.2136963399251302 Test Loss: 0.4819765625\n",
      "Epoch: 3051 Training Loss: 0.21391689215766058 Test Loss: 0.4823177083333333\n",
      "Epoch: 3052 Training Loss: 0.2140415276421441 Test Loss: 0.48183289930555556\n",
      "Epoch: 3053 Training Loss: 0.2141321055094401 Test Loss: 0.4806008029513889\n",
      "Epoch: 3054 Training Loss: 0.21419668918185764 Test Loss: 0.47912934027777776\n",
      "Epoch: 3055 Training Loss: 0.21409554036458334 Test Loss: 0.4784660915798611\n",
      "Epoch: 3056 Training Loss: 0.2139133538140191 Test Loss: 0.4789084201388889\n",
      "Epoch: 3057 Training Loss: 0.21375506422254775 Test Loss: 0.4802782118055556\n",
      "Epoch: 3058 Training Loss: 0.21366393364800348 Test Loss: 0.48232432725694446\n",
      "Epoch: 3059 Training Loss: 0.2136888224283854 Test Loss: 0.4842336154513889\n",
      "Epoch: 3060 Training Loss: 0.21376656087239584 Test Loss: 0.4864697265625\n",
      "Epoch: 3061 Training Loss: 0.21383504231770833 Test Loss: 0.4879340277777778\n",
      "Epoch: 3062 Training Loss: 0.21384984673394097 Test Loss: 0.4878909505208333\n",
      "Epoch: 3063 Training Loss: 0.21379957071940103 Test Loss: 0.4866899956597222\n",
      "Epoch: 3064 Training Loss: 0.21368214416503906 Test Loss: 0.4852418077256944\n",
      "Epoch: 3065 Training Loss: 0.2135942637125651 Test Loss: 0.4840488823784722\n",
      "Epoch: 3066 Training Loss: 0.21369857279459636 Test Loss: 0.4830182834201389\n",
      "Epoch: 3067 Training Loss: 0.2139791497124566 Test Loss: 0.4826552734375\n",
      "Epoch: 3068 Training Loss: 0.2144286380343967 Test Loss: 0.4855352105034722\n",
      "Epoch: 3069 Training Loss: 0.21439869689941407 Test Loss: 0.48545855034722224\n",
      "Epoch: 3070 Training Loss: 0.2143622775607639 Test Loss: 0.486052734375\n",
      "Epoch: 3071 Training Loss: 0.21424159071180557 Test Loss: 0.48594319661458335\n",
      "Epoch: 3072 Training Loss: 0.21406966484917533 Test Loss: 0.48548432074652775\n",
      "Epoch: 3073 Training Loss: 0.21376245625813803 Test Loss: 0.48479817708333334\n",
      "Epoch: 3074 Training Loss: 0.2135054439968533 Test Loss: 0.48389252387152776\n",
      "Epoch: 3075 Training Loss: 0.21327508714463975 Test Loss: 0.48372227647569443\n",
      "Epoch: 3076 Training Loss: 0.21312357584635416 Test Loss: 0.4832941623263889\n",
      "Epoch: 3077 Training Loss: 0.21302552625868054 Test Loss: 0.4829637586805556\n",
      "Epoch: 3078 Training Loss: 0.21295733473036024 Test Loss: 0.4816327039930556\n",
      "Epoch: 3079 Training Loss: 0.21295523240831163 Test Loss: 0.48055870225694447\n",
      "Epoch: 3080 Training Loss: 0.21300139702690973 Test Loss: 0.4793922526041667\n",
      "Epoch: 3081 Training Loss: 0.2132665761311849 Test Loss: 0.4779835611979167\n",
      "Epoch: 3082 Training Loss: 0.21364981926812066 Test Loss: 0.4772228190104167\n",
      "Epoch: 3083 Training Loss: 0.2144703843858507 Test Loss: 0.47784315321180554\n",
      "Epoch: 3084 Training Loss: 0.21587491861979166 Test Loss: 0.49323095703125\n",
      "Epoch: 3085 Training Loss: 0.21914130147298178 Test Loss: 0.5288841145833333\n",
      "Epoch: 3086 Training Loss: 0.22504713609483507 Test Loss: 0.5053067491319444\n",
      "Epoch: 3087 Training Loss: 0.22532649570041233 Test Loss: 0.47395610894097223\n",
      "Epoch: 3088 Training Loss: 0.21777345106336807 Test Loss: 0.4777703450520833\n",
      "Epoch: 3089 Training Loss: 0.21373301866319444 Test Loss: 0.4768142361111111\n",
      "Epoch: 3090 Training Loss: 0.21390555148654514 Test Loss: 0.48829437934027775\n",
      "Epoch: 3091 Training Loss: 0.2155011172824436 Test Loss: 0.48225743272569443\n",
      "Epoch: 3092 Training Loss: 0.21578373548719618 Test Loss: 0.4747214626736111\n",
      "Epoch: 3093 Training Loss: 0.21522533331976995 Test Loss: 0.47483924696180557\n",
      "Epoch: 3094 Training Loss: 0.21492845492892795 Test Loss: 0.47791661241319444\n",
      "Epoch: 3095 Training Loss: 0.21542371453179254 Test Loss: 0.48257703993055556\n",
      "Epoch: 3096 Training Loss: 0.21662180582682292 Test Loss: 0.48616449652777777\n",
      "Epoch: 3097 Training Loss: 0.2181643998887804 Test Loss: 0.4865024956597222\n",
      "Epoch: 3098 Training Loss: 0.21980428568522137 Test Loss: 0.48435167100694443\n",
      "Epoch: 3099 Training Loss: 0.22142163933648004 Test Loss: 0.47980506727430555\n",
      "Epoch: 3100 Training Loss: 0.22290933397081164 Test Loss: 0.4735692816840278\n",
      "Epoch: 3101 Training Loss: 0.22363794962565103 Test Loss: 0.4685968967013889\n",
      "Epoch: 3102 Training Loss: 0.22295205010308158 Test Loss: 0.4683434787326389\n",
      "Epoch: 3103 Training Loss: 0.22091329277886285 Test Loss: 0.47035546875\n",
      "Epoch: 3104 Training Loss: 0.21829525926378038 Test Loss: 0.47186056857638886\n",
      "Epoch: 3105 Training Loss: 0.21583187696668837 Test Loss: 0.4726444769965278\n",
      "Epoch: 3106 Training Loss: 0.21384244961208768 Test Loss: 0.4728325737847222\n",
      "Epoch: 3107 Training Loss: 0.21242050001356336 Test Loss: 0.4732556423611111\n",
      "Epoch: 3108 Training Loss: 0.211506349351671 Test Loss: 0.4739153103298611\n",
      "Epoch: 3109 Training Loss: 0.21100767347547744 Test Loss: 0.4745368381076389\n",
      "Epoch: 3110 Training Loss: 0.21077478875054254 Test Loss: 0.47503634982638887\n",
      "Epoch: 3111 Training Loss: 0.21069494120279947 Test Loss: 0.4750099826388889\n",
      "Epoch: 3112 Training Loss: 0.2106811303032769 Test Loss: 0.47451236979166667\n",
      "Epoch: 3113 Training Loss: 0.21070020887586804 Test Loss: 0.4739927300347222\n",
      "Epoch: 3114 Training Loss: 0.21073342725965713 Test Loss: 0.47336387803819446\n",
      "Epoch: 3115 Training Loss: 0.21076605563693576 Test Loss: 0.47290435112847223\n",
      "Epoch: 3116 Training Loss: 0.21078215026855468 Test Loss: 0.4725580512152778\n",
      "Epoch: 3117 Training Loss: 0.21080842759874133 Test Loss: 0.4723337131076389\n",
      "Epoch: 3118 Training Loss: 0.21086166042751736 Test Loss: 0.4723714192708333\n",
      "Epoch: 3119 Training Loss: 0.21093192715115017 Test Loss: 0.4725125868055556\n",
      "Epoch: 3120 Training Loss: 0.2110414598253038 Test Loss: 0.47278184678819446\n",
      "Epoch: 3121 Training Loss: 0.21118704223632812 Test Loss: 0.4731365559895833\n",
      "Epoch: 3122 Training Loss: 0.21137495761447483 Test Loss: 0.47333474392361113\n",
      "Epoch: 3123 Training Loss: 0.2115861121283637 Test Loss: 0.4737920464409722\n",
      "Epoch: 3124 Training Loss: 0.21182572258843316 Test Loss: 0.47413948567708336\n",
      "Epoch: 3125 Training Loss: 0.21209873962402342 Test Loss: 0.4742541775173611\n",
      "Epoch: 3126 Training Loss: 0.2124039001464844 Test Loss: 0.4742814670138889\n",
      "Epoch: 3127 Training Loss: 0.21274532402886284 Test Loss: 0.47437830946180554\n",
      "Epoch: 3128 Training Loss: 0.21313292270236545 Test Loss: 0.4741512586805556\n",
      "Epoch: 3129 Training Loss: 0.2135576697455512 Test Loss: 0.4741808810763889\n",
      "Epoch: 3130 Training Loss: 0.21402709452311197 Test Loss: 0.4743272026909722\n",
      "Epoch: 3131 Training Loss: 0.21456798638237848 Test Loss: 0.47533783637152777\n",
      "Epoch: 3132 Training Loss: 0.21519671800401477 Test Loss: 0.47649012586805556\n",
      "Epoch: 3133 Training Loss: 0.2159249810112847 Test Loss: 0.47816807725694443\n",
      "Epoch: 3134 Training Loss: 0.21677859157986112 Test Loss: 0.47963134765625\n",
      "Epoch: 3135 Training Loss: 0.2177100084092882 Test Loss: 0.48115646701388887\n",
      "Epoch: 3136 Training Loss: 0.21870829942491318 Test Loss: 0.4825045030381944\n",
      "Epoch: 3137 Training Loss: 0.21971111891004774 Test Loss: 0.4834021809895833\n",
      "Epoch: 3138 Training Loss: 0.22061456976996527 Test Loss: 0.4830905490451389\n",
      "Epoch: 3139 Training Loss: 0.2213058844672309 Test Loss: 0.48120638020833334\n",
      "Epoch: 3140 Training Loss: 0.22160210333930122 Test Loss: 0.47787098524305555\n",
      "Epoch: 3141 Training Loss: 0.22139191012912326 Test Loss: 0.47503765190972225\n",
      "Epoch: 3142 Training Loss: 0.2205875498453776 Test Loss: 0.47317138671875\n",
      "Epoch: 3143 Training Loss: 0.2192400682237413 Test Loss: 0.4723049587673611\n",
      "Epoch: 3144 Training Loss: 0.21758756679958768 Test Loss: 0.472119140625\n",
      "Epoch: 3145 Training Loss: 0.21591138882107205 Test Loss: 0.47260970052083334\n",
      "Epoch: 3146 Training Loss: 0.21443623860677083 Test Loss: 0.47297770182291665\n",
      "Epoch: 3147 Training Loss: 0.21322374301486546 Test Loss: 0.4731400824652778\n",
      "Epoch: 3148 Training Loss: 0.21230378044976128 Test Loss: 0.4733577473958333\n",
      "Epoch: 3149 Training Loss: 0.21165439690483942 Test Loss: 0.47338134765625\n",
      "Epoch: 3150 Training Loss: 0.2112452121310764 Test Loss: 0.4731617838541667\n",
      "Epoch: 3151 Training Loss: 0.21103306240505643 Test Loss: 0.4728389214409722\n",
      "Epoch: 3152 Training Loss: 0.21095883687337239 Test Loss: 0.47277240668402776\n",
      "Epoch: 3153 Training Loss: 0.21101288180881075 Test Loss: 0.4725285373263889\n",
      "Epoch: 3154 Training Loss: 0.21114643351236978 Test Loss: 0.4721876627604167\n",
      "Epoch: 3155 Training Loss: 0.21135584513346353 Test Loss: 0.47194932725694444\n",
      "Epoch: 3156 Training Loss: 0.21163993496365016 Test Loss: 0.4718968098958333\n",
      "Epoch: 3157 Training Loss: 0.21197634887695313 Test Loss: 0.47179031032986113\n",
      "Epoch: 3158 Training Loss: 0.21236411539713543 Test Loss: 0.47194764539930556\n",
      "Epoch: 3159 Training Loss: 0.21279169209798177 Test Loss: 0.47228770616319443\n",
      "Epoch: 3160 Training Loss: 0.21324946933322483 Test Loss: 0.47304286024305553\n",
      "Epoch: 3161 Training Loss: 0.2136964619954427 Test Loss: 0.47428125\n",
      "Epoch: 3162 Training Loss: 0.21415124003092448 Test Loss: 0.476572265625\n",
      "Epoch: 3163 Training Loss: 0.21456385972764758 Test Loss: 0.47938590494791666\n",
      "Epoch: 3164 Training Loss: 0.21495703125 Test Loss: 0.48285948350694446\n",
      "Epoch: 3165 Training Loss: 0.2153214399549696 Test Loss: 0.48668901909722223\n",
      "Epoch: 3166 Training Loss: 0.2156475372314453 Test Loss: 0.4895784505208333\n",
      "Epoch: 3167 Training Loss: 0.21588499959309895 Test Loss: 0.4903750542534722\n",
      "Epoch: 3168 Training Loss: 0.21596792602539064 Test Loss: 0.48815684678819443\n",
      "Epoch: 3169 Training Loss: 0.2158740980360243 Test Loss: 0.48393999565972223\n",
      "Epoch: 3170 Training Loss: 0.21551019795735676 Test Loss: 0.47857329644097224\n",
      "Epoch: 3171 Training Loss: 0.21493001302083334 Test Loss: 0.47322439236111113\n",
      "Epoch: 3172 Training Loss: 0.21420949300130207 Test Loss: 0.4688383246527778\n",
      "Epoch: 3173 Training Loss: 0.2134675513373481 Test Loss: 0.46563416883680553\n",
      "Epoch: 3174 Training Loss: 0.2127778049045139 Test Loss: 0.46362163628472225\n",
      "Epoch: 3175 Training Loss: 0.21219407145182292 Test Loss: 0.4625263129340278\n",
      "Epoch: 3176 Training Loss: 0.2117328830295139 Test Loss: 0.46211143663194443\n",
      "Epoch: 3177 Training Loss: 0.21138016594780815 Test Loss: 0.46211555989583336\n",
      "Epoch: 3178 Training Loss: 0.2111070319281684 Test Loss: 0.46234022352430554\n",
      "Epoch: 3179 Training Loss: 0.21090387810601127 Test Loss: 0.4627383897569444\n",
      "Epoch: 3180 Training Loss: 0.21073514811197916 Test Loss: 0.4631904296875\n",
      "Epoch: 3181 Training Loss: 0.21057789272732205 Test Loss: 0.4637888997395833\n",
      "Epoch: 3182 Training Loss: 0.2104203847249349 Test Loss: 0.4643987630208333\n",
      "Epoch: 3183 Training Loss: 0.2102603505452474 Test Loss: 0.46500005425347224\n",
      "Epoch: 3184 Training Loss: 0.21008937411838108 Test Loss: 0.46558946397569445\n",
      "Epoch: 3185 Training Loss: 0.20989634704589843 Test Loss: 0.4658759223090278\n",
      "Epoch: 3186 Training Loss: 0.20966515265570745 Test Loss: 0.4660853949652778\n",
      "Epoch: 3187 Training Loss: 0.2094303249782986 Test Loss: 0.466318359375\n",
      "Epoch: 3188 Training Loss: 0.20919295586480036 Test Loss: 0.46627256944444445\n",
      "Epoch: 3189 Training Loss: 0.2089546390109592 Test Loss: 0.4663831922743056\n",
      "Epoch: 3190 Training Loss: 0.20871718343098958 Test Loss: 0.4665508355034722\n",
      "Epoch: 3191 Training Loss: 0.20847264777289495 Test Loss: 0.4667373046875\n",
      "Epoch: 3192 Training Loss: 0.2082311248779297 Test Loss: 0.46676649305555556\n",
      "Epoch: 3193 Training Loss: 0.20800267028808594 Test Loss: 0.4667555338541667\n",
      "Epoch: 3194 Training Loss: 0.2078071543375651 Test Loss: 0.46681553819444443\n",
      "Epoch: 3195 Training Loss: 0.20763019137912325 Test Loss: 0.4668006184895833\n",
      "Epoch: 3196 Training Loss: 0.2074747602674696 Test Loss: 0.4668515625\n",
      "Epoch: 3197 Training Loss: 0.20734453667534722 Test Loss: 0.4669111328125\n",
      "Epoch: 3198 Training Loss: 0.2072356499565972 Test Loss: 0.4668474392361111\n",
      "Epoch: 3199 Training Loss: 0.20715613640679253 Test Loss: 0.46683995225694447\n",
      "Epoch: 3200 Training Loss: 0.20709511990017362 Test Loss: 0.46684223090277777\n",
      "Epoch: 3201 Training Loss: 0.20705686611599392 Test Loss: 0.46683121744791667\n",
      "Epoch: 3202 Training Loss: 0.20703644476996527 Test Loss: 0.4668671332465278\n",
      "Epoch: 3203 Training Loss: 0.20703316752115886 Test Loss: 0.46698459201388887\n",
      "Epoch: 3204 Training Loss: 0.20705133395724826 Test Loss: 0.467185546875\n",
      "Epoch: 3205 Training Loss: 0.20709698316786024 Test Loss: 0.4673844401041667\n",
      "Epoch: 3206 Training Loss: 0.20715956454806858 Test Loss: 0.46764670138888886\n",
      "Epoch: 3207 Training Loss: 0.20724483405219185 Test Loss: 0.4680705295138889\n",
      "Epoch: 3208 Training Loss: 0.20735106404622397 Test Loss: 0.468533203125\n",
      "Epoch: 3209 Training Loss: 0.20747882927788627 Test Loss: 0.4691601019965278\n",
      "Epoch: 3210 Training Loss: 0.20763433329264322 Test Loss: 0.46980815972222223\n",
      "Epoch: 3211 Training Loss: 0.2078114505343967 Test Loss: 0.47038975694444446\n",
      "Epoch: 3212 Training Loss: 0.20801850043402778 Test Loss: 0.4710804036458333\n",
      "Epoch: 3213 Training Loss: 0.20824209425184462 Test Loss: 0.4718247612847222\n",
      "Epoch: 3214 Training Loss: 0.20849894375271266 Test Loss: 0.4726369357638889\n",
      "Epoch: 3215 Training Loss: 0.20878526645236545 Test Loss: 0.47344520399305556\n",
      "Epoch: 3216 Training Loss: 0.20910052659776476 Test Loss: 0.4742629123263889\n",
      "Epoch: 3217 Training Loss: 0.20945184156629773 Test Loss: 0.4750667317708333\n",
      "Epoch: 3218 Training Loss: 0.20984149000379773 Test Loss: 0.4757998589409722\n",
      "Epoch: 3219 Training Loss: 0.21027488538953992 Test Loss: 0.47640147569444447\n",
      "Epoch: 3220 Training Loss: 0.2107382134331597 Test Loss: 0.47681852213541664\n",
      "Epoch: 3221 Training Loss: 0.21123903062608507 Test Loss: 0.47715538194444446\n",
      "Epoch: 3222 Training Loss: 0.21174610222710505 Test Loss: 0.4774410807291667\n",
      "Epoch: 3223 Training Loss: 0.2122549811469184 Test Loss: 0.4776100802951389\n",
      "Epoch: 3224 Training Loss: 0.21277294413248699 Test Loss: 0.47812858072916664\n",
      "Epoch: 3225 Training Loss: 0.213281980726454 Test Loss: 0.47898828125\n",
      "Epoch: 3226 Training Loss: 0.21377057393391927 Test Loss: 0.4803972439236111\n",
      "Epoch: 3227 Training Loss: 0.21419366115993924 Test Loss: 0.48244080946180556\n",
      "Epoch: 3228 Training Loss: 0.2145592770046658 Test Loss: 0.48502528211805557\n",
      "Epoch: 3229 Training Loss: 0.21480882093641493 Test Loss: 0.4875244140625\n",
      "Epoch: 3230 Training Loss: 0.21500424194335938 Test Loss: 0.4893888888888889\n",
      "Epoch: 3231 Training Loss: 0.21513553195529514 Test Loss: 0.49001573350694444\n",
      "Epoch: 3232 Training Loss: 0.21521492513020835 Test Loss: 0.48845345052083333\n",
      "Epoch: 3233 Training Loss: 0.21516306220160591 Test Loss: 0.48556271701388887\n",
      "Epoch: 3234 Training Loss: 0.2150306667751736 Test Loss: 0.48224256727430553\n",
      "Epoch: 3235 Training Loss: 0.21480117967393664 Test Loss: 0.47952430555555553\n",
      "Epoch: 3236 Training Loss: 0.21449551222059463 Test Loss: 0.47724593098958334\n",
      "Epoch: 3237 Training Loss: 0.21412947252061632 Test Loss: 0.47565467664930555\n",
      "Epoch: 3238 Training Loss: 0.21370718553331164 Test Loss: 0.47460286458333334\n",
      "Epoch: 3239 Training Loss: 0.2132264116075304 Test Loss: 0.47353016493055555\n",
      "Epoch: 3240 Training Loss: 0.21271270243326823 Test Loss: 0.4728154839409722\n",
      "Epoch: 3241 Training Loss: 0.21217660522460938 Test Loss: 0.47187055121527777\n",
      "Epoch: 3242 Training Loss: 0.2116217312282986 Test Loss: 0.4711310763888889\n",
      "Epoch: 3243 Training Loss: 0.21105111524793838 Test Loss: 0.47035118272569443\n",
      "Epoch: 3244 Training Loss: 0.21048080444335937 Test Loss: 0.4697062174479167\n",
      "Epoch: 3245 Training Loss: 0.20991979132758246 Test Loss: 0.46920773654513886\n",
      "Epoch: 3246 Training Loss: 0.2093874003092448 Test Loss: 0.46860101996527775\n",
      "Epoch: 3247 Training Loss: 0.20889156426323785 Test Loss: 0.46827598741319443\n",
      "Epoch: 3248 Training Loss: 0.20843580627441408 Test Loss: 0.46819108072916665\n",
      "Epoch: 3249 Training Loss: 0.2080205349392361 Test Loss: 0.4680569661458333\n",
      "Epoch: 3250 Training Loss: 0.20764168124728732 Test Loss: 0.4678259548611111\n",
      "Epoch: 3251 Training Loss: 0.20729180060492622 Test Loss: 0.4678547634548611\n",
      "Epoch: 3252 Training Loss: 0.2069800313313802 Test Loss: 0.4678880208333333\n",
      "Epoch: 3253 Training Loss: 0.2066965806749132 Test Loss: 0.46810118272569445\n",
      "Epoch: 3254 Training Loss: 0.20644575670030382 Test Loss: 0.4679548611111111\n",
      "Epoch: 3255 Training Loss: 0.20620624287923178 Test Loss: 0.4680625\n",
      "Epoch: 3256 Training Loss: 0.20599077690972223 Test Loss: 0.46806548394097225\n",
      "Epoch: 3257 Training Loss: 0.2058069339328342 Test Loss: 0.46835020616319445\n",
      "Epoch: 3258 Training Loss: 0.20564207797580294 Test Loss: 0.4685817057291667\n",
      "Epoch: 3259 Training Loss: 0.20548874070909287 Test Loss: 0.4688528645833333\n",
      "Epoch: 3260 Training Loss: 0.20535306633843317 Test Loss: 0.4691723090277778\n",
      "Epoch: 3261 Training Loss: 0.20522879197862412 Test Loss: 0.46937727864583334\n",
      "Epoch: 3262 Training Loss: 0.20511148749457464 Test Loss: 0.4696909722222222\n",
      "Epoch: 3263 Training Loss: 0.2050028296576606 Test Loss: 0.46977739800347224\n",
      "Epoch: 3264 Training Loss: 0.20490269809299044 Test Loss: 0.4699572482638889\n",
      "Epoch: 3265 Training Loss: 0.20480692545572918 Test Loss: 0.47016644965277776\n",
      "Epoch: 3266 Training Loss: 0.20472628614637586 Test Loss: 0.47047081163194443\n",
      "Epoch: 3267 Training Loss: 0.20464674546983508 Test Loss: 0.4708696831597222\n",
      "Epoch: 3268 Training Loss: 0.20457498338487412 Test Loss: 0.47095882161458336\n",
      "Epoch: 3269 Training Loss: 0.20451295640733508 Test Loss: 0.471158203125\n",
      "Epoch: 3270 Training Loss: 0.2044638146294488 Test Loss: 0.4714243706597222\n",
      "Epoch: 3271 Training Loss: 0.20441707526312933 Test Loss: 0.47159982638888887\n",
      "Epoch: 3272 Training Loss: 0.2043717498779297 Test Loss: 0.47170648871527776\n",
      "Epoch: 3273 Training Loss: 0.2043467271592882 Test Loss: 0.4716667751736111\n",
      "Epoch: 3274 Training Loss: 0.204333492702908 Test Loss: 0.4718213433159722\n",
      "Epoch: 3275 Training Loss: 0.20431410725911459 Test Loss: 0.47188134765625\n",
      "Epoch: 3276 Training Loss: 0.20430608791775173 Test Loss: 0.47197786458333335\n",
      "Epoch: 3277 Training Loss: 0.20429556952582464 Test Loss: 0.4720126953125\n",
      "Epoch: 3278 Training Loss: 0.20429753960503472 Test Loss: 0.4721896158854167\n",
      "Epoch: 3279 Training Loss: 0.2043038770887587 Test Loss: 0.4722004123263889\n",
      "Epoch: 3280 Training Loss: 0.20432304043240018 Test Loss: 0.4722048611111111\n",
      "Epoch: 3281 Training Loss: 0.20435276455349394 Test Loss: 0.4724963650173611\n",
      "Epoch: 3282 Training Loss: 0.20436053297254775 Test Loss: 0.472693359375\n",
      "Epoch: 3283 Training Loss: 0.2043713616265191 Test Loss: 0.4730440538194444\n",
      "Epoch: 3284 Training Loss: 0.20438750372992623 Test Loss: 0.47359326171875\n",
      "Epoch: 3285 Training Loss: 0.2044085456000434 Test Loss: 0.4742241753472222\n",
      "Epoch: 3286 Training Loss: 0.20444330681694878 Test Loss: 0.47454296875\n",
      "Epoch: 3287 Training Loss: 0.20449170600043404 Test Loss: 0.4750378146701389\n",
      "Epoch: 3288 Training Loss: 0.20454443528917102 Test Loss: 0.4753294270833333\n",
      "Epoch: 3289 Training Loss: 0.20462086825900608 Test Loss: 0.4762253689236111\n",
      "Epoch: 3290 Training Loss: 0.2047063530815972 Test Loss: 0.47689127604166665\n",
      "Epoch: 3291 Training Loss: 0.20480884297688803 Test Loss: 0.4772843967013889\n",
      "Epoch: 3292 Training Loss: 0.20491211615668403 Test Loss: 0.47778917100694446\n",
      "Epoch: 3293 Training Loss: 0.20502200147840713 Test Loss: 0.47833729383680557\n",
      "Epoch: 3294 Training Loss: 0.20515750122070311 Test Loss: 0.4787663845486111\n",
      "Epoch: 3295 Training Loss: 0.20530783928765192 Test Loss: 0.47909798177083335\n",
      "Epoch: 3296 Training Loss: 0.20548116217719184 Test Loss: 0.47941465928819443\n",
      "Epoch: 3297 Training Loss: 0.20566909620496962 Test Loss: 0.4795606011284722\n",
      "Epoch: 3298 Training Loss: 0.20587302822536893 Test Loss: 0.4795575086805556\n",
      "Epoch: 3299 Training Loss: 0.20609385511610243 Test Loss: 0.4791923828125\n",
      "Epoch: 3300 Training Loss: 0.2063156009250217 Test Loss: 0.47858154296875\n",
      "Epoch: 3301 Training Loss: 0.20654383171929253 Test Loss: 0.47729595269097225\n",
      "Epoch: 3302 Training Loss: 0.20680367363823784 Test Loss: 0.47568923611111114\n",
      "Epoch: 3303 Training Loss: 0.20702335781521267 Test Loss: 0.47393646918402776\n",
      "Epoch: 3304 Training Loss: 0.2072173563639323 Test Loss: 0.4719711371527778\n",
      "Epoch: 3305 Training Loss: 0.2073762427435981 Test Loss: 0.4706193576388889\n",
      "Epoch: 3306 Training Loss: 0.20752618747287327 Test Loss: 0.46902284071180556\n",
      "Epoch: 3307 Training Loss: 0.2076464352077908 Test Loss: 0.46824430338541667\n",
      "Epoch: 3308 Training Loss: 0.20772000122070314 Test Loss: 0.4679806315104167\n",
      "Epoch: 3309 Training Loss: 0.20772038947211371 Test Loss: 0.46802235243055557\n",
      "Epoch: 3310 Training Loss: 0.20768208143446182 Test Loss: 0.4690954318576389\n",
      "Epoch: 3311 Training Loss: 0.2075798848470052 Test Loss: 0.4700250108506944\n",
      "Epoch: 3312 Training Loss: 0.2074524909125434 Test Loss: 0.4710509982638889\n",
      "Epoch: 3313 Training Loss: 0.20728357442220052 Test Loss: 0.47187114800347224\n",
      "Epoch: 3314 Training Loss: 0.2070672115749783 Test Loss: 0.47235557725694444\n",
      "Epoch: 3315 Training Loss: 0.2068362087673611 Test Loss: 0.4723671875\n",
      "Epoch: 3316 Training Loss: 0.20658783298068575 Test Loss: 0.47224663628472224\n",
      "Epoch: 3317 Training Loss: 0.20632762993706596 Test Loss: 0.47189328342013886\n",
      "Epoch: 3318 Training Loss: 0.20606803554958766 Test Loss: 0.4715908745659722\n",
      "Epoch: 3319 Training Loss: 0.20579288736979168 Test Loss: 0.47106336805555554\n",
      "Epoch: 3320 Training Loss: 0.20551483154296876 Test Loss: 0.4708739691840278\n",
      "Epoch: 3321 Training Loss: 0.2052798139784071 Test Loss: 0.4705076497395833\n",
      "Epoch: 3322 Training Loss: 0.2050541008843316 Test Loss: 0.47033463541666665\n",
      "Epoch: 3323 Training Loss: 0.2048620825873481 Test Loss: 0.47018809678819445\n",
      "Epoch: 3324 Training Loss: 0.20468997361924912 Test Loss: 0.4702775607638889\n",
      "Epoch: 3325 Training Loss: 0.20454102240668404 Test Loss: 0.47025157335069445\n",
      "Epoch: 3326 Training Loss: 0.20440861002604166 Test Loss: 0.4705008680555556\n",
      "Epoch: 3327 Training Loss: 0.2042963612874349 Test Loss: 0.470552734375\n",
      "Epoch: 3328 Training Loss: 0.20419818454318575 Test Loss: 0.4708127712673611\n",
      "Epoch: 3329 Training Loss: 0.20410051981608074 Test Loss: 0.47102473958333335\n",
      "Epoch: 3330 Training Loss: 0.20402392578125 Test Loss: 0.4712800564236111\n",
      "Epoch: 3331 Training Loss: 0.2039571058485243 Test Loss: 0.47169618055555557\n",
      "Epoch: 3332 Training Loss: 0.20389180501302084 Test Loss: 0.47190814887152777\n",
      "Epoch: 3333 Training Loss: 0.2038507520887587 Test Loss: 0.4721598849826389\n",
      "Epoch: 3334 Training Loss: 0.2038250478108724 Test Loss: 0.47241069878472225\n",
      "Epoch: 3335 Training Loss: 0.2038087666829427 Test Loss: 0.4726526692708333\n",
      "Epoch: 3336 Training Loss: 0.20380242750379773 Test Loss: 0.47272412109375\n",
      "Epoch: 3337 Training Loss: 0.20380965677897137 Test Loss: 0.4728076171875\n",
      "Epoch: 3338 Training Loss: 0.20382257927788627 Test Loss: 0.47307394748263887\n",
      "Epoch: 3339 Training Loss: 0.20384241570366754 Test Loss: 0.47339029947916667\n",
      "Epoch: 3340 Training Loss: 0.2038185306125217 Test Loss: 0.47349332682291667\n",
      "Epoch: 3341 Training Loss: 0.20378091430664064 Test Loss: 0.4738244357638889\n",
      "Epoch: 3342 Training Loss: 0.20375390116373698 Test Loss: 0.47427983940972224\n",
      "Epoch: 3343 Training Loss: 0.2037240431043837 Test Loss: 0.4744990234375\n",
      "Epoch: 3344 Training Loss: 0.20370889790852864 Test Loss: 0.47481114366319443\n",
      "Epoch: 3345 Training Loss: 0.20371719360351562 Test Loss: 0.4750625542534722\n",
      "Epoch: 3346 Training Loss: 0.20373931545681423 Test Loss: 0.4751628146701389\n",
      "Epoch: 3347 Training Loss: 0.20378795369466146 Test Loss: 0.47528217230902775\n",
      "Epoch: 3348 Training Loss: 0.2038544226752387 Test Loss: 0.4753509657118056\n",
      "Epoch: 3349 Training Loss: 0.2039375440809462 Test Loss: 0.47535026041666667\n",
      "Epoch: 3350 Training Loss: 0.2040453338623047 Test Loss: 0.4753152126736111\n",
      "Epoch: 3351 Training Loss: 0.2041734398735894 Test Loss: 0.47517822265625\n",
      "Epoch: 3352 Training Loss: 0.20433526272243924 Test Loss: 0.4752199435763889\n",
      "Epoch: 3353 Training Loss: 0.2045192684597439 Test Loss: 0.4752528754340278\n",
      "Epoch: 3354 Training Loss: 0.20472689649793838 Test Loss: 0.47521592881944447\n",
      "Epoch: 3355 Training Loss: 0.20496192423502604 Test Loss: 0.47531591796875\n",
      "Epoch: 3356 Training Loss: 0.20522884792751736 Test Loss: 0.47558447265625\n",
      "Epoch: 3357 Training Loss: 0.20555216132269966 Test Loss: 0.4758331705729167\n",
      "Epoch: 3358 Training Loss: 0.20592627800835503 Test Loss: 0.4762373046875\n",
      "Epoch: 3359 Training Loss: 0.2063452385796441 Test Loss: 0.4763397894965278\n",
      "Epoch: 3360 Training Loss: 0.20682868957519532 Test Loss: 0.4764373372395833\n",
      "Epoch: 3361 Training Loss: 0.20740614488389758 Test Loss: 0.47678797743055557\n",
      "Epoch: 3362 Training Loss: 0.20810975138346355 Test Loss: 0.4777892252604167\n",
      "Epoch: 3363 Training Loss: 0.20900706142849393 Test Loss: 0.47926063368055555\n",
      "Epoch: 3364 Training Loss: 0.21024788581000434 Test Loss: 0.47919862196180557\n",
      "Epoch: 3365 Training Loss: 0.2116702168782552 Test Loss: 0.47844748263888887\n",
      "Epoch: 3366 Training Loss: 0.21339014519585503 Test Loss: 0.4761432834201389\n",
      "Epoch: 3367 Training Loss: 0.21540497165256076 Test Loss: 0.47315901692708334\n",
      "Epoch: 3368 Training Loss: 0.21760064358181425 Test Loss: 0.4707548828125\n",
      "Epoch: 3369 Training Loss: 0.22001065402560763 Test Loss: 0.47047330729166664\n",
      "Epoch: 3370 Training Loss: 0.22216155497233073 Test Loss: 0.47430186631944443\n",
      "Epoch: 3371 Training Loss: 0.22397662692599826 Test Loss: 0.479103515625\n",
      "Epoch: 3372 Training Loss: 0.22551089986165365 Test Loss: 0.48020279947916666\n",
      "Epoch: 3373 Training Loss: 0.22625181918674045 Test Loss: 0.47823980034722224\n",
      "Epoch: 3374 Training Loss: 0.22571232265896268 Test Loss: 0.47767692057291666\n",
      "Epoch: 3375 Training Loss: 0.22411650763617622 Test Loss: 0.47632416449652776\n",
      "Epoch: 3376 Training Loss: 0.22214916314019098 Test Loss: 0.4800419379340278\n",
      "Epoch: 3377 Training Loss: 0.22112622918023003 Test Loss: 0.48437033420138886\n",
      "Epoch: 3378 Training Loss: 0.22020802646213108 Test Loss: 0.48446723090277777\n",
      "Epoch: 3379 Training Loss: 0.21938389587402343 Test Loss: 0.48631043836805554\n",
      "Epoch: 3380 Training Loss: 0.21902906121148003 Test Loss: 0.4903181966145833\n",
      "Epoch: 3381 Training Loss: 0.21875929599338106 Test Loss: 0.4883662109375\n",
      "Epoch: 3382 Training Loss: 0.2184560326470269 Test Loss: 0.4865427517361111\n",
      "Epoch: 3383 Training Loss: 0.21850686984592013 Test Loss: 0.4882956814236111\n",
      "Epoch: 3384 Training Loss: 0.21856129286024306 Test Loss: 0.4865192057291667\n",
      "Epoch: 3385 Training Loss: 0.21881592475043402 Test Loss: 0.4830779079861111\n",
      "Epoch: 3386 Training Loss: 0.21964576043023004 Test Loss: 0.4843842230902778\n",
      "Epoch: 3387 Training Loss: 0.22077167087131078 Test Loss: 0.47583843315972224\n",
      "Epoch: 3388 Training Loss: 0.22160279337565103 Test Loss: 0.47534038628472225\n",
      "Epoch: 3389 Training Loss: 0.22280105421278212 Test Loss: 0.47628716362847223\n",
      "Epoch: 3390 Training Loss: 0.22348595852322048 Test Loss: 0.46313075086805555\n",
      "Epoch: 3391 Training Loss: 0.22412599012586806 Test Loss: 0.46850119357638886\n",
      "Epoch: 3392 Training Loss: 0.2243050028483073 Test Loss: 0.46886317274305556\n",
      "Epoch: 3393 Training Loss: 0.22419517686631946 Test Loss: 0.46511610243055557\n",
      "Epoch: 3394 Training Loss: 0.22377707756890192 Test Loss: 0.46847352430555556\n",
      "Epoch: 3395 Training Loss: 0.2231081814236111 Test Loss: 0.46863362630208333\n",
      "Epoch: 3396 Training Loss: 0.22232713996039496 Test Loss: 0.46296549479166665\n",
      "Epoch: 3397 Training Loss: 0.22190054999457465 Test Loss: 0.4640759548611111\n",
      "Epoch: 3398 Training Loss: 0.2213307359483507 Test Loss: 0.45972265625\n",
      "Epoch: 3399 Training Loss: 0.22097715589735242 Test Loss: 0.4559892578125\n",
      "Epoch: 3400 Training Loss: 0.22052844746907552 Test Loss: 0.4571731228298611\n",
      "Epoch: 3401 Training Loss: 0.21939288160536025 Test Loss: 0.4558464626736111\n",
      "Epoch: 3402 Training Loss: 0.21789856635199653 Test Loss: 0.4546970486111111\n",
      "Epoch: 3403 Training Loss: 0.21618965657552083 Test Loss: 0.45504600694444447\n",
      "Epoch: 3404 Training Loss: 0.2142908749050564 Test Loss: 0.4546176215277778\n",
      "Epoch: 3405 Training Loss: 0.21237209235297308 Test Loss: 0.452231689453125\n",
      "Epoch: 3406 Training Loss: 0.2106553955078125 Test Loss: 0.4511855197482639\n",
      "Epoch: 3407 Training Loss: 0.20919027879503038 Test Loss: 0.45086518012152776\n",
      "Epoch: 3408 Training Loss: 0.20792475891113282 Test Loss: 0.4503117404513889\n",
      "Epoch: 3409 Training Loss: 0.2068507588704427 Test Loss: 0.44979635959201386\n",
      "Epoch: 3410 Training Loss: 0.2059636450873481 Test Loss: 0.449599853515625\n",
      "Epoch: 3411 Training Loss: 0.20522764078776043 Test Loss: 0.44962706163194444\n",
      "Epoch: 3412 Training Loss: 0.20462096320258247 Test Loss: 0.44968958875868054\n",
      "Epoch: 3413 Training Loss: 0.20410039774576824 Test Loss: 0.4499198676215278\n",
      "Epoch: 3414 Training Loss: 0.20364825948079426 Test Loss: 0.450076904296875\n",
      "Epoch: 3415 Training Loss: 0.2032532738579644 Test Loss: 0.4502979871961806\n",
      "Epoch: 3416 Training Loss: 0.20290177747938368 Test Loss: 0.45056380208333335\n",
      "Epoch: 3417 Training Loss: 0.2025860833062066 Test Loss: 0.45077143012152776\n",
      "Epoch: 3418 Training Loss: 0.20230214267306856 Test Loss: 0.45098790147569445\n",
      "Epoch: 3419 Training Loss: 0.20204440307617189 Test Loss: 0.45122450086805554\n",
      "Epoch: 3420 Training Loss: 0.2018100789388021 Test Loss: 0.45144520399305554\n",
      "Epoch: 3421 Training Loss: 0.20158997599283854 Test Loss: 0.45171956380208333\n",
      "Epoch: 3422 Training Loss: 0.20139012824164496 Test Loss: 0.45193478732638886\n",
      "Epoch: 3423 Training Loss: 0.20120306057400172 Test Loss: 0.4520734592013889\n",
      "Epoch: 3424 Training Loss: 0.20102780490451388 Test Loss: 0.45230064561631944\n",
      "Epoch: 3425 Training Loss: 0.2008582051595052 Test Loss: 0.4524570583767361\n",
      "Epoch: 3426 Training Loss: 0.2006951446533203 Test Loss: 0.45262171766493053\n",
      "Epoch: 3427 Training Loss: 0.20054767354329428 Test Loss: 0.45270266384548613\n",
      "Epoch: 3428 Training Loss: 0.2004123992919922 Test Loss: 0.45288422309027776\n",
      "Epoch: 3429 Training Loss: 0.20028120930989585 Test Loss: 0.45308414713541667\n",
      "Epoch: 3430 Training Loss: 0.20015645005967883 Test Loss: 0.4532521701388889\n",
      "Epoch: 3431 Training Loss: 0.2000320553249783 Test Loss: 0.45344276258680555\n",
      "Epoch: 3432 Training Loss: 0.19991453552246094 Test Loss: 0.4536142849392361\n",
      "Epoch: 3433 Training Loss: 0.1998002217610677 Test Loss: 0.45375477430555555\n",
      "Epoch: 3434 Training Loss: 0.19968274603949654 Test Loss: 0.45390766059027776\n",
      "Epoch: 3435 Training Loss: 0.1995679931640625 Test Loss: 0.45408534071180556\n",
      "Epoch: 3436 Training Loss: 0.19945960320366754 Test Loss: 0.4542365993923611\n",
      "Epoch: 3437 Training Loss: 0.19935792880588107 Test Loss: 0.45436078559027776\n",
      "Epoch: 3438 Training Loss: 0.1992502950032552 Test Loss: 0.45450908745659724\n",
      "Epoch: 3439 Training Loss: 0.1991485561794705 Test Loss: 0.45465779622395835\n",
      "Epoch: 3440 Training Loss: 0.19904948425292968 Test Loss: 0.45478702799479165\n",
      "Epoch: 3441 Training Loss: 0.1989542270236545 Test Loss: 0.45490771484375\n",
      "Epoch: 3442 Training Loss: 0.19886373562282986 Test Loss: 0.45504050021701387\n",
      "Epoch: 3443 Training Loss: 0.19877205403645834 Test Loss: 0.4551212565104167\n",
      "Epoch: 3444 Training Loss: 0.1986825408935547 Test Loss: 0.45527577039930556\n",
      "Epoch: 3445 Training Loss: 0.19859426879882813 Test Loss: 0.45533995225694446\n",
      "Epoch: 3446 Training Loss: 0.1985086466471354 Test Loss: 0.4554677734375\n",
      "Epoch: 3447 Training Loss: 0.19842752583821616 Test Loss: 0.45556358506944444\n",
      "Epoch: 3448 Training Loss: 0.1983424767388238 Test Loss: 0.45570545789930555\n",
      "Epoch: 3449 Training Loss: 0.19825916883680555 Test Loss: 0.45584114583333335\n",
      "Epoch: 3450 Training Loss: 0.1981807386610243 Test Loss: 0.4558715277777778\n",
      "Epoch: 3451 Training Loss: 0.19810554334852432 Test Loss: 0.4559893120659722\n",
      "Epoch: 3452 Training Loss: 0.19802903747558595 Test Loss: 0.45612999131944443\n",
      "Epoch: 3453 Training Loss: 0.19795704650878906 Test Loss: 0.4562180447048611\n",
      "Epoch: 3454 Training Loss: 0.1978819088406033 Test Loss: 0.45628727213541664\n",
      "Epoch: 3455 Training Loss: 0.1978091295030382 Test Loss: 0.4564238823784722\n",
      "Epoch: 3456 Training Loss: 0.1977365434434679 Test Loss: 0.4564947916666667\n",
      "Epoch: 3457 Training Loss: 0.19766526963975695 Test Loss: 0.4566164822048611\n",
      "Epoch: 3458 Training Loss: 0.1975992889404297 Test Loss: 0.45667393663194444\n",
      "Epoch: 3459 Training Loss: 0.1975291019015842 Test Loss: 0.45688661024305555\n",
      "Epoch: 3460 Training Loss: 0.19745522732204862 Test Loss: 0.4568691948784722\n",
      "Epoch: 3461 Training Loss: 0.19739398532443575 Test Loss: 0.45701421440972223\n",
      "Epoch: 3462 Training Loss: 0.19732242499457464 Test Loss: 0.45712364366319447\n",
      "Epoch: 3463 Training Loss: 0.19726134067111545 Test Loss: 0.45721343315972224\n",
      "Epoch: 3464 Training Loss: 0.19719132317437066 Test Loss: 0.45727034505208336\n",
      "Epoch: 3465 Training Loss: 0.1971289554172092 Test Loss: 0.45737125651041666\n",
      "Epoch: 3466 Training Loss: 0.19706619432237413 Test Loss: 0.45744080946180554\n",
      "Epoch: 3467 Training Loss: 0.19700764465332032 Test Loss: 0.45754649522569446\n",
      "Epoch: 3468 Training Loss: 0.19694319661458334 Test Loss: 0.4577058376736111\n",
      "Epoch: 3469 Training Loss: 0.1968833991156684 Test Loss: 0.45768196614583334\n",
      "Epoch: 3470 Training Loss: 0.19682516649034287 Test Loss: 0.4578156467013889\n",
      "Epoch: 3471 Training Loss: 0.19676538594563803 Test Loss: 0.4579430338541667\n",
      "Epoch: 3472 Training Loss: 0.1967038099500868 Test Loss: 0.4579050021701389\n",
      "Epoch: 3473 Training Loss: 0.19665005154079862 Test Loss: 0.4580320095486111\n",
      "Epoch: 3474 Training Loss: 0.1965874565972222 Test Loss: 0.4582080078125\n",
      "Epoch: 3475 Training Loss: 0.19653289116753472 Test Loss: 0.45821028645833334\n",
      "Epoch: 3476 Training Loss: 0.19647746107313369 Test Loss: 0.45823193359375\n",
      "Epoch: 3477 Training Loss: 0.19641920471191407 Test Loss: 0.4584386935763889\n",
      "Epoch: 3478 Training Loss: 0.19636182149251302 Test Loss: 0.45845822482638887\n",
      "Epoch: 3479 Training Loss: 0.19630978054470485 Test Loss: 0.45840196397569444\n",
      "Epoch: 3480 Training Loss: 0.19625340610080294 Test Loss: 0.45856423611111113\n",
      "Epoch: 3481 Training Loss: 0.19620342508951824 Test Loss: 0.45879242621527777\n",
      "Epoch: 3482 Training Loss: 0.19614857143825956 Test Loss: 0.45879486762152777\n",
      "Epoch: 3483 Training Loss: 0.19609739515516494 Test Loss: 0.4586348741319444\n",
      "Epoch: 3484 Training Loss: 0.1960433654785156 Test Loss: 0.45873556857638886\n",
      "Epoch: 3485 Training Loss: 0.1959918721516927 Test Loss: 0.459091796875\n",
      "Epoch: 3486 Training Loss: 0.1959396735297309 Test Loss: 0.459076171875\n",
      "Epoch: 3487 Training Loss: 0.19588948906792536 Test Loss: 0.4589190538194444\n",
      "Epoch: 3488 Training Loss: 0.19583760409884982 Test Loss: 0.4589567057291667\n",
      "Epoch: 3489 Training Loss: 0.19579012722439237 Test Loss: 0.45928233506944444\n",
      "Epoch: 3490 Training Loss: 0.19574016994900173 Test Loss: 0.4594645724826389\n",
      "Epoch: 3491 Training Loss: 0.19569293382432726 Test Loss: 0.45926459418402776\n",
      "Epoch: 3492 Training Loss: 0.1956416812472873 Test Loss: 0.4591203884548611\n",
      "Epoch: 3493 Training Loss: 0.19559562344021267 Test Loss: 0.4594017469618056\n",
      "Epoch: 3494 Training Loss: 0.19554728529188367 Test Loss: 0.4597209743923611\n",
      "Epoch: 3495 Training Loss: 0.1955015648735894 Test Loss: 0.45979676649305556\n",
      "Epoch: 3496 Training Loss: 0.19545125155978732 Test Loss: 0.4594918619791667\n",
      "Epoch: 3497 Training Loss: 0.19540524122450087 Test Loss: 0.4594055447048611\n",
      "Epoch: 3498 Training Loss: 0.19535834757486978 Test Loss: 0.45968766276041667\n",
      "Epoch: 3499 Training Loss: 0.19531312052408853 Test Loss: 0.46012852647569447\n",
      "Epoch: 3500 Training Loss: 0.19526903279622396 Test Loss: 0.46015711805555554\n",
      "Epoch: 3501 Training Loss: 0.19522359042697482 Test Loss: 0.4597549913194444\n",
      "Epoch: 3502 Training Loss: 0.1951773156060113 Test Loss: 0.45963812934027776\n",
      "Epoch: 3503 Training Loss: 0.19513175455729168 Test Loss: 0.4599103732638889\n",
      "Epoch: 3504 Training Loss: 0.19508650377061632 Test Loss: 0.46043766276041664\n",
      "Epoch: 3505 Training Loss: 0.1950465342203776 Test Loss: 0.4607611219618056\n",
      "Epoch: 3506 Training Loss: 0.19499900817871094 Test Loss: 0.4604103732638889\n",
      "Epoch: 3507 Training Loss: 0.1949570770263672 Test Loss: 0.45996061197916666\n",
      "Epoch: 3508 Training Loss: 0.19491188388400607 Test Loss: 0.4598630642361111\n",
      "Epoch: 3509 Training Loss: 0.1948720177544488 Test Loss: 0.4601299370659722\n",
      "Epoch: 3510 Training Loss: 0.19483237542046442 Test Loss: 0.46088438585069447\n",
      "Epoch: 3511 Training Loss: 0.1947925279405382 Test Loss: 0.46146690538194446\n",
      "Epoch: 3512 Training Loss: 0.19474907938639324 Test Loss: 0.46110899522569443\n",
      "Epoch: 3513 Training Loss: 0.19470962693956162 Test Loss: 0.46043419053819445\n",
      "Epoch: 3514 Training Loss: 0.19466627163357206 Test Loss: 0.46010205078125\n",
      "Epoch: 3515 Training Loss: 0.19462411668565538 Test Loss: 0.4602179361979167\n",
      "Epoch: 3516 Training Loss: 0.19458787706163194 Test Loss: 0.46077506510416666\n",
      "Epoch: 3517 Training Loss: 0.19455264282226561 Test Loss: 0.4617375217013889\n",
      "Epoch: 3518 Training Loss: 0.1945173865424262 Test Loss: 0.4624035373263889\n",
      "Epoch: 3519 Training Loss: 0.19447911071777343 Test Loss: 0.46169287109375\n",
      "Epoch: 3520 Training Loss: 0.19444371710883246 Test Loss: 0.46074864366319446\n",
      "Epoch: 3521 Training Loss: 0.19440781148274738 Test Loss: 0.46044639756944444\n",
      "Epoch: 3522 Training Loss: 0.1943619181315104 Test Loss: 0.4605189887152778\n",
      "Epoch: 3523 Training Loss: 0.19433157348632812 Test Loss: 0.46071788194444446\n",
      "Epoch: 3524 Training Loss: 0.19431133185492622 Test Loss: 0.4620952690972222\n",
      "Epoch: 3525 Training Loss: 0.19428553432888454 Test Loss: 0.4635756293402778\n",
      "Epoch: 3526 Training Loss: 0.1942535620795356 Test Loss: 0.4634538845486111\n",
      "Epoch: 3527 Training Loss: 0.19422488911946614 Test Loss: 0.46164632161458335\n",
      "Epoch: 3528 Training Loss: 0.19420259602864584 Test Loss: 0.4607717013888889\n",
      "Epoch: 3529 Training Loss: 0.19415799458821614 Test Loss: 0.46083740234375\n",
      "Epoch: 3530 Training Loss: 0.19411454094780817 Test Loss: 0.46079606119791666\n",
      "Epoch: 3531 Training Loss: 0.19408072916666666 Test Loss: 0.46185888671875\n",
      "Epoch: 3532 Training Loss: 0.19404708862304687 Test Loss: 0.46427012803819445\n",
      "Epoch: 3533 Training Loss: 0.19402948845757378 Test Loss: 0.46478602430555555\n",
      "Epoch: 3534 Training Loss: 0.1939905039469401 Test Loss: 0.4626118706597222\n",
      "Epoch: 3535 Training Loss: 0.19396181064181858 Test Loss: 0.4611971028645833\n",
      "Epoch: 3536 Training Loss: 0.19390789964463975 Test Loss: 0.46126220703125\n",
      "Epoch: 3537 Training Loss: 0.19385539245605468 Test Loss: 0.4612006293402778\n",
      "Epoch: 3538 Training Loss: 0.19382205369737412 Test Loss: 0.4619584418402778\n",
      "Epoch: 3539 Training Loss: 0.19378861151801216 Test Loss: 0.46469547526041666\n",
      "Epoch: 3540 Training Loss: 0.19377866787380643 Test Loss: 0.4656965060763889\n",
      "Epoch: 3541 Training Loss: 0.1937428215874566 Test Loss: 0.46309559461805555\n",
      "Epoch: 3542 Training Loss: 0.1937095981174045 Test Loss: 0.461578125\n",
      "Epoch: 3543 Training Loss: 0.19365087381998697 Test Loss: 0.46174500868055557\n",
      "Epoch: 3544 Training Loss: 0.19359604899088542 Test Loss: 0.46159168836805553\n",
      "Epoch: 3545 Training Loss: 0.19356192694769966 Test Loss: 0.46258436414930554\n",
      "Epoch: 3546 Training Loss: 0.19352303229437934 Test Loss: 0.46551524522569443\n",
      "Epoch: 3547 Training Loss: 0.1935119171142578 Test Loss: 0.4660061306423611\n",
      "Epoch: 3548 Training Loss: 0.1934821539984809 Test Loss: 0.46308148871527777\n",
      "Epoch: 3549 Training Loss: 0.19344711473253037 Test Loss: 0.4619148220486111\n",
      "Epoch: 3550 Training Loss: 0.19339222717285157 Test Loss: 0.46218668619791664\n",
      "Epoch: 3551 Training Loss: 0.19336053805881076 Test Loss: 0.46181749131944444\n",
      "Epoch: 3552 Training Loss: 0.19331453959147135 Test Loss: 0.46391438802083335\n",
      "Epoch: 3553 Training Loss: 0.19327542622884114 Test Loss: 0.4666846788194444\n",
      "Epoch: 3554 Training Loss: 0.1932455308702257 Test Loss: 0.4653739691840278\n",
      "Epoch: 3555 Training Loss: 0.19320863850911457 Test Loss: 0.46274055989583335\n",
      "Epoch: 3556 Training Loss: 0.19316454230414495 Test Loss: 0.4623627387152778\n",
      "Epoch: 3557 Training Loss: 0.1931184590657552 Test Loss: 0.46239453125\n",
      "Epoch: 3558 Training Loss: 0.19308122931586372 Test Loss: 0.46269498697916667\n",
      "Epoch: 3559 Training Loss: 0.19302946133083768 Test Loss: 0.46577311197916665\n",
      "Epoch: 3560 Training Loss: 0.19302605353461372 Test Loss: 0.46718429904513886\n",
      "Epoch: 3561 Training Loss: 0.19299040222167968 Test Loss: 0.46439637586805554\n",
      "Epoch: 3562 Training Loss: 0.19295430501302083 Test Loss: 0.46271837022569445\n",
      "Epoch: 3563 Training Loss: 0.1929048563639323 Test Loss: 0.4630132921006944\n",
      "Epoch: 3564 Training Loss: 0.19286739264594183 Test Loss: 0.4626025390625\n",
      "Epoch: 3565 Training Loss: 0.19281594000922309 Test Loss: 0.4643838975694444\n",
      "Epoch: 3566 Training Loss: 0.19277789815266927 Test Loss: 0.4674089084201389\n",
      "Epoch: 3567 Training Loss: 0.19275263298882378 Test Loss: 0.4663517252604167\n",
      "Epoch: 3568 Training Loss: 0.1927225087483724 Test Loss: 0.463376953125\n",
      "Epoch: 3569 Training Loss: 0.1926897226969401 Test Loss: 0.4630945095486111\n",
      "Epoch: 3570 Training Loss: 0.19265653483072917 Test Loss: 0.4631458875868056\n",
      "Epoch: 3571 Training Loss: 0.1926237114800347 Test Loss: 0.46357345920138887\n",
      "Epoch: 3572 Training Loss: 0.1925630306667752 Test Loss: 0.46687288411458333\n",
      "Epoch: 3573 Training Loss: 0.19255502319335938 Test Loss: 0.4679299587673611\n",
      "Epoch: 3574 Training Loss: 0.19251537068684896 Test Loss: 0.4644955512152778\n",
      "Epoch: 3575 Training Loss: 0.19248077392578125 Test Loss: 0.46324164496527775\n",
      "Epoch: 3576 Training Loss: 0.19243536207411024 Test Loss: 0.46371896701388887\n",
      "Epoch: 3577 Training Loss: 0.19240116882324218 Test Loss: 0.463396484375\n",
      "Epoch: 3578 Training Loss: 0.1923343709309896 Test Loss: 0.46622205946180556\n",
      "Epoch: 3579 Training Loss: 0.19232104661729602 Test Loss: 0.46845372178819444\n",
      "Epoch: 3580 Training Loss: 0.19227828131781685 Test Loss: 0.46537190755208335\n",
      "Epoch: 3581 Training Loss: 0.19223738606770832 Test Loss: 0.46360400390625\n",
      "Epoch: 3582 Training Loss: 0.19219242350260415 Test Loss: 0.4639219292534722\n",
      "Epoch: 3583 Training Loss: 0.19216741943359375 Test Loss: 0.46369254557291667\n",
      "Epoch: 3584 Training Loss: 0.1921142832438151 Test Loss: 0.46558940972222224\n",
      "Epoch: 3585 Training Loss: 0.19209268697102866 Test Loss: 0.4685989583333333\n",
      "Epoch: 3586 Training Loss: 0.1920630120171441 Test Loss: 0.46674549696180556\n",
      "Epoch: 3587 Training Loss: 0.19201600646972655 Test Loss: 0.4641162651909722\n",
      "Epoch: 3588 Training Loss: 0.19197263929578992 Test Loss: 0.4641257595486111\n",
      "Epoch: 3589 Training Loss: 0.19195164489746094 Test Loss: 0.46392594401041665\n",
      "Epoch: 3590 Training Loss: 0.1919233907063802 Test Loss: 0.46490011935763886\n",
      "Epoch: 3591 Training Loss: 0.19187947760687935 Test Loss: 0.46809434678819445\n",
      "Epoch: 3592 Training Loss: 0.19187390984429253 Test Loss: 0.46833474392361113\n",
      "Epoch: 3593 Training Loss: 0.19185003662109376 Test Loss: 0.4652199435763889\n",
      "Epoch: 3594 Training Loss: 0.19182999844021267 Test Loss: 0.46435405815972225\n",
      "Epoch: 3595 Training Loss: 0.19180671691894532 Test Loss: 0.4644559461805556\n",
      "Epoch: 3596 Training Loss: 0.19175783114963108 Test Loss: 0.46445621744791665\n",
      "Epoch: 3597 Training Loss: 0.19170323181152343 Test Loss: 0.4676296115451389\n",
      "Epoch: 3598 Training Loss: 0.1916976030137804 Test Loss: 0.46908582899305556\n",
      "Epoch: 3599 Training Loss: 0.19166887580023872 Test Loss: 0.4657248263888889\n",
      "Epoch: 3600 Training Loss: 0.19168411593967014 Test Loss: 0.46476866319444443\n",
      "Epoch: 3601 Training Loss: 0.19169580925835503 Test Loss: 0.46526036241319446\n",
      "Epoch: 3602 Training Loss: 0.19166886562771268 Test Loss: 0.46485460069444445\n",
      "Epoch: 3603 Training Loss: 0.19160193718804253 Test Loss: 0.46828819444444447\n",
      "Epoch: 3604 Training Loss: 0.19160628933376736 Test Loss: 0.46896142578125\n",
      "Epoch: 3605 Training Loss: 0.19156241522894965 Test Loss: 0.4650521918402778\n",
      "Epoch: 3606 Training Loss: 0.19155316331651476 Test Loss: 0.46469563802083336\n",
      "Epoch: 3607 Training Loss: 0.19159041002061633 Test Loss: 0.46455126953125\n",
      "Epoch: 3608 Training Loss: 0.1916006622314453 Test Loss: 0.46575721571180556\n",
      "Epoch: 3609 Training Loss: 0.1915896725124783 Test Loss: 0.46798692491319444\n",
      "Epoch: 3610 Training Loss: 0.19156642659505208 Test Loss: 0.46672900390625\n",
      "Epoch: 3611 Training Loss: 0.19159788513183593 Test Loss: 0.46575786675347225\n",
      "Epoch: 3612 Training Loss: 0.19172569105360243 Test Loss: 0.4662438151041667\n",
      "Epoch: 3613 Training Loss: 0.1918223910861545 Test Loss: 0.4655707465277778\n",
      "Epoch: 3614 Training Loss: 0.19184710015190973 Test Loss: 0.46748025173611113\n",
      "Epoch: 3615 Training Loss: 0.19189362080891928 Test Loss: 0.4685896267361111\n",
      "Epoch: 3616 Training Loss: 0.19198546006944445 Test Loss: 0.4659244791666667\n",
      "Epoch: 3617 Training Loss: 0.19213490634494357 Test Loss: 0.46440445963541666\n",
      "Epoch: 3618 Training Loss: 0.19226672024197047 Test Loss: 0.46451893446180553\n",
      "Epoch: 3619 Training Loss: 0.19223328314887153 Test Loss: 0.4660774197048611\n",
      "Epoch: 3620 Training Loss: 0.19214928012424046 Test Loss: 0.46717220052083336\n",
      "Epoch: 3621 Training Loss: 0.19198470730251735 Test Loss: 0.4666013454861111\n",
      "Epoch: 3622 Training Loss: 0.19176310390896267 Test Loss: 0.4671459418402778\n",
      "Epoch: 3623 Training Loss: 0.19168782212999133 Test Loss: 0.46692236328125\n",
      "Epoch: 3624 Training Loss: 0.19163827684190538 Test Loss: 0.4657751193576389\n",
      "Epoch: 3625 Training Loss: 0.19144852701822918 Test Loss: 0.4652172309027778\n",
      "Epoch: 3626 Training Loss: 0.1913022206624349 Test Loss: 0.4650927734375\n",
      "Epoch: 3627 Training Loss: 0.19126185607910157 Test Loss: 0.4647690972222222\n",
      "Epoch: 3628 Training Loss: 0.19120185173882379 Test Loss: 0.4653473307291667\n",
      "Epoch: 3629 Training Loss: 0.19113487921820746 Test Loss: 0.46637548828125\n",
      "Epoch: 3630 Training Loss: 0.19118034193250869 Test Loss: 0.46740234375\n",
      "Epoch: 3631 Training Loss: 0.19128343878851997 Test Loss: 0.46746744791666667\n",
      "Epoch: 3632 Training Loss: 0.19136346605088975 Test Loss: 0.46656184895833336\n",
      "Epoch: 3633 Training Loss: 0.19147747972276477 Test Loss: 0.46629470486111113\n",
      "Epoch: 3634 Training Loss: 0.19161360507541234 Test Loss: 0.46669862196180556\n",
      "Epoch: 3635 Training Loss: 0.19170694478352865 Test Loss: 0.4663996310763889\n",
      "Epoch: 3636 Training Loss: 0.1918154517279731 Test Loss: 0.4670337456597222\n",
      "Epoch: 3637 Training Loss: 0.19188558112250434 Test Loss: 0.4668123372395833\n",
      "Epoch: 3638 Training Loss: 0.1918689897325304 Test Loss: 0.46516758897569443\n",
      "Epoch: 3639 Training Loss: 0.19174785698784721 Test Loss: 0.4666723090277778\n",
      "Epoch: 3640 Training Loss: 0.19165047030978732 Test Loss: 0.4700707465277778\n",
      "Epoch: 3641 Training Loss: 0.19155890740288628 Test Loss: 0.4717178819444444\n",
      "Epoch: 3642 Training Loss: 0.1913643086751302 Test Loss: 0.47049918619791664\n",
      "Epoch: 3643 Training Loss: 0.19117292955186632 Test Loss: 0.4682062717013889\n",
      "Epoch: 3644 Training Loss: 0.19115440368652345 Test Loss: 0.46693299696180557\n",
      "Epoch: 3645 Training Loss: 0.19132991366916233 Test Loss: 0.46616167534722225\n",
      "Epoch: 3646 Training Loss: 0.19156309509277344 Test Loss: 0.46526877170138886\n",
      "Epoch: 3647 Training Loss: 0.19171315341525608 Test Loss: 0.46499761284722224\n",
      "Epoch: 3648 Training Loss: 0.19169616190592448 Test Loss: 0.46727479383680554\n",
      "Epoch: 3649 Training Loss: 0.19162182447645398 Test Loss: 0.4722185872395833\n",
      "Epoch: 3650 Training Loss: 0.19170180765787762 Test Loss: 0.47371712239583336\n",
      "Epoch: 3651 Training Loss: 0.1918614705403646 Test Loss: 0.4693079427083333\n",
      "Epoch: 3652 Training Loss: 0.19173646884494358 Test Loss: 0.4670871853298611\n",
      "Epoch: 3653 Training Loss: 0.19156934611002605 Test Loss: 0.4687185329861111\n",
      "Epoch: 3654 Training Loss: 0.19163574896918403 Test Loss: 0.4675003255208333\n",
      "Epoch: 3655 Training Loss: 0.19164998202853734 Test Loss: 0.4653072916666667\n",
      "Epoch: 3656 Training Loss: 0.19145390319824218 Test Loss: 0.4662615017361111\n",
      "Epoch: 3657 Training Loss: 0.19133246527777778 Test Loss: 0.46984375\n",
      "Epoch: 3658 Training Loss: 0.19130013359917536 Test Loss: 0.47269276258680554\n",
      "Epoch: 3659 Training Loss: 0.19119008043077257 Test Loss: 0.4724189453125\n",
      "Epoch: 3660 Training Loss: 0.19097374131944445 Test Loss: 0.47053634982638887\n",
      "Epoch: 3661 Training Loss: 0.19079662068684897 Test Loss: 0.46927294921875\n",
      "Epoch: 3662 Training Loss: 0.19077562459309896 Test Loss: 0.46702582465277775\n",
      "Epoch: 3663 Training Loss: 0.19081500074598523 Test Loss: 0.4658777669270833\n",
      "Epoch: 3664 Training Loss: 0.19082647705078126 Test Loss: 0.46746115451388887\n",
      "Epoch: 3665 Training Loss: 0.1908739267985026 Test Loss: 0.4706814778645833\n",
      "Epoch: 3666 Training Loss: 0.19094280497233074 Test Loss: 0.47462749565972223\n",
      "Epoch: 3667 Training Loss: 0.19097279527452257 Test Loss: 0.4761482747395833\n",
      "Epoch: 3668 Training Loss: 0.19100765652126736 Test Loss: 0.4725089518229167\n",
      "Epoch: 3669 Training Loss: 0.19111121453179253 Test Loss: 0.4683432074652778\n",
      "Epoch: 3670 Training Loss: 0.19130425008138022 Test Loss: 0.4677128363715278\n",
      "Epoch: 3671 Training Loss: 0.19149356418185765 Test Loss: 0.46750108506944443\n",
      "Epoch: 3672 Training Loss: 0.19163960605197483 Test Loss: 0.4701697048611111\n",
      "Epoch: 3673 Training Loss: 0.191812013414171 Test Loss: 0.47314713541666664\n",
      "Epoch: 3674 Training Loss: 0.19203260464138455 Test Loss: 0.47055490451388887\n",
      "Epoch: 3675 Training Loss: 0.19229141404893663 Test Loss: 0.46752208116319444\n",
      "Epoch: 3676 Training Loss: 0.19258407592773438 Test Loss: 0.4662494032118056\n",
      "Epoch: 3677 Training Loss: 0.1928822241889106 Test Loss: 0.466783203125\n",
      "Epoch: 3678 Training Loss: 0.19324259270562066 Test Loss: 0.4685627170138889\n",
      "Epoch: 3679 Training Loss: 0.1937269778781467 Test Loss: 0.4672637261284722\n",
      "Epoch: 3680 Training Loss: 0.19422925482855902 Test Loss: 0.4656602105034722\n",
      "Epoch: 3681 Training Loss: 0.19470538330078124 Test Loss: 0.4655885959201389\n",
      "Epoch: 3682 Training Loss: 0.19508899943033856 Test Loss: 0.46555745442708335\n",
      "Epoch: 3683 Training Loss: 0.19540587531195747 Test Loss: 0.4646847873263889\n",
      "Epoch: 3684 Training Loss: 0.19568804762098524 Test Loss: 0.4631509331597222\n",
      "Epoch: 3685 Training Loss: 0.19580687798394097 Test Loss: 0.4625153537326389\n",
      "Epoch: 3686 Training Loss: 0.19571382819281685 Test Loss: 0.46244878472222223\n",
      "Epoch: 3687 Training Loss: 0.1954866943359375 Test Loss: 0.4624365234375\n",
      "Epoch: 3688 Training Loss: 0.19515744018554687 Test Loss: 0.4632156032986111\n",
      "Epoch: 3689 Training Loss: 0.1947503441704644 Test Loss: 0.4638908420138889\n",
      "Epoch: 3690 Training Loss: 0.19424772135416668 Test Loss: 0.46377701822916667\n",
      "Epoch: 3691 Training Loss: 0.19366411505805123 Test Loss: 0.4628912760416667\n",
      "Epoch: 3692 Training Loss: 0.19306655714246962 Test Loss: 0.4623102756076389\n",
      "Epoch: 3693 Training Loss: 0.19248104688856338 Test Loss: 0.46248838975694445\n",
      "Epoch: 3694 Training Loss: 0.19190472242567275 Test Loss: 0.4630114474826389\n",
      "Epoch: 3695 Training Loss: 0.1913753136528863 Test Loss: 0.46324527994791664\n",
      "Epoch: 3696 Training Loss: 0.19090680270724827 Test Loss: 0.4632865668402778\n",
      "Epoch: 3697 Training Loss: 0.19051253933376736 Test Loss: 0.46333355034722223\n",
      "Epoch: 3698 Training Loss: 0.19019524976942273 Test Loss: 0.4636232638888889\n",
      "Epoch: 3699 Training Loss: 0.1899426998562283 Test Loss: 0.46406532118055555\n",
      "Epoch: 3700 Training Loss: 0.18972566392686632 Test Loss: 0.4647088216145833\n",
      "Epoch: 3701 Training Loss: 0.1895445319281684 Test Loss: 0.4650391710069444\n",
      "Epoch: 3702 Training Loss: 0.1893933342827691 Test Loss: 0.46525059678819447\n",
      "Epoch: 3703 Training Loss: 0.1892712894015842 Test Loss: 0.4655176866319444\n",
      "Epoch: 3704 Training Loss: 0.18917028299967448 Test Loss: 0.4656463758680556\n",
      "Epoch: 3705 Training Loss: 0.18908257548014323 Test Loss: 0.46577110460069443\n",
      "Epoch: 3706 Training Loss: 0.18899665154351128 Test Loss: 0.4657865668402778\n",
      "Epoch: 3707 Training Loss: 0.18891541375054252 Test Loss: 0.46599457465277777\n",
      "Epoch: 3708 Training Loss: 0.1888387468126085 Test Loss: 0.4659755316840278\n",
      "Epoch: 3709 Training Loss: 0.18876510620117187 Test Loss: 0.46576752387152776\n",
      "Epoch: 3710 Training Loss: 0.18870951165093316 Test Loss: 0.4652099066840278\n",
      "Epoch: 3711 Training Loss: 0.18867622884114582 Test Loss: 0.46457834201388887\n",
      "Epoch: 3712 Training Loss: 0.1886510026719835 Test Loss: 0.46394563802083333\n",
      "Epoch: 3713 Training Loss: 0.18863705444335938 Test Loss: 0.4639269205729167\n",
      "Epoch: 3714 Training Loss: 0.18864931233723958 Test Loss: 0.46407676866319447\n",
      "Epoch: 3715 Training Loss: 0.1887149912516276 Test Loss: 0.46422271050347225\n",
      "Epoch: 3716 Training Loss: 0.18879338582356772 Test Loss: 0.46566525607638887\n",
      "Epoch: 3717 Training Loss: 0.18887783983018663 Test Loss: 0.4694725477430556\n",
      "Epoch: 3718 Training Loss: 0.1889972432454427 Test Loss: 0.4702869466145833\n",
      "Epoch: 3719 Training Loss: 0.18910065375434026 Test Loss: 0.4652821180555556\n",
      "Epoch: 3720 Training Loss: 0.18910305955674914 Test Loss: 0.4634398871527778\n",
      "Epoch: 3721 Training Loss: 0.18902501254611545 Test Loss: 0.46332975260416664\n",
      "Epoch: 3722 Training Loss: 0.18897049967447915 Test Loss: 0.46384483506944446\n",
      "Epoch: 3723 Training Loss: 0.18885335286458332 Test Loss: 0.46878618706597225\n",
      "Epoch: 3724 Training Loss: 0.18887644619411892 Test Loss: 0.46875954861111113\n",
      "Epoch: 3725 Training Loss: 0.18886575317382812 Test Loss: 0.46416048177083336\n",
      "Epoch: 3726 Training Loss: 0.18882314893934463 Test Loss: 0.4635517578125\n",
      "Epoch: 3727 Training Loss: 0.18880242580837672 Test Loss: 0.46397265625\n",
      "Epoch: 3728 Training Loss: 0.18878472900390625 Test Loss: 0.46540825737847225\n",
      "Epoch: 3729 Training Loss: 0.1887838406032986 Test Loss: 0.4681433376736111\n",
      "Epoch: 3730 Training Loss: 0.18879770236545138 Test Loss: 0.46700667317708333\n",
      "Epoch: 3731 Training Loss: 0.188811277601454 Test Loss: 0.4638511284722222\n",
      "Epoch: 3732 Training Loss: 0.1888418494330512 Test Loss: 0.4633135308159722\n",
      "Epoch: 3733 Training Loss: 0.18889727783203125 Test Loss: 0.46330094401041666\n",
      "Epoch: 3734 Training Loss: 0.18890544976128473 Test Loss: 0.4647203776041667\n",
      "Epoch: 3735 Training Loss: 0.18890695359971788 Test Loss: 0.46660221354166664\n",
      "Epoch: 3736 Training Loss: 0.18894216579861112 Test Loss: 0.4652013346354167\n",
      "Epoch: 3737 Training Loss: 0.18902042812771266 Test Loss: 0.46293587239583334\n",
      "Epoch: 3738 Training Loss: 0.18912349955240884 Test Loss: 0.4628802083333333\n",
      "Epoch: 3739 Training Loss: 0.18927808973524304 Test Loss: 0.46350748697916666\n",
      "Epoch: 3740 Training Loss: 0.18945623440212675 Test Loss: 0.4648572591145833\n",
      "Epoch: 3741 Training Loss: 0.18967816501193577 Test Loss: 0.4659267578125\n",
      "Epoch: 3742 Training Loss: 0.18996335856119792 Test Loss: 0.4636909722222222\n",
      "Epoch: 3743 Training Loss: 0.19022493998209636 Test Loss: 0.4620720486111111\n",
      "Epoch: 3744 Training Loss: 0.19043976169162327 Test Loss: 0.46244222005208335\n",
      "Epoch: 3745 Training Loss: 0.19062697007921006 Test Loss: 0.4638695746527778\n",
      "Epoch: 3746 Training Loss: 0.19080234951443142 Test Loss: 0.4655115017361111\n",
      "Epoch: 3747 Training Loss: 0.1909788835313585 Test Loss: 0.4649050021701389\n",
      "Epoch: 3748 Training Loss: 0.19121087985568577 Test Loss: 0.46317247178819443\n",
      "Epoch: 3749 Training Loss: 0.19136656867133248 Test Loss: 0.4623351779513889\n",
      "Epoch: 3750 Training Loss: 0.19130337863498265 Test Loss: 0.4649010416666667\n",
      "Epoch: 3751 Training Loss: 0.19119762674967447 Test Loss: 0.4653103841145833\n",
      "Epoch: 3752 Training Loss: 0.1911687503390842 Test Loss: 0.46206846788194444\n",
      "Epoch: 3753 Training Loss: 0.1910034417046441 Test Loss: 0.46128504774305557\n",
      "Epoch: 3754 Training Loss: 0.1905993160671658 Test Loss: 0.46603331163194445\n",
      "Epoch: 3755 Training Loss: 0.19027566528320314 Test Loss: 0.4703504774305556\n",
      "Epoch: 3756 Training Loss: 0.19014953782823352 Test Loss: 0.4682255859375\n",
      "Epoch: 3757 Training Loss: 0.1900139634874132 Test Loss: 0.46434635416666664\n",
      "Epoch: 3758 Training Loss: 0.18974825710720486 Test Loss: 0.463712890625\n",
      "Epoch: 3759 Training Loss: 0.18936337619357638 Test Loss: 0.4653794487847222\n",
      "Epoch: 3760 Training Loss: 0.1891090850830078 Test Loss: 0.4646853298611111\n",
      "Epoch: 3761 Training Loss: 0.18892306179470486 Test Loss: 0.46242925347222225\n",
      "Epoch: 3762 Training Loss: 0.1886712409125434 Test Loss: 0.4631448025173611\n",
      "Epoch: 3763 Training Loss: 0.1884468553331163 Test Loss: 0.46542333984375\n",
      "Epoch: 3764 Training Loss: 0.18831130811903213 Test Loss: 0.46638297526041667\n",
      "Epoch: 3765 Training Loss: 0.18818955993652345 Test Loss: 0.4656216362847222\n",
      "Epoch: 3766 Training Loss: 0.18805459764268664 Test Loss: 0.46478114149305555\n",
      "Epoch: 3767 Training Loss: 0.18792951287163628 Test Loss: 0.4650595703125\n",
      "Epoch: 3768 Training Loss: 0.18785819668240017 Test Loss: 0.46544645182291666\n",
      "Epoch: 3769 Training Loss: 0.18780299038357204 Test Loss: 0.46529551866319446\n",
      "Epoch: 3770 Training Loss: 0.1877386966281467 Test Loss: 0.46550965711805553\n",
      "Epoch: 3771 Training Loss: 0.18766974555121527 Test Loss: 0.4665310872395833\n",
      "Epoch: 3772 Training Loss: 0.18763792080349392 Test Loss: 0.4685416666666667\n",
      "Epoch: 3773 Training Loss: 0.18763434516059027 Test Loss: 0.4709163953993056\n",
      "Epoch: 3774 Training Loss: 0.18763447401258682 Test Loss: 0.4728080512152778\n",
      "Epoch: 3775 Training Loss: 0.1876681620279948 Test Loss: 0.4723887261284722\n",
      "Epoch: 3776 Training Loss: 0.18768679640028213 Test Loss: 0.4706623806423611\n",
      "Epoch: 3777 Training Loss: 0.1877171427408854 Test Loss: 0.46999251302083334\n",
      "Epoch: 3778 Training Loss: 0.18773810662163629 Test Loss: 0.4705260416666667\n",
      "Epoch: 3779 Training Loss: 0.18771402147081162 Test Loss: 0.47004573567708335\n",
      "Epoch: 3780 Training Loss: 0.1876380377875434 Test Loss: 0.46913997395833335\n",
      "Epoch: 3781 Training Loss: 0.18754793294270833 Test Loss: 0.4686511501736111\n",
      "Epoch: 3782 Training Loss: 0.18746190728081596 Test Loss: 0.46887586805555553\n",
      "Epoch: 3783 Training Loss: 0.18739488220214845 Test Loss: 0.4690851779513889\n",
      "Epoch: 3784 Training Loss: 0.18733875020345053 Test Loss: 0.4688894314236111\n",
      "Epoch: 3785 Training Loss: 0.18729498630099825 Test Loss: 0.46858241102430553\n",
      "Epoch: 3786 Training Loss: 0.18726571146647136 Test Loss: 0.4684626736111111\n",
      "Epoch: 3787 Training Loss: 0.18725908745659722 Test Loss: 0.4683898654513889\n",
      "Epoch: 3788 Training Loss: 0.18727551947699653 Test Loss: 0.4681838650173611\n",
      "Epoch: 3789 Training Loss: 0.1873099907769097 Test Loss: 0.46830615234375\n",
      "Epoch: 3790 Training Loss: 0.18734527587890626 Test Loss: 0.46843782552083335\n",
      "Epoch: 3791 Training Loss: 0.1873979271782769 Test Loss: 0.46888829210069444\n",
      "Epoch: 3792 Training Loss: 0.18749804009331597 Test Loss: 0.46894466145833336\n",
      "Epoch: 3793 Training Loss: 0.1876451619466146 Test Loss: 0.46778499348958336\n",
      "Epoch: 3794 Training Loss: 0.18782016838921442 Test Loss: 0.4661413302951389\n",
      "Epoch: 3795 Training Loss: 0.18798258293999565 Test Loss: 0.46524186197916667\n",
      "Epoch: 3796 Training Loss: 0.18812769571940105 Test Loss: 0.4654887152777778\n",
      "Epoch: 3797 Training Loss: 0.18826932610405817 Test Loss: 0.46786626519097224\n",
      "Epoch: 3798 Training Loss: 0.1884397464328342 Test Loss: 0.4713330620659722\n",
      "Epoch: 3799 Training Loss: 0.18864327663845487 Test Loss: 0.47181564670138887\n",
      "Epoch: 3800 Training Loss: 0.18879276360405817 Test Loss: 0.4677814670138889\n",
      "Epoch: 3801 Training Loss: 0.18890983412000867 Test Loss: 0.46486393229166667\n",
      "Epoch: 3802 Training Loss: 0.18904254319932726 Test Loss: 0.46335997178819444\n",
      "Epoch: 3803 Training Loss: 0.18921006774902344 Test Loss: 0.4632095269097222\n",
      "Epoch: 3804 Training Loss: 0.1893818155924479 Test Loss: 0.46436018880208335\n",
      "Epoch: 3805 Training Loss: 0.1895839080810547 Test Loss: 0.46299207899305556\n",
      "Epoch: 3806 Training Loss: 0.18961238437228733 Test Loss: 0.46194840494791667\n",
      "Epoch: 3807 Training Loss: 0.18953487989637588 Test Loss: 0.4644148220486111\n",
      "Epoch: 3808 Training Loss: 0.18926431613498265 Test Loss: 0.46723177083333334\n",
      "Epoch: 3809 Training Loss: 0.18893907843695745 Test Loss: 0.46822639973958335\n",
      "Epoch: 3810 Training Loss: 0.1886702626546224 Test Loss: 0.46757953559027776\n",
      "Epoch: 3811 Training Loss: 0.18842652384440103 Test Loss: 0.46801123046875\n",
      "Epoch: 3812 Training Loss: 0.18819063313802084 Test Loss: 0.46930392795138887\n",
      "Epoch: 3813 Training Loss: 0.1879678988986545 Test Loss: 0.46880164930555557\n",
      "Epoch: 3814 Training Loss: 0.1878111826578776 Test Loss: 0.46683430989583335\n",
      "Epoch: 3815 Training Loss: 0.18766952684190538 Test Loss: 0.4653363172743056\n",
      "Epoch: 3816 Training Loss: 0.18752357991536459 Test Loss: 0.46487364366319445\n",
      "Epoch: 3817 Training Loss: 0.18739087422688802 Test Loss: 0.4653660481770833\n",
      "Epoch: 3818 Training Loss: 0.18725823296440972 Test Loss: 0.46568419053819443\n",
      "Epoch: 3819 Training Loss: 0.18716609191894532 Test Loss: 0.46580126953125\n",
      "Epoch: 3820 Training Loss: 0.18714183722601996 Test Loss: 0.46572694227430556\n",
      "Epoch: 3821 Training Loss: 0.18715464274088542 Test Loss: 0.4654084201388889\n",
      "Epoch: 3822 Training Loss: 0.18715142652723524 Test Loss: 0.4650202907986111\n",
      "Epoch: 3823 Training Loss: 0.187105962117513 Test Loss: 0.46529036458333334\n",
      "Epoch: 3824 Training Loss: 0.1870196261935764 Test Loss: 0.4662282986111111\n",
      "Epoch: 3825 Training Loss: 0.1869825914171007 Test Loss: 0.46679698350694443\n",
      "Epoch: 3826 Training Loss: 0.18701441107855904 Test Loss: 0.46535753038194444\n",
      "Epoch: 3827 Training Loss: 0.18706593661838108 Test Loss: 0.4636359592013889\n",
      "Epoch: 3828 Training Loss: 0.18705821058485242 Test Loss: 0.4628215603298611\n",
      "Epoch: 3829 Training Loss: 0.18703306579589843 Test Loss: 0.4638628472222222\n",
      "Epoch: 3830 Training Loss: 0.1870702853732639 Test Loss: 0.46571462673611114\n",
      "Epoch: 3831 Training Loss: 0.1871638675265842 Test Loss: 0.46668446180555556\n",
      "Epoch: 3832 Training Loss: 0.1872552744547526 Test Loss: 0.4681488172743056\n",
      "Epoch: 3833 Training Loss: 0.1871507059733073 Test Loss: 0.4702486979166667\n",
      "Epoch: 3834 Training Loss: 0.18695569525824654 Test Loss: 0.47022439236111113\n",
      "Epoch: 3835 Training Loss: 0.18678990851508245 Test Loss: 0.4686521267361111\n",
      "Epoch: 3836 Training Loss: 0.18666348436143662 Test Loss: 0.46684852430555557\n",
      "Epoch: 3837 Training Loss: 0.18657147386338976 Test Loss: 0.46557584635416666\n",
      "Epoch: 3838 Training Loss: 0.18650236172146267 Test Loss: 0.4654233940972222\n",
      "Epoch: 3839 Training Loss: 0.18641471523708766 Test Loss: 0.46661881510416664\n",
      "Epoch: 3840 Training Loss: 0.1862955356174045 Test Loss: 0.46754801432291665\n",
      "Epoch: 3841 Training Loss: 0.1862152353922526 Test Loss: 0.4669475368923611\n",
      "Epoch: 3842 Training Loss: 0.18620721266004775 Test Loss: 0.46541541883680554\n",
      "Epoch: 3843 Training Loss: 0.18619866604275173 Test Loss: 0.46459315321180555\n",
      "Epoch: 3844 Training Loss: 0.18618399726019966 Test Loss: 0.4646567925347222\n",
      "Epoch: 3845 Training Loss: 0.1861516571044922 Test Loss: 0.46489078776041665\n",
      "Epoch: 3846 Training Loss: 0.18615162150065104 Test Loss: 0.46466286892361114\n",
      "Epoch: 3847 Training Loss: 0.18615018886990017 Test Loss: 0.4643950737847222\n",
      "Epoch: 3848 Training Loss: 0.18612127007378473 Test Loss: 0.46492371961805556\n",
      "Epoch: 3849 Training Loss: 0.18612998792860244 Test Loss: 0.46576622178819443\n",
      "Epoch: 3850 Training Loss: 0.18614441935221354 Test Loss: 0.46686930338541666\n",
      "Epoch: 3851 Training Loss: 0.18615848626030815 Test Loss: 0.467353515625\n",
      "Epoch: 3852 Training Loss: 0.18617469278971355 Test Loss: 0.4674861653645833\n",
      "Epoch: 3853 Training Loss: 0.1861894056532118 Test Loss: 0.4672833116319444\n",
      "Epoch: 3854 Training Loss: 0.18620556810167102 Test Loss: 0.46794764539930556\n",
      "Epoch: 3855 Training Loss: 0.18621387227376301 Test Loss: 0.4678034939236111\n",
      "Epoch: 3856 Training Loss: 0.1862447764078776 Test Loss: 0.46694509548611113\n",
      "Epoch: 3857 Training Loss: 0.18627579922146267 Test Loss: 0.4664739040798611\n",
      "Epoch: 3858 Training Loss: 0.18628773837619358 Test Loss: 0.46690706380208336\n",
      "Epoch: 3859 Training Loss: 0.18628910319010417 Test Loss: 0.46777430555555555\n",
      "Epoch: 3860 Training Loss: 0.1862999284532335 Test Loss: 0.46845985243055555\n",
      "Epoch: 3861 Training Loss: 0.18633682929144965 Test Loss: 0.4689934895833333\n",
      "Epoch: 3862 Training Loss: 0.18639215087890626 Test Loss: 0.46906043836805555\n",
      "Epoch: 3863 Training Loss: 0.18648603142632378 Test Loss: 0.4693962673611111\n",
      "Epoch: 3864 Training Loss: 0.18660657925075955 Test Loss: 0.46980333116319445\n",
      "Epoch: 3865 Training Loss: 0.1867524888780382 Test Loss: 0.46963172743055553\n",
      "Epoch: 3866 Training Loss: 0.18682697041829427 Test Loss: 0.4680299479166667\n",
      "Epoch: 3867 Training Loss: 0.18687443033854167 Test Loss: 0.4663492838541667\n",
      "Epoch: 3868 Training Loss: 0.18688352796766494 Test Loss: 0.46569200303819447\n",
      "Epoch: 3869 Training Loss: 0.186873291015625 Test Loss: 0.4653337673611111\n",
      "Epoch: 3870 Training Loss: 0.18687014600965712 Test Loss: 0.4647981770833333\n",
      "Epoch: 3871 Training Loss: 0.1868720516628689 Test Loss: 0.464341796875\n",
      "Epoch: 3872 Training Loss: 0.1868529357910156 Test Loss: 0.46432628038194446\n",
      "Epoch: 3873 Training Loss: 0.18686708407931857 Test Loss: 0.4650336371527778\n",
      "Epoch: 3874 Training Loss: 0.18697365315755207 Test Loss: 0.46533425564236114\n",
      "Epoch: 3875 Training Loss: 0.1870897742377387 Test Loss: 0.4652760959201389\n",
      "Epoch: 3876 Training Loss: 0.18716368442111544 Test Loss: 0.4657400173611111\n",
      "Epoch: 3877 Training Loss: 0.18720095655653213 Test Loss: 0.4660941840277778\n",
      "Epoch: 3878 Training Loss: 0.18727914428710937 Test Loss: 0.46577870008680555\n",
      "Epoch: 3879 Training Loss: 0.18742842441134983 Test Loss: 0.4653676215277778\n",
      "Epoch: 3880 Training Loss: 0.18762261454264323 Test Loss: 0.4657900933159722\n",
      "Epoch: 3881 Training Loss: 0.18784242248535157 Test Loss: 0.4665979275173611\n",
      "Epoch: 3882 Training Loss: 0.18812898932562935 Test Loss: 0.46862141927083334\n",
      "Epoch: 3883 Training Loss: 0.18851590813530816 Test Loss: 0.47056141493055553\n",
      "Epoch: 3884 Training Loss: 0.18902206420898438 Test Loss: 0.47259977213541665\n",
      "Epoch: 3885 Training Loss: 0.18958788045247396 Test Loss: 0.47422992621527776\n",
      "Epoch: 3886 Training Loss: 0.19027655198838975 Test Loss: 0.4749015842013889\n",
      "Epoch: 3887 Training Loss: 0.19111447482638888 Test Loss: 0.47487055121527777\n",
      "Epoch: 3888 Training Loss: 0.19226952107747397 Test Loss: 0.4745784505208333\n",
      "Epoch: 3889 Training Loss: 0.19370850626627603 Test Loss: 0.4742191840277778\n",
      "Epoch: 3890 Training Loss: 0.19571473015679253 Test Loss: 0.47474864366319447\n",
      "Epoch: 3891 Training Loss: 0.1984559800889757 Test Loss: 0.47454210069444447\n",
      "Epoch: 3892 Training Loss: 0.20160107421875 Test Loss: 0.47186865234375\n",
      "Epoch: 3893 Training Loss: 0.20585352918836805 Test Loss: 0.47265028211805554\n",
      "Epoch: 3894 Training Loss: 0.21195831637912327 Test Loss: 0.48973111979166667\n",
      "Epoch: 3895 Training Loss: 0.2175067884657118 Test Loss: 0.4820421006944444\n",
      "Epoch: 3896 Training Loss: 0.21439811197916667 Test Loss: 0.5041940646701389\n",
      "Epoch: 3897 Training Loss: 0.20569209967719185 Test Loss: 0.49845171440972225\n",
      "Epoch: 3898 Training Loss: 0.19912891642252603 Test Loss: 0.4832720811631944\n",
      "Epoch: 3899 Training Loss: 0.19518609110514323 Test Loss: 0.4688525390625\n",
      "Epoch: 3900 Training Loss: 0.19238387722439237 Test Loss: 0.46700667317708333\n",
      "Epoch: 3901 Training Loss: 0.190440672132704 Test Loss: 0.45784966362847224\n",
      "Epoch: 3902 Training Loss: 0.18882196384006075 Test Loss: 0.45385107421875\n",
      "Epoch: 3903 Training Loss: 0.18756035698784723 Test Loss: 0.45265296766493057\n",
      "Epoch: 3904 Training Loss: 0.18664173889160157 Test Loss: 0.45216476779513887\n",
      "Epoch: 3905 Training Loss: 0.18602921379937065 Test Loss: 0.4523595106336806\n",
      "Epoch: 3906 Training Loss: 0.1857011006673177 Test Loss: 0.4530503472222222\n",
      "Epoch: 3907 Training Loss: 0.18552123175726998 Test Loss: 0.4541605902777778\n",
      "Epoch: 3908 Training Loss: 0.1854210900200738 Test Loss: 0.45534190538194447\n",
      "Epoch: 3909 Training Loss: 0.1853540717230903 Test Loss: 0.45636691623263886\n",
      "Epoch: 3910 Training Loss: 0.18530138651529948 Test Loss: 0.4570375434027778\n",
      "Epoch: 3911 Training Loss: 0.18524292839898004 Test Loss: 0.4573967556423611\n",
      "Epoch: 3912 Training Loss: 0.18517476230197483 Test Loss: 0.45726106770833336\n",
      "Epoch: 3913 Training Loss: 0.18508957248263888 Test Loss: 0.4568166775173611\n",
      "Epoch: 3914 Training Loss: 0.18497579108344184 Test Loss: 0.45605935329861114\n",
      "Epoch: 3915 Training Loss: 0.18482625325520832 Test Loss: 0.4551611870659722\n",
      "Epoch: 3916 Training Loss: 0.18463948228624133 Test Loss: 0.45452403428819443\n",
      "Epoch: 3917 Training Loss: 0.1843961893717448 Test Loss: 0.45427861870659725\n",
      "Epoch: 3918 Training Loss: 0.18413529290093317 Test Loss: 0.4542650824652778\n",
      "Epoch: 3919 Training Loss: 0.18388102383083768 Test Loss: 0.45459488932291664\n",
      "Epoch: 3920 Training Loss: 0.18361667209201388 Test Loss: 0.4550496148003472\n",
      "Epoch: 3921 Training Loss: 0.1833509318033854 Test Loss: 0.45543245442708336\n",
      "Epoch: 3922 Training Loss: 0.1831147749159071 Test Loss: 0.4557002495659722\n",
      "Epoch: 3923 Training Loss: 0.18291387600368925 Test Loss: 0.4557502170138889\n",
      "Epoch: 3924 Training Loss: 0.1827433098687066 Test Loss: 0.4557204861111111\n",
      "Epoch: 3925 Training Loss: 0.182613035413954 Test Loss: 0.4556409505208333\n",
      "Epoch: 3926 Training Loss: 0.18252552117241755 Test Loss: 0.4556340060763889\n",
      "Epoch: 3927 Training Loss: 0.18247282240125867 Test Loss: 0.45549755859375\n",
      "Epoch: 3928 Training Loss: 0.18245279608832465 Test Loss: 0.45538975694444445\n",
      "Epoch: 3929 Training Loss: 0.18245963711208768 Test Loss: 0.45543782552083334\n",
      "Epoch: 3930 Training Loss: 0.18249115329318577 Test Loss: 0.4555015190972222\n",
      "Epoch: 3931 Training Loss: 0.18254972839355468 Test Loss: 0.45569297960069444\n",
      "Epoch: 3932 Training Loss: 0.18262986246744792 Test Loss: 0.4560435112847222\n",
      "Epoch: 3933 Training Loss: 0.18272391594780815 Test Loss: 0.456603515625\n",
      "Epoch: 3934 Training Loss: 0.18285745069715711 Test Loss: 0.45731260850694444\n",
      "Epoch: 3935 Training Loss: 0.18304354858398436 Test Loss: 0.4586062282986111\n",
      "Epoch: 3936 Training Loss: 0.18329965549045138 Test Loss: 0.4607492404513889\n",
      "Epoch: 3937 Training Loss: 0.1836496327718099 Test Loss: 0.46345833333333336\n",
      "Epoch: 3938 Training Loss: 0.18413879564073352 Test Loss: 0.46735438368055554\n",
      "Epoch: 3939 Training Loss: 0.18481768798828124 Test Loss: 0.4711261935763889\n",
      "Epoch: 3940 Training Loss: 0.18562998962402344 Test Loss: 0.4700762261284722\n",
      "Epoch: 3941 Training Loss: 0.18641746012369792 Test Loss: 0.46267323133680555\n",
      "Epoch: 3942 Training Loss: 0.18681524827745224 Test Loss: 0.45723128255208334\n",
      "Epoch: 3943 Training Loss: 0.1867382541232639 Test Loss: 0.46142437065972225\n",
      "Epoch: 3944 Training Loss: 0.18638004557291668 Test Loss: 0.46876009114583334\n",
      "Epoch: 3945 Training Loss: 0.18579094780815972 Test Loss: 0.4685651041666667\n",
      "Epoch: 3946 Training Loss: 0.18489258999294705 Test Loss: 0.462939453125\n",
      "Epoch: 3947 Training Loss: 0.18405308363172743 Test Loss: 0.4586385091145833\n",
      "Epoch: 3948 Training Loss: 0.18363390435112847 Test Loss: 0.4572970920138889\n",
      "Epoch: 3949 Training Loss: 0.1835885060628255 Test Loss: 0.45809461805555557\n",
      "Epoch: 3950 Training Loss: 0.1837350124782986 Test Loss: 0.4591533203125\n",
      "Epoch: 3951 Training Loss: 0.18389666578504774 Test Loss: 0.45991487630208333\n",
      "Epoch: 3952 Training Loss: 0.18401470608181425 Test Loss: 0.46088514539930553\n",
      "Epoch: 3953 Training Loss: 0.1841031765407986 Test Loss: 0.4617290581597222\n",
      "Epoch: 3954 Training Loss: 0.1841904788547092 Test Loss: 0.4629854058159722\n",
      "Epoch: 3955 Training Loss: 0.1842838338216146 Test Loss: 0.4641892361111111\n",
      "Epoch: 3956 Training Loss: 0.18436327446831596 Test Loss: 0.4650040147569444\n",
      "Epoch: 3957 Training Loss: 0.18442967393663195 Test Loss: 0.46539898003472224\n",
      "Epoch: 3958 Training Loss: 0.18449590894911025 Test Loss: 0.46511393229166664\n",
      "Epoch: 3959 Training Loss: 0.18457608032226563 Test Loss: 0.46461832682291665\n",
      "Epoch: 3960 Training Loss: 0.18471239386664495 Test Loss: 0.46385210503472224\n",
      "Epoch: 3961 Training Loss: 0.18492623223198784 Test Loss: 0.46331005859375\n",
      "Epoch: 3962 Training Loss: 0.18523211839463977 Test Loss: 0.4633007269965278\n",
      "Epoch: 3963 Training Loss: 0.18567533196343317 Test Loss: 0.46326963975694446\n",
      "Epoch: 3964 Training Loss: 0.18628070237901476 Test Loss: 0.46374370659722225\n",
      "Epoch: 3965 Training Loss: 0.1870402052137587 Test Loss: 0.46406407335069444\n",
      "Epoch: 3966 Training Loss: 0.18798718600802952 Test Loss: 0.4634465603298611\n",
      "Epoch: 3967 Training Loss: 0.1891880154079861 Test Loss: 0.4629123806423611\n",
      "Epoch: 3968 Training Loss: 0.19074696011013456 Test Loss: 0.4611120334201389\n",
      "Epoch: 3969 Training Loss: 0.1926811235215929 Test Loss: 0.4583451605902778\n",
      "Epoch: 3970 Training Loss: 0.19491631571451823 Test Loss: 0.4556960177951389\n",
      "Epoch: 3971 Training Loss: 0.1972007819281684 Test Loss: 0.45624256727430557\n",
      "Epoch: 3972 Training Loss: 0.1990323011610243 Test Loss: 0.46164295789930554\n",
      "Epoch: 3973 Training Loss: 0.20006080966525608 Test Loss: 0.4687746853298611\n",
      "Epoch: 3974 Training Loss: 0.20025755988226995 Test Loss: 0.4711592339409722\n",
      "Epoch: 3975 Training Loss: 0.19961207411024307 Test Loss: 0.4684052734375\n",
      "Epoch: 3976 Training Loss: 0.19768090650770398 Test Loss: 0.46285579427083334\n",
      "Epoch: 3977 Training Loss: 0.19481606207953558 Test Loss: 0.4578408745659722\n",
      "Epoch: 3978 Training Loss: 0.19226595560709636 Test Loss: 0.4548797200520833\n",
      "Epoch: 3979 Training Loss: 0.19069460720486112 Test Loss: 0.454252685546875\n",
      "Epoch: 3980 Training Loss: 0.18997759331597222 Test Loss: 0.45421348741319445\n",
      "Epoch: 3981 Training Loss: 0.18982405598958332 Test Loss: 0.45507508680555553\n",
      "Epoch: 3982 Training Loss: 0.19002959357367621 Test Loss: 0.45654090711805556\n",
      "Epoch: 3983 Training Loss: 0.19050342983669705 Test Loss: 0.4587570529513889\n",
      "Epoch: 3984 Training Loss: 0.19115152825249565 Test Loss: 0.46122569444444445\n",
      "Epoch: 3985 Training Loss: 0.19191502380371095 Test Loss: 0.46369867621527777\n",
      "Epoch: 3986 Training Loss: 0.19273087395562066 Test Loss: 0.46465961371527775\n",
      "Epoch: 3987 Training Loss: 0.1935173831515842 Test Loss: 0.4632214626736111\n",
      "Epoch: 3988 Training Loss: 0.1941919403076172 Test Loss: 0.45959798177083333\n",
      "Epoch: 3989 Training Loss: 0.19453256056043836 Test Loss: 0.45428938802083335\n",
      "Epoch: 3990 Training Loss: 0.19428812154134115 Test Loss: 0.4494615885416667\n",
      "Epoch: 3991 Training Loss: 0.19340491739908855 Test Loss: 0.4468046603732639\n",
      "Epoch: 3992 Training Loss: 0.19207581753200956 Test Loss: 0.4465104709201389\n",
      "Epoch: 3993 Training Loss: 0.19066306559244792 Test Loss: 0.44751223415798613\n",
      "Epoch: 3994 Training Loss: 0.18938199530707464 Test Loss: 0.44897837999131945\n",
      "Epoch: 3995 Training Loss: 0.18833956739637586 Test Loss: 0.4500872124565972\n",
      "Epoch: 3996 Training Loss: 0.187572262234158 Test Loss: 0.45074180772569444\n",
      "Epoch: 3997 Training Loss: 0.18703900655110678 Test Loss: 0.45147129991319446\n",
      "Epoch: 3998 Training Loss: 0.18668887498643663 Test Loss: 0.4519663899739583\n",
      "Epoch: 3999 Training Loss: 0.18648369344075522 Test Loss: 0.4523720703125\n",
      "Epoch: 4000 Training Loss: 0.18639341735839843 Test Loss: 0.4525235731336806\n",
      "Epoch: 4001 Training Loss: 0.18638237677680122 Test Loss: 0.4525790201822917\n",
      "Epoch: 4002 Training Loss: 0.1864718271891276 Test Loss: 0.45246053059895835\n",
      "Epoch: 4003 Training Loss: 0.18661434258355034 Test Loss: 0.4521921115451389\n",
      "Epoch: 4004 Training Loss: 0.18681880527072484 Test Loss: 0.45185834418402776\n",
      "Epoch: 4005 Training Loss: 0.18707894727918836 Test Loss: 0.4515619574652778\n",
      "Epoch: 4006 Training Loss: 0.1873783959282769 Test Loss: 0.4513576931423611\n",
      "Epoch: 4007 Training Loss: 0.18772755093044705 Test Loss: 0.4510535481770833\n",
      "Epoch: 4008 Training Loss: 0.18812550354003907 Test Loss: 0.4509098307291667\n",
      "Epoch: 4009 Training Loss: 0.18854981655544706 Test Loss: 0.45099617513020834\n",
      "Epoch: 4010 Training Loss: 0.18900480143229167 Test Loss: 0.45174026150173613\n",
      "Epoch: 4011 Training Loss: 0.18948463270399304 Test Loss: 0.45290890842013887\n",
      "Epoch: 4012 Training Loss: 0.18997997199164496 Test Loss: 0.45471788194444446\n",
      "Epoch: 4013 Training Loss: 0.1904939643012153 Test Loss: 0.45701361762152776\n",
      "Epoch: 4014 Training Loss: 0.19105372450086805 Test Loss: 0.45954188368055554\n",
      "Epoch: 4015 Training Loss: 0.19167479451497396 Test Loss: 0.46171717664930556\n",
      "Epoch: 4016 Training Loss: 0.1923240254720052 Test Loss: 0.46290565321180555\n",
      "Epoch: 4017 Training Loss: 0.19298308987087673 Test Loss: 0.46216259765625\n",
      "Epoch: 4018 Training Loss: 0.1935612063937717 Test Loss: 0.4594299045138889\n",
      "Epoch: 4019 Training Loss: 0.19395705159505208 Test Loss: 0.45545133463541665\n",
      "Epoch: 4020 Training Loss: 0.1940599585639106 Test Loss: 0.4516514214409722\n",
      "Epoch: 4021 Training Loss: 0.19385439724392362 Test Loss: 0.4486276584201389\n",
      "Epoch: 4022 Training Loss: 0.19338544379340278 Test Loss: 0.4470270182291667\n",
      "Epoch: 4023 Training Loss: 0.19283561197916665 Test Loss: 0.44681689453125\n",
      "Epoch: 4024 Training Loss: 0.19243695407443576 Test Loss: 0.4478292914496528\n",
      "Epoch: 4025 Training Loss: 0.19230215284559463 Test Loss: 0.449636962890625\n",
      "Epoch: 4026 Training Loss: 0.19248893907335068 Test Loss: 0.451931640625\n",
      "Epoch: 4027 Training Loss: 0.19300775146484375 Test Loss: 0.45489778645833334\n",
      "Epoch: 4028 Training Loss: 0.1938946550157335 Test Loss: 0.4586823459201389\n",
      "Epoch: 4029 Training Loss: 0.19522172376844618 Test Loss: 0.4637752821180556\n",
      "Epoch: 4030 Training Loss: 0.19715819295247397 Test Loss: 0.47041227213541664\n",
      "Epoch: 4031 Training Loss: 0.1998013441297743 Test Loss: 0.4765760091145833\n",
      "Epoch: 4032 Training Loss: 0.20322230868869356 Test Loss: 0.47780441623263886\n",
      "Epoch: 4033 Training Loss: 0.20703190104166666 Test Loss: 0.46919032118055554\n",
      "Epoch: 4034 Training Loss: 0.20963702223036024 Test Loss: 0.4548062065972222\n",
      "Epoch: 4035 Training Loss: 0.20894652472601996 Test Loss: 0.44654752604166664\n",
      "Epoch: 4036 Training Loss: 0.20499582078721787 Test Loss: 0.44699129231770834\n",
      "Epoch: 4037 Training Loss: 0.20021217515733508 Test Loss: 0.4488355848524306\n",
      "Epoch: 4038 Training Loss: 0.1962860073513455 Test Loss: 0.4491249457465278\n",
      "Epoch: 4039 Training Loss: 0.19337976921929254 Test Loss: 0.4493403049045139\n",
      "Epoch: 4040 Training Loss: 0.1915050591362847 Test Loss: 0.44978379991319445\n",
      "Epoch: 4041 Training Loss: 0.19044376966688367 Test Loss: 0.4505947265625\n",
      "Epoch: 4042 Training Loss: 0.18993351236979167 Test Loss: 0.45157161458333334\n",
      "Epoch: 4043 Training Loss: 0.1897876705593533 Test Loss: 0.4523812662760417\n",
      "Epoch: 4044 Training Loss: 0.18986465284559462 Test Loss: 0.4528474934895833\n",
      "Epoch: 4045 Training Loss: 0.1900575968424479 Test Loss: 0.45288492838541666\n",
      "Epoch: 4046 Training Loss: 0.1903057623969184 Test Loss: 0.45253209092881946\n",
      "Epoch: 4047 Training Loss: 0.19053720262315538 Test Loss: 0.451451416015625\n",
      "Epoch: 4048 Training Loss: 0.19072003851996527 Test Loss: 0.45020442708333336\n",
      "Epoch: 4049 Training Loss: 0.19084815470377603 Test Loss: 0.4485875922309028\n",
      "Epoch: 4050 Training Loss: 0.19083350965711807 Test Loss: 0.44704264322916665\n",
      "Epoch: 4051 Training Loss: 0.1906965569390191 Test Loss: 0.4455326063368056\n",
      "Epoch: 4052 Training Loss: 0.19045787387424046 Test Loss: 0.44451323784722224\n",
      "Epoch: 4053 Training Loss: 0.1901255832248264 Test Loss: 0.4439066026475694\n",
      "Epoch: 4054 Training Loss: 0.18971045600043404 Test Loss: 0.44373814561631947\n",
      "Epoch: 4055 Training Loss: 0.18924822319878473 Test Loss: 0.4439455023871528\n",
      "Epoch: 4056 Training Loss: 0.18875750054253473 Test Loss: 0.4444648980034722\n",
      "Epoch: 4057 Training Loss: 0.1882648671468099 Test Loss: 0.4450898166232639\n",
      "Epoch: 4058 Training Loss: 0.18778163146972657 Test Loss: 0.4456869303385417\n",
      "Epoch: 4059 Training Loss: 0.1873371090359158 Test Loss: 0.44638170030381946\n",
      "Epoch: 4060 Training Loss: 0.18697080654568143 Test Loss: 0.4471730685763889\n",
      "Epoch: 4061 Training Loss: 0.1866643269856771 Test Loss: 0.4483141547309028\n",
      "Epoch: 4062 Training Loss: 0.18644467163085937 Test Loss: 0.44933289930555553\n",
      "Epoch: 4063 Training Loss: 0.18629267205132377 Test Loss: 0.4505481228298611\n",
      "Epoch: 4064 Training Loss: 0.1862158949110243 Test Loss: 0.45188981119791666\n",
      "Epoch: 4065 Training Loss: 0.18621449449327257 Test Loss: 0.45318462456597225\n",
      "Epoch: 4066 Training Loss: 0.1862710723876953 Test Loss: 0.45453746202256945\n",
      "Epoch: 4067 Training Loss: 0.18637028333875869 Test Loss: 0.4560165473090278\n",
      "Epoch: 4068 Training Loss: 0.1865409884982639 Test Loss: 0.45753157552083334\n",
      "Epoch: 4069 Training Loss: 0.186770021226671 Test Loss: 0.45881380208333333\n",
      "Epoch: 4070 Training Loss: 0.18706378512912325 Test Loss: 0.46007042100694445\n",
      "Epoch: 4071 Training Loss: 0.18739438544379342 Test Loss: 0.4611904296875\n",
      "Epoch: 4072 Training Loss: 0.18777391560872395 Test Loss: 0.46215348307291665\n",
      "Epoch: 4073 Training Loss: 0.18818195597330728 Test Loss: 0.4628669704861111\n",
      "Epoch: 4074 Training Loss: 0.18865889485677084 Test Loss: 0.4635947265625\n",
      "Epoch: 4075 Training Loss: 0.1891417999267578 Test Loss: 0.46394954427083335\n",
      "Epoch: 4076 Training Loss: 0.18967208184136286 Test Loss: 0.4639541558159722\n",
      "Epoch: 4077 Training Loss: 0.19025585259331598 Test Loss: 0.4639765625\n",
      "Epoch: 4078 Training Loss: 0.19084917534722223 Test Loss: 0.46386729600694443\n",
      "Epoch: 4079 Training Loss: 0.1914758012559679 Test Loss: 0.4638009982638889\n",
      "Epoch: 4080 Training Loss: 0.192157472398546 Test Loss: 0.46382996961805556\n",
      "Epoch: 4081 Training Loss: 0.19288720533582898 Test Loss: 0.4644028862847222\n",
      "Epoch: 4082 Training Loss: 0.19372478569878473 Test Loss: 0.46604812282986113\n",
      "Epoch: 4083 Training Loss: 0.19473539055718317 Test Loss: 0.46888351779513887\n",
      "Epoch: 4084 Training Loss: 0.1959664052327474 Test Loss: 0.47273600260416665\n",
      "Epoch: 4085 Training Loss: 0.19751900058322483 Test Loss: 0.4761715494791667\n",
      "Epoch: 4086 Training Loss: 0.19956695048014322 Test Loss: 0.4774410807291667\n",
      "Epoch: 4087 Training Loss: 0.2022146504720052 Test Loss: 0.4742155490451389\n",
      "Epoch: 4088 Training Loss: 0.2054669172498915 Test Loss: 0.4648395182291667\n",
      "Epoch: 4089 Training Loss: 0.20898372904459636 Test Loss: 0.4531856011284722\n",
      "Epoch: 4090 Training Loss: 0.21190026346842447 Test Loss: 0.44349074978298614\n",
      "Epoch: 4091 Training Loss: 0.21287515597873263 Test Loss: 0.44082568359375\n",
      "Epoch: 4092 Training Loss: 0.2112146945529514 Test Loss: 0.4389701877170139\n",
      "Epoch: 4093 Training Loss: 0.20770282830132378 Test Loss: 0.4385015462239583\n",
      "Epoch: 4094 Training Loss: 0.20329126485188803 Test Loss: 0.4390068630642361\n",
      "Epoch: 4095 Training Loss: 0.19864457363552518 Test Loss: 0.43616921657986113\n",
      "Epoch: 4096 Training Loss: 0.1948316141764323 Test Loss: 0.43304131401909723\n",
      "Epoch: 4097 Training Loss: 0.19221188524034288 Test Loss: 0.4326178385416667\n",
      "Epoch: 4098 Training Loss: 0.19037698872884115 Test Loss: 0.4322485894097222\n",
      "Epoch: 4099 Training Loss: 0.18883969285753038 Test Loss: 0.4313699544270833\n",
      "Epoch: 4100 Training Loss: 0.18791490512424044 Test Loss: 0.43060536024305557\n",
      "Epoch: 4101 Training Loss: 0.18721824476453994 Test Loss: 0.4316567925347222\n",
      "Epoch: 4102 Training Loss: 0.18661187405056423 Test Loss: 0.43202701822916667\n",
      "Epoch: 4103 Training Loss: 0.18600487094455295 Test Loss: 0.4315295681423611\n",
      "Epoch: 4104 Training Loss: 0.18555852254231772 Test Loss: 0.4315364040798611\n",
      "Epoch: 4105 Training Loss: 0.18517071533203125 Test Loss: 0.4320822482638889\n",
      "Epoch: 4106 Training Loss: 0.18482162306043837 Test Loss: 0.43263446723090276\n",
      "Epoch: 4107 Training Loss: 0.18450767008463542 Test Loss: 0.4330258246527778\n",
      "Epoch: 4108 Training Loss: 0.1841942104763455 Test Loss: 0.4333741861979167\n",
      "Epoch: 4109 Training Loss: 0.18388417222764758 Test Loss: 0.4336351996527778\n",
      "Epoch: 4110 Training Loss: 0.18360052490234374 Test Loss: 0.4339124348958333\n",
      "Epoch: 4111 Training Loss: 0.18332687208387588 Test Loss: 0.434192626953125\n",
      "Epoch: 4112 Training Loss: 0.18306839667426214 Test Loss: 0.4344963650173611\n",
      "Epoch: 4113 Training Loss: 0.18282699245876735 Test Loss: 0.4347529025607639\n",
      "Epoch: 4114 Training Loss: 0.18259871758355034 Test Loss: 0.435031494140625\n",
      "Epoch: 4115 Training Loss: 0.18237832980685764 Test Loss: 0.4352977430555556\n",
      "Epoch: 4116 Training Loss: 0.1821710442437066 Test Loss: 0.43555381944444443\n",
      "Epoch: 4117 Training Loss: 0.1819678988986545 Test Loss: 0.4358011881510417\n",
      "Epoch: 4118 Training Loss: 0.18177174377441407 Test Loss: 0.43605872938368057\n",
      "Epoch: 4119 Training Loss: 0.18158065626356337 Test Loss: 0.43630289713541665\n",
      "Epoch: 4120 Training Loss: 0.18139685228135852 Test Loss: 0.43652745225694445\n",
      "Epoch: 4121 Training Loss: 0.1812251739501953 Test Loss: 0.4367675238715278\n",
      "Epoch: 4122 Training Loss: 0.18105851067437065 Test Loss: 0.43696267361111113\n",
      "Epoch: 4123 Training Loss: 0.18089785597059463 Test Loss: 0.43719154188368053\n",
      "Epoch: 4124 Training Loss: 0.18074555121527777 Test Loss: 0.43745933702256945\n",
      "Epoch: 4125 Training Loss: 0.1805944298638238 Test Loss: 0.4376311577690972\n",
      "Epoch: 4126 Training Loss: 0.18044371032714843 Test Loss: 0.43781537543402776\n",
      "Epoch: 4127 Training Loss: 0.18030279371473523 Test Loss: 0.437983642578125\n",
      "Epoch: 4128 Training Loss: 0.18017093912760418 Test Loss: 0.4381179470486111\n",
      "Epoch: 4129 Training Loss: 0.18003717041015624 Test Loss: 0.43829877387152777\n",
      "Epoch: 4130 Training Loss: 0.17991027153862849 Test Loss: 0.4384522298177083\n",
      "Epoch: 4131 Training Loss: 0.17978448147243922 Test Loss: 0.4385676540798611\n",
      "Epoch: 4132 Training Loss: 0.17966380988226996 Test Loss: 0.43875048828125\n",
      "Epoch: 4133 Training Loss: 0.1795476294623481 Test Loss: 0.4388609754774306\n",
      "Epoch: 4134 Training Loss: 0.179428707546658 Test Loss: 0.4389665256076389\n",
      "Epoch: 4135 Training Loss: 0.17931868659125433 Test Loss: 0.4391591254340278\n",
      "Epoch: 4136 Training Loss: 0.17921559143066407 Test Loss: 0.4392468804253472\n",
      "Epoch: 4137 Training Loss: 0.17911101955837674 Test Loss: 0.43933072916666666\n",
      "Epoch: 4138 Training Loss: 0.17901185268825956 Test Loss: 0.43938498263888887\n",
      "Epoch: 4139 Training Loss: 0.178915037367079 Test Loss: 0.4395185818142361\n",
      "Epoch: 4140 Training Loss: 0.1788241950141059 Test Loss: 0.4396455620659722\n",
      "Epoch: 4141 Training Loss: 0.17873854064941405 Test Loss: 0.43980213758680553\n",
      "Epoch: 4142 Training Loss: 0.17865147569444445 Test Loss: 0.43987046983506944\n",
      "Epoch: 4143 Training Loss: 0.17856959025065103 Test Loss: 0.439927734375\n",
      "Epoch: 4144 Training Loss: 0.17848552958170574 Test Loss: 0.4399428982204861\n",
      "Epoch: 4145 Training Loss: 0.17840462917751737 Test Loss: 0.4400630425347222\n",
      "Epoch: 4146 Training Loss: 0.17833180067274307 Test Loss: 0.44018077256944443\n",
      "Epoch: 4147 Training Loss: 0.17825687662760417 Test Loss: 0.4402798936631944\n",
      "Epoch: 4148 Training Loss: 0.17818518914116754 Test Loss: 0.44034044053819443\n",
      "Epoch: 4149 Training Loss: 0.17811768934461805 Test Loss: 0.4404704861111111\n",
      "Epoch: 4150 Training Loss: 0.17804605102539062 Test Loss: 0.44053537326388886\n",
      "Epoch: 4151 Training Loss: 0.17797630479600696 Test Loss: 0.4406648220486111\n",
      "Epoch: 4152 Training Loss: 0.17791535949707032 Test Loss: 0.44070551215277776\n",
      "Epoch: 4153 Training Loss: 0.17785047912597657 Test Loss: 0.44078721788194447\n",
      "Epoch: 4154 Training Loss: 0.17778690253363716 Test Loss: 0.4408322482638889\n",
      "Epoch: 4155 Training Loss: 0.17773003980848523 Test Loss: 0.4409559733072917\n",
      "Epoch: 4156 Training Loss: 0.17767019992404515 Test Loss: 0.44105881076388886\n",
      "Epoch: 4157 Training Loss: 0.17761131456163196 Test Loss: 0.4411297471788194\n",
      "Epoch: 4158 Training Loss: 0.17755201721191408 Test Loss: 0.44126866319444447\n",
      "Epoch: 4159 Training Loss: 0.17749586825900607 Test Loss: 0.44133854166666664\n",
      "Epoch: 4160 Training Loss: 0.17744099595811633 Test Loss: 0.44138121202256947\n",
      "Epoch: 4161 Training Loss: 0.17738747829861112 Test Loss: 0.44143733723958334\n",
      "Epoch: 4162 Training Loss: 0.17733939615885416 Test Loss: 0.44156363932291665\n",
      "Epoch: 4163 Training Loss: 0.17728892178005642 Test Loss: 0.44166487630208334\n",
      "Epoch: 4164 Training Loss: 0.177234864976671 Test Loss: 0.4417514105902778\n",
      "Epoch: 4165 Training Loss: 0.1771864284939236 Test Loss: 0.44182115342881945\n",
      "Epoch: 4166 Training Loss: 0.17714047241210937 Test Loss: 0.44193543836805554\n",
      "Epoch: 4167 Training Loss: 0.17709325493706599 Test Loss: 0.44196625434027775\n",
      "Epoch: 4168 Training Loss: 0.1770466037326389 Test Loss: 0.44205387369791665\n",
      "Epoch: 4169 Training Loss: 0.17700161234537762 Test Loss: 0.4422028537326389\n",
      "Epoch: 4170 Training Loss: 0.17695343017578125 Test Loss: 0.44227528211805556\n",
      "Epoch: 4171 Training Loss: 0.1769083014594184 Test Loss: 0.4423134494357639\n",
      "Epoch: 4172 Training Loss: 0.1768666280110677 Test Loss: 0.44241810438368057\n",
      "Epoch: 4173 Training Loss: 0.17682713487413196 Test Loss: 0.4425260687934028\n",
      "Epoch: 4174 Training Loss: 0.17678672960069444 Test Loss: 0.4425417751736111\n",
      "Epoch: 4175 Training Loss: 0.17674664306640625 Test Loss: 0.4425798611111111\n",
      "Epoch: 4176 Training Loss: 0.1767047136094835 Test Loss: 0.44271812608506944\n",
      "Epoch: 4177 Training Loss: 0.1766633046468099 Test Loss: 0.4428126898871528\n",
      "Epoch: 4178 Training Loss: 0.17662348090277777 Test Loss: 0.4428926052517361\n",
      "Epoch: 4179 Training Loss: 0.17658498636881512 Test Loss: 0.44290608723958336\n",
      "Epoch: 4180 Training Loss: 0.17655020141601563 Test Loss: 0.44301991102430555\n",
      "Epoch: 4181 Training Loss: 0.1765147976345486 Test Loss: 0.44311127387152777\n",
      "Epoch: 4182 Training Loss: 0.17647718811035157 Test Loss: 0.44311756727430557\n",
      "Epoch: 4183 Training Loss: 0.17643736945258245 Test Loss: 0.4431928439670139\n",
      "Epoch: 4184 Training Loss: 0.17640468512641058 Test Loss: 0.4433789605034722\n",
      "Epoch: 4185 Training Loss: 0.17636895751953124 Test Loss: 0.44346148003472224\n",
      "Epoch: 4186 Training Loss: 0.1763333028157552 Test Loss: 0.44343253580729164\n",
      "Epoch: 4187 Training Loss: 0.17629618665907118 Test Loss: 0.44345464409722224\n",
      "Epoch: 4188 Training Loss: 0.17626241726345487 Test Loss: 0.44354964192708335\n",
      "Epoch: 4189 Training Loss: 0.17623245069715712 Test Loss: 0.4437526584201389\n",
      "Epoch: 4190 Training Loss: 0.17620177205403645 Test Loss: 0.44383555772569444\n",
      "Epoch: 4191 Training Loss: 0.176166261461046 Test Loss: 0.4437743055555556\n",
      "Epoch: 4192 Training Loss: 0.17612917921278212 Test Loss: 0.4438251139322917\n",
      "Epoch: 4193 Training Loss: 0.1760965796576606 Test Loss: 0.44391419813368055\n",
      "Epoch: 4194 Training Loss: 0.17606834242078992 Test Loss: 0.44413631184895835\n",
      "Epoch: 4195 Training Loss: 0.17603963724772134 Test Loss: 0.44423692491319444\n",
      "Epoch: 4196 Training Loss: 0.1760078413221571 Test Loss: 0.4441339789496528\n",
      "Epoch: 4197 Training Loss: 0.1759699469672309 Test Loss: 0.4441182183159722\n",
      "Epoch: 4198 Training Loss: 0.17593807644314235 Test Loss: 0.4441413302951389\n",
      "Epoch: 4199 Training Loss: 0.17591271124945745 Test Loss: 0.4443003472222222\n",
      "Epoch: 4200 Training Loss: 0.17588729519314236 Test Loss: 0.44460074869791666\n",
      "Epoch: 4201 Training Loss: 0.17585949367947049 Test Loss: 0.4447696668836806\n",
      "Epoch: 4202 Training Loss: 0.17582779439290364 Test Loss: 0.4445885959201389\n",
      "Epoch: 4203 Training Loss: 0.17578705681694878 Test Loss: 0.444531005859375\n",
      "Epoch: 4204 Training Loss: 0.17575551520453558 Test Loss: 0.44455731879340277\n",
      "Epoch: 4205 Training Loss: 0.17573189290364583 Test Loss: 0.4445574001736111\n",
      "Epoch: 4206 Training Loss: 0.17571494717068142 Test Loss: 0.44489455837673614\n",
      "Epoch: 4207 Training Loss: 0.1756904788547092 Test Loss: 0.44535221354166665\n",
      "Epoch: 4208 Training Loss: 0.17566741095648872 Test Loss: 0.44514238823784724\n",
      "Epoch: 4209 Training Loss: 0.1756322258843316 Test Loss: 0.4448545193142361\n",
      "Epoch: 4210 Training Loss: 0.1755907965766059 Test Loss: 0.4449665798611111\n",
      "Epoch: 4211 Training Loss: 0.17556096733940973 Test Loss: 0.445017578125\n",
      "Epoch: 4212 Training Loss: 0.17555385504828558 Test Loss: 0.44492431640625\n",
      "Epoch: 4213 Training Loss: 0.17553961012098523 Test Loss: 0.4457108832465278\n",
      "Epoch: 4214 Training Loss: 0.17551932779947918 Test Loss: 0.44624178059895836\n",
      "Epoch: 4215 Training Loss: 0.17550195821126302 Test Loss: 0.4457150607638889\n",
      "Epoch: 4216 Training Loss: 0.17546197170681424 Test Loss: 0.4452054850260417\n",
      "Epoch: 4217 Training Loss: 0.1754192623562283 Test Loss: 0.44545125325520835\n",
      "Epoch: 4218 Training Loss: 0.17539415317111545 Test Loss: 0.4455372450086806\n",
      "Epoch: 4219 Training Loss: 0.1753897705078125 Test Loss: 0.4452693142361111\n",
      "Epoch: 4220 Training Loss: 0.17537507968478733 Test Loss: 0.44615250651041666\n",
      "Epoch: 4221 Training Loss: 0.17536677890353733 Test Loss: 0.4473791232638889\n",
      "Epoch: 4222 Training Loss: 0.1753614959716797 Test Loss: 0.44651453993055557\n",
      "Epoch: 4223 Training Loss: 0.17532452392578124 Test Loss: 0.44559263780381947\n",
      "Epoch: 4224 Training Loss: 0.17527257453070746 Test Loss: 0.4460276150173611\n",
      "Epoch: 4225 Training Loss: 0.17524486456976995 Test Loss: 0.4461261935763889\n",
      "Epoch: 4226 Training Loss: 0.17525174967447918 Test Loss: 0.4456706271701389\n",
      "Epoch: 4227 Training Loss: 0.1752308807373047 Test Loss: 0.44688129340277777\n",
      "Epoch: 4228 Training Loss: 0.17523167928059896 Test Loss: 0.44834998914930557\n",
      "Epoch: 4229 Training Loss: 0.17521806844075521 Test Loss: 0.44682096354166667\n",
      "Epoch: 4230 Training Loss: 0.17516826544867622 Test Loss: 0.4458727756076389\n",
      "Epoch: 4231 Training Loss: 0.17509994506835938 Test Loss: 0.446763671875\n",
      "Epoch: 4232 Training Loss: 0.17507887098524305 Test Loss: 0.4465727267795139\n",
      "Epoch: 4233 Training Loss: 0.17508385213216146 Test Loss: 0.44626624891493055\n",
      "Epoch: 4234 Training Loss: 0.17505481296115452 Test Loss: 0.44823638237847224\n",
      "Epoch: 4235 Training Loss: 0.17506556023491754 Test Loss: 0.44869325086805556\n",
      "Epoch: 4236 Training Loss: 0.17504091220431858 Test Loss: 0.4467581922743056\n",
      "Epoch: 4237 Training Loss: 0.17497643534342447 Test Loss: 0.4466231011284722\n",
      "Epoch: 4238 Training Loss: 0.17491534932454428 Test Loss: 0.4473681640625\n",
      "Epoch: 4239 Training Loss: 0.17491680399576823 Test Loss: 0.44668497721354167\n",
      "Epoch: 4240 Training Loss: 0.17489737786187065 Test Loss: 0.4475075141059028\n",
      "Epoch: 4241 Training Loss: 0.1748928765190972 Test Loss: 0.44957731119791666\n",
      "Epoch: 4242 Training Loss: 0.17489812554253473 Test Loss: 0.4483589138454861\n",
      "Epoch: 4243 Training Loss: 0.17485355292426216 Test Loss: 0.44689849175347224\n",
      "Epoch: 4244 Training Loss: 0.1747732425265842 Test Loss: 0.447584228515625\n",
      "Epoch: 4245 Training Loss: 0.17474830118815105 Test Loss: 0.4476908908420139\n",
      "Epoch: 4246 Training Loss: 0.17475690375434028 Test Loss: 0.447259765625\n",
      "Epoch: 4247 Training Loss: 0.1747205115424262 Test Loss: 0.44937608506944443\n",
      "Epoch: 4248 Training Loss: 0.17474083794487846 Test Loss: 0.4499129231770833\n",
      "Epoch: 4249 Training Loss: 0.17471970621744792 Test Loss: 0.4477921549479167\n",
      "Epoch: 4250 Training Loss: 0.17466529676649306 Test Loss: 0.44768351236979165\n",
      "Epoch: 4251 Training Loss: 0.17459397888183595 Test Loss: 0.448419677734375\n",
      "Epoch: 4252 Training Loss: 0.17460472785101996 Test Loss: 0.44781046549479164\n",
      "Epoch: 4253 Training Loss: 0.1745884484185113 Test Loss: 0.4489145236545139\n",
      "Epoch: 4254 Training Loss: 0.17460062323676215 Test Loss: 0.45119414605034724\n",
      "Epoch: 4255 Training Loss: 0.17461052788628473 Test Loss: 0.44921714952256947\n",
      "Epoch: 4256 Training Loss: 0.17456152004665798 Test Loss: 0.4480626627604167\n",
      "Epoch: 4257 Training Loss: 0.17448184373643663 Test Loss: 0.44913037109375\n",
      "Epoch: 4258 Training Loss: 0.1744709947374132 Test Loss: 0.4486101888020833\n",
      "Epoch: 4259 Training Loss: 0.17445093621148003 Test Loss: 0.4491360677083333\n",
      "Epoch: 4260 Training Loss: 0.17445032755533854 Test Loss: 0.45140901692708335\n",
      "Epoch: 4261 Training Loss: 0.17447003682454426 Test Loss: 0.4501449381510417\n",
      "Epoch: 4262 Training Loss: 0.17442015584309895 Test Loss: 0.44856244574652776\n",
      "Epoch: 4263 Training Loss: 0.1743643307156033 Test Loss: 0.44945827907986113\n",
      "Epoch: 4264 Training Loss: 0.17434593370225696 Test Loss: 0.44913916015625\n",
      "Epoch: 4265 Training Loss: 0.174345216539171 Test Loss: 0.44941129557291665\n",
      "Epoch: 4266 Training Loss: 0.1743677012125651 Test Loss: 0.4510878363715278\n",
      "Epoch: 4267 Training Loss: 0.17435845608181424 Test Loss: 0.450239990234375\n",
      "Epoch: 4268 Training Loss: 0.17430908033582898 Test Loss: 0.4485419379340278\n",
      "Epoch: 4269 Training Loss: 0.17427397833930122 Test Loss: 0.4491786566840278\n",
      "Epoch: 4270 Training Loss: 0.17429007805718316 Test Loss: 0.4489781901041667\n",
      "Epoch: 4271 Training Loss: 0.17433179389105902 Test Loss: 0.44856024848090276\n",
      "Epoch: 4272 Training Loss: 0.17435819837782118 Test Loss: 0.45008379448784724\n",
      "Epoch: 4273 Training Loss: 0.17434183926052518 Test Loss: 0.4493813747829861\n",
      "Epoch: 4274 Training Loss: 0.17427764044867622 Test Loss: 0.44796516927083335\n",
      "Epoch: 4275 Training Loss: 0.17430817159016926 Test Loss: 0.4485578342013889\n",
      "Epoch: 4276 Training Loss: 0.17440896945529513 Test Loss: 0.4485091688368056\n",
      "Epoch: 4277 Training Loss: 0.17444213019476998 Test Loss: 0.4481577690972222\n",
      "Epoch: 4278 Training Loss: 0.17440648566351996 Test Loss: 0.44866875542534723\n",
      "Epoch: 4279 Training Loss: 0.1743691694471571 Test Loss: 0.4487581380208333\n",
      "Epoch: 4280 Training Loss: 0.17433076816134982 Test Loss: 0.4495595703125\n",
      "Epoch: 4281 Training Loss: 0.17429001871744793 Test Loss: 0.4507507052951389\n",
      "Epoch: 4282 Training Loss: 0.174244138929579 Test Loss: 0.4505757378472222\n",
      "Epoch: 4283 Training Loss: 0.1741859622531467 Test Loss: 0.45016411675347223\n",
      "Epoch: 4284 Training Loss: 0.17412011210123698 Test Loss: 0.4503489040798611\n",
      "Epoch: 4285 Training Loss: 0.17407225375705296 Test Loss: 0.4498149956597222\n",
      "Epoch: 4286 Training Loss: 0.17405598958333332 Test Loss: 0.44982948133680556\n",
      "Epoch: 4287 Training Loss: 0.17405966525607638 Test Loss: 0.4511025390625\n",
      "Epoch: 4288 Training Loss: 0.17407085333930122 Test Loss: 0.4522034505208333\n",
      "Epoch: 4289 Training Loss: 0.17407964579264323 Test Loss: 0.4515863444010417\n",
      "Epoch: 4290 Training Loss: 0.17408790588378906 Test Loss: 0.451688232421875\n",
      "Epoch: 4291 Training Loss: 0.17410093349880643 Test Loss: 0.4521126302083333\n",
      "Epoch: 4292 Training Loss: 0.17411161634657119 Test Loss: 0.4506126844618056\n",
      "Epoch: 4293 Training Loss: 0.17407888624403212 Test Loss: 0.4502687174479167\n",
      "Epoch: 4294 Training Loss: 0.17407057529025607 Test Loss: 0.45191438802083334\n",
      "Epoch: 4295 Training Loss: 0.17413253953721788 Test Loss: 0.45175037977430554\n",
      "Epoch: 4296 Training Loss: 0.17419203864203558 Test Loss: 0.45260687934027777\n",
      "Epoch: 4297 Training Loss: 0.1742408396402995 Test Loss: 0.45414217122395834\n",
      "Epoch: 4298 Training Loss: 0.174344239976671 Test Loss: 0.4520419379340278\n",
      "Epoch: 4299 Training Loss: 0.17440001593695748 Test Loss: 0.45191259765625\n",
      "Epoch: 4300 Training Loss: 0.17440752326117623 Test Loss: 0.4532284613715278\n",
      "Epoch: 4301 Training Loss: 0.174429441663954 Test Loss: 0.45191015625\n",
      "Epoch: 4302 Training Loss: 0.17449529859754775 Test Loss: 0.4529859212239583\n",
      "Epoch: 4303 Training Loss: 0.17464922417534723 Test Loss: 0.452767822265625\n",
      "Epoch: 4304 Training Loss: 0.17477964613172742 Test Loss: 0.44990462239583334\n",
      "Epoch: 4305 Training Loss: 0.17488230556911893 Test Loss: 0.4515581868489583\n",
      "Epoch: 4306 Training Loss: 0.17502599080403647 Test Loss: 0.45343888346354166\n",
      "Epoch: 4307 Training Loss: 0.17524256049262152 Test Loss: 0.45453751627604166\n",
      "Epoch: 4308 Training Loss: 0.1753678012424045 Test Loss: 0.4533277994791667\n",
      "Epoch: 4309 Training Loss: 0.1752283935546875 Test Loss: 0.45283799913194445\n",
      "Epoch: 4310 Training Loss: 0.17517871602376303 Test Loss: 0.45223662651909724\n",
      "Epoch: 4311 Training Loss: 0.17519295247395833 Test Loss: 0.4511698676215278\n",
      "Epoch: 4312 Training Loss: 0.17510391743977866 Test Loss: 0.4524474283854167\n",
      "Epoch: 4313 Training Loss: 0.1751344485812717 Test Loss: 0.45421413845486114\n",
      "Epoch: 4314 Training Loss: 0.17508653937445748 Test Loss: 0.4536405707465278\n",
      "Epoch: 4315 Training Loss: 0.17491456773546007 Test Loss: 0.45287934027777776\n",
      "Epoch: 4316 Training Loss: 0.17488146464029947 Test Loss: 0.45253336588541665\n",
      "Epoch: 4317 Training Loss: 0.17496241590711806 Test Loss: 0.4530579427083333\n",
      "Epoch: 4318 Training Loss: 0.17511630079481336 Test Loss: 0.4536019151475694\n",
      "Epoch: 4319 Training Loss: 0.1753763156467014 Test Loss: 0.45357915581597225\n",
      "Epoch: 4320 Training Loss: 0.17559319220648872 Test Loss: 0.4535097113715278\n",
      "Epoch: 4321 Training Loss: 0.17578161790635852 Test Loss: 0.45362277560763886\n",
      "Epoch: 4322 Training Loss: 0.17603739590115017 Test Loss: 0.4533132595486111\n",
      "Epoch: 4323 Training Loss: 0.17631807793511284 Test Loss: 0.45306000434027777\n",
      "Epoch: 4324 Training Loss: 0.17663075595431857 Test Loss: 0.45316506618923613\n",
      "Epoch: 4325 Training Loss: 0.17691451687282986 Test Loss: 0.453345703125\n",
      "Epoch: 4326 Training Loss: 0.17719914754231772 Test Loss: 0.45392550998263886\n",
      "Epoch: 4327 Training Loss: 0.17755169338650173 Test Loss: 0.45464691840277777\n",
      "Epoch: 4328 Training Loss: 0.17783969285753037 Test Loss: 0.4531312934027778\n",
      "Epoch: 4329 Training Loss: 0.17788995361328125 Test Loss: 0.4504678276909722\n",
      "Epoch: 4330 Training Loss: 0.17790103488498263 Test Loss: 0.45080379231770834\n",
      "Epoch: 4331 Training Loss: 0.17802411227756076 Test Loss: 0.45240980360243055\n",
      "Epoch: 4332 Training Loss: 0.1782020009358724 Test Loss: 0.4519758029513889\n",
      "Epoch: 4333 Training Loss: 0.17823321363661024 Test Loss: 0.4514486762152778\n",
      "Epoch: 4334 Training Loss: 0.17815770128038194 Test Loss: 0.45045084635416666\n",
      "Epoch: 4335 Training Loss: 0.17796092393663193 Test Loss: 0.4498897026909722\n",
      "Epoch: 4336 Training Loss: 0.17756907484266493 Test Loss: 0.4499020453559028\n",
      "Epoch: 4337 Training Loss: 0.17706112162272136 Test Loss: 0.45077617730034725\n",
      "Epoch: 4338 Training Loss: 0.17658844333224827 Test Loss: 0.4523349609375\n",
      "Epoch: 4339 Training Loss: 0.1761910196940104 Test Loss: 0.4522641872829861\n",
      "Epoch: 4340 Training Loss: 0.1758694576687283 Test Loss: 0.4519499782986111\n",
      "Epoch: 4341 Training Loss: 0.17565530565049914 Test Loss: 0.452203369140625\n",
      "Epoch: 4342 Training Loss: 0.17552365281846788 Test Loss: 0.4523434516059028\n",
      "Epoch: 4343 Training Loss: 0.1754172566731771 Test Loss: 0.45223318142361113\n",
      "Epoch: 4344 Training Loss: 0.17532590908474394 Test Loss: 0.45147813585069446\n",
      "Epoch: 4345 Training Loss: 0.17526626925998265 Test Loss: 0.4503876410590278\n",
      "Epoch: 4346 Training Loss: 0.17527227952745225 Test Loss: 0.4495724826388889\n",
      "Epoch: 4347 Training Loss: 0.17533624267578124 Test Loss: 0.44884185112847225\n",
      "Epoch: 4348 Training Loss: 0.17539996676974826 Test Loss: 0.44804877387152775\n",
      "Epoch: 4349 Training Loss: 0.17544525316026477 Test Loss: 0.4478167317708333\n",
      "Epoch: 4350 Training Loss: 0.17548340691460504 Test Loss: 0.4476950141059028\n",
      "Epoch: 4351 Training Loss: 0.17551920064290363 Test Loss: 0.44755726453993055\n",
      "Epoch: 4352 Training Loss: 0.1755279066297743 Test Loss: 0.44795906575520833\n",
      "Epoch: 4353 Training Loss: 0.17548765903049046 Test Loss: 0.44829242621527776\n",
      "Epoch: 4354 Training Loss: 0.17546321953667535 Test Loss: 0.4483615451388889\n",
      "Epoch: 4355 Training Loss: 0.17550337388780382 Test Loss: 0.44852259657118054\n",
      "Epoch: 4356 Training Loss: 0.1755525834825304 Test Loss: 0.4479335123697917\n",
      "Epoch: 4357 Training Loss: 0.1755931905110677 Test Loss: 0.44705940755208334\n",
      "Epoch: 4358 Training Loss: 0.17558460150824654 Test Loss: 0.446908203125\n",
      "Epoch: 4359 Training Loss: 0.17548552619086372 Test Loss: 0.44753323025173614\n",
      "Epoch: 4360 Training Loss: 0.1753236355251736 Test Loss: 0.4479424370659722\n",
      "Epoch: 4361 Training Loss: 0.1751067131890191 Test Loss: 0.44879706488715276\n",
      "Epoch: 4362 Training Loss: 0.17488897365993925 Test Loss: 0.44996207682291667\n",
      "Epoch: 4363 Training Loss: 0.17473797437879773 Test Loss: 0.45034586588541664\n",
      "Epoch: 4364 Training Loss: 0.17462002393934462 Test Loss: 0.44975037977430554\n",
      "Epoch: 4365 Training Loss: 0.1745086958143446 Test Loss: 0.4488137478298611\n",
      "Epoch: 4366 Training Loss: 0.17438731892903644 Test Loss: 0.4484288736979167\n",
      "Epoch: 4367 Training Loss: 0.17428076850043403 Test Loss: 0.44859087456597224\n",
      "Epoch: 4368 Training Loss: 0.17420605977376302 Test Loss: 0.4487574869791667\n",
      "Epoch: 4369 Training Loss: 0.17415701124403213 Test Loss: 0.44920979817708334\n",
      "Epoch: 4370 Training Loss: 0.17414184061686197 Test Loss: 0.44972211371527776\n",
      "Epoch: 4371 Training Loss: 0.17410608418782553 Test Loss: 0.4493204752604167\n",
      "Epoch: 4372 Training Loss: 0.17402344428168404 Test Loss: 0.44814626736111113\n",
      "Epoch: 4373 Training Loss: 0.17396719868977864 Test Loss: 0.448100341796875\n",
      "Epoch: 4374 Training Loss: 0.1739676038953993 Test Loss: 0.4482878689236111\n",
      "Epoch: 4375 Training Loss: 0.17405060323079427 Test Loss: 0.448029296875\n",
      "Epoch: 4376 Training Loss: 0.17415140618218317 Test Loss: 0.4481931694878472\n",
      "Epoch: 4377 Training Loss: 0.17425019666883682 Test Loss: 0.4503690863715278\n",
      "Epoch: 4378 Training Loss: 0.17432554456922744 Test Loss: 0.4519435763888889\n",
      "Epoch: 4379 Training Loss: 0.1743041771782769 Test Loss: 0.4509514702690972\n",
      "Epoch: 4380 Training Loss: 0.17414867485894098 Test Loss: 0.44927582465277777\n",
      "Epoch: 4381 Training Loss: 0.1740048590766059 Test Loss: 0.4487756890190972\n",
      "Epoch: 4382 Training Loss: 0.17403226725260418 Test Loss: 0.4487199978298611\n",
      "Epoch: 4383 Training Loss: 0.17421256849500869 Test Loss: 0.44780463324652775\n",
      "Epoch: 4384 Training Loss: 0.17435264587402344 Test Loss: 0.44675244140625\n",
      "Epoch: 4385 Training Loss: 0.174376708984375 Test Loss: 0.448259521484375\n",
      "Epoch: 4386 Training Loss: 0.17443224928114148 Test Loss: 0.45122252061631946\n",
      "Epoch: 4387 Training Loss: 0.1746197560628255 Test Loss: 0.45241615125868057\n",
      "Epoch: 4388 Training Loss: 0.17472638278537325 Test Loss: 0.4511395128038194\n",
      "Epoch: 4389 Training Loss: 0.17462342664930555 Test Loss: 0.44875667317708334\n",
      "Epoch: 4390 Training Loss: 0.17450311957465278 Test Loss: 0.44761778428819443\n",
      "Epoch: 4391 Training Loss: 0.1744869842529297 Test Loss: 0.4466504448784722\n",
      "Epoch: 4392 Training Loss: 0.17447528754340277 Test Loss: 0.44650830078125\n",
      "Epoch: 4393 Training Loss: 0.17444239468044706 Test Loss: 0.4492888726128472\n",
      "Epoch: 4394 Training Loss: 0.17447043355305988 Test Loss: 0.4527274848090278\n",
      "Epoch: 4395 Training Loss: 0.17453685506184896 Test Loss: 0.45388346354166664\n",
      "Epoch: 4396 Training Loss: 0.1744034915500217 Test Loss: 0.45109296332465276\n",
      "Epoch: 4397 Training Loss: 0.17415077379014757 Test Loss: 0.4487218967013889\n",
      "Epoch: 4398 Training Loss: 0.17396514553493925 Test Loss: 0.4479336751302083\n",
      "Epoch: 4399 Training Loss: 0.17386805386013454 Test Loss: 0.4472873263888889\n",
      "Epoch: 4400 Training Loss: 0.17377688768174912 Test Loss: 0.44802479383680555\n",
      "Epoch: 4401 Training Loss: 0.1737391086154514 Test Loss: 0.45073133680555555\n",
      "Epoch: 4402 Training Loss: 0.17382358974880643 Test Loss: 0.4528538411458333\n",
      "Epoch: 4403 Training Loss: 0.17391685485839845 Test Loss: 0.45084673394097224\n",
      "Epoch: 4404 Training Loss: 0.1738786332872179 Test Loss: 0.44868880208333334\n",
      "Epoch: 4405 Training Loss: 0.17373505147298177 Test Loss: 0.4483764377170139\n",
      "Epoch: 4406 Training Loss: 0.1736089867485894 Test Loss: 0.4475375434027778\n",
      "Epoch: 4407 Training Loss: 0.17352562628851997 Test Loss: 0.4479509548611111\n",
      "Epoch: 4408 Training Loss: 0.17350961134168838 Test Loss: 0.44997601996527775\n",
      "Epoch: 4409 Training Loss: 0.17357090928819444 Test Loss: 0.45078081597222225\n",
      "Epoch: 4410 Training Loss: 0.17367320421006943 Test Loss: 0.4501018337673611\n",
      "Epoch: 4411 Training Loss: 0.1737262674967448 Test Loss: 0.4494054361979167\n",
      "Epoch: 4412 Training Loss: 0.17368277147081163 Test Loss: 0.4489064398871528\n",
      "Epoch: 4413 Training Loss: 0.17361332533094617 Test Loss: 0.44917431640625\n",
      "Epoch: 4414 Training Loss: 0.17361063639322916 Test Loss: 0.44957796223958335\n",
      "Epoch: 4415 Training Loss: 0.1736373545328776 Test Loss: 0.449538818359375\n",
      "Epoch: 4416 Training Loss: 0.17363900926378037 Test Loss: 0.45084174262152776\n",
      "Epoch: 4417 Training Loss: 0.17369634670681425 Test Loss: 0.4529726019965278\n",
      "Epoch: 4418 Training Loss: 0.1737999538845486 Test Loss: 0.45366270616319443\n",
      "Epoch: 4419 Training Loss: 0.17383118693033855 Test Loss: 0.45421986219618055\n",
      "Epoch: 4420 Training Loss: 0.173797115749783 Test Loss: 0.4542042914496528\n",
      "Epoch: 4421 Training Loss: 0.1737390374077691 Test Loss: 0.45340660264756943\n",
      "Epoch: 4422 Training Loss: 0.17370944213867187 Test Loss: 0.4528729926215278\n",
      "Epoch: 4423 Training Loss: 0.1736968722873264 Test Loss: 0.4527654079861111\n",
      "Epoch: 4424 Training Loss: 0.17370279100206162 Test Loss: 0.453445556640625\n",
      "Epoch: 4425 Training Loss: 0.17379281446668837 Test Loss: 0.4548427463107639\n",
      "Epoch: 4426 Training Loss: 0.17394466824001736 Test Loss: 0.4536381564670139\n",
      "Epoch: 4427 Training Loss: 0.17412729899088542 Test Loss: 0.4514117296006944\n",
      "Epoch: 4428 Training Loss: 0.1743020765516493 Test Loss: 0.4509744194878472\n",
      "Epoch: 4429 Training Loss: 0.1744449496799045 Test Loss: 0.452140869140625\n",
      "Epoch: 4430 Training Loss: 0.17451087103949653 Test Loss: 0.45393543836805555\n",
      "Epoch: 4431 Training Loss: 0.17455093892415366 Test Loss: 0.45677577039930556\n",
      "Epoch: 4432 Training Loss: 0.17457915412055122 Test Loss: 0.45963736979166664\n",
      "Epoch: 4433 Training Loss: 0.17458851284450955 Test Loss: 0.4626107313368056\n",
      "Epoch: 4434 Training Loss: 0.17459680684407552 Test Loss: 0.4660920138888889\n",
      "Epoch: 4435 Training Loss: 0.17464657084147137 Test Loss: 0.46791254340277777\n",
      "Epoch: 4436 Training Loss: 0.17472005377875435 Test Loss: 0.4695977647569444\n",
      "Epoch: 4437 Training Loss: 0.1747724388970269 Test Loss: 0.4701057942708333\n",
      "Epoch: 4438 Training Loss: 0.17480400933159723 Test Loss: 0.46856976996527777\n",
      "Epoch: 4439 Training Loss: 0.1747976362440321 Test Loss: 0.46581483289930553\n",
      "Epoch: 4440 Training Loss: 0.1746990017361111 Test Loss: 0.46320355902777777\n",
      "Epoch: 4441 Training Loss: 0.17455535719129775 Test Loss: 0.4617494032118056\n",
      "Epoch: 4442 Training Loss: 0.1744508836534288 Test Loss: 0.46057275390625\n",
      "Epoch: 4443 Training Loss: 0.1744368387858073 Test Loss: 0.4589260525173611\n",
      "Epoch: 4444 Training Loss: 0.17451544528537327 Test Loss: 0.4572938368055556\n",
      "Epoch: 4445 Training Loss: 0.17468684217664932 Test Loss: 0.45687733289930554\n",
      "Epoch: 4446 Training Loss: 0.17494693840874567 Test Loss: 0.4570763888888889\n",
      "Epoch: 4447 Training Loss: 0.17529786851671006 Test Loss: 0.4577255859375\n",
      "Epoch: 4448 Training Loss: 0.17571846347384984 Test Loss: 0.45875428602430557\n",
      "Epoch: 4449 Training Loss: 0.1762260030110677 Test Loss: 0.4592518988715278\n",
      "Epoch: 4450 Training Loss: 0.17683486599392362 Test Loss: 0.45902501085069447\n",
      "Epoch: 4451 Training Loss: 0.1775736083984375 Test Loss: 0.45830588107638887\n",
      "Epoch: 4452 Training Loss: 0.1784156714545356 Test Loss: 0.45685948350694444\n",
      "Epoch: 4453 Training Loss: 0.1793655039469401 Test Loss: 0.45610980902777776\n",
      "Epoch: 4454 Training Loss: 0.18045393880208332 Test Loss: 0.4567407769097222\n",
      "Epoch: 4455 Training Loss: 0.18182661268446182 Test Loss: 0.4590270724826389\n",
      "Epoch: 4456 Training Loss: 0.18353614298502605 Test Loss: 0.4629546440972222\n",
      "Epoch: 4457 Training Loss: 0.18576249016655816 Test Loss: 0.4657105577256944\n",
      "Epoch: 4458 Training Loss: 0.18857636515299478 Test Loss: 0.46668739149305555\n",
      "Epoch: 4459 Training Loss: 0.192086420694987 Test Loss: 0.46886805555555555\n",
      "Epoch: 4460 Training Loss: 0.1956163584391276 Test Loss: 0.4627272677951389\n",
      "Epoch: 4461 Training Loss: 0.196544430202908 Test Loss: 0.4563427191840278\n",
      "Epoch: 4462 Training Loss: 0.1945667199028863 Test Loss: 0.45463818359375\n",
      "Epoch: 4463 Training Loss: 0.19023965454101563 Test Loss: 0.44862052408854164\n",
      "Epoch: 4464 Training Loss: 0.18532035319010418 Test Loss: 0.45018218315972225\n",
      "Epoch: 4465 Training Loss: 0.18088528781467014 Test Loss: 0.4488195258246528\n",
      "Epoch: 4466 Training Loss: 0.177587404039171 Test Loss: 0.4465595160590278\n",
      "Epoch: 4467 Training Loss: 0.17549063958062067 Test Loss: 0.44423130967881946\n",
      "Epoch: 4468 Training Loss: 0.17423511420355903 Test Loss: 0.44311219618055553\n",
      "Epoch: 4469 Training Loss: 0.17351375155978732 Test Loss: 0.44322417534722225\n",
      "Epoch: 4470 Training Loss: 0.17316128540039064 Test Loss: 0.44326277669270836\n",
      "Epoch: 4471 Training Loss: 0.17299634975857206 Test Loss: 0.4433516981336806\n",
      "Epoch: 4472 Training Loss: 0.17295470513237848 Test Loss: 0.4435575358072917\n",
      "Epoch: 4473 Training Loss: 0.17294749959309896 Test Loss: 0.4436730685763889\n",
      "Epoch: 4474 Training Loss: 0.17297924296061198 Test Loss: 0.44383810763888887\n",
      "Epoch: 4475 Training Loss: 0.1730325232611762 Test Loss: 0.44401223415798613\n",
      "Epoch: 4476 Training Loss: 0.17310569254557293 Test Loss: 0.44439116753472224\n",
      "Epoch: 4477 Training Loss: 0.17317277018229166 Test Loss: 0.44440842013888887\n",
      "Epoch: 4478 Training Loss: 0.17320328267415364 Test Loss: 0.4448267144097222\n",
      "Epoch: 4479 Training Loss: 0.1732269524468316 Test Loss: 0.44536702473958334\n",
      "Epoch: 4480 Training Loss: 0.17322713555230035 Test Loss: 0.4460902506510417\n",
      "Epoch: 4481 Training Loss: 0.1732250756157769 Test Loss: 0.44692258029513887\n",
      "Epoch: 4482 Training Loss: 0.17318384636773004 Test Loss: 0.44806043836805554\n",
      "Epoch: 4483 Training Loss: 0.17315752834743925 Test Loss: 0.449181640625\n",
      "Epoch: 4484 Training Loss: 0.17313483005099825 Test Loss: 0.45016883680555553\n",
      "Epoch: 4485 Training Loss: 0.1731597900390625 Test Loss: 0.450787353515625\n",
      "Epoch: 4486 Training Loss: 0.17322630988226997 Test Loss: 0.4511934136284722\n",
      "Epoch: 4487 Training Loss: 0.1733226064046224 Test Loss: 0.45003032769097223\n",
      "Epoch: 4488 Training Loss: 0.17347897508409288 Test Loss: 0.4520082194010417\n",
      "Epoch: 4489 Training Loss: 0.17352955796983507 Test Loss: 0.45378331163194446\n",
      "Epoch: 4490 Training Loss: 0.17370465426974827 Test Loss: 0.45570865885416667\n",
      "Epoch: 4491 Training Loss: 0.17398655700683593 Test Loss: 0.4574251302083333\n",
      "Epoch: 4492 Training Loss: 0.17435282050238715 Test Loss: 0.4595736762152778\n",
      "Epoch: 4493 Training Loss: 0.1749187503390842 Test Loss: 0.46265120442708335\n",
      "Epoch: 4494 Training Loss: 0.17584621005588108 Test Loss: 0.46656456163194443\n",
      "Epoch: 4495 Training Loss: 0.1774764387342665 Test Loss: 0.46889762369791665\n",
      "Epoch: 4496 Training Loss: 0.1803446723090278 Test Loss: 0.4626360134548611\n",
      "Epoch: 4497 Training Loss: 0.18438970608181424 Test Loss: 0.47014680989583335\n",
      "Epoch: 4498 Training Loss: 0.18778490532769096 Test Loss: 0.4905815972222222\n",
      "Epoch: 4499 Training Loss: 0.1895283932156033 Test Loss: 0.47466004774305554\n",
      "Epoch: 4500 Training Loss: 0.1838926476372613 Test Loss: 0.4644978298611111\n",
      "Epoch: 4501 Training Loss: 0.18178811984592014 Test Loss: 0.46156591796875\n",
      "Epoch: 4502 Training Loss: 0.18309104410807292 Test Loss: 0.45994466145833335\n",
      "Epoch: 4503 Training Loss: 0.18503230963812933 Test Loss: 0.45994146050347223\n",
      "Epoch: 4504 Training Loss: 0.18653071933322482 Test Loss: 0.4574494357638889\n",
      "Epoch: 4505 Training Loss: 0.1877136416965061 Test Loss: 0.4488179253472222\n",
      "Epoch: 4506 Training Loss: 0.1887447526719835 Test Loss: 0.4414091796875\n",
      "Epoch: 4507 Training Loss: 0.18921285332573784 Test Loss: 0.4394327799479167\n",
      "Epoch: 4508 Training Loss: 0.18863224453396268 Test Loss: 0.4421008572048611\n",
      "Epoch: 4509 Training Loss: 0.18708211771647135 Test Loss: 0.4443098415798611\n",
      "Epoch: 4510 Training Loss: 0.18505653381347656 Test Loss: 0.4431991644965278\n",
      "Epoch: 4511 Training Loss: 0.1829182840983073 Test Loss: 0.4407665201822917\n",
      "Epoch: 4512 Training Loss: 0.18100186496310763 Test Loss: 0.43878645833333335\n",
      "Epoch: 4513 Training Loss: 0.17945983378092448 Test Loss: 0.4378178439670139\n",
      "Epoch: 4514 Training Loss: 0.17835611131456164 Test Loss: 0.43817930772569447\n",
      "Epoch: 4515 Training Loss: 0.177672604031033 Test Loss: 0.4394233127170139\n",
      "Epoch: 4516 Training Loss: 0.1773537089029948 Test Loss: 0.4410629340277778\n",
      "Epoch: 4517 Training Loss: 0.1773271959092882 Test Loss: 0.44315500217013887\n",
      "Epoch: 4518 Training Loss: 0.17750467088487412 Test Loss: 0.44537689887152776\n",
      "Epoch: 4519 Training Loss: 0.17781238979763456 Test Loss: 0.44723263888888887\n",
      "Epoch: 4520 Training Loss: 0.1782074737548828 Test Loss: 0.4495230305989583\n",
      "Epoch: 4521 Training Loss: 0.17868322245279947 Test Loss: 0.4517788899739583\n",
      "Epoch: 4522 Training Loss: 0.17920613606770833 Test Loss: 0.4532063259548611\n",
      "Epoch: 4523 Training Loss: 0.17966266038682727 Test Loss: 0.45417119683159723\n",
      "Epoch: 4524 Training Loss: 0.1799430711534288 Test Loss: 0.4541642795138889\n",
      "Epoch: 4525 Training Loss: 0.18002123514811197 Test Loss: 0.4528231608072917\n",
      "Epoch: 4526 Training Loss: 0.17982364739312065 Test Loss: 0.45079771592881945\n",
      "Epoch: 4527 Training Loss: 0.17936693488226996 Test Loss: 0.44922325303819444\n",
      "Epoch: 4528 Training Loss: 0.17874950832790798 Test Loss: 0.44803727213541666\n",
      "Epoch: 4529 Training Loss: 0.17801463656955296 Test Loss: 0.4471611599392361\n",
      "Epoch: 4530 Training Loss: 0.17722857157389324 Test Loss: 0.4465431315104167\n",
      "Epoch: 4531 Training Loss: 0.17642396206325955 Test Loss: 0.4458583170572917\n",
      "Epoch: 4532 Training Loss: 0.17565805053710937 Test Loss: 0.44499479166666667\n",
      "Epoch: 4533 Training Loss: 0.17495894538031684 Test Loss: 0.44439689127604165\n",
      "Epoch: 4534 Training Loss: 0.17432546657986112 Test Loss: 0.4438809678819444\n",
      "Epoch: 4535 Training Loss: 0.17374998813205295 Test Loss: 0.4434637858072917\n",
      "Epoch: 4536 Training Loss: 0.17324054463704427 Test Loss: 0.4429681803385417\n",
      "Epoch: 4537 Training Loss: 0.17280410936143664 Test Loss: 0.44271158854166665\n",
      "Epoch: 4538 Training Loss: 0.1724207950168186 Test Loss: 0.4424020182291667\n",
      "Epoch: 4539 Training Loss: 0.17208510504828559 Test Loss: 0.4423121473524306\n",
      "Epoch: 4540 Training Loss: 0.1717919413248698 Test Loss: 0.44205707465277777\n",
      "Epoch: 4541 Training Loss: 0.17154734971788194 Test Loss: 0.4420211859809028\n",
      "Epoch: 4542 Training Loss: 0.1713413543701172 Test Loss: 0.442120849609375\n",
      "Epoch: 4543 Training Loss: 0.17116695658365885 Test Loss: 0.44207267252604165\n",
      "Epoch: 4544 Training Loss: 0.17101244608561197 Test Loss: 0.44216666666666665\n",
      "Epoch: 4545 Training Loss: 0.1708856438530816 Test Loss: 0.4421559244791667\n",
      "Epoch: 4546 Training Loss: 0.1707736579047309 Test Loss: 0.44225423177083334\n",
      "Epoch: 4547 Training Loss: 0.17067659166124133 Test Loss: 0.44252449544270833\n",
      "Epoch: 4548 Training Loss: 0.17059507581922742 Test Loss: 0.4425898980034722\n",
      "Epoch: 4549 Training Loss: 0.17051756625705294 Test Loss: 0.4427131618923611\n",
      "Epoch: 4550 Training Loss: 0.17045321485731338 Test Loss: 0.442888671875\n",
      "Epoch: 4551 Training Loss: 0.17039949713812935 Test Loss: 0.44288514539930557\n",
      "Epoch: 4552 Training Loss: 0.17035213555230036 Test Loss: 0.4430394965277778\n",
      "Epoch: 4553 Training Loss: 0.17030833435058593 Test Loss: 0.4432314724392361\n",
      "Epoch: 4554 Training Loss: 0.17027402920193144 Test Loss: 0.44332817925347223\n",
      "Epoch: 4555 Training Loss: 0.17024705674913193 Test Loss: 0.44355333116319445\n",
      "Epoch: 4556 Training Loss: 0.17022476874457465 Test Loss: 0.44372865125868055\n",
      "Epoch: 4557 Training Loss: 0.1702110816107856 Test Loss: 0.4439375542534722\n",
      "Epoch: 4558 Training Loss: 0.17020311652289496 Test Loss: 0.44425062391493053\n",
      "Epoch: 4559 Training Loss: 0.1701944376627604 Test Loss: 0.4445420193142361\n",
      "Epoch: 4560 Training Loss: 0.1701934339735243 Test Loss: 0.4447365180121528\n",
      "Epoch: 4561 Training Loss: 0.17018890041775173 Test Loss: 0.4449502224392361\n",
      "Epoch: 4562 Training Loss: 0.17018665737575955 Test Loss: 0.44522401258680555\n",
      "Epoch: 4563 Training Loss: 0.17019062127007378 Test Loss: 0.4454783528645833\n",
      "Epoch: 4564 Training Loss: 0.1701894819471571 Test Loss: 0.4455789388020833\n",
      "Epoch: 4565 Training Loss: 0.17018634880913627 Test Loss: 0.4457564019097222\n",
      "Epoch: 4566 Training Loss: 0.17019656541612413 Test Loss: 0.445828369140625\n",
      "Epoch: 4567 Training Loss: 0.17020460849338107 Test Loss: 0.4461220431857639\n",
      "Epoch: 4568 Training Loss: 0.17021822950575086 Test Loss: 0.4464198947482639\n",
      "Epoch: 4569 Training Loss: 0.17024085998535157 Test Loss: 0.44662098524305555\n",
      "Epoch: 4570 Training Loss: 0.17026472981770832 Test Loss: 0.44682413736979165\n",
      "Epoch: 4571 Training Loss: 0.17028761461046008 Test Loss: 0.44699256727430553\n",
      "Epoch: 4572 Training Loss: 0.17031793891059027 Test Loss: 0.4472120768229167\n",
      "Epoch: 4573 Training Loss: 0.17036326938205296 Test Loss: 0.44757242838541667\n",
      "Epoch: 4574 Training Loss: 0.1704108428955078 Test Loss: 0.44778955078125\n",
      "Epoch: 4575 Training Loss: 0.1704654812282986 Test Loss: 0.44826529947916666\n",
      "Epoch: 4576 Training Loss: 0.1705332539876302 Test Loss: 0.44866547309027777\n",
      "Epoch: 4577 Training Loss: 0.17059732225206165 Test Loss: 0.44921158854166665\n",
      "Epoch: 4578 Training Loss: 0.1706679450141059 Test Loss: 0.4495290256076389\n",
      "Epoch: 4579 Training Loss: 0.17074994235568577 Test Loss: 0.44999793836805557\n",
      "Epoch: 4580 Training Loss: 0.1708519321017795 Test Loss: 0.4504329698350694\n",
      "Epoch: 4581 Training Loss: 0.17095865546332464 Test Loss: 0.4508419596354167\n",
      "Epoch: 4582 Training Loss: 0.17106924438476562 Test Loss: 0.4513272569444444\n",
      "Epoch: 4583 Training Loss: 0.17119939676920573 Test Loss: 0.4519962022569444\n",
      "Epoch: 4584 Training Loss: 0.17133543395996093 Test Loss: 0.45273182508680554\n",
      "Epoch: 4585 Training Loss: 0.1714994913736979 Test Loss: 0.4535221354166667\n",
      "Epoch: 4586 Training Loss: 0.17169012790256077 Test Loss: 0.4544812825520833\n",
      "Epoch: 4587 Training Loss: 0.17188781229654948 Test Loss: 0.4552389865451389\n",
      "Epoch: 4588 Training Loss: 0.17212925381130642 Test Loss: 0.4566106228298611\n",
      "Epoch: 4589 Training Loss: 0.17242663913302952 Test Loss: 0.45795008680555555\n",
      "Epoch: 4590 Training Loss: 0.17272993977864584 Test Loss: 0.4592765299479167\n",
      "Epoch: 4591 Training Loss: 0.17308087327745225 Test Loss: 0.46061968315972224\n",
      "Epoch: 4592 Training Loss: 0.17345236036512587 Test Loss: 0.461978515625\n",
      "Epoch: 4593 Training Loss: 0.17388057284884983 Test Loss: 0.4633226996527778\n",
      "Epoch: 4594 Training Loss: 0.17438429599338107 Test Loss: 0.4643332248263889\n",
      "Epoch: 4595 Training Loss: 0.17497425672743055 Test Loss: 0.46474522569444443\n",
      "Epoch: 4596 Training Loss: 0.17562483554416233 Test Loss: 0.46481022135416666\n",
      "Epoch: 4597 Training Loss: 0.1764289059109158 Test Loss: 0.46445621744791665\n",
      "Epoch: 4598 Training Loss: 0.17738740878634982 Test Loss: 0.4622012261284722\n",
      "Epoch: 4599 Training Loss: 0.1784606662326389 Test Loss: 0.45915342881944443\n",
      "Epoch: 4600 Training Loss: 0.17965437655978733 Test Loss: 0.45671739366319447\n",
      "Epoch: 4601 Training Loss: 0.18102383253309462 Test Loss: 0.45747298177083334\n",
      "Epoch: 4602 Training Loss: 0.1826284908718533 Test Loss: 0.45989659288194445\n",
      "Epoch: 4603 Training Loss: 0.18452206251356337 Test Loss: 0.4582126736111111\n",
      "Epoch: 4604 Training Loss: 0.18677551947699653 Test Loss: 0.45266107855902776\n",
      "Epoch: 4605 Training Loss: 0.1895788285997179 Test Loss: 0.44708935546875\n",
      "Epoch: 4606 Training Loss: 0.19279911126030816 Test Loss: 0.44248592122395836\n",
      "Epoch: 4607 Training Loss: 0.19514071146647136 Test Loss: 0.439999755859375\n",
      "Epoch: 4608 Training Loss: 0.195167728000217 Test Loss: 0.44139350043402775\n",
      "Epoch: 4609 Training Loss: 0.19327536180284288 Test Loss: 0.4451134982638889\n",
      "Epoch: 4610 Training Loss: 0.19022943962944877 Test Loss: 0.447197265625\n",
      "Epoch: 4611 Training Loss: 0.18649253336588542 Test Loss: 0.44654210069444444\n",
      "Epoch: 4612 Training Loss: 0.18275645277235242 Test Loss: 0.4438057183159722\n",
      "Epoch: 4613 Training Loss: 0.1796956532796224 Test Loss: 0.4407390407986111\n",
      "Epoch: 4614 Training Loss: 0.1774664984809028 Test Loss: 0.4389231228298611\n",
      "Epoch: 4615 Training Loss: 0.17592688496907552 Test Loss: 0.43778675672743056\n",
      "Epoch: 4616 Training Loss: 0.1748944600423177 Test Loss: 0.43729188368055555\n",
      "Epoch: 4617 Training Loss: 0.17418726603190104 Test Loss: 0.43728173828125\n",
      "Epoch: 4618 Training Loss: 0.17369272867838542 Test Loss: 0.43736075846354167\n",
      "Epoch: 4619 Training Loss: 0.17334086100260418 Test Loss: 0.4374801974826389\n",
      "Epoch: 4620 Training Loss: 0.17308918592664932 Test Loss: 0.43803255208333336\n",
      "Epoch: 4621 Training Loss: 0.17290301344129774 Test Loss: 0.43840831163194444\n",
      "Epoch: 4622 Training Loss: 0.17276929558648005 Test Loss: 0.4387368706597222\n",
      "Epoch: 4623 Training Loss: 0.1726825968424479 Test Loss: 0.43927162000868053\n",
      "Epoch: 4624 Training Loss: 0.17263876512315537 Test Loss: 0.43970372178819445\n",
      "Epoch: 4625 Training Loss: 0.17263424004448785 Test Loss: 0.4401367458767361\n",
      "Epoch: 4626 Training Loss: 0.17266613260904948 Test Loss: 0.44049207899305554\n",
      "Epoch: 4627 Training Loss: 0.17272992621527777 Test Loss: 0.44081551106770833\n",
      "Epoch: 4628 Training Loss: 0.17283545600043404 Test Loss: 0.4411017795138889\n",
      "Epoch: 4629 Training Loss: 0.17299693976508246 Test Loss: 0.44131906467013887\n",
      "Epoch: 4630 Training Loss: 0.17320430840386286 Test Loss: 0.44152031792534724\n",
      "Epoch: 4631 Training Loss: 0.1734713609483507 Test Loss: 0.44148665364583334\n",
      "Epoch: 4632 Training Loss: 0.1738143310546875 Test Loss: 0.4413155924479167\n",
      "Epoch: 4633 Training Loss: 0.17424364217122396 Test Loss: 0.44085899522569444\n",
      "Epoch: 4634 Training Loss: 0.1747756822374132 Test Loss: 0.44045903862847224\n",
      "Epoch: 4635 Training Loss: 0.17544488016764323 Test Loss: 0.4395203450520833\n",
      "Epoch: 4636 Training Loss: 0.1762902577718099 Test Loss: 0.43821853298611113\n",
      "Epoch: 4637 Training Loss: 0.17736390516493056 Test Loss: 0.436966552734375\n",
      "Epoch: 4638 Training Loss: 0.17871148851182725 Test Loss: 0.4359339735243056\n",
      "Epoch: 4639 Training Loss: 0.1804105038113064 Test Loss: 0.43560186089409725\n",
      "Epoch: 4640 Training Loss: 0.18245389980740018 Test Loss: 0.43695263671875\n",
      "Epoch: 4641 Training Loss: 0.1847626020643446 Test Loss: 0.44215421549479167\n",
      "Epoch: 4642 Training Loss: 0.187142823961046 Test Loss: 0.45175010850694447\n",
      "Epoch: 4643 Training Loss: 0.1892772488064236 Test Loss: 0.46106553819444446\n",
      "Epoch: 4644 Training Loss: 0.1909170922173394 Test Loss: 0.4621342230902778\n",
      "Epoch: 4645 Training Loss: 0.1916652577718099 Test Loss: 0.4526661783854167\n",
      "Epoch: 4646 Training Loss: 0.1904654032389323 Test Loss: 0.4423783908420139\n",
      "Epoch: 4647 Training Loss: 0.1871445549858941 Test Loss: 0.4364699164496528\n",
      "Epoch: 4648 Training Loss: 0.18306492614746095 Test Loss: 0.43454296875\n",
      "Epoch: 4649 Training Loss: 0.17964595709906683 Test Loss: 0.4356388888888889\n",
      "Epoch: 4650 Training Loss: 0.17731739468044705 Test Loss: 0.4374763454861111\n",
      "Epoch: 4651 Training Loss: 0.17588889736599392 Test Loss: 0.43925797526041666\n",
      "Epoch: 4652 Training Loss: 0.17513020324707032 Test Loss: 0.44027012803819443\n",
      "Epoch: 4653 Training Loss: 0.17482348293728298 Test Loss: 0.4408971896701389\n",
      "Epoch: 4654 Training Loss: 0.17480532667371962 Test Loss: 0.44141796875\n",
      "Epoch: 4655 Training Loss: 0.17499122450086804 Test Loss: 0.44185856119791667\n",
      "Epoch: 4656 Training Loss: 0.17533508809407553 Test Loss: 0.4421330837673611\n",
      "Epoch: 4657 Training Loss: 0.17579563734266493 Test Loss: 0.44232166883680557\n",
      "Epoch: 4658 Training Loss: 0.1764094950358073 Test Loss: 0.44209342447916666\n",
      "Epoch: 4659 Training Loss: 0.17716248575846355 Test Loss: 0.44147010633680556\n",
      "Epoch: 4660 Training Loss: 0.17807364569769965 Test Loss: 0.4403162434895833\n",
      "Epoch: 4661 Training Loss: 0.1791480678982205 Test Loss: 0.4381825629340278\n",
      "Epoch: 4662 Training Loss: 0.18037137010362414 Test Loss: 0.43512299262152776\n",
      "Epoch: 4663 Training Loss: 0.18162772962782117 Test Loss: 0.43147178819444443\n",
      "Epoch: 4664 Training Loss: 0.18268134053548177 Test Loss: 0.428330078125\n",
      "Epoch: 4665 Training Loss: 0.1832608608669705 Test Loss: 0.42636338975694443\n",
      "Epoch: 4666 Training Loss: 0.18321976386176214 Test Loss: 0.4265381130642361\n",
      "Epoch: 4667 Training Loss: 0.1825010240342882 Test Loss: 0.42861121961805554\n",
      "Epoch: 4668 Training Loss: 0.18125454542371963 Test Loss: 0.4303994411892361\n",
      "Epoch: 4669 Training Loss: 0.17967219882541233 Test Loss: 0.4311719021267361\n",
      "Epoch: 4670 Training Loss: 0.17792012532552084 Test Loss: 0.43079562717013886\n",
      "Epoch: 4671 Training Loss: 0.176198238796658 Test Loss: 0.42958704969618056\n",
      "Epoch: 4672 Training Loss: 0.174576414320204 Test Loss: 0.42846351453993053\n",
      "Epoch: 4673 Training Loss: 0.17317157660590277 Test Loss: 0.4274634060329861\n",
      "Epoch: 4674 Training Loss: 0.17199393039279515 Test Loss: 0.42673649088541665\n",
      "Epoch: 4675 Training Loss: 0.17104090881347656 Test Loss: 0.42623084852430554\n",
      "Epoch: 4676 Training Loss: 0.17028019205729167 Test Loss: 0.42604560004340275\n",
      "Epoch: 4677 Training Loss: 0.1696814897325304 Test Loss: 0.4259359809027778\n",
      "Epoch: 4678 Training Loss: 0.1692068074544271 Test Loss: 0.4259284125434028\n",
      "Epoch: 4679 Training Loss: 0.1688231404622396 Test Loss: 0.4260647243923611\n",
      "Epoch: 4680 Training Loss: 0.16851192389594183 Test Loss: 0.42613818359375\n",
      "Epoch: 4681 Training Loss: 0.16825310940212673 Test Loss: 0.4263244357638889\n",
      "Epoch: 4682 Training Loss: 0.16803217400444878 Test Loss: 0.4265049370659722\n",
      "Epoch: 4683 Training Loss: 0.16784730360243055 Test Loss: 0.42671818033854164\n",
      "Epoch: 4684 Training Loss: 0.16768504333496093 Test Loss: 0.42690473090277775\n",
      "Epoch: 4685 Training Loss: 0.1675428941514757 Test Loss: 0.42713053385416666\n",
      "Epoch: 4686 Training Loss: 0.16741348266601563 Test Loss: 0.4272886827256944\n",
      "Epoch: 4687 Training Loss: 0.1672957780626085 Test Loss: 0.4275352647569444\n",
      "Epoch: 4688 Training Loss: 0.16719097730848526 Test Loss: 0.427779296875\n",
      "Epoch: 4689 Training Loss: 0.1670936041937934 Test Loss: 0.4280284830729167\n",
      "Epoch: 4690 Training Loss: 0.16700556945800782 Test Loss: 0.4281494140625\n",
      "Epoch: 4691 Training Loss: 0.166919918484158 Test Loss: 0.42827788628472224\n",
      "Epoch: 4692 Training Loss: 0.16684380933973525 Test Loss: 0.4284833441840278\n",
      "Epoch: 4693 Training Loss: 0.1667711402045356 Test Loss: 0.42867087131076387\n",
      "Epoch: 4694 Training Loss: 0.166704098171658 Test Loss: 0.42880555555555555\n",
      "Epoch: 4695 Training Loss: 0.16663831753200956 Test Loss: 0.428993408203125\n",
      "Epoch: 4696 Training Loss: 0.1665795644124349 Test Loss: 0.4291575792100694\n",
      "Epoch: 4697 Training Loss: 0.1665203586154514 Test Loss: 0.4292678493923611\n",
      "Epoch: 4698 Training Loss: 0.16646733940972222 Test Loss: 0.4294042697482639\n",
      "Epoch: 4699 Training Loss: 0.1664166293674045 Test Loss: 0.4295010036892361\n",
      "Epoch: 4700 Training Loss: 0.16636516486273872 Test Loss: 0.42966758897569446\n",
      "Epoch: 4701 Training Loss: 0.1663172370062934 Test Loss: 0.4298439670138889\n",
      "Epoch: 4702 Training Loss: 0.16627061292860243 Test Loss: 0.43000276692708334\n",
      "Epoch: 4703 Training Loss: 0.1662259250217014 Test Loss: 0.4300939398871528\n",
      "Epoch: 4704 Training Loss: 0.16618500603569877 Test Loss: 0.4302033962673611\n",
      "Epoch: 4705 Training Loss: 0.16614328850640192 Test Loss: 0.43039024522569447\n",
      "Epoch: 4706 Training Loss: 0.16610629272460936 Test Loss: 0.43049454752604166\n",
      "Epoch: 4707 Training Loss: 0.1660664978027344 Test Loss: 0.4306356879340278\n",
      "Epoch: 4708 Training Loss: 0.16602908494737414 Test Loss: 0.43073822699652775\n",
      "Epoch: 4709 Training Loss: 0.1659956529405382 Test Loss: 0.4308210991753472\n",
      "Epoch: 4710 Training Loss: 0.16596066792805989 Test Loss: 0.43093869357638886\n",
      "Epoch: 4711 Training Loss: 0.1659258592393663 Test Loss: 0.43106819661458334\n",
      "Epoch: 4712 Training Loss: 0.1658917253282335 Test Loss: 0.43122184244791667\n",
      "Epoch: 4713 Training Loss: 0.16585897827148438 Test Loss: 0.4313137478298611\n",
      "Epoch: 4714 Training Loss: 0.16582881503634983 Test Loss: 0.4313855523003472\n",
      "Epoch: 4715 Training Loss: 0.16579876539442273 Test Loss: 0.4315537109375\n",
      "Epoch: 4716 Training Loss: 0.16577032301161024 Test Loss: 0.43171346028645835\n",
      "Epoch: 4717 Training Loss: 0.16573870849609376 Test Loss: 0.4318031955295139\n",
      "Epoch: 4718 Training Loss: 0.16570949130588108 Test Loss: 0.43191541883680556\n",
      "Epoch: 4719 Training Loss: 0.1656809370252821 Test Loss: 0.4319963650173611\n",
      "Epoch: 4720 Training Loss: 0.16565401034884983 Test Loss: 0.43203095160590277\n",
      "Epoch: 4721 Training Loss: 0.16562437438964844 Test Loss: 0.43212744140625\n",
      "Epoch: 4722 Training Loss: 0.16559161037868925 Test Loss: 0.4323162163628472\n",
      "Epoch: 4723 Training Loss: 0.16556722683376737 Test Loss: 0.4324865451388889\n",
      "Epoch: 4724 Training Loss: 0.1655420345730252 Test Loss: 0.4324743381076389\n",
      "Epoch: 4725 Training Loss: 0.1655162573920356 Test Loss: 0.4325781792534722\n",
      "Epoch: 4726 Training Loss: 0.16549212646484374 Test Loss: 0.4327758517795139\n",
      "Epoch: 4727 Training Loss: 0.1654702860514323 Test Loss: 0.4328432888454861\n",
      "Epoch: 4728 Training Loss: 0.16544366285536025 Test Loss: 0.43289246961805555\n",
      "Epoch: 4729 Training Loss: 0.1654162851969401 Test Loss: 0.43308444552951386\n",
      "Epoch: 4730 Training Loss: 0.16539144897460936 Test Loss: 0.43324875217013886\n",
      "Epoch: 4731 Training Loss: 0.165368654039171 Test Loss: 0.43328087022569445\n",
      "Epoch: 4732 Training Loss: 0.1653439687093099 Test Loss: 0.4333664008246528\n",
      "Epoch: 4733 Training Loss: 0.16531831868489583 Test Loss: 0.43342713758680557\n",
      "Epoch: 4734 Training Loss: 0.16529865349663628 Test Loss: 0.43361393229166667\n",
      "Epoch: 4735 Training Loss: 0.1652819112141927 Test Loss: 0.43369222005208335\n",
      "Epoch: 4736 Training Loss: 0.16525432840983073 Test Loss: 0.43369211154513887\n",
      "Epoch: 4737 Training Loss: 0.1652244381374783 Test Loss: 0.4339319390190972\n",
      "Epoch: 4738 Training Loss: 0.1652012719048394 Test Loss: 0.43410997178819444\n",
      "Epoch: 4739 Training Loss: 0.1651823238796658 Test Loss: 0.4343251953125\n",
      "Epoch: 4740 Training Loss: 0.16516501024034289 Test Loss: 0.43438069661458334\n",
      "Epoch: 4741 Training Loss: 0.1651470218234592 Test Loss: 0.43439100477430553\n",
      "Epoch: 4742 Training Loss: 0.1651268310546875 Test Loss: 0.4344751247829861\n",
      "Epoch: 4743 Training Loss: 0.1651121334499783 Test Loss: 0.4345829806857639\n",
      "Epoch: 4744 Training Loss: 0.16509678819444445 Test Loss: 0.43448876953125\n",
      "Epoch: 4745 Training Loss: 0.16507540045844185 Test Loss: 0.4344529351128472\n",
      "Epoch: 4746 Training Loss: 0.16504554239908853 Test Loss: 0.4345910915798611\n",
      "Epoch: 4747 Training Loss: 0.16501819695366754 Test Loss: 0.4348828667534722\n",
      "Epoch: 4748 Training Loss: 0.16499637688530816 Test Loss: 0.4352211100260417\n",
      "Epoch: 4749 Training Loss: 0.1649814910888672 Test Loss: 0.4356123046875\n",
      "Epoch: 4750 Training Loss: 0.16497817823621963 Test Loss: 0.43589735243055555\n",
      "Epoch: 4751 Training Loss: 0.1649840579562717 Test Loss: 0.4359538302951389\n",
      "Epoch: 4752 Training Loss: 0.16499424913194444 Test Loss: 0.43598817274305557\n",
      "Epoch: 4753 Training Loss: 0.16500709872775607 Test Loss: 0.4357178005642361\n",
      "Epoch: 4754 Training Loss: 0.1650158487955729 Test Loss: 0.43557967122395835\n",
      "Epoch: 4755 Training Loss: 0.1650145975748698 Test Loss: 0.43522669813368053\n",
      "Epoch: 4756 Training Loss: 0.16500456068250868 Test Loss: 0.43520979817708333\n",
      "Epoch: 4757 Training Loss: 0.1650009070502387 Test Loss: 0.4355255805121528\n",
      "Epoch: 4758 Training Loss: 0.16501025390625 Test Loss: 0.4358366970486111\n",
      "Epoch: 4759 Training Loss: 0.16501751369900175 Test Loss: 0.4364768608940972\n",
      "Epoch: 4760 Training Loss: 0.16503519524468316 Test Loss: 0.43766910807291665\n",
      "Epoch: 4761 Training Loss: 0.16510955641004774 Test Loss: 0.43885441080729165\n",
      "Epoch: 4762 Training Loss: 0.1652579345703125 Test Loss: 0.43934228515625\n",
      "Epoch: 4763 Training Loss: 0.16543257141113282 Test Loss: 0.43815220811631944\n",
      "Epoch: 4764 Training Loss: 0.16553020731608073 Test Loss: 0.4366979709201389\n",
      "Epoch: 4765 Training Loss: 0.16554488457573785 Test Loss: 0.4359922146267361\n",
      "Epoch: 4766 Training Loss: 0.16557576497395834 Test Loss: 0.4361563313802083\n",
      "Epoch: 4767 Training Loss: 0.1656392313639323 Test Loss: 0.43679318576388887\n",
      "Epoch: 4768 Training Loss: 0.16562123955620658 Test Loss: 0.4372084418402778\n",
      "Epoch: 4769 Training Loss: 0.16550028313530815 Test Loss: 0.4391099175347222\n",
      "Epoch: 4770 Training Loss: 0.1655328674316406 Test Loss: 0.4412163628472222\n",
      "Epoch: 4771 Training Loss: 0.16575361972384983 Test Loss: 0.4408623589409722\n",
      "Epoch: 4772 Training Loss: 0.16580594889322917 Test Loss: 0.43843739149305555\n",
      "Epoch: 4773 Training Loss: 0.16565122816297742 Test Loss: 0.43703553602430556\n",
      "Epoch: 4774 Training Loss: 0.16553680589463976 Test Loss: 0.4366950412326389\n",
      "Epoch: 4775 Training Loss: 0.16542257012261286 Test Loss: 0.437115478515625\n",
      "Epoch: 4776 Training Loss: 0.1652071295844184 Test Loss: 0.4385541449652778\n",
      "Epoch: 4777 Training Loss: 0.16510440233018664 Test Loss: 0.4401815592447917\n",
      "Epoch: 4778 Training Loss: 0.16521573723687066 Test Loss: 0.44031358506944446\n",
      "Epoch: 4779 Training Loss: 0.16526887851291233 Test Loss: 0.4390060221354167\n",
      "Epoch: 4780 Training Loss: 0.1651830054389106 Test Loss: 0.4381955295138889\n",
      "Epoch: 4781 Training Loss: 0.16509934828016493 Test Loss: 0.43730121527777777\n",
      "Epoch: 4782 Training Loss: 0.16502435133192275 Test Loss: 0.43732066514756945\n",
      "Epoch: 4783 Training Loss: 0.16490250142415364 Test Loss: 0.43873014322916665\n",
      "Epoch: 4784 Training Loss: 0.16482415771484374 Test Loss: 0.4404721137152778\n",
      "Epoch: 4785 Training Loss: 0.16490118577745225 Test Loss: 0.44063346354166666\n",
      "Epoch: 4786 Training Loss: 0.1649982215033637 Test Loss: 0.4399772135416667\n",
      "Epoch: 4787 Training Loss: 0.1650206010606554 Test Loss: 0.4403785807291667\n",
      "Epoch: 4788 Training Loss: 0.1650024634467231 Test Loss: 0.4385436740451389\n",
      "Epoch: 4789 Training Loss: 0.1649517788357205 Test Loss: 0.4377758517795139\n",
      "Epoch: 4790 Training Loss: 0.16485595703125 Test Loss: 0.439255859375\n",
      "Epoch: 4791 Training Loss: 0.16475867207845052 Test Loss: 0.44060907660590276\n",
      "Epoch: 4792 Training Loss: 0.16476933797200521 Test Loss: 0.44127506510416664\n",
      "Epoch: 4793 Training Loss: 0.1648867424858941 Test Loss: 0.44202579752604165\n",
      "Epoch: 4794 Training Loss: 0.16499000040690104 Test Loss: 0.44196980794270835\n",
      "Epoch: 4795 Training Loss: 0.16497372097439236 Test Loss: 0.439439453125\n",
      "Epoch: 4796 Training Loss: 0.16491292317708334 Test Loss: 0.4386504991319444\n",
      "Epoch: 4797 Training Loss: 0.16483650716145834 Test Loss: 0.43946034071180556\n",
      "Epoch: 4798 Training Loss: 0.16477005513509116 Test Loss: 0.4399504123263889\n",
      "Epoch: 4799 Training Loss: 0.1646921624077691 Test Loss: 0.4415438910590278\n",
      "Epoch: 4800 Training Loss: 0.1647413584391276 Test Loss: 0.4430779351128472\n",
      "Epoch: 4801 Training Loss: 0.16486848958333333 Test Loss: 0.44308539496527777\n",
      "Epoch: 4802 Training Loss: 0.1649372355143229 Test Loss: 0.441118408203125\n",
      "Epoch: 4803 Training Loss: 0.16491124810112848 Test Loss: 0.4389606119791667\n",
      "Epoch: 4804 Training Loss: 0.16487659878200955 Test Loss: 0.43831727430555556\n",
      "Epoch: 4805 Training Loss: 0.1649029778374566 Test Loss: 0.43896196831597223\n",
      "Epoch: 4806 Training Loss: 0.16495902845594618 Test Loss: 0.43970247395833334\n",
      "Epoch: 4807 Training Loss: 0.16495870123969184 Test Loss: 0.44102435980902777\n",
      "Epoch: 4808 Training Loss: 0.16498476664225262 Test Loss: 0.44302680121527777\n",
      "Epoch: 4809 Training Loss: 0.165079833984375 Test Loss: 0.44268787977430557\n",
      "Epoch: 4810 Training Loss: 0.16512796868218316 Test Loss: 0.4402214626736111\n",
      "Epoch: 4811 Training Loss: 0.1651218973795573 Test Loss: 0.4380854763454861\n",
      "Epoch: 4812 Training Loss: 0.1651791297064887 Test Loss: 0.43797100151909724\n",
      "Epoch: 4813 Training Loss: 0.1653431125217014 Test Loss: 0.4397377658420139\n",
      "Epoch: 4814 Training Loss: 0.16548685370551217 Test Loss: 0.4413048231336806\n",
      "Epoch: 4815 Training Loss: 0.16546134948730468 Test Loss: 0.4420512424045139\n",
      "Epoch: 4816 Training Loss: 0.1653623775906033 Test Loss: 0.4440925564236111\n",
      "Epoch: 4817 Training Loss: 0.16536429172092013 Test Loss: 0.44447694227430556\n",
      "Epoch: 4818 Training Loss: 0.16536060418023005 Test Loss: 0.4432809787326389\n",
      "Epoch: 4819 Training Loss: 0.16520074971516926 Test Loss: 0.4414121365017361\n",
      "Epoch: 4820 Training Loss: 0.16500139872233072 Test Loss: 0.4398750813802083\n",
      "Epoch: 4821 Training Loss: 0.1648774939643012 Test Loss: 0.439860107421875\n",
      "Epoch: 4822 Training Loss: 0.16478256564670138 Test Loss: 0.4404203559027778\n",
      "Epoch: 4823 Training Loss: 0.16470213148328994 Test Loss: 0.4416851942274306\n",
      "Epoch: 4824 Training Loss: 0.16469061279296876 Test Loss: 0.44367881944444443\n",
      "Epoch: 4825 Training Loss: 0.1647892557779948 Test Loss: 0.44424321831597224\n",
      "Epoch: 4826 Training Loss: 0.16488001844618055 Test Loss: 0.44299088541666665\n",
      "Epoch: 4827 Training Loss: 0.16487395562065973 Test Loss: 0.44151920572916664\n",
      "Epoch: 4828 Training Loss: 0.16486087544759115 Test Loss: 0.4405567491319444\n",
      "Epoch: 4829 Training Loss: 0.1648609585232205 Test Loss: 0.43980156792534725\n",
      "Epoch: 4830 Training Loss: 0.16483982510036893 Test Loss: 0.44079313151041666\n",
      "Epoch: 4831 Training Loss: 0.164813234117296 Test Loss: 0.44336268446180555\n",
      "Epoch: 4832 Training Loss: 0.16485227966308594 Test Loss: 0.44514892578125\n",
      "Epoch: 4833 Training Loss: 0.1650132853190104 Test Loss: 0.4459599609375\n",
      "Epoch: 4834 Training Loss: 0.16520187038845485 Test Loss: 0.4460780978732639\n",
      "Epoch: 4835 Training Loss: 0.1653150600857205 Test Loss: 0.4425918511284722\n",
      "Epoch: 4836 Training Loss: 0.16525843132866752 Test Loss: 0.44058688693576387\n",
      "Epoch: 4837 Training Loss: 0.16523001268174914 Test Loss: 0.4423982747395833\n",
      "Epoch: 4838 Training Loss: 0.1653001013861762 Test Loss: 0.44232703993055555\n",
      "Epoch: 4839 Training Loss: 0.16529545762803818 Test Loss: 0.444625\n",
      "Epoch: 4840 Training Loss: 0.16535096401638455 Test Loss: 0.44654896375868053\n",
      "Epoch: 4841 Training Loss: 0.16547669643825955 Test Loss: 0.44540771484375\n",
      "Epoch: 4842 Training Loss: 0.16555276997884114 Test Loss: 0.44437635633680556\n",
      "Epoch: 4843 Training Loss: 0.16554878234863282 Test Loss: 0.44245570203993057\n",
      "Epoch: 4844 Training Loss: 0.16558008321126302 Test Loss: 0.4418671603732639\n",
      "Epoch: 4845 Training Loss: 0.1656884036593967 Test Loss: 0.4424682888454861\n",
      "Epoch: 4846 Training Loss: 0.16581363254123263 Test Loss: 0.44417713758680555\n",
      "Epoch: 4847 Training Loss: 0.16595653957790799 Test Loss: 0.4459918619791667\n",
      "Epoch: 4848 Training Loss: 0.16620145840115017 Test Loss: 0.44683382161458335\n",
      "Epoch: 4849 Training Loss: 0.16650515916612413 Test Loss: 0.44556095377604166\n",
      "Epoch: 4850 Training Loss: 0.16674134826660156 Test Loss: 0.44398082139756945\n",
      "Epoch: 4851 Training Loss: 0.16693922085232205 Test Loss: 0.44341270616319445\n",
      "Epoch: 4852 Training Loss: 0.16717977735731337 Test Loss: 0.44426540798611114\n",
      "Epoch: 4853 Training Loss: 0.16743702528211807 Test Loss: 0.445865234375\n",
      "Epoch: 4854 Training Loss: 0.16767455206976997 Test Loss: 0.44783756510416667\n",
      "Epoch: 4855 Training Loss: 0.16804349772135416 Test Loss: 0.4480510525173611\n",
      "Epoch: 4856 Training Loss: 0.16855486382378473 Test Loss: 0.44668207465277776\n",
      "Epoch: 4857 Training Loss: 0.16908094618055555 Test Loss: 0.44556309678819445\n",
      "Epoch: 4858 Training Loss: 0.16957110426161023 Test Loss: 0.4455513509114583\n",
      "Epoch: 4859 Training Loss: 0.16995271809895834 Test Loss: 0.4464013129340278\n",
      "Epoch: 4860 Training Loss: 0.17013110690646702 Test Loss: 0.4465807291666667\n",
      "Epoch: 4861 Training Loss: 0.17010743374294704 Test Loss: 0.44715907118055553\n",
      "Epoch: 4862 Training Loss: 0.16996468268500434 Test Loss: 0.4477088758680556\n",
      "Epoch: 4863 Training Loss: 0.16980242750379773 Test Loss: 0.4482839084201389\n",
      "Epoch: 4864 Training Loss: 0.1696212378607856 Test Loss: 0.44866316731770833\n",
      "Epoch: 4865 Training Loss: 0.16942445712619358 Test Loss: 0.44889860026041667\n",
      "Epoch: 4866 Training Loss: 0.16921568637424045 Test Loss: 0.4493443196614583\n",
      "Epoch: 4867 Training Loss: 0.1689886491563585 Test Loss: 0.45032893880208336\n",
      "Epoch: 4868 Training Loss: 0.16878860812717014 Test Loss: 0.4507870551215278\n",
      "Epoch: 4869 Training Loss: 0.16865731811523438 Test Loss: 0.45142857530381947\n",
      "Epoch: 4870 Training Loss: 0.16861241658528645 Test Loss: 0.45186672634548614\n",
      "Epoch: 4871 Training Loss: 0.16863130866156684 Test Loss: 0.45235739474826386\n",
      "Epoch: 4872 Training Loss: 0.16873022969563803 Test Loss: 0.45280284288194445\n",
      "Epoch: 4873 Training Loss: 0.16891385057237412 Test Loss: 0.4534707302517361\n",
      "Epoch: 4874 Training Loss: 0.16919026523166233 Test Loss: 0.45464371744791665\n",
      "Epoch: 4875 Training Loss: 0.169562257554796 Test Loss: 0.4557224934895833\n",
      "Epoch: 4876 Training Loss: 0.17000741407606337 Test Loss: 0.45706174045138886\n",
      "Epoch: 4877 Training Loss: 0.17049742635091145 Test Loss: 0.4583636067708333\n",
      "Epoch: 4878 Training Loss: 0.17106416490342882 Test Loss: 0.46006781684027775\n",
      "Epoch: 4879 Training Loss: 0.17170278761121963 Test Loss: 0.46172998046875\n",
      "Epoch: 4880 Training Loss: 0.17243918863932292 Test Loss: 0.4633924153645833\n",
      "Epoch: 4881 Training Loss: 0.17331720140245227 Test Loss: 0.4646357421875\n",
      "Epoch: 4882 Training Loss: 0.1743561537000868 Test Loss: 0.4661668294270833\n",
      "Epoch: 4883 Training Loss: 0.1756148393419054 Test Loss: 0.46813623046875\n",
      "Epoch: 4884 Training Loss: 0.1771336466471354 Test Loss: 0.47134407552083335\n",
      "Epoch: 4885 Training Loss: 0.17909300571017794 Test Loss: 0.4776804470486111\n",
      "Epoch: 4886 Training Loss: 0.18169761657714845 Test Loss: 0.4845902777777778\n",
      "Epoch: 4887 Training Loss: 0.18517417399088543 Test Loss: 0.48946158854166666\n",
      "Epoch: 4888 Training Loss: 0.18974151950412327 Test Loss: 0.48994189453125\n",
      "Epoch: 4889 Training Loss: 0.19592099338107638 Test Loss: 0.48317333984375\n",
      "Epoch: 4890 Training Loss: 0.20366131761338976 Test Loss: 0.4696595052083333\n",
      "Epoch: 4891 Training Loss: 0.2103074188232422 Test Loss: 0.4574195963541667\n",
      "Epoch: 4892 Training Loss: 0.21004521179199218 Test Loss: 0.46002745225694447\n",
      "Epoch: 4893 Training Loss: 0.2036044498019748 Test Loss: 0.46030099826388887\n",
      "Epoch: 4894 Training Loss: 0.19529153951009115 Test Loss: 0.44924823676215275\n",
      "Epoch: 4895 Training Loss: 0.1876529337565104 Test Loss: 0.4421169704861111\n",
      "Epoch: 4896 Training Loss: 0.18318960910373264 Test Loss: 0.44049918619791667\n",
      "Epoch: 4897 Training Loss: 0.18117778862847222 Test Loss: 0.43901578776041666\n",
      "Epoch: 4898 Training Loss: 0.1803475036621094 Test Loss: 0.43757752821180557\n",
      "Epoch: 4899 Training Loss: 0.18003518846299912 Test Loss: 0.43705845811631944\n",
      "Epoch: 4900 Training Loss: 0.17984149509006075 Test Loss: 0.43667914496527777\n",
      "Epoch: 4901 Training Loss: 0.17965120442708332 Test Loss: 0.43640733506944446\n",
      "Epoch: 4902 Training Loss: 0.1794683888753255 Test Loss: 0.4355290798611111\n",
      "Epoch: 4903 Training Loss: 0.17930546569824218 Test Loss: 0.4346131998697917\n",
      "Epoch: 4904 Training Loss: 0.17922838338216146 Test Loss: 0.43340587022569443\n",
      "Epoch: 4905 Training Loss: 0.1793347947862413 Test Loss: 0.43255528428819445\n",
      "Epoch: 4906 Training Loss: 0.1797271287706163 Test Loss: 0.43161876085069445\n",
      "Epoch: 4907 Training Loss: 0.18051084730360242 Test Loss: 0.43044731987847223\n",
      "Epoch: 4908 Training Loss: 0.18165193854437933 Test Loss: 0.42997450086805555\n",
      "Epoch: 4909 Training Loss: 0.18310379197862414 Test Loss: 0.431042236328125\n",
      "Epoch: 4910 Training Loss: 0.1847026841905382 Test Loss: 0.43491764322916665\n",
      "Epoch: 4911 Training Loss: 0.18614018419053818 Test Loss: 0.4423283420138889\n",
      "Epoch: 4912 Training Loss: 0.18707278442382813 Test Loss: 0.4514250217013889\n",
      "Epoch: 4913 Training Loss: 0.18742237684461804 Test Loss: 0.45683121744791666\n",
      "Epoch: 4914 Training Loss: 0.18712136501736112 Test Loss: 0.4531496310763889\n",
      "Epoch: 4915 Training Loss: 0.18593628607855903 Test Loss: 0.4424619411892361\n",
      "Epoch: 4916 Training Loss: 0.18382154168023004 Test Loss: 0.4316555447048611\n",
      "Epoch: 4917 Training Loss: 0.18126937527126735 Test Loss: 0.42498299153645835\n",
      "Epoch: 4918 Training Loss: 0.17901271057128906 Test Loss: 0.42190291341145836\n",
      "Epoch: 4919 Training Loss: 0.1773631337483724 Test Loss: 0.42087972005208335\n",
      "Epoch: 4920 Training Loss: 0.17631060112847222 Test Loss: 0.4215006510416667\n",
      "Epoch: 4921 Training Loss: 0.1757095709906684 Test Loss: 0.42295081922743055\n",
      "Epoch: 4922 Training Loss: 0.17540509033203125 Test Loss: 0.424824462890625\n",
      "Epoch: 4923 Training Loss: 0.17527271694607205 Test Loss: 0.42695836046006946\n",
      "Epoch: 4924 Training Loss: 0.1752257283528646 Test Loss: 0.42939312065972224\n",
      "Epoch: 4925 Training Loss: 0.17524303351508247 Test Loss: 0.4315377875434028\n",
      "Epoch: 4926 Training Loss: 0.17523070949978298 Test Loss: 0.43365641276041667\n",
      "Epoch: 4927 Training Loss: 0.17519308641221787 Test Loss: 0.43603133138020833\n",
      "Epoch: 4928 Training Loss: 0.17513277011447484 Test Loss: 0.4381494140625\n",
      "Epoch: 4929 Training Loss: 0.17503958977593315 Test Loss: 0.43951524522569446\n",
      "Epoch: 4930 Training Loss: 0.17486352878146702 Test Loss: 0.440134765625\n",
      "Epoch: 4931 Training Loss: 0.1745762159559462 Test Loss: 0.4393273654513889\n",
      "Epoch: 4932 Training Loss: 0.1742196536593967 Test Loss: 0.4378252766927083\n",
      "Epoch: 4933 Training Loss: 0.17379033576117622 Test Loss: 0.4356676974826389\n",
      "Epoch: 4934 Training Loss: 0.17333800421820747 Test Loss: 0.43315386284722224\n",
      "Epoch: 4935 Training Loss: 0.17288324822319878 Test Loss: 0.4307882215711806\n",
      "Epoch: 4936 Training Loss: 0.17245874701605904 Test Loss: 0.4289372287326389\n",
      "Epoch: 4937 Training Loss: 0.1720765414767795 Test Loss: 0.4274836154513889\n",
      "Epoch: 4938 Training Loss: 0.1717667965359158 Test Loss: 0.42645567491319447\n",
      "Epoch: 4939 Training Loss: 0.17150635782877605 Test Loss: 0.4257814398871528\n",
      "Epoch: 4940 Training Loss: 0.1713040720621745 Test Loss: 0.42538145616319445\n",
      "Epoch: 4941 Training Loss: 0.17113683064778645 Test Loss: 0.42534136284722224\n",
      "Epoch: 4942 Training Loss: 0.17100440639919706 Test Loss: 0.4258122829861111\n",
      "Epoch: 4943 Training Loss: 0.17087737189398872 Test Loss: 0.42619536675347225\n",
      "Epoch: 4944 Training Loss: 0.17077303059895832 Test Loss: 0.42673206922743057\n",
      "Epoch: 4945 Training Loss: 0.17066579691569012 Test Loss: 0.4273650173611111\n",
      "Epoch: 4946 Training Loss: 0.17056861538357204 Test Loss: 0.42799327256944447\n",
      "Epoch: 4947 Training Loss: 0.17049380323621963 Test Loss: 0.42854237196180556\n",
      "Epoch: 4948 Training Loss: 0.17041018676757813 Test Loss: 0.4290951605902778\n",
      "Epoch: 4949 Training Loss: 0.1703526899549696 Test Loss: 0.42957427300347223\n",
      "Epoch: 4950 Training Loss: 0.17030572679307726 Test Loss: 0.42993720160590276\n",
      "Epoch: 4951 Training Loss: 0.17028131273057726 Test Loss: 0.43005143229166665\n",
      "Epoch: 4952 Training Loss: 0.17026615566677517 Test Loss: 0.42990809461805557\n",
      "Epoch: 4953 Training Loss: 0.17026242065429686 Test Loss: 0.4294582790798611\n",
      "Epoch: 4954 Training Loss: 0.17024896748860677 Test Loss: 0.42893202039930556\n",
      "Epoch: 4955 Training Loss: 0.17022178480360242 Test Loss: 0.4278251681857639\n",
      "Epoch: 4956 Training Loss: 0.17018077256944444 Test Loss: 0.42680246310763886\n",
      "Epoch: 4957 Training Loss: 0.17012098354763455 Test Loss: 0.4256258138020833\n",
      "Epoch: 4958 Training Loss: 0.17006327141655816 Test Loss: 0.42446771918402776\n",
      "Epoch: 4959 Training Loss: 0.16999177890353734 Test Loss: 0.42323166232638887\n",
      "Epoch: 4960 Training Loss: 0.16992072211371528 Test Loss: 0.42219639756944444\n",
      "Epoch: 4961 Training Loss: 0.16986005316840277 Test Loss: 0.42155194769965276\n",
      "Epoch: 4962 Training Loss: 0.16982376607259114 Test Loss: 0.4211691623263889\n",
      "Epoch: 4963 Training Loss: 0.16981251695421007 Test Loss: 0.42119395616319444\n",
      "Epoch: 4964 Training Loss: 0.16981248135036892 Test Loss: 0.4216494411892361\n",
      "Epoch: 4965 Training Loss: 0.16983944363064235 Test Loss: 0.42241053602430556\n",
      "Epoch: 4966 Training Loss: 0.16990481397840712 Test Loss: 0.423369140625\n",
      "Epoch: 4967 Training Loss: 0.17001141187879773 Test Loss: 0.424803955078125\n",
      "Epoch: 4968 Training Loss: 0.17012890794542102 Test Loss: 0.42639735243055554\n",
      "Epoch: 4969 Training Loss: 0.17027578565809462 Test Loss: 0.42800792100694446\n",
      "Epoch: 4970 Training Loss: 0.1704533928765191 Test Loss: 0.42962459309895834\n",
      "Epoch: 4971 Training Loss: 0.17061270480685764 Test Loss: 0.4307699652777778\n",
      "Epoch: 4972 Training Loss: 0.1707675543891059 Test Loss: 0.4317325846354167\n",
      "Epoch: 4973 Training Loss: 0.17091872999403213 Test Loss: 0.432109619140625\n",
      "Epoch: 4974 Training Loss: 0.1710330318874783 Test Loss: 0.43198787434895836\n",
      "Epoch: 4975 Training Loss: 0.17111469353569878 Test Loss: 0.43149867078993054\n",
      "Epoch: 4976 Training Loss: 0.17110822719997829 Test Loss: 0.4306715494791667\n",
      "Epoch: 4977 Training Loss: 0.1710392795138889 Test Loss: 0.42963902452256947\n",
      "Epoch: 4978 Training Loss: 0.17089283921983506 Test Loss: 0.4286541069878472\n",
      "Epoch: 4979 Training Loss: 0.17068841044108074 Test Loss: 0.42774297417534723\n",
      "Epoch: 4980 Training Loss: 0.1704634975857205 Test Loss: 0.4270527072482639\n",
      "Epoch: 4981 Training Loss: 0.17022790357801648 Test Loss: 0.4266928982204861\n",
      "Epoch: 4982 Training Loss: 0.16998401387532552 Test Loss: 0.4266067165798611\n",
      "Epoch: 4983 Training Loss: 0.16973629421657987 Test Loss: 0.4269069010416667\n",
      "Epoch: 4984 Training Loss: 0.1694989251030816 Test Loss: 0.42738734266493056\n",
      "Epoch: 4985 Training Loss: 0.169277589586046 Test Loss: 0.42793077256944445\n",
      "Epoch: 4986 Training Loss: 0.16910362413194444 Test Loss: 0.4283785807291667\n",
      "Epoch: 4987 Training Loss: 0.16895457797580296 Test Loss: 0.4286676432291667\n",
      "Epoch: 4988 Training Loss: 0.16883613925509983 Test Loss: 0.429126708984375\n",
      "Epoch: 4989 Training Loss: 0.1687501983642578 Test Loss: 0.42945947265625\n",
      "Epoch: 4990 Training Loss: 0.1686793450249566 Test Loss: 0.4296554904513889\n",
      "Epoch: 4991 Training Loss: 0.16861881679958768 Test Loss: 0.4299164225260417\n",
      "Epoch: 4992 Training Loss: 0.1685671657986111 Test Loss: 0.43011328125\n",
      "Epoch: 4993 Training Loss: 0.16854021708170572 Test Loss: 0.43025830078125\n",
      "Epoch: 4994 Training Loss: 0.16853299458821613 Test Loss: 0.43049351671006947\n",
      "Epoch: 4995 Training Loss: 0.16856437344021266 Test Loss: 0.4308057183159722\n",
      "Epoch: 4996 Training Loss: 0.16863031514485677 Test Loss: 0.43118592664930555\n",
      "Epoch: 4997 Training Loss: 0.16874083794487849 Test Loss: 0.43155457899305555\n",
      "Epoch: 4998 Training Loss: 0.16890415785047744 Test Loss: 0.4319686414930556\n",
      "Epoch: 4999 Training Loss: 0.16911927286783854 Test Loss: 0.4326496310763889\n",
      "Epoch: 5000 Training Loss: 0.1693843977186415 Test Loss: 0.4334437934027778\n",
      "Epoch: 5001 Training Loss: 0.1697051289876302 Test Loss: 0.43405552842881945\n",
      "Epoch: 5002 Training Loss: 0.1700981174045139 Test Loss: 0.43501169162326386\n",
      "Epoch: 5003 Training Loss: 0.1705781521267361 Test Loss: 0.4360945095486111\n",
      "Epoch: 5004 Training Loss: 0.17114985656738282 Test Loss: 0.4372376030815972\n",
      "Epoch: 5005 Training Loss: 0.17182491387261284 Test Loss: 0.4383739963107639\n",
      "Epoch: 5006 Training Loss: 0.17258608500162761 Test Loss: 0.43960191514756947\n",
      "Epoch: 5007 Training Loss: 0.17345573086208768 Test Loss: 0.44012489149305556\n",
      "Epoch: 5008 Training Loss: 0.17446500481499566 Test Loss: 0.44068117947048613\n",
      "Epoch: 5009 Training Loss: 0.175608150906033 Test Loss: 0.4409172634548611\n",
      "Epoch: 5010 Training Loss: 0.17687250434027776 Test Loss: 0.4422120768229167\n",
      "Epoch: 5011 Training Loss: 0.17834742567274306 Test Loss: 0.44454964192708335\n",
      "Epoch: 5012 Training Loss: 0.18012137688530816 Test Loss: 0.44797840711805553\n",
      "Epoch: 5013 Training Loss: 0.18217095269097222 Test Loss: 0.45087505425347224\n",
      "Epoch: 5014 Training Loss: 0.18433442179361978 Test Loss: 0.45608344184027777\n",
      "Epoch: 5015 Training Loss: 0.18714772372775607 Test Loss: 0.46144287109375\n",
      "Epoch: 5016 Training Loss: 0.1901871303982205 Test Loss: 0.46135888671875\n",
      "Epoch: 5017 Training Loss: 0.19290883382161458 Test Loss: 0.44693085394965276\n",
      "Epoch: 5018 Training Loss: 0.19339691501193576 Test Loss: 0.4287067328559028\n",
      "Epoch: 5019 Training Loss: 0.1905544145372179 Test Loss: 0.42335980902777776\n",
      "Epoch: 5020 Training Loss: 0.18556808471679687 Test Loss: 0.42516159396701386\n",
      "Epoch: 5021 Training Loss: 0.1805893775092231 Test Loss: 0.4248871527777778\n",
      "Epoch: 5022 Training Loss: 0.17649599032931856 Test Loss: 0.4242173665364583\n",
      "Epoch: 5023 Training Loss: 0.17336084323459203 Test Loss: 0.42522553168402777\n",
      "Epoch: 5024 Training Loss: 0.17121803283691406 Test Loss: 0.4249058702256944\n",
      "Epoch: 5025 Training Loss: 0.16983063591851127 Test Loss: 0.42354351128472223\n",
      "Epoch: 5026 Training Loss: 0.1688353237575955 Test Loss: 0.42249110243055554\n",
      "Epoch: 5027 Training Loss: 0.16808373684353298 Test Loss: 0.42185432942708334\n",
      "Epoch: 5028 Training Loss: 0.16751598273383247 Test Loss: 0.42148206922743053\n",
      "Epoch: 5029 Training Loss: 0.16704882134331597 Test Loss: 0.42133219401041666\n",
      "Epoch: 5030 Training Loss: 0.1666574215359158 Test Loss: 0.4215104709201389\n",
      "Epoch: 5031 Training Loss: 0.16630899386935763 Test Loss: 0.42168269856770835\n",
      "Epoch: 5032 Training Loss: 0.16599569532606337 Test Loss: 0.4219146050347222\n",
      "Epoch: 5033 Training Loss: 0.16570382520887586 Test Loss: 0.42200661892361113\n",
      "Epoch: 5034 Training Loss: 0.16542630343967013 Test Loss: 0.42219417317708335\n",
      "Epoch: 5035 Training Loss: 0.1651683112250434 Test Loss: 0.42231776258680553\n",
      "Epoch: 5036 Training Loss: 0.16491884019639758 Test Loss: 0.42235194227430556\n",
      "Epoch: 5037 Training Loss: 0.16468481614854602 Test Loss: 0.42247200520833333\n",
      "Epoch: 5038 Training Loss: 0.1644655287000868 Test Loss: 0.42250716145833334\n",
      "Epoch: 5039 Training Loss: 0.1642496117485894 Test Loss: 0.4225328233506944\n",
      "Epoch: 5040 Training Loss: 0.1640414818657769 Test Loss: 0.42252685546875\n",
      "Epoch: 5041 Training Loss: 0.16384243435329862 Test Loss: 0.4225131293402778\n",
      "Epoch: 5042 Training Loss: 0.16365850999620227 Test Loss: 0.4225646701388889\n",
      "Epoch: 5043 Training Loss: 0.1634794616699219 Test Loss: 0.42252232530381945\n",
      "Epoch: 5044 Training Loss: 0.1633063693576389 Test Loss: 0.42256138780381947\n",
      "Epoch: 5045 Training Loss: 0.16314036729600695 Test Loss: 0.4226266547309028\n",
      "Epoch: 5046 Training Loss: 0.16298875257703993 Test Loss: 0.42256966145833336\n",
      "Epoch: 5047 Training Loss: 0.1628390842013889 Test Loss: 0.4226697048611111\n",
      "Epoch: 5048 Training Loss: 0.16270301649305555 Test Loss: 0.42262934027777777\n",
      "Epoch: 5049 Training Loss: 0.1625658162434896 Test Loss: 0.42272368706597224\n",
      "Epoch: 5050 Training Loss: 0.16243686591254342 Test Loss: 0.42273741319444447\n",
      "Epoch: 5051 Training Loss: 0.1623138682047526 Test Loss: 0.422777099609375\n",
      "Epoch: 5052 Training Loss: 0.16219896782769097 Test Loss: 0.42284339735243054\n",
      "Epoch: 5053 Training Loss: 0.16208789740668403 Test Loss: 0.4229034288194444\n",
      "Epoch: 5054 Training Loss: 0.16198057725694445 Test Loss: 0.42300721571180555\n",
      "Epoch: 5055 Training Loss: 0.16187803988986546 Test Loss: 0.4231655544704861\n",
      "Epoch: 5056 Training Loss: 0.16178011915418836 Test Loss: 0.42310175238715275\n",
      "Epoch: 5057 Training Loss: 0.16168473307291667 Test Loss: 0.4231968315972222\n",
      "Epoch: 5058 Training Loss: 0.16159153578016494 Test Loss: 0.42329524739583335\n",
      "Epoch: 5059 Training Loss: 0.16150767686631945 Test Loss: 0.4233577473958333\n",
      "Epoch: 5060 Training Loss: 0.16142417568630643 Test Loss: 0.42344813368055556\n",
      "Epoch: 5061 Training Loss: 0.16134316168891058 Test Loss: 0.4235635036892361\n",
      "Epoch: 5062 Training Loss: 0.16126541815863715 Test Loss: 0.4236753472222222\n",
      "Epoch: 5063 Training Loss: 0.16118937004937067 Test Loss: 0.4237160373263889\n",
      "Epoch: 5064 Training Loss: 0.16111570739746095 Test Loss: 0.4238034939236111\n",
      "Epoch: 5065 Training Loss: 0.16104101223415798 Test Loss: 0.4239509819878472\n",
      "Epoch: 5066 Training Loss: 0.1609719509548611 Test Loss: 0.423978271484375\n",
      "Epoch: 5067 Training Loss: 0.16090624152289495 Test Loss: 0.4240509711371528\n",
      "Epoch: 5068 Training Loss: 0.1608415747748481 Test Loss: 0.42415890842013887\n",
      "Epoch: 5069 Training Loss: 0.16078036668565537 Test Loss: 0.4242869466145833\n",
      "Epoch: 5070 Training Loss: 0.16072378709581164 Test Loss: 0.4243359375\n",
      "Epoch: 5071 Training Loss: 0.1606655561659071 Test Loss: 0.4244033203125\n",
      "Epoch: 5072 Training Loss: 0.160606689453125 Test Loss: 0.42454451497395834\n",
      "Epoch: 5073 Training Loss: 0.1605526140001085 Test Loss: 0.4247013617621528\n",
      "Epoch: 5074 Training Loss: 0.16050400967068143 Test Loss: 0.42471001519097223\n",
      "Epoch: 5075 Training Loss: 0.16045734151204427 Test Loss: 0.42476953125\n",
      "Epoch: 5076 Training Loss: 0.16040401373969185 Test Loss: 0.4248152126736111\n",
      "Epoch: 5077 Training Loss: 0.16035257805718317 Test Loss: 0.4249707845052083\n",
      "Epoch: 5078 Training Loss: 0.16031094360351564 Test Loss: 0.425078125\n",
      "Epoch: 5079 Training Loss: 0.16027145894368489 Test Loss: 0.42500927734375\n",
      "Epoch: 5080 Training Loss: 0.16023075188530816 Test Loss: 0.4250661892361111\n",
      "Epoch: 5081 Training Loss: 0.1601855214436849 Test Loss: 0.42512250434027776\n",
      "Epoch: 5082 Training Loss: 0.16014061143663194 Test Loss: 0.4252125922309028\n",
      "Epoch: 5083 Training Loss: 0.160106689453125 Test Loss: 0.42531019422743055\n",
      "Epoch: 5084 Training Loss: 0.16007669745551215 Test Loss: 0.4252105034722222\n",
      "Epoch: 5085 Training Loss: 0.1600443064371745 Test Loss: 0.42512198893229164\n",
      "Epoch: 5086 Training Loss: 0.16001197306315104 Test Loss: 0.42523101128472224\n",
      "Epoch: 5087 Training Loss: 0.15997836303710938 Test Loss: 0.42534022352430556\n",
      "Epoch: 5088 Training Loss: 0.15995154486762153 Test Loss: 0.4254111056857639\n",
      "Epoch: 5089 Training Loss: 0.15993021308051214 Test Loss: 0.42528990342881945\n",
      "Epoch: 5090 Training Loss: 0.15991260952419706 Test Loss: 0.4252402615017361\n",
      "Epoch: 5091 Training Loss: 0.159892825656467 Test Loss: 0.4251864691840278\n",
      "Epoch: 5092 Training Loss: 0.15986799621582032 Test Loss: 0.42520418294270834\n",
      "Epoch: 5093 Training Loss: 0.15985333251953124 Test Loss: 0.42524327256944444\n",
      "Epoch: 5094 Training Loss: 0.1598395300971137 Test Loss: 0.42532023111979167\n",
      "Epoch: 5095 Training Loss: 0.15983719889322917 Test Loss: 0.42531947157118055\n",
      "Epoch: 5096 Training Loss: 0.15983428955078124 Test Loss: 0.42549061414930556\n",
      "Epoch: 5097 Training Loss: 0.15983587646484376 Test Loss: 0.4255748969184028\n",
      "Epoch: 5098 Training Loss: 0.15983463880750867 Test Loss: 0.42564898003472224\n",
      "Epoch: 5099 Training Loss: 0.15983475748697917 Test Loss: 0.42581667751736113\n",
      "Epoch: 5100 Training Loss: 0.15982908460828993 Test Loss: 0.4260676812065972\n",
      "Epoch: 5101 Training Loss: 0.1598284166124132 Test Loss: 0.42635972764756946\n",
      "Epoch: 5102 Training Loss: 0.15982018195258246 Test Loss: 0.426472412109375\n",
      "Epoch: 5103 Training Loss: 0.15980742730034722 Test Loss: 0.42665294053819447\n",
      "Epoch: 5104 Training Loss: 0.15979010179307726 Test Loss: 0.4268689236111111\n",
      "Epoch: 5105 Training Loss: 0.15977720642089843 Test Loss: 0.42713926866319446\n",
      "Epoch: 5106 Training Loss: 0.15976563517252604 Test Loss: 0.42735628255208336\n",
      "Epoch: 5107 Training Loss: 0.15974975416395398 Test Loss: 0.4275421006944444\n",
      "Epoch: 5108 Training Loss: 0.15973006523980035 Test Loss: 0.4276952853732639\n",
      "Epoch: 5109 Training Loss: 0.15970494757758247 Test Loss: 0.42782112630208335\n",
      "Epoch: 5110 Training Loss: 0.1596811286078559 Test Loss: 0.4279965277777778\n",
      "Epoch: 5111 Training Loss: 0.15965652804904515 Test Loss: 0.42828841145833335\n",
      "Epoch: 5112 Training Loss: 0.15963316006130643 Test Loss: 0.42860983615451387\n",
      "Epoch: 5113 Training Loss: 0.15961678568522136 Test Loss: 0.4291357421875\n",
      "Epoch: 5114 Training Loss: 0.15961092122395834 Test Loss: 0.4291270616319444\n",
      "Epoch: 5115 Training Loss: 0.1595958980984158 Test Loss: 0.4289449055989583\n",
      "Epoch: 5116 Training Loss: 0.1595746629503038 Test Loss: 0.4289194878472222\n",
      "Epoch: 5117 Training Loss: 0.15955103386773004 Test Loss: 0.4289709201388889\n",
      "Epoch: 5118 Training Loss: 0.15953583102756078 Test Loss: 0.42918158637152776\n",
      "Epoch: 5119 Training Loss: 0.15952029588487412 Test Loss: 0.42958968098958333\n",
      "Epoch: 5120 Training Loss: 0.15950272115071615 Test Loss: 0.43053702799479165\n",
      "Epoch: 5121 Training Loss: 0.15951295301649304 Test Loss: 0.4314994032118056\n",
      "Epoch: 5122 Training Loss: 0.15953993225097657 Test Loss: 0.4313682183159722\n",
      "Epoch: 5123 Training Loss: 0.15954139709472656 Test Loss: 0.43033197699652775\n",
      "Epoch: 5124 Training Loss: 0.1595187055799696 Test Loss: 0.42997748480902775\n",
      "Epoch: 5125 Training Loss: 0.15949755350748698 Test Loss: 0.4304231228298611\n",
      "Epoch: 5126 Training Loss: 0.15949220445421006 Test Loss: 0.4306948784722222\n",
      "Epoch: 5127 Training Loss: 0.15948454115125868 Test Loss: 0.43082047526041667\n",
      "Epoch: 5128 Training Loss: 0.15946722073025174 Test Loss: 0.4325348036024306\n",
      "Epoch: 5129 Training Loss: 0.15951349046495225 Test Loss: 0.4341979709201389\n",
      "Epoch: 5130 Training Loss: 0.1595421346028646 Test Loss: 0.4326120334201389\n",
      "Epoch: 5131 Training Loss: 0.1595350087483724 Test Loss: 0.43078116861979165\n",
      "Epoch: 5132 Training Loss: 0.15949474419487847 Test Loss: 0.4311975368923611\n",
      "Epoch: 5133 Training Loss: 0.15945640563964844 Test Loss: 0.4316740451388889\n",
      "Epoch: 5134 Training Loss: 0.15942464701334635 Test Loss: 0.4315002712673611\n",
      "Epoch: 5135 Training Loss: 0.15939196268717448 Test Loss: 0.43347059461805554\n",
      "Epoch: 5136 Training Loss: 0.15944441731770834 Test Loss: 0.4350806206597222\n",
      "Epoch: 5137 Training Loss: 0.15945773315429687 Test Loss: 0.4326621365017361\n",
      "Epoch: 5138 Training Loss: 0.15942391120062935 Test Loss: 0.4310791558159722\n",
      "Epoch: 5139 Training Loss: 0.1593904334174262 Test Loss: 0.43171500651041667\n",
      "Epoch: 5140 Training Loss: 0.15935164388020834 Test Loss: 0.43173828125\n",
      "Epoch: 5141 Training Loss: 0.15932588704427084 Test Loss: 0.43239908854166664\n",
      "Epoch: 5142 Training Loss: 0.15934032524956598 Test Loss: 0.4345715603298611\n",
      "Epoch: 5143 Training Loss: 0.1593777058919271 Test Loss: 0.43359814453125\n",
      "Epoch: 5144 Training Loss: 0.15936581590440538 Test Loss: 0.4313274739583333\n",
      "Epoch: 5145 Training Loss: 0.15935975986056858 Test Loss: 0.4314494357638889\n",
      "Epoch: 5146 Training Loss: 0.15936014980740018 Test Loss: 0.43159282769097224\n",
      "Epoch: 5147 Training Loss: 0.1593561537000868 Test Loss: 0.43150906032986114\n",
      "Epoch: 5148 Training Loss: 0.15935633850097655 Test Loss: 0.4333872612847222\n",
      "Epoch: 5149 Training Loss: 0.159368894788954 Test Loss: 0.43339675564236113\n",
      "Epoch: 5150 Training Loss: 0.15937948608398436 Test Loss: 0.4309074978298611\n",
      "Epoch: 5151 Training Loss: 0.15938776312934028 Test Loss: 0.43038004557291665\n",
      "Epoch: 5152 Training Loss: 0.15942760213216145 Test Loss: 0.4306844075520833\n",
      "Epoch: 5153 Training Loss: 0.15945997619628907 Test Loss: 0.43057421875\n",
      "Epoch: 5154 Training Loss: 0.15944620598687065 Test Loss: 0.43128626844618057\n",
      "Epoch: 5155 Training Loss: 0.1594402601453993 Test Loss: 0.4313573404947917\n",
      "Epoch: 5156 Training Loss: 0.1594796668158637 Test Loss: 0.43049614800347225\n",
      "Epoch: 5157 Training Loss: 0.1595152113172743 Test Loss: 0.43112315538194446\n",
      "Epoch: 5158 Training Loss: 0.1595361531575521 Test Loss: 0.43135677083333335\n",
      "Epoch: 5159 Training Loss: 0.15952926635742187 Test Loss: 0.4315014105902778\n",
      "Epoch: 5160 Training Loss: 0.15952200995551216 Test Loss: 0.43235658094618057\n",
      "Epoch: 5161 Training Loss: 0.1595527615017361 Test Loss: 0.43254893663194444\n",
      "Epoch: 5162 Training Loss: 0.15959632873535157 Test Loss: 0.4317377658420139\n",
      "Epoch: 5163 Training Loss: 0.15962674119737413 Test Loss: 0.4313186577690972\n",
      "Epoch: 5164 Training Loss: 0.15963100009494358 Test Loss: 0.43150672743055557\n",
      "Epoch: 5165 Training Loss: 0.1596294470893012 Test Loss: 0.43217106119791665\n",
      "Epoch: 5166 Training Loss: 0.15962176005045572 Test Loss: 0.43361691623263887\n",
      "Epoch: 5167 Training Loss: 0.15965929328070746 Test Loss: 0.43430973307291665\n",
      "Epoch: 5168 Training Loss: 0.15972425164116755 Test Loss: 0.4330537651909722\n",
      "Epoch: 5169 Training Loss: 0.1597515631781684 Test Loss: 0.43188883463541666\n",
      "Epoch: 5170 Training Loss: 0.1597660132514106 Test Loss: 0.4322501898871528\n",
      "Epoch: 5171 Training Loss: 0.1597763434516059 Test Loss: 0.43261802842881947\n",
      "Epoch: 5172 Training Loss: 0.15979037814670138 Test Loss: 0.4339303927951389\n",
      "Epoch: 5173 Training Loss: 0.1598406236436632 Test Loss: 0.4354538302951389\n",
      "Epoch: 5174 Training Loss: 0.15992753261990017 Test Loss: 0.43473090277777776\n",
      "Epoch: 5175 Training Loss: 0.16000293646918404 Test Loss: 0.4330802408854167\n",
      "Epoch: 5176 Training Loss: 0.16006717088487413 Test Loss: 0.4325935329861111\n",
      "Epoch: 5177 Training Loss: 0.16011920844184027 Test Loss: 0.4332356228298611\n",
      "Epoch: 5178 Training Loss: 0.16015852695041233 Test Loss: 0.43404481336805556\n",
      "Epoch: 5179 Training Loss: 0.16020196702745226 Test Loss: 0.4356851128472222\n",
      "Epoch: 5180 Training Loss: 0.1603182101779514 Test Loss: 0.43585120985243053\n",
      "Epoch: 5181 Training Loss: 0.16045643615722657 Test Loss: 0.4340167100694444\n",
      "Epoch: 5182 Training Loss: 0.16059525214301215 Test Loss: 0.4333980305989583\n",
      "Epoch: 5183 Training Loss: 0.16073903232150608 Test Loss: 0.4342236056857639\n",
      "Epoch: 5184 Training Loss: 0.16088489786783855 Test Loss: 0.4345661892361111\n",
      "Epoch: 5185 Training Loss: 0.16102415974934897 Test Loss: 0.43640836588541665\n",
      "Epoch: 5186 Training Loss: 0.1612386915418837 Test Loss: 0.437580078125\n",
      "Epoch: 5187 Training Loss: 0.16152350870768228 Test Loss: 0.4364326714409722\n",
      "Epoch: 5188 Training Loss: 0.16187839253743488 Test Loss: 0.43641691080729167\n",
      "Epoch: 5189 Training Loss: 0.16229991997612847 Test Loss: 0.4378220486111111\n",
      "Epoch: 5190 Training Loss: 0.16276302083333333 Test Loss: 0.43908539496527776\n",
      "Epoch: 5191 Training Loss: 0.16330066257052953 Test Loss: 0.44122081163194443\n",
      "Epoch: 5192 Training Loss: 0.1640742662217882 Test Loss: 0.44174769422743054\n",
      "Epoch: 5193 Training Loss: 0.16501726616753473 Test Loss: 0.4413029242621528\n",
      "Epoch: 5194 Training Loss: 0.16615477667914497 Test Loss: 0.44261783854166664\n",
      "Epoch: 5195 Training Loss: 0.16753121948242186 Test Loss: 0.44456947157118054\n",
      "Epoch: 5196 Training Loss: 0.1692216576470269 Test Loss: 0.44674376085069445\n",
      "Epoch: 5197 Training Loss: 0.17130440606011285 Test Loss: 0.44550895182291667\n",
      "Epoch: 5198 Training Loss: 0.17376013522677952 Test Loss: 0.44334749348958336\n",
      "Epoch: 5199 Training Loss: 0.17668600124782985 Test Loss: 0.4353525119357639\n",
      "Epoch: 5200 Training Loss: 0.17963890753851997 Test Loss: 0.4279001193576389\n",
      "Epoch: 5201 Training Loss: 0.181214357164171 Test Loss: 0.42941840277777776\n",
      "Epoch: 5202 Training Loss: 0.180129396226671 Test Loss: 0.43774110243055553\n",
      "Epoch: 5203 Training Loss: 0.17732100084092883 Test Loss: 0.4408722873263889\n",
      "Epoch: 5204 Training Loss: 0.17377595350477432 Test Loss: 0.43520014105902777\n",
      "Epoch: 5205 Training Loss: 0.1699919162326389 Test Loss: 0.43022987196180557\n",
      "Epoch: 5206 Training Loss: 0.16680214436848959 Test Loss: 0.42846242947048613\n",
      "Epoch: 5207 Training Loss: 0.1646738501654731 Test Loss: 0.4282821451822917\n",
      "Epoch: 5208 Training Loss: 0.16344814893934462 Test Loss: 0.4283667534722222\n",
      "Epoch: 5209 Training Loss: 0.16280051676432292 Test Loss: 0.4282779134114583\n",
      "Epoch: 5210 Training Loss: 0.16249138726128473 Test Loss: 0.42843321397569445\n",
      "Epoch: 5211 Training Loss: 0.1623686794704861 Test Loss: 0.4286449652777778\n",
      "Epoch: 5212 Training Loss: 0.16234520128038193 Test Loss: 0.42898974609375\n",
      "Epoch: 5213 Training Loss: 0.16236682807074654 Test Loss: 0.4291566297743056\n",
      "Epoch: 5214 Training Loss: 0.1624151119656033 Test Loss: 0.42945735677083335\n",
      "Epoch: 5215 Training Loss: 0.16245820617675782 Test Loss: 0.42961219618055557\n",
      "Epoch: 5216 Training Loss: 0.1625276607937283 Test Loss: 0.4298495551215278\n",
      "Epoch: 5217 Training Loss: 0.16261049058702257 Test Loss: 0.43007872178819445\n",
      "Epoch: 5218 Training Loss: 0.16270891825358072 Test Loss: 0.43050927734375\n",
      "Epoch: 5219 Training Loss: 0.16283983527289497 Test Loss: 0.43112000868055556\n",
      "Epoch: 5220 Training Loss: 0.16301887003580728 Test Loss: 0.4317606879340278\n",
      "Epoch: 5221 Training Loss: 0.1632418721516927 Test Loss: 0.43253678385416666\n",
      "Epoch: 5222 Training Loss: 0.16352904086642794 Test Loss: 0.4333355034722222\n",
      "Epoch: 5223 Training Loss: 0.16387998962402345 Test Loss: 0.4341506076388889\n",
      "Epoch: 5224 Training Loss: 0.1643156467013889 Test Loss: 0.43490804036458336\n",
      "Epoch: 5225 Training Loss: 0.16486454433865017 Test Loss: 0.4351136067708333\n",
      "Epoch: 5226 Training Loss: 0.16555579460991754 Test Loss: 0.4354599066840278\n",
      "Epoch: 5227 Training Loss: 0.16637287055121527 Test Loss: 0.4350998806423611\n",
      "Epoch: 5228 Training Loss: 0.1672982652452257 Test Loss: 0.43421278211805553\n",
      "Epoch: 5229 Training Loss: 0.1683103281656901 Test Loss: 0.43189152018229165\n",
      "Epoch: 5230 Training Loss: 0.16931782531738282 Test Loss: 0.4288125542534722\n",
      "Epoch: 5231 Training Loss: 0.1701614244249132 Test Loss: 0.42702048068576387\n",
      "Epoch: 5232 Training Loss: 0.17066266547309028 Test Loss: 0.42734120008680554\n",
      "Epoch: 5233 Training Loss: 0.17075233968098957 Test Loss: 0.42978396267361113\n",
      "Epoch: 5234 Training Loss: 0.17048373413085938 Test Loss: 0.43195903862847224\n",
      "Epoch: 5235 Training Loss: 0.17002577718098957 Test Loss: 0.4315442165798611\n",
      "Epoch: 5236 Training Loss: 0.16951756117078992 Test Loss: 0.4291061197916667\n",
      "Epoch: 5237 Training Loss: 0.16901573859320745 Test Loss: 0.42652167426215276\n",
      "Epoch: 5238 Training Loss: 0.1686253628200955 Test Loss: 0.4234295518663194\n",
      "Epoch: 5239 Training Loss: 0.1684076402452257 Test Loss: 0.4223722873263889\n",
      "Epoch: 5240 Training Loss: 0.16836293029785157 Test Loss: 0.4227623969184028\n",
      "Epoch: 5241 Training Loss: 0.1684776831732856 Test Loss: 0.4244010959201389\n",
      "Epoch: 5242 Training Loss: 0.16865543280707465 Test Loss: 0.42675889756944446\n",
      "Epoch: 5243 Training Loss: 0.16881527031792534 Test Loss: 0.42901041666666667\n",
      "Epoch: 5244 Training Loss: 0.16897249348958332 Test Loss: 0.4312744140625\n",
      "Epoch: 5245 Training Loss: 0.1691045430501302 Test Loss: 0.43185362413194445\n",
      "Epoch: 5246 Training Loss: 0.1691303202311198 Test Loss: 0.4310230034722222\n",
      "Epoch: 5247 Training Loss: 0.16898743184407553 Test Loss: 0.428953369140625\n",
      "Epoch: 5248 Training Loss: 0.16859062872992622 Test Loss: 0.42626407877604167\n",
      "Epoch: 5249 Training Loss: 0.1678816884358724 Test Loss: 0.4240728081597222\n",
      "Epoch: 5250 Training Loss: 0.16702304246690539 Test Loss: 0.4227048068576389\n",
      "Epoch: 5251 Training Loss: 0.16623325941297742 Test Loss: 0.4226749945746528\n",
      "Epoch: 5252 Training Loss: 0.16561889139811198 Test Loss: 0.4231226399739583\n",
      "Epoch: 5253 Training Loss: 0.16527937655978733 Test Loss: 0.42368212890625\n",
      "Epoch: 5254 Training Loss: 0.16520628526475695 Test Loss: 0.4243231336805556\n",
      "Epoch: 5255 Training Loss: 0.16537386406792534 Test Loss: 0.4248005642361111\n",
      "Epoch: 5256 Training Loss: 0.16578624131944444 Test Loss: 0.4255104709201389\n",
      "Epoch: 5257 Training Loss: 0.1663780042860243 Test Loss: 0.42573876953125\n",
      "Epoch: 5258 Training Loss: 0.16712955220540365 Test Loss: 0.4252878689236111\n",
      "Epoch: 5259 Training Loss: 0.16812976413302952 Test Loss: 0.4259198676215278\n",
      "Epoch: 5260 Training Loss: 0.16944150797526042 Test Loss: 0.4281174045138889\n",
      "Epoch: 5261 Training Loss: 0.17114077589246962 Test Loss: 0.43179090711805557\n",
      "Epoch: 5262 Training Loss: 0.17332886250813803 Test Loss: 0.43814534505208336\n",
      "Epoch: 5263 Training Loss: 0.17612523057725696 Test Loss: 0.44652620442708335\n",
      "Epoch: 5264 Training Loss: 0.1794935336642795 Test Loss: 0.45369148763020833\n",
      "Epoch: 5265 Training Loss: 0.18318376837836373 Test Loss: 0.45534722222222224\n",
      "Epoch: 5266 Training Loss: 0.1864775661892361 Test Loss: 0.4466328396267361\n",
      "Epoch: 5267 Training Loss: 0.18777274237738714 Test Loss: 0.4344093967013889\n",
      "Epoch: 5268 Training Loss: 0.18592661878797742 Test Loss: 0.42589371744791665\n",
      "Epoch: 5269 Training Loss: 0.1819611545138889 Test Loss: 0.4219599066840278\n",
      "Epoch: 5270 Training Loss: 0.1778312004937066 Test Loss: 0.42055474175347224\n",
      "Epoch: 5271 Training Loss: 0.17450177680121529 Test Loss: 0.4204287651909722\n",
      "Epoch: 5272 Training Loss: 0.1722183837890625 Test Loss: 0.42213123914930556\n",
      "Epoch: 5273 Training Loss: 0.17074406602647568 Test Loss: 0.42368836805555554\n",
      "Epoch: 5274 Training Loss: 0.1698543752034505 Test Loss: 0.4256846245659722\n",
      "Epoch: 5275 Training Loss: 0.16934325832790798 Test Loss: 0.42717247178819445\n",
      "Epoch: 5276 Training Loss: 0.16903436787923176 Test Loss: 0.42784488932291664\n",
      "Epoch: 5277 Training Loss: 0.16883246188693576 Test Loss: 0.4278546006944444\n",
      "Epoch: 5278 Training Loss: 0.1687082977294922 Test Loss: 0.42798546006944443\n",
      "Epoch: 5279 Training Loss: 0.16861283874511718 Test Loss: 0.4279723307291667\n",
      "Epoch: 5280 Training Loss: 0.1684968753390842 Test Loss: 0.4275814344618056\n",
      "Epoch: 5281 Training Loss: 0.16839259847005209 Test Loss: 0.42771245659722223\n",
      "Epoch: 5282 Training Loss: 0.16827486334906683 Test Loss: 0.428085205078125\n",
      "Epoch: 5283 Training Loss: 0.16818676418728298 Test Loss: 0.4290522732204861\n",
      "Epoch: 5284 Training Loss: 0.16811824544270834 Test Loss: 0.4299342990451389\n",
      "Epoch: 5285 Training Loss: 0.16812022908528645 Test Loss: 0.43106580946180556\n",
      "Epoch: 5286 Training Loss: 0.16821071370442708 Test Loss: 0.43222347005208334\n",
      "Epoch: 5287 Training Loss: 0.16838478257921008 Test Loss: 0.4336505805121528\n",
      "Epoch: 5288 Training Loss: 0.16862958102756076 Test Loss: 0.4345135091145833\n",
      "Epoch: 5289 Training Loss: 0.1689487525092231 Test Loss: 0.4349861924913194\n",
      "Epoch: 5290 Training Loss: 0.16934310065375435 Test Loss: 0.43485218641493056\n",
      "Epoch: 5291 Training Loss: 0.16972822909884983 Test Loss: 0.4340295681423611\n",
      "Epoch: 5292 Training Loss: 0.1700538567437066 Test Loss: 0.43254988606770833\n",
      "Epoch: 5293 Training Loss: 0.17029067145453558 Test Loss: 0.43054625108506944\n",
      "Epoch: 5294 Training Loss: 0.1703990224202474 Test Loss: 0.42844346788194443\n",
      "Epoch: 5295 Training Loss: 0.17041634623209637 Test Loss: 0.4267424045138889\n",
      "Epoch: 5296 Training Loss: 0.17032194349500868 Test Loss: 0.4254211697048611\n",
      "Epoch: 5297 Training Loss: 0.17016613091362848 Test Loss: 0.42476513671875\n",
      "Epoch: 5298 Training Loss: 0.16994737921820746 Test Loss: 0.42466200086805556\n",
      "Epoch: 5299 Training Loss: 0.16971805487738714 Test Loss: 0.42506749131944443\n",
      "Epoch: 5300 Training Loss: 0.16953875562879775 Test Loss: 0.42571625434027777\n",
      "Epoch: 5301 Training Loss: 0.16940882195366752 Test Loss: 0.4264373372395833\n",
      "Epoch: 5302 Training Loss: 0.16938597615559897 Test Loss: 0.42702186414930554\n",
      "Epoch: 5303 Training Loss: 0.16949367099338108 Test Loss: 0.42747271050347224\n",
      "Epoch: 5304 Training Loss: 0.1697510019938151 Test Loss: 0.42661802842881946\n",
      "Epoch: 5305 Training Loss: 0.17018650478786893 Test Loss: 0.42510725911458336\n",
      "Epoch: 5306 Training Loss: 0.1707937706841363 Test Loss: 0.42308447265625\n",
      "Epoch: 5307 Training Loss: 0.17152040269639757 Test Loss: 0.4206571994357639\n",
      "Epoch: 5308 Training Loss: 0.17232735697428386 Test Loss: 0.41868853081597224\n",
      "Epoch: 5309 Training Loss: 0.17294965955946182 Test Loss: 0.417739501953125\n",
      "Epoch: 5310 Training Loss: 0.17333107333713108 Test Loss: 0.41868115234375\n",
      "Epoch: 5311 Training Loss: 0.1734576399061415 Test Loss: 0.42135384114583335\n",
      "Epoch: 5312 Training Loss: 0.17343031480577256 Test Loss: 0.42537483723958336\n",
      "Epoch: 5313 Training Loss: 0.1732745107014974 Test Loss: 0.42755604383680557\n",
      "Epoch: 5314 Training Loss: 0.1728680911593967 Test Loss: 0.4290518663194444\n",
      "Epoch: 5315 Training Loss: 0.17232767401801216 Test Loss: 0.4287565104166667\n",
      "Epoch: 5316 Training Loss: 0.17170887247721353 Test Loss: 0.42823890516493057\n",
      "Epoch: 5317 Training Loss: 0.17081968010796442 Test Loss: 0.42734239366319443\n",
      "Epoch: 5318 Training Loss: 0.16983937242296007 Test Loss: 0.42479733615451387\n",
      "Epoch: 5319 Training Loss: 0.16887026977539063 Test Loss: 0.4235525716145833\n",
      "Epoch: 5320 Training Loss: 0.16816728719075522 Test Loss: 0.4242353515625\n",
      "Epoch: 5321 Training Loss: 0.1676101294623481 Test Loss: 0.42575718858506945\n",
      "Epoch: 5322 Training Loss: 0.16723072814941406 Test Loss: 0.4274304470486111\n",
      "Epoch: 5323 Training Loss: 0.16701193406846787 Test Loss: 0.42821828884548613\n",
      "Epoch: 5324 Training Loss: 0.1670008324517144 Test Loss: 0.42969970703125\n",
      "Epoch: 5325 Training Loss: 0.1672084469265408 Test Loss: 0.4318278537326389\n",
      "Epoch: 5326 Training Loss: 0.16756336296929253 Test Loss: 0.43415736219618056\n",
      "Epoch: 5327 Training Loss: 0.16804835001627605 Test Loss: 0.43623928493923614\n",
      "Epoch: 5328 Training Loss: 0.16865298461914063 Test Loss: 0.437734375\n",
      "Epoch: 5329 Training Loss: 0.16937703789605035 Test Loss: 0.43846356879340276\n",
      "Epoch: 5330 Training Loss: 0.17020143127441406 Test Loss: 0.43834684244791666\n",
      "Epoch: 5331 Training Loss: 0.17107805379231772 Test Loss: 0.43754899088541666\n",
      "Epoch: 5332 Training Loss: 0.17200556945800782 Test Loss: 0.43584879557291667\n",
      "Epoch: 5333 Training Loss: 0.17286394924587672 Test Loss: 0.4334741753472222\n",
      "Epoch: 5334 Training Loss: 0.17346877543131511 Test Loss: 0.4300359700520833\n",
      "Epoch: 5335 Training Loss: 0.17376735093858506 Test Loss: 0.42654212782118056\n",
      "Epoch: 5336 Training Loss: 0.17367760213216146 Test Loss: 0.4241086697048611\n",
      "Epoch: 5337 Training Loss: 0.1732091301812066 Test Loss: 0.42212868923611113\n",
      "Epoch: 5338 Training Loss: 0.1723199259440104 Test Loss: 0.42083534071180556\n",
      "Epoch: 5339 Training Loss: 0.17111979166666666 Test Loss: 0.4195725368923611\n",
      "Epoch: 5340 Training Loss: 0.16975225151909723 Test Loss: 0.41863850911458333\n",
      "Epoch: 5341 Training Loss: 0.16829468282063803 Test Loss: 0.4176646321614583\n",
      "Epoch: 5342 Training Loss: 0.16686092631022134 Test Loss: 0.4167880316840278\n",
      "Epoch: 5343 Training Loss: 0.16550031026204426 Test Loss: 0.41598594835069447\n",
      "Epoch: 5344 Training Loss: 0.1642808363172743 Test Loss: 0.41516924370659725\n",
      "Epoch: 5345 Training Loss: 0.1632180650499132 Test Loss: 0.4146648220486111\n",
      "Epoch: 5346 Training Loss: 0.16230485026041666 Test Loss: 0.4142072211371528\n",
      "Epoch: 5347 Training Loss: 0.16153013271755642 Test Loss: 0.41388465711805555\n",
      "Epoch: 5348 Training Loss: 0.16089086574978298 Test Loss: 0.4137028266059028\n",
      "Epoch: 5349 Training Loss: 0.16035787963867187 Test Loss: 0.4134765625\n",
      "Epoch: 5350 Training Loss: 0.15991551038953994 Test Loss: 0.41340052625868057\n",
      "Epoch: 5351 Training Loss: 0.1595428212483724 Test Loss: 0.41336300998263886\n",
      "Epoch: 5352 Training Loss: 0.15923846944173178 Test Loss: 0.4133102756076389\n",
      "Epoch: 5353 Training Loss: 0.1589890662299262 Test Loss: 0.413310546875\n",
      "Epoch: 5354 Training Loss: 0.15877913750542536 Test Loss: 0.41330362955729166\n",
      "Epoch: 5355 Training Loss: 0.1586004672580295 Test Loss: 0.41337242296006943\n",
      "Epoch: 5356 Training Loss: 0.15844509039984808 Test Loss: 0.4134844563802083\n",
      "Epoch: 5357 Training Loss: 0.15830534023708767 Test Loss: 0.41357826063368053\n",
      "Epoch: 5358 Training Loss: 0.15818536207411024 Test Loss: 0.4137795681423611\n",
      "Epoch: 5359 Training Loss: 0.158080813937717 Test Loss: 0.41383946397569443\n",
      "Epoch: 5360 Training Loss: 0.15798881700303818 Test Loss: 0.414032958984375\n",
      "Epoch: 5361 Training Loss: 0.15790646023220486 Test Loss: 0.41418888346354166\n",
      "Epoch: 5362 Training Loss: 0.15783184305826822 Test Loss: 0.4143176540798611\n",
      "Epoch: 5363 Training Loss: 0.1577618645562066 Test Loss: 0.41447710503472224\n",
      "Epoch: 5364 Training Loss: 0.15769844394259983 Test Loss: 0.41465706380208334\n",
      "Epoch: 5365 Training Loss: 0.15763526407877604 Test Loss: 0.41482077365451386\n",
      "Epoch: 5366 Training Loss: 0.1575843065049913 Test Loss: 0.41506865776909724\n",
      "Epoch: 5367 Training Loss: 0.15753323533799912 Test Loss: 0.41526416015625\n",
      "Epoch: 5368 Training Loss: 0.15749183654785157 Test Loss: 0.41553379991319445\n",
      "Epoch: 5369 Training Loss: 0.15745348273383247 Test Loss: 0.41572569444444446\n",
      "Epoch: 5370 Training Loss: 0.15741571892632378 Test Loss: 0.416044921875\n",
      "Epoch: 5371 Training Loss: 0.15737352667914498 Test Loss: 0.41627867296006943\n",
      "Epoch: 5372 Training Loss: 0.15734161716037326 Test Loss: 0.41646864149305557\n",
      "Epoch: 5373 Training Loss: 0.15731240844726563 Test Loss: 0.41675130208333333\n",
      "Epoch: 5374 Training Loss: 0.1572870093451606 Test Loss: 0.4170519748263889\n",
      "Epoch: 5375 Training Loss: 0.15726161363389757 Test Loss: 0.4173787434895833\n",
      "Epoch: 5376 Training Loss: 0.1572367706298828 Test Loss: 0.41768229166666665\n",
      "Epoch: 5377 Training Loss: 0.1572206573486328 Test Loss: 0.4179142523871528\n",
      "Epoch: 5378 Training Loss: 0.15719887288411458 Test Loss: 0.41818473307291665\n",
      "Epoch: 5379 Training Loss: 0.15719220309787327 Test Loss: 0.4185004069010417\n",
      "Epoch: 5380 Training Loss: 0.15718233066134982 Test Loss: 0.41890983072916665\n",
      "Epoch: 5381 Training Loss: 0.15717381117078993 Test Loss: 0.41921858723958333\n",
      "Epoch: 5382 Training Loss: 0.15716779242621529 Test Loss: 0.41964208984375\n",
      "Epoch: 5383 Training Loss: 0.15715700954861112 Test Loss: 0.4199273003472222\n",
      "Epoch: 5384 Training Loss: 0.15715528869628906 Test Loss: 0.42020713975694446\n",
      "Epoch: 5385 Training Loss: 0.1571488071017795 Test Loss: 0.42045594618055554\n",
      "Epoch: 5386 Training Loss: 0.15715092298719618 Test Loss: 0.4207428927951389\n",
      "Epoch: 5387 Training Loss: 0.1571423814561632 Test Loss: 0.4208754340277778\n",
      "Epoch: 5388 Training Loss: 0.15714317830403646 Test Loss: 0.42111143663194445\n",
      "Epoch: 5389 Training Loss: 0.15713941277398003 Test Loss: 0.42125105794270834\n",
      "Epoch: 5390 Training Loss: 0.1571480255126953 Test Loss: 0.42139412434895834\n",
      "Epoch: 5391 Training Loss: 0.1571553005642361 Test Loss: 0.42154286024305554\n",
      "Epoch: 5392 Training Loss: 0.15718465338812934 Test Loss: 0.42172374131944446\n",
      "Epoch: 5393 Training Loss: 0.1572151845296224 Test Loss: 0.4217745225694444\n",
      "Epoch: 5394 Training Loss: 0.15725933159722222 Test Loss: 0.42199083116319447\n",
      "Epoch: 5395 Training Loss: 0.15730395846896703 Test Loss: 0.4221830783420139\n",
      "Epoch: 5396 Training Loss: 0.15736749437120226 Test Loss: 0.4224861111111111\n",
      "Epoch: 5397 Training Loss: 0.15742015075683594 Test Loss: 0.4227752278645833\n",
      "Epoch: 5398 Training Loss: 0.15747964477539061 Test Loss: 0.42311827256944445\n",
      "Epoch: 5399 Training Loss: 0.15755177307128906 Test Loss: 0.42342420789930557\n",
      "Epoch: 5400 Training Loss: 0.157625486585829 Test Loss: 0.4237330729166667\n",
      "Epoch: 5401 Training Loss: 0.15771205647786457 Test Loss: 0.424132080078125\n",
      "Epoch: 5402 Training Loss: 0.1577970954047309 Test Loss: 0.42446687825520835\n",
      "Epoch: 5403 Training Loss: 0.15788705105251735 Test Loss: 0.4247980685763889\n",
      "Epoch: 5404 Training Loss: 0.15798141479492187 Test Loss: 0.42493077256944445\n",
      "Epoch: 5405 Training Loss: 0.15809467909071181 Test Loss: 0.42522601996527776\n",
      "Epoch: 5406 Training Loss: 0.15820833672417534 Test Loss: 0.4253997395833333\n",
      "Epoch: 5407 Training Loss: 0.15833678351508246 Test Loss: 0.4256633572048611\n",
      "Epoch: 5408 Training Loss: 0.15846733601888022 Test Loss: 0.42579155815972225\n",
      "Epoch: 5409 Training Loss: 0.15861097378200956 Test Loss: 0.42585712348090277\n",
      "Epoch: 5410 Training Loss: 0.1587683529324002 Test Loss: 0.42603504774305556\n",
      "Epoch: 5411 Training Loss: 0.15892366197374133 Test Loss: 0.4260068088107639\n",
      "Epoch: 5412 Training Loss: 0.15909791564941406 Test Loss: 0.4260188530815972\n",
      "Epoch: 5413 Training Loss: 0.15929372999403213 Test Loss: 0.4259198676215278\n",
      "Epoch: 5414 Training Loss: 0.15951378377278647 Test Loss: 0.4257314181857639\n",
      "Epoch: 5415 Training Loss: 0.15979405890570747 Test Loss: 0.42573583984375\n",
      "Epoch: 5416 Training Loss: 0.16011485968695746 Test Loss: 0.42590294053819444\n",
      "Epoch: 5417 Training Loss: 0.16049537489149304 Test Loss: 0.4258914116753472\n",
      "Epoch: 5418 Training Loss: 0.16092840067545572 Test Loss: 0.42557817925347224\n",
      "Epoch: 5419 Training Loss: 0.16145241122775608 Test Loss: 0.42490155707465277\n",
      "Epoch: 5420 Training Loss: 0.1620098843044705 Test Loss: 0.42549815538194447\n",
      "Epoch: 5421 Training Loss: 0.1626939934624566 Test Loss: 0.42749007161458336\n",
      "Epoch: 5422 Training Loss: 0.16360076395670572 Test Loss: 0.4303680555555556\n",
      "Epoch: 5423 Training Loss: 0.16483753289116754 Test Loss: 0.43487000868055553\n",
      "Epoch: 5424 Training Loss: 0.16651417371961805 Test Loss: 0.4384826931423611\n",
      "Epoch: 5425 Training Loss: 0.16872974650065103 Test Loss: 0.4301861979166667\n",
      "Epoch: 5426 Training Loss: 0.17039060635036893 Test Loss: 0.42308878580729165\n",
      "Epoch: 5427 Training Loss: 0.17073013814290364 Test Loss: 0.44537272135416667\n",
      "Epoch: 5428 Training Loss: 0.1701609141031901 Test Loss: 0.4400448676215278\n",
      "Epoch: 5429 Training Loss: 0.16919956970214844 Test Loss: 0.43294816080729165\n",
      "Epoch: 5430 Training Loss: 0.1685810309516059 Test Loss: 0.432767333984375\n",
      "Epoch: 5431 Training Loss: 0.16824109903971354 Test Loss: 0.4286875542534722\n",
      "Epoch: 5432 Training Loss: 0.16786856248643664 Test Loss: 0.42553575303819446\n",
      "Epoch: 5433 Training Loss: 0.16726423984103733 Test Loss: 0.42414599609375\n",
      "Epoch: 5434 Training Loss: 0.1664080115424262 Test Loss: 0.42282212999131946\n",
      "Epoch: 5435 Training Loss: 0.16537278578016493 Test Loss: 0.4211313747829861\n",
      "Epoch: 5436 Training Loss: 0.1643015441894531 Test Loss: 0.4194613986545139\n",
      "Epoch: 5437 Training Loss: 0.16328116353352864 Test Loss: 0.41807996961805555\n",
      "Epoch: 5438 Training Loss: 0.1623634067111545 Test Loss: 0.41729020182291665\n",
      "Epoch: 5439 Training Loss: 0.1616036088731554 Test Loss: 0.4174021267361111\n",
      "Epoch: 5440 Training Loss: 0.1609976077609592 Test Loss: 0.41786067708333335\n",
      "Epoch: 5441 Training Loss: 0.1605315619574653 Test Loss: 0.4185247938368056\n",
      "Epoch: 5442 Training Loss: 0.1601902058919271 Test Loss: 0.4191933051215278\n",
      "Epoch: 5443 Training Loss: 0.15995781962076822 Test Loss: 0.4198921440972222\n",
      "Epoch: 5444 Training Loss: 0.1597925533718533 Test Loss: 0.4204312065972222\n",
      "Epoch: 5445 Training Loss: 0.15970255364312066 Test Loss: 0.4209313422309028\n",
      "Epoch: 5446 Training Loss: 0.15966545952690972 Test Loss: 0.42141582573784725\n",
      "Epoch: 5447 Training Loss: 0.159681644015842 Test Loss: 0.4219002821180556\n",
      "Epoch: 5448 Training Loss: 0.1597326202392578 Test Loss: 0.42201326497395836\n",
      "Epoch: 5449 Training Loss: 0.15979798210991752 Test Loss: 0.42205753580729166\n",
      "Epoch: 5450 Training Loss: 0.15987728712293836 Test Loss: 0.42195621744791667\n",
      "Epoch: 5451 Training Loss: 0.1599496053059896 Test Loss: 0.4217112358940972\n",
      "Epoch: 5452 Training Loss: 0.16001852586534288 Test Loss: 0.42160267469618057\n",
      "Epoch: 5453 Training Loss: 0.16007396613226996 Test Loss: 0.42120893012152777\n",
      "Epoch: 5454 Training Loss: 0.16011665344238282 Test Loss: 0.42069835069444445\n",
      "Epoch: 5455 Training Loss: 0.16015431213378906 Test Loss: 0.420067138671875\n",
      "Epoch: 5456 Training Loss: 0.16019011603461372 Test Loss: 0.41972596571180554\n",
      "Epoch: 5457 Training Loss: 0.16022750515407985 Test Loss: 0.41927728949652776\n",
      "Epoch: 5458 Training Loss: 0.1602723117404514 Test Loss: 0.419356689453125\n",
      "Epoch: 5459 Training Loss: 0.16032494099934896 Test Loss: 0.41948592122395834\n",
      "Epoch: 5460 Training Loss: 0.16038651869032117 Test Loss: 0.4199696994357639\n",
      "Epoch: 5461 Training Loss: 0.16045029873318142 Test Loss: 0.420633544921875\n",
      "Epoch: 5462 Training Loss: 0.1605168219672309 Test Loss: 0.42153724500868056\n",
      "Epoch: 5463 Training Loss: 0.16057294379340278 Test Loss: 0.4221814778645833\n",
      "Epoch: 5464 Training Loss: 0.16062042744954427 Test Loss: 0.4229352484809028\n",
      "Epoch: 5465 Training Loss: 0.16065474446614583 Test Loss: 0.4242363552517361\n",
      "Epoch: 5466 Training Loss: 0.16068386501736112 Test Loss: 0.42513807508680557\n",
      "Epoch: 5467 Training Loss: 0.16071443515353734 Test Loss: 0.42590424262152776\n",
      "Epoch: 5468 Training Loss: 0.1607580040825738 Test Loss: 0.42646142578125\n",
      "Epoch: 5469 Training Loss: 0.1608026140001085 Test Loss: 0.4267548285590278\n",
      "Epoch: 5470 Training Loss: 0.16083380805121528 Test Loss: 0.42682834201388886\n",
      "Epoch: 5471 Training Loss: 0.16087800937228733 Test Loss: 0.42656787109375\n",
      "Epoch: 5472 Training Loss: 0.16092626274956598 Test Loss: 0.425857666015625\n",
      "Epoch: 5473 Training Loss: 0.16097534688313803 Test Loss: 0.4248148600260417\n",
      "Epoch: 5474 Training Loss: 0.16102321031358507 Test Loss: 0.42376896158854166\n",
      "Epoch: 5475 Training Loss: 0.16106767272949218 Test Loss: 0.422643798828125\n",
      "Epoch: 5476 Training Loss: 0.16111047871907552 Test Loss: 0.42167545572916665\n",
      "Epoch: 5477 Training Loss: 0.1611573978000217 Test Loss: 0.4207769639756944\n",
      "Epoch: 5478 Training Loss: 0.16120694986979167 Test Loss: 0.41999446614583336\n",
      "Epoch: 5479 Training Loss: 0.16124063958062065 Test Loss: 0.41946270073784725\n",
      "Epoch: 5480 Training Loss: 0.16127994113498265 Test Loss: 0.4192297634548611\n",
      "Epoch: 5481 Training Loss: 0.16131900533040364 Test Loss: 0.41900238715277777\n",
      "Epoch: 5482 Training Loss: 0.16135958184136284 Test Loss: 0.41870893012152777\n",
      "Epoch: 5483 Training Loss: 0.1614187232123481 Test Loss: 0.4182616102430556\n",
      "Epoch: 5484 Training Loss: 0.1614986572265625 Test Loss: 0.41773600260416666\n",
      "Epoch: 5485 Training Loss: 0.16159925503200956 Test Loss: 0.41703374565972223\n",
      "Epoch: 5486 Training Loss: 0.1617146775987413 Test Loss: 0.41643663194444447\n",
      "Epoch: 5487 Training Loss: 0.1618472408718533 Test Loss: 0.4158141818576389\n",
      "Epoch: 5488 Training Loss: 0.16199734157986112 Test Loss: 0.41534282769097225\n",
      "Epoch: 5489 Training Loss: 0.16214638943142362 Test Loss: 0.41511729600694447\n",
      "Epoch: 5490 Training Loss: 0.16228229098849825 Test Loss: 0.41532381184895834\n",
      "Epoch: 5491 Training Loss: 0.16239558580186633 Test Loss: 0.416009765625\n",
      "Epoch: 5492 Training Loss: 0.16248067220052084 Test Loss: 0.41684700520833334\n",
      "Epoch: 5493 Training Loss: 0.16256154208713108 Test Loss: 0.4182273220486111\n",
      "Epoch: 5494 Training Loss: 0.16262369452582465 Test Loss: 0.419486572265625\n",
      "Epoch: 5495 Training Loss: 0.16268426005045572 Test Loss: 0.4207868381076389\n",
      "Epoch: 5496 Training Loss: 0.16272316826714409 Test Loss: 0.42182855902777777\n",
      "Epoch: 5497 Training Loss: 0.1627373979356554 Test Loss: 0.4223520236545139\n",
      "Epoch: 5498 Training Loss: 0.16272911071777343 Test Loss: 0.42308311631944445\n",
      "Epoch: 5499 Training Loss: 0.16271347893608942 Test Loss: 0.42384825303819446\n",
      "Epoch: 5500 Training Loss: 0.16267452155219184 Test Loss: 0.42458699544270834\n",
      "Epoch: 5501 Training Loss: 0.16262603420681424 Test Loss: 0.42520686848958333\n",
      "Epoch: 5502 Training Loss: 0.1625766381157769 Test Loss: 0.42573046875\n",
      "Epoch: 5503 Training Loss: 0.16251011318630643 Test Loss: 0.4263965386284722\n",
      "Epoch: 5504 Training Loss: 0.16243391757541234 Test Loss: 0.4273214789496528\n",
      "Epoch: 5505 Training Loss: 0.16235125054253471 Test Loss: 0.4280475802951389\n",
      "Epoch: 5506 Training Loss: 0.1622866397433811 Test Loss: 0.42864586046006947\n",
      "Epoch: 5507 Training Loss: 0.1622366231282552 Test Loss: 0.42946663411458336\n",
      "Epoch: 5508 Training Loss: 0.16222188313802083 Test Loss: 0.4300259060329861\n",
      "Epoch: 5509 Training Loss: 0.16220335727267796 Test Loss: 0.43042632378472223\n",
      "Epoch: 5510 Training Loss: 0.16221464029947916 Test Loss: 0.43085302734375\n",
      "Epoch: 5511 Training Loss: 0.16224941168891058 Test Loss: 0.43085422092013886\n",
      "Epoch: 5512 Training Loss: 0.16227891370985242 Test Loss: 0.4306661512586806\n",
      "Epoch: 5513 Training Loss: 0.16230959065755207 Test Loss: 0.43022810872395834\n",
      "Epoch: 5514 Training Loss: 0.16234198167588976 Test Loss: 0.42930002170138887\n",
      "Epoch: 5515 Training Loss: 0.1623866458468967 Test Loss: 0.42832459852430554\n",
      "Epoch: 5516 Training Loss: 0.16244097900390625 Test Loss: 0.42722398546006946\n",
      "Epoch: 5517 Training Loss: 0.1624975077311198 Test Loss: 0.42607177734375\n",
      "Epoch: 5518 Training Loss: 0.16254729207356772 Test Loss: 0.4246292588975694\n",
      "Epoch: 5519 Training Loss: 0.16258044772677951 Test Loss: 0.42329912651909724\n",
      "Epoch: 5520 Training Loss: 0.16259206983778213 Test Loss: 0.42213563368055557\n",
      "Epoch: 5521 Training Loss: 0.16256573994954426 Test Loss: 0.4209319390190972\n",
      "Epoch: 5522 Training Loss: 0.16250816853841146 Test Loss: 0.42015681966145835\n",
      "Epoch: 5523 Training Loss: 0.16239421759711373 Test Loss: 0.4194490559895833\n",
      "Epoch: 5524 Training Loss: 0.1622276628282335 Test Loss: 0.4189551866319444\n",
      "Epoch: 5525 Training Loss: 0.16201704576280382 Test Loss: 0.41870079210069444\n",
      "Epoch: 5526 Training Loss: 0.16177988857693143 Test Loss: 0.41883368598090276\n",
      "Epoch: 5527 Training Loss: 0.16151449584960936 Test Loss: 0.4190396592881944\n",
      "Epoch: 5528 Training Loss: 0.16123209126790364 Test Loss: 0.41956765407986113\n",
      "Epoch: 5529 Training Loss: 0.16091518825954862 Test Loss: 0.4203004014756944\n",
      "Epoch: 5530 Training Loss: 0.1605872056749132 Test Loss: 0.42102886284722224\n",
      "Epoch: 5531 Training Loss: 0.16028815205891928 Test Loss: 0.42152815755208334\n",
      "Epoch: 5532 Training Loss: 0.16000601196289063 Test Loss: 0.421876708984375\n",
      "Epoch: 5533 Training Loss: 0.15973395284016928 Test Loss: 0.42237022569444443\n",
      "Epoch: 5534 Training Loss: 0.15948209635416666 Test Loss: 0.4226365017361111\n",
      "Epoch: 5535 Training Loss: 0.1592412634955512 Test Loss: 0.42265706380208334\n",
      "Epoch: 5536 Training Loss: 0.15899454752604167 Test Loss: 0.4224943576388889\n",
      "Epoch: 5537 Training Loss: 0.15876833089192707 Test Loss: 0.42213340928819443\n",
      "Epoch: 5538 Training Loss: 0.15856332227918837 Test Loss: 0.4218749457465278\n",
      "Epoch: 5539 Training Loss: 0.15837014770507812 Test Loss: 0.421577392578125\n",
      "Epoch: 5540 Training Loss: 0.1581661868625217 Test Loss: 0.42125569661458334\n",
      "Epoch: 5541 Training Loss: 0.15796640184190539 Test Loss: 0.4210296766493056\n",
      "Epoch: 5542 Training Loss: 0.15778219265407986 Test Loss: 0.42078108723958335\n",
      "Epoch: 5543 Training Loss: 0.15760503980848525 Test Loss: 0.4205378689236111\n",
      "Epoch: 5544 Training Loss: 0.1574524620903863 Test Loss: 0.42030555555555554\n",
      "Epoch: 5545 Training Loss: 0.1573151601155599 Test Loss: 0.42016324869791666\n",
      "Epoch: 5546 Training Loss: 0.15719300503200956 Test Loss: 0.4201193576388889\n",
      "Epoch: 5547 Training Loss: 0.15709304979112412 Test Loss: 0.4199546440972222\n",
      "Epoch: 5548 Training Loss: 0.1570048082139757 Test Loss: 0.41991696506076387\n",
      "Epoch: 5549 Training Loss: 0.15693578084309895 Test Loss: 0.41993929036458333\n",
      "Epoch: 5550 Training Loss: 0.15687034606933595 Test Loss: 0.4198251681857639\n",
      "Epoch: 5551 Training Loss: 0.15681498040093317 Test Loss: 0.41979204644097223\n",
      "Epoch: 5552 Training Loss: 0.15676871405707465 Test Loss: 0.4198200412326389\n",
      "Epoch: 5553 Training Loss: 0.1567229224310981 Test Loss: 0.41981016710069446\n",
      "Epoch: 5554 Training Loss: 0.1567043999565972 Test Loss: 0.4199686957465278\n",
      "Epoch: 5555 Training Loss: 0.1566998087565104 Test Loss: 0.4200934787326389\n",
      "Epoch: 5556 Training Loss: 0.15670607164171008 Test Loss: 0.4203276909722222\n",
      "Epoch: 5557 Training Loss: 0.15671849738226998 Test Loss: 0.42044737413194444\n",
      "Epoch: 5558 Training Loss: 0.1567326151529948 Test Loss: 0.42059488932291667\n",
      "Epoch: 5559 Training Loss: 0.15675991651746962 Test Loss: 0.4207322048611111\n",
      "Epoch: 5560 Training Loss: 0.15679646301269531 Test Loss: 0.42096815321180553\n",
      "Epoch: 5561 Training Loss: 0.15684715270996094 Test Loss: 0.42144189453125\n",
      "Epoch: 5562 Training Loss: 0.15690384419759115 Test Loss: 0.4219426812065972\n",
      "Epoch: 5563 Training Loss: 0.15698536003960503 Test Loss: 0.4223980305989583\n",
      "Epoch: 5564 Training Loss: 0.1570715806749132 Test Loss: 0.42302726236979166\n",
      "Epoch: 5565 Training Loss: 0.15717454189724392 Test Loss: 0.4235527072482639\n",
      "Epoch: 5566 Training Loss: 0.1572752193874783 Test Loss: 0.42402870008680554\n",
      "Epoch: 5567 Training Loss: 0.1573785925971137 Test Loss: 0.42451310221354166\n",
      "Epoch: 5568 Training Loss: 0.15750752427842882 Test Loss: 0.4248750542534722\n",
      "Epoch: 5569 Training Loss: 0.15763272942437065 Test Loss: 0.4254768608940972\n",
      "Epoch: 5570 Training Loss: 0.1577844746907552 Test Loss: 0.4260202094184028\n",
      "Epoch: 5571 Training Loss: 0.15795072428385418 Test Loss: 0.42648719618055553\n",
      "Epoch: 5572 Training Loss: 0.15812805853949652 Test Loss: 0.4269758843315972\n",
      "Epoch: 5573 Training Loss: 0.15830887518988715 Test Loss: 0.4273147243923611\n",
      "Epoch: 5574 Training Loss: 0.1585183868408203 Test Loss: 0.4278747287326389\n",
      "Epoch: 5575 Training Loss: 0.15873878818088108 Test Loss: 0.42843451605902777\n",
      "Epoch: 5576 Training Loss: 0.1589655999077691 Test Loss: 0.4286626247829861\n",
      "Epoch: 5577 Training Loss: 0.15920368109809027 Test Loss: 0.42886528862847223\n",
      "Epoch: 5578 Training Loss: 0.15943883260091146 Test Loss: 0.4293914930555556\n",
      "Epoch: 5579 Training Loss: 0.15970882331000433 Test Loss: 0.4299755859375\n",
      "Epoch: 5580 Training Loss: 0.15998919508192275 Test Loss: 0.43059619140625\n",
      "Epoch: 5581 Training Loss: 0.16029082573784723 Test Loss: 0.4311335991753472\n",
      "Epoch: 5582 Training Loss: 0.16058379618326823 Test Loss: 0.4316997341579861\n",
      "Epoch: 5583 Training Loss: 0.16087851969401043 Test Loss: 0.4326623263888889\n",
      "Epoch: 5584 Training Loss: 0.16120543077256944 Test Loss: 0.4339191623263889\n",
      "Epoch: 5585 Training Loss: 0.16153914388020835 Test Loss: 0.4341950954861111\n",
      "Epoch: 5586 Training Loss: 0.16184142388237846 Test Loss: 0.43439360894097223\n",
      "Epoch: 5587 Training Loss: 0.1621547139485677 Test Loss: 0.434859375\n",
      "Epoch: 5588 Training Loss: 0.16251262919108073 Test Loss: 0.4357388509114583\n",
      "Epoch: 5589 Training Loss: 0.1628952111138238 Test Loss: 0.4366357964409722\n",
      "Epoch: 5590 Training Loss: 0.163215823703342 Test Loss: 0.4375652126736111\n",
      "Epoch: 5591 Training Loss: 0.16351712036132812 Test Loss: 0.4381635199652778\n",
      "Epoch: 5592 Training Loss: 0.16379413011338975 Test Loss: 0.43793524848090276\n",
      "Epoch: 5593 Training Loss: 0.16402969360351563 Test Loss: 0.4358129069010417\n",
      "Epoch: 5594 Training Loss: 0.16425768364800347 Test Loss: 0.43368522135416665\n",
      "Epoch: 5595 Training Loss: 0.16438841756184897 Test Loss: 0.43247960069444447\n",
      "Epoch: 5596 Training Loss: 0.1644900919596354 Test Loss: 0.4325635850694444\n",
      "Epoch: 5597 Training Loss: 0.16450572204589844 Test Loss: 0.43343934461805556\n",
      "Epoch: 5598 Training Loss: 0.16449068196614583 Test Loss: 0.4312656792534722\n",
      "Epoch: 5599 Training Loss: 0.1643709259033203 Test Loss: 0.42818986002604165\n",
      "Epoch: 5600 Training Loss: 0.1640674811469184 Test Loss: 0.4281870388454861\n",
      "Epoch: 5601 Training Loss: 0.16366971164279515 Test Loss: 0.43033273654513887\n",
      "Epoch: 5602 Training Loss: 0.1632627207438151 Test Loss: 0.43359998914930553\n",
      "Epoch: 5603 Training Loss: 0.16285241021050348 Test Loss: 0.4325847710503472\n",
      "Epoch: 5604 Training Loss: 0.16223392740885417 Test Loss: 0.4290075954861111\n",
      "Epoch: 5605 Training Loss: 0.16158167182074654 Test Loss: 0.4259453125\n",
      "Epoch: 5606 Training Loss: 0.16105460272894964 Test Loss: 0.4239971245659722\n",
      "Epoch: 5607 Training Loss: 0.16066111585828993 Test Loss: 0.42322279188368056\n",
      "Epoch: 5608 Training Loss: 0.1604107920328776 Test Loss: 0.4232847493489583\n",
      "Epoch: 5609 Training Loss: 0.16025350613064235 Test Loss: 0.4234089084201389\n",
      "Epoch: 5610 Training Loss: 0.16015662807888456 Test Loss: 0.42371286349826387\n",
      "Epoch: 5611 Training Loss: 0.16011892700195313 Test Loss: 0.42380479600694443\n",
      "Epoch: 5612 Training Loss: 0.1601159430609809 Test Loss: 0.42372303602430555\n",
      "Epoch: 5613 Training Loss: 0.16014703030056424 Test Loss: 0.42350775824652775\n",
      "Epoch: 5614 Training Loss: 0.16021378411187065 Test Loss: 0.42323494466145833\n",
      "Epoch: 5615 Training Loss: 0.16030855645073785 Test Loss: 0.4225965983072917\n",
      "Epoch: 5616 Training Loss: 0.1604394310845269 Test Loss: 0.42184998914930555\n",
      "Epoch: 5617 Training Loss: 0.16058896721733942 Test Loss: 0.4209950900607639\n",
      "Epoch: 5618 Training Loss: 0.1607647213406033 Test Loss: 0.42026136610243053\n",
      "Epoch: 5619 Training Loss: 0.16097579277886284 Test Loss: 0.41979052734375\n",
      "Epoch: 5620 Training Loss: 0.1611934102376302 Test Loss: 0.41934426540798614\n",
      "Epoch: 5621 Training Loss: 0.1614095696343316 Test Loss: 0.4189988335503472\n",
      "Epoch: 5622 Training Loss: 0.16163407389322917 Test Loss: 0.4187754177517361\n",
      "Epoch: 5623 Training Loss: 0.16186760965983074 Test Loss: 0.4187041558159722\n",
      "Epoch: 5624 Training Loss: 0.16214769490559897 Test Loss: 0.41851958550347224\n",
      "Epoch: 5625 Training Loss: 0.16246509806315104 Test Loss: 0.4183925509982639\n",
      "Epoch: 5626 Training Loss: 0.16282433064778645 Test Loss: 0.4182448459201389\n",
      "Epoch: 5627 Training Loss: 0.16325602383083768 Test Loss: 0.4184364420572917\n",
      "Epoch: 5628 Training Loss: 0.16376221720377604 Test Loss: 0.4188955620659722\n",
      "Epoch: 5629 Training Loss: 0.16437249586317274 Test Loss: 0.4192862413194444\n",
      "Epoch: 5630 Training Loss: 0.1651117163764106 Test Loss: 0.41996169704861114\n",
      "Epoch: 5631 Training Loss: 0.1659763963487413 Test Loss: 0.4207781032986111\n",
      "Epoch: 5632 Training Loss: 0.16699073282877605 Test Loss: 0.4220505913628472\n",
      "Epoch: 5633 Training Loss: 0.16815691460503474 Test Loss: 0.4242548285590278\n",
      "Epoch: 5634 Training Loss: 0.16942717997233073 Test Loss: 0.42700059678819446\n",
      "Epoch: 5635 Training Loss: 0.17072264607747395 Test Loss: 0.4306120334201389\n",
      "Epoch: 5636 Training Loss: 0.17193394809299045 Test Loss: 0.4338033854166667\n",
      "Epoch: 5637 Training Loss: 0.17282623291015625 Test Loss: 0.43548396809895834\n",
      "Epoch: 5638 Training Loss: 0.1732258589002821 Test Loss: 0.4349939778645833\n",
      "Epoch: 5639 Training Loss: 0.17295354885525174 Test Loss: 0.43260191514756946\n",
      "Epoch: 5640 Training Loss: 0.17194105529785156 Test Loss: 0.43011577690972225\n",
      "Epoch: 5641 Training Loss: 0.17035914950900607 Test Loss: 0.4282046169704861\n",
      "Epoch: 5642 Training Loss: 0.16853101603190104 Test Loss: 0.42649869791666667\n",
      "Epoch: 5643 Training Loss: 0.16673882717556424 Test Loss: 0.42515361870659724\n",
      "Epoch: 5644 Training Loss: 0.16515355088975694 Test Loss: 0.42390321180555557\n",
      "Epoch: 5645 Training Loss: 0.16382291327582466 Test Loss: 0.42339491102430554\n",
      "Epoch: 5646 Training Loss: 0.16273507012261285 Test Loss: 0.4231621365017361\n",
      "Epoch: 5647 Training Loss: 0.16188014221191407 Test Loss: 0.423357177734375\n",
      "Epoch: 5648 Training Loss: 0.16122095913357204 Test Loss: 0.4235920681423611\n",
      "Epoch: 5649 Training Loss: 0.1607417721218533 Test Loss: 0.4239192437065972\n",
      "Epoch: 5650 Training Loss: 0.16041695319281685 Test Loss: 0.4242339409722222\n",
      "Epoch: 5651 Training Loss: 0.16021317545572916 Test Loss: 0.42461566840277776\n",
      "Epoch: 5652 Training Loss: 0.16011799791124132 Test Loss: 0.425119873046875\n",
      "Epoch: 5653 Training Loss: 0.16009955173068577 Test Loss: 0.42554866536458336\n",
      "Epoch: 5654 Training Loss: 0.16013625420464409 Test Loss: 0.42568229166666666\n",
      "Epoch: 5655 Training Loss: 0.16022078450520832 Test Loss: 0.425991455078125\n",
      "Epoch: 5656 Training Loss: 0.16033650207519531 Test Loss: 0.4261650390625\n",
      "Epoch: 5657 Training Loss: 0.16047452629937067 Test Loss: 0.4263303493923611\n",
      "Epoch: 5658 Training Loss: 0.1606380394829644 Test Loss: 0.42630821397569446\n",
      "Epoch: 5659 Training Loss: 0.16081744045681423 Test Loss: 0.42624598524305557\n",
      "Epoch: 5660 Training Loss: 0.16097359212239584 Test Loss: 0.42591780598958334\n",
      "Epoch: 5661 Training Loss: 0.16115204196506078 Test Loss: 0.42585603841145836\n",
      "Epoch: 5662 Training Loss: 0.16135813903808593 Test Loss: 0.4259342990451389\n",
      "Epoch: 5663 Training Loss: 0.1615845489501953 Test Loss: 0.42590806749131943\n",
      "Epoch: 5664 Training Loss: 0.161812742445204 Test Loss: 0.4262121310763889\n",
      "Epoch: 5665 Training Loss: 0.16204323154025607 Test Loss: 0.42665361870659724\n",
      "Epoch: 5666 Training Loss: 0.16229565938313803 Test Loss: 0.42739556206597223\n",
      "Epoch: 5667 Training Loss: 0.16258817884657117 Test Loss: 0.42821525065104166\n",
      "Epoch: 5668 Training Loss: 0.16292400614420574 Test Loss: 0.42938026258680556\n",
      "Epoch: 5669 Training Loss: 0.16334920586480034 Test Loss: 0.43056627061631947\n",
      "Epoch: 5670 Training Loss: 0.16385446166992187 Test Loss: 0.4320264214409722\n",
      "Epoch: 5671 Training Loss: 0.1644959225124783 Test Loss: 0.43408265516493055\n",
      "Epoch: 5672 Training Loss: 0.1653088107638889 Test Loss: 0.4363361002604167\n",
      "Epoch: 5673 Training Loss: 0.16629105631510416 Test Loss: 0.43894775390625\n",
      "Epoch: 5674 Training Loss: 0.16745543585883246 Test Loss: 0.44119230143229166\n",
      "Epoch: 5675 Training Loss: 0.16873162163628472 Test Loss: 0.4413543023003472\n",
      "Epoch: 5676 Training Loss: 0.17000347561306423 Test Loss: 0.43916118706597224\n",
      "Epoch: 5677 Training Loss: 0.17109219699435763 Test Loss: 0.43452650282118055\n",
      "Epoch: 5678 Training Loss: 0.171739496866862 Test Loss: 0.42950987413194447\n",
      "Epoch: 5679 Training Loss: 0.17171429612901476 Test Loss: 0.42450263129340277\n",
      "Epoch: 5680 Training Loss: 0.17100454542371962 Test Loss: 0.42036317274305557\n",
      "Epoch: 5681 Training Loss: 0.1696601376003689 Test Loss: 0.4175310872395833\n",
      "Epoch: 5682 Training Loss: 0.16794197421603732 Test Loss: 0.4153143174913194\n",
      "Epoch: 5683 Training Loss: 0.16603328111436633 Test Loss: 0.41377601453993057\n",
      "Epoch: 5684 Training Loss: 0.16417071702745226 Test Loss: 0.4122437065972222\n",
      "Epoch: 5685 Training Loss: 0.16247089301215278 Test Loss: 0.41123046875\n",
      "Epoch: 5686 Training Loss: 0.16100811937120227 Test Loss: 0.41038983832465276\n",
      "Epoch: 5687 Training Loss: 0.15978204345703126 Test Loss: 0.40991661241319444\n",
      "Epoch: 5688 Training Loss: 0.15876183064778646 Test Loss: 0.40955485026041666\n",
      "Epoch: 5689 Training Loss: 0.15791475762261284 Test Loss: 0.4092350802951389\n",
      "Epoch: 5690 Training Loss: 0.15721693589952257 Test Loss: 0.40927940538194446\n",
      "Epoch: 5691 Training Loss: 0.15663616265190972 Test Loss: 0.4092488606770833\n",
      "Epoch: 5692 Training Loss: 0.15615462239583333 Test Loss: 0.4093294270833333\n",
      "Epoch: 5693 Training Loss: 0.1557374742296007 Test Loss: 0.4092985297309028\n",
      "Epoch: 5694 Training Loss: 0.15537199910481772 Test Loss: 0.4093741590711806\n",
      "Epoch: 5695 Training Loss: 0.15505563354492188 Test Loss: 0.40939946831597224\n",
      "Epoch: 5696 Training Loss: 0.1547824249267578 Test Loss: 0.40950206163194447\n",
      "Epoch: 5697 Training Loss: 0.154539554172092 Test Loss: 0.40960107421875\n",
      "Epoch: 5698 Training Loss: 0.15431817796495226 Test Loss: 0.40961612955729165\n",
      "Epoch: 5699 Training Loss: 0.1541131540934245 Test Loss: 0.40969767252604167\n",
      "Epoch: 5700 Training Loss: 0.15392657979329427 Test Loss: 0.40970817057291664\n",
      "Epoch: 5701 Training Loss: 0.1537625189887153 Test Loss: 0.40981184895833334\n",
      "Epoch: 5702 Training Loss: 0.1536131100124783 Test Loss: 0.409901611328125\n",
      "Epoch: 5703 Training Loss: 0.1534745313856337 Test Loss: 0.4100260416666667\n",
      "Epoch: 5704 Training Loss: 0.15334767489963108 Test Loss: 0.41006700303819443\n",
      "Epoch: 5705 Training Loss: 0.1532334984673394 Test Loss: 0.4101990017361111\n",
      "Epoch: 5706 Training Loss: 0.15312666490342883 Test Loss: 0.41024763997395836\n",
      "Epoch: 5707 Training Loss: 0.1530268096923828 Test Loss: 0.41041731770833334\n",
      "Epoch: 5708 Training Loss: 0.1529350314670139 Test Loss: 0.4105298936631944\n",
      "Epoch: 5709 Training Loss: 0.15285552469889324 Test Loss: 0.41066349283854164\n",
      "Epoch: 5710 Training Loss: 0.15277962748209636 Test Loss: 0.4107841525607639\n",
      "Epoch: 5711 Training Loss: 0.15271305168999566 Test Loss: 0.41085259331597224\n",
      "Epoch: 5712 Training Loss: 0.15265115695529513 Test Loss: 0.4109574924045139\n",
      "Epoch: 5713 Training Loss: 0.1525932379828559 Test Loss: 0.41109239366319444\n",
      "Epoch: 5714 Training Loss: 0.1525367685953776 Test Loss: 0.41122021484375\n",
      "Epoch: 5715 Training Loss: 0.15248692660861546 Test Loss: 0.4113992513020833\n",
      "Epoch: 5716 Training Loss: 0.1524420844184028 Test Loss: 0.41157655164930557\n",
      "Epoch: 5717 Training Loss: 0.15239813741048178 Test Loss: 0.41173282877604167\n",
      "Epoch: 5718 Training Loss: 0.15235970730251736 Test Loss: 0.41188001844618055\n",
      "Epoch: 5719 Training Loss: 0.152326412624783 Test Loss: 0.41201768663194444\n",
      "Epoch: 5720 Training Loss: 0.15229629177517362 Test Loss: 0.41217588975694447\n",
      "Epoch: 5721 Training Loss: 0.15226795959472655 Test Loss: 0.4123247884114583\n",
      "Epoch: 5722 Training Loss: 0.15224138895670572 Test Loss: 0.41245703125\n",
      "Epoch: 5723 Training Loss: 0.15221436903211805 Test Loss: 0.4126769748263889\n",
      "Epoch: 5724 Training Loss: 0.152193601820204 Test Loss: 0.41282967122395836\n",
      "Epoch: 5725 Training Loss: 0.1521740943060981 Test Loss: 0.4130546332465278\n",
      "Epoch: 5726 Training Loss: 0.1521600358751085 Test Loss: 0.41322376844618053\n",
      "Epoch: 5727 Training Loss: 0.1521485070122613 Test Loss: 0.4133004557291667\n",
      "Epoch: 5728 Training Loss: 0.1521387464735243 Test Loss: 0.41351665581597224\n",
      "Epoch: 5729 Training Loss: 0.1521287367078993 Test Loss: 0.4136205512152778\n",
      "Epoch: 5730 Training Loss: 0.1521222364637587 Test Loss: 0.41379020182291665\n",
      "Epoch: 5731 Training Loss: 0.15211786397298177 Test Loss: 0.41396034071180554\n",
      "Epoch: 5732 Training Loss: 0.15211608378092448 Test Loss: 0.4142025824652778\n",
      "Epoch: 5733 Training Loss: 0.15211440022786457 Test Loss: 0.41431749131944445\n",
      "Epoch: 5734 Training Loss: 0.15211469014485676 Test Loss: 0.4144986707899306\n",
      "Epoch: 5735 Training Loss: 0.15211805046929253 Test Loss: 0.4146452365451389\n",
      "Epoch: 5736 Training Loss: 0.15212474060058595 Test Loss: 0.4148149685329861\n",
      "Epoch: 5737 Training Loss: 0.15213282097710504 Test Loss: 0.41505238172743053\n",
      "Epoch: 5738 Training Loss: 0.15214413282606337 Test Loss: 0.4151881510416667\n",
      "Epoch: 5739 Training Loss: 0.15215211147732205 Test Loss: 0.41539954969618054\n",
      "Epoch: 5740 Training Loss: 0.15216514756944444 Test Loss: 0.41557118055555553\n",
      "Epoch: 5741 Training Loss: 0.15217840576171876 Test Loss: 0.41574701605902775\n",
      "Epoch: 5742 Training Loss: 0.15219326782226564 Test Loss: 0.415861328125\n",
      "Epoch: 5743 Training Loss: 0.15221514383951823 Test Loss: 0.41604039171006946\n",
      "Epoch: 5744 Training Loss: 0.1522383083767361 Test Loss: 0.4161991373697917\n",
      "Epoch: 5745 Training Loss: 0.15226372951931424 Test Loss: 0.41636561414930556\n",
      "Epoch: 5746 Training Loss: 0.15228721449110244 Test Loss: 0.41662611219618056\n",
      "Epoch: 5747 Training Loss: 0.1523203379313151 Test Loss: 0.41679698350694444\n",
      "Epoch: 5748 Training Loss: 0.15234803093804253 Test Loss: 0.41691126844618054\n",
      "Epoch: 5749 Training Loss: 0.1523834228515625 Test Loss: 0.41706022135416665\n",
      "Epoch: 5750 Training Loss: 0.15242924329969618 Test Loss: 0.41715890842013886\n",
      "Epoch: 5751 Training Loss: 0.15247635396321615 Test Loss: 0.41720912000868055\n",
      "Epoch: 5752 Training Loss: 0.152533448961046 Test Loss: 0.41730436197916665\n",
      "Epoch: 5753 Training Loss: 0.15258394707573786 Test Loss: 0.41730582682291667\n",
      "Epoch: 5754 Training Loss: 0.15264321051703558 Test Loss: 0.41735020616319446\n",
      "Epoch: 5755 Training Loss: 0.15270877414279513 Test Loss: 0.4173221028645833\n",
      "Epoch: 5756 Training Loss: 0.15277938334147134 Test Loss: 0.41727460394965277\n",
      "Epoch: 5757 Training Loss: 0.15285934956868488 Test Loss: 0.41731743706597224\n",
      "Epoch: 5758 Training Loss: 0.1529601101345486 Test Loss: 0.41728203667534725\n",
      "Epoch: 5759 Training Loss: 0.15306561109754774 Test Loss: 0.41733100043402777\n",
      "Epoch: 5760 Training Loss: 0.15318257480197484 Test Loss: 0.41722721354166664\n",
      "Epoch: 5761 Training Loss: 0.15331450398763022 Test Loss: 0.41717312282986113\n",
      "Epoch: 5762 Training Loss: 0.15344913736979165 Test Loss: 0.417109375\n",
      "Epoch: 5763 Training Loss: 0.15361000400119357 Test Loss: 0.4169371744791667\n",
      "Epoch: 5764 Training Loss: 0.15379614427354601 Test Loss: 0.41698757595486113\n",
      "Epoch: 5765 Training Loss: 0.15399126349555123 Test Loss: 0.41696283637152776\n",
      "Epoch: 5766 Training Loss: 0.1542257046169705 Test Loss: 0.41704481336805554\n",
      "Epoch: 5767 Training Loss: 0.15447260199652776 Test Loss: 0.41703917100694443\n",
      "Epoch: 5768 Training Loss: 0.15474628702799478 Test Loss: 0.4171057671440972\n",
      "Epoch: 5769 Training Loss: 0.15505272759331598 Test Loss: 0.417156982421875\n",
      "Epoch: 5770 Training Loss: 0.15539217122395832 Test Loss: 0.41727685546875\n",
      "Epoch: 5771 Training Loss: 0.1557793935139974 Test Loss: 0.4173003200954861\n",
      "Epoch: 5772 Training Loss: 0.15621983167860243 Test Loss: 0.4173677300347222\n",
      "Epoch: 5773 Training Loss: 0.15673897806803386 Test Loss: 0.41756925455729166\n",
      "Epoch: 5774 Training Loss: 0.15734864468044704 Test Loss: 0.41770149739583334\n",
      "Epoch: 5775 Training Loss: 0.158082275390625 Test Loss: 0.41801725260416667\n",
      "Epoch: 5776 Training Loss: 0.15895836046006945 Test Loss: 0.4187418619791667\n",
      "Epoch: 5777 Training Loss: 0.16001898701985678 Test Loss: 0.4199860568576389\n",
      "Epoch: 5778 Training Loss: 0.16131027730305988 Test Loss: 0.42227880859375\n",
      "Epoch: 5779 Training Loss: 0.1628575202094184 Test Loss: 0.42568717447916665\n",
      "Epoch: 5780 Training Loss: 0.16463529629177517 Test Loss: 0.43045439995659723\n",
      "Epoch: 5781 Training Loss: 0.16667722405327692 Test Loss: 0.43628146701388887\n",
      "Epoch: 5782 Training Loss: 0.16893834431966145 Test Loss: 0.4424034559461806\n",
      "Epoch: 5783 Training Loss: 0.17132178921169705 Test Loss: 0.4465387912326389\n",
      "Epoch: 5784 Training Loss: 0.17363775295681425 Test Loss: 0.44524834526909723\n",
      "Epoch: 5785 Training Loss: 0.1750442386203342 Test Loss: 0.4371259223090278\n",
      "Epoch: 5786 Training Loss: 0.17416717698838977 Test Loss: 0.42802210828993054\n",
      "Epoch: 5787 Training Loss: 0.17138320922851563 Test Loss: 0.420911376953125\n",
      "Epoch: 5788 Training Loss: 0.16764646572536893 Test Loss: 0.41680897352430557\n",
      "Epoch: 5789 Training Loss: 0.1639462161593967 Test Loss: 0.41364849175347224\n",
      "Epoch: 5790 Training Loss: 0.16090010918511286 Test Loss: 0.4118312717013889\n",
      "Epoch: 5791 Training Loss: 0.15861854553222657 Test Loss: 0.4111181911892361\n",
      "Epoch: 5792 Training Loss: 0.15699232313368056 Test Loss: 0.41086924913194445\n",
      "Epoch: 5793 Training Loss: 0.15588308885362412 Test Loss: 0.4109497612847222\n",
      "Epoch: 5794 Training Loss: 0.15515830824110244 Test Loss: 0.4113356119791667\n",
      "Epoch: 5795 Training Loss: 0.1546894768608941 Test Loss: 0.4117233615451389\n",
      "Epoch: 5796 Training Loss: 0.15439554172092013 Test Loss: 0.41236311848958335\n",
      "Epoch: 5797 Training Loss: 0.15422715589735242 Test Loss: 0.4127410210503472\n",
      "Epoch: 5798 Training Loss: 0.15414309692382813 Test Loss: 0.41315633138020835\n",
      "Epoch: 5799 Training Loss: 0.15410755242241753 Test Loss: 0.41341259765625\n",
      "Epoch: 5800 Training Loss: 0.15410340203179254 Test Loss: 0.4136291775173611\n",
      "Epoch: 5801 Training Loss: 0.15413312445746527 Test Loss: 0.4138205023871528\n",
      "Epoch: 5802 Training Loss: 0.15418413967556424 Test Loss: 0.4139740397135417\n",
      "Epoch: 5803 Training Loss: 0.15424615308973524 Test Loss: 0.4140755208333333\n",
      "Epoch: 5804 Training Loss: 0.15432435268825956 Test Loss: 0.4143672417534722\n",
      "Epoch: 5805 Training Loss: 0.1544136708577474 Test Loss: 0.4145428059895833\n",
      "Epoch: 5806 Training Loss: 0.1545245123969184 Test Loss: 0.4150725368923611\n",
      "Epoch: 5807 Training Loss: 0.15465155707465278 Test Loss: 0.4153192545572917\n",
      "Epoch: 5808 Training Loss: 0.15480406867133248 Test Loss: 0.41593074544270836\n",
      "Epoch: 5809 Training Loss: 0.15497915818956162 Test Loss: 0.4167876247829861\n",
      "Epoch: 5810 Training Loss: 0.1551609836154514 Test Loss: 0.4174126519097222\n",
      "Epoch: 5811 Training Loss: 0.15538414849175347 Test Loss: 0.41846063910590275\n",
      "Epoch: 5812 Training Loss: 0.15564260694715712 Test Loss: 0.4195033637152778\n",
      "Epoch: 5813 Training Loss: 0.15594268629286023 Test Loss: 0.42054676649305556\n",
      "Epoch: 5814 Training Loss: 0.1562724338107639 Test Loss: 0.4216592339409722\n",
      "Epoch: 5815 Training Loss: 0.15663030158148872 Test Loss: 0.42252986653645835\n",
      "Epoch: 5816 Training Loss: 0.15701952446831596 Test Loss: 0.42318193901909723\n",
      "Epoch: 5817 Training Loss: 0.15744384087456598 Test Loss: 0.42355653211805555\n",
      "Epoch: 5818 Training Loss: 0.1578945024278429 Test Loss: 0.4234644097222222\n",
      "Epoch: 5819 Training Loss: 0.15836358133951822 Test Loss: 0.42304345703125\n",
      "Epoch: 5820 Training Loss: 0.1588266855875651 Test Loss: 0.4217748209635417\n",
      "Epoch: 5821 Training Loss: 0.15929253641764324 Test Loss: 0.41996402994791665\n",
      "Epoch: 5822 Training Loss: 0.15973067559136284 Test Loss: 0.4179729817708333\n",
      "Epoch: 5823 Training Loss: 0.160105219523112 Test Loss: 0.4156386990017361\n",
      "Epoch: 5824 Training Loss: 0.16039327324761285 Test Loss: 0.41293717447916667\n",
      "Epoch: 5825 Training Loss: 0.16057263014051648 Test Loss: 0.41076009114583334\n",
      "Epoch: 5826 Training Loss: 0.16058011372884115 Test Loss: 0.40987906901041665\n",
      "Epoch: 5827 Training Loss: 0.16051212395562067 Test Loss: 0.41090418836805553\n",
      "Epoch: 5828 Training Loss: 0.1603251766628689 Test Loss: 0.4122724880642361\n",
      "Epoch: 5829 Training Loss: 0.15995376925998264 Test Loss: 0.41300203450520834\n",
      "Epoch: 5830 Training Loss: 0.15947416178385418 Test Loss: 0.41358376736111113\n",
      "Epoch: 5831 Training Loss: 0.1589290042453342 Test Loss: 0.41376323784722224\n",
      "Epoch: 5832 Training Loss: 0.15835193549262153 Test Loss: 0.4136561414930556\n",
      "Epoch: 5833 Training Loss: 0.15776206970214843 Test Loss: 0.41370833333333334\n",
      "Epoch: 5834 Training Loss: 0.1571725599500868 Test Loss: 0.4140311957465278\n",
      "Epoch: 5835 Training Loss: 0.15659491814507379 Test Loss: 0.4140853949652778\n",
      "Epoch: 5836 Training Loss: 0.15605511813693576 Test Loss: 0.41426399739583336\n",
      "Epoch: 5837 Training Loss: 0.15558025614420573 Test Loss: 0.4149179416232639\n",
      "Epoch: 5838 Training Loss: 0.15513709852430554 Test Loss: 0.41569970703125\n",
      "Epoch: 5839 Training Loss: 0.15475580003526476 Test Loss: 0.4161346028645833\n",
      "Epoch: 5840 Training Loss: 0.15443978542751735 Test Loss: 0.4165588650173611\n",
      "Epoch: 5841 Training Loss: 0.1541858401828342 Test Loss: 0.4172992621527778\n",
      "Epoch: 5842 Training Loss: 0.15398885599772136 Test Loss: 0.4180094401041667\n",
      "Epoch: 5843 Training Loss: 0.15383424207899304 Test Loss: 0.41844835069444447\n",
      "Epoch: 5844 Training Loss: 0.15372178649902343 Test Loss: 0.4183895128038194\n",
      "Epoch: 5845 Training Loss: 0.1536390635172526 Test Loss: 0.4182499457465278\n",
      "Epoch: 5846 Training Loss: 0.15357603624131944 Test Loss: 0.4180105523003472\n",
      "Epoch: 5847 Training Loss: 0.15351417371961806 Test Loss: 0.4175949435763889\n",
      "Epoch: 5848 Training Loss: 0.1534567142062717 Test Loss: 0.41696598307291666\n",
      "Epoch: 5849 Training Loss: 0.15340389166937934 Test Loss: 0.4163102213541667\n",
      "Epoch: 5850 Training Loss: 0.15335104878743488 Test Loss: 0.41569411892361113\n",
      "Epoch: 5851 Training Loss: 0.1532958238389757 Test Loss: 0.4150102267795139\n",
      "Epoch: 5852 Training Loss: 0.15325309244791666 Test Loss: 0.4144331597222222\n",
      "Epoch: 5853 Training Loss: 0.15320836385091147 Test Loss: 0.41415079752604167\n",
      "Epoch: 5854 Training Loss: 0.15316182963053385 Test Loss: 0.41395087348090276\n",
      "Epoch: 5855 Training Loss: 0.15310123867458766 Test Loss: 0.4137843967013889\n",
      "Epoch: 5856 Training Loss: 0.15304742262098525 Test Loss: 0.41394788953993056\n",
      "Epoch: 5857 Training Loss: 0.1529849344889323 Test Loss: 0.41405479600694445\n",
      "Epoch: 5858 Training Loss: 0.15290938991970487 Test Loss: 0.41434874131944444\n",
      "Epoch: 5859 Training Loss: 0.1528495381673177 Test Loss: 0.41473909505208334\n",
      "Epoch: 5860 Training Loss: 0.15279157850477432 Test Loss: 0.41517472330729166\n",
      "Epoch: 5861 Training Loss: 0.1527327117919922 Test Loss: 0.41549452039930557\n",
      "Epoch: 5862 Training Loss: 0.15269095018174914 Test Loss: 0.4160163302951389\n",
      "Epoch: 5863 Training Loss: 0.1526507805718316 Test Loss: 0.41651302083333336\n",
      "Epoch: 5864 Training Loss: 0.15261237589518228 Test Loss: 0.4169428168402778\n",
      "Epoch: 5865 Training Loss: 0.15258984883626303 Test Loss: 0.4173137749565972\n",
      "Epoch: 5866 Training Loss: 0.15257307942708334 Test Loss: 0.41759554036458335\n",
      "Epoch: 5867 Training Loss: 0.15255170355902778 Test Loss: 0.41768584526909724\n",
      "Epoch: 5868 Training Loss: 0.15254285176595053 Test Loss: 0.4177752278645833\n",
      "Epoch: 5869 Training Loss: 0.15254428948296442 Test Loss: 0.41762909613715277\n",
      "Epoch: 5870 Training Loss: 0.15255365159776477 Test Loss: 0.41739716254340276\n",
      "Epoch: 5871 Training Loss: 0.15257862684461806 Test Loss: 0.4169658745659722\n",
      "Epoch: 5872 Training Loss: 0.15262457444932725 Test Loss: 0.4165317111545139\n",
      "Epoch: 5873 Training Loss: 0.15269075690375433 Test Loss: 0.4159213595920139\n",
      "Epoch: 5874 Training Loss: 0.1527942623562283 Test Loss: 0.41561518012152776\n",
      "Epoch: 5875 Training Loss: 0.15294842868381076 Test Loss: 0.4156726616753472\n",
      "Epoch: 5876 Training Loss: 0.1530949215359158 Test Loss: 0.4165048828125\n",
      "Epoch: 5877 Training Loss: 0.1530830756293403 Test Loss: 0.4161288791232639\n",
      "Epoch: 5878 Training Loss: 0.1530365431043837 Test Loss: 0.4158993869357639\n",
      "Epoch: 5879 Training Loss: 0.15301556057400173 Test Loss: 0.41600358072916666\n",
      "Epoch: 5880 Training Loss: 0.15301852586534287 Test Loss: 0.41579315863715277\n",
      "Epoch: 5881 Training Loss: 0.15305276489257813 Test Loss: 0.4158899197048611\n",
      "Epoch: 5882 Training Loss: 0.1531075185139974 Test Loss: 0.41606591796875\n",
      "Epoch: 5883 Training Loss: 0.15317151217990452 Test Loss: 0.41610660807291666\n",
      "Epoch: 5884 Training Loss: 0.15324166870117187 Test Loss: 0.41627099609375\n",
      "Epoch: 5885 Training Loss: 0.15332874382866754 Test Loss: 0.4163005642361111\n",
      "Epoch: 5886 Training Loss: 0.1534362538655599 Test Loss: 0.41641246202256943\n",
      "Epoch: 5887 Training Loss: 0.15355643378363715 Test Loss: 0.41644927300347223\n",
      "Epoch: 5888 Training Loss: 0.15370117865668403 Test Loss: 0.416251953125\n",
      "Epoch: 5889 Training Loss: 0.1538751983642578 Test Loss: 0.41596063910590275\n",
      "Epoch: 5890 Training Loss: 0.1540894029405382 Test Loss: 0.41554774305555553\n",
      "Epoch: 5891 Training Loss: 0.1543254140218099 Test Loss: 0.414979736328125\n",
      "Epoch: 5892 Training Loss: 0.15459287855360243 Test Loss: 0.4142551812065972\n",
      "Epoch: 5893 Training Loss: 0.15488926357693142 Test Loss: 0.41345128038194445\n",
      "Epoch: 5894 Training Loss: 0.1552236328125 Test Loss: 0.41242409939236113\n",
      "Epoch: 5895 Training Loss: 0.15558282979329427 Test Loss: 0.41104313151041666\n",
      "Epoch: 5896 Training Loss: 0.1559712643093533 Test Loss: 0.41011390516493057\n",
      "Epoch: 5897 Training Loss: 0.15637975396050346 Test Loss: 0.40971237521701387\n",
      "Epoch: 5898 Training Loss: 0.1568175048828125 Test Loss: 0.4101904296875\n",
      "Epoch: 5899 Training Loss: 0.15727270846896702 Test Loss: 0.411197265625\n",
      "Epoch: 5900 Training Loss: 0.15774828084309897 Test Loss: 0.41272748480902777\n",
      "Epoch: 5901 Training Loss: 0.15823091464572483 Test Loss: 0.4147351345486111\n",
      "Epoch: 5902 Training Loss: 0.1587086876763238 Test Loss: 0.41665272352430555\n",
      "Epoch: 5903 Training Loss: 0.15913099670410155 Test Loss: 0.4180724826388889\n",
      "Epoch: 5904 Training Loss: 0.15944959682888454 Test Loss: 0.41868212890625\n",
      "Epoch: 5905 Training Loss: 0.15959414503309463 Test Loss: 0.41812630208333335\n",
      "Epoch: 5906 Training Loss: 0.15949811469184028 Test Loss: 0.4167047526041667\n",
      "Epoch: 5907 Training Loss: 0.15912713623046876 Test Loss: 0.4148336588541667\n",
      "Epoch: 5908 Training Loss: 0.1584857872856988 Test Loss: 0.4132378743489583\n",
      "Epoch: 5909 Training Loss: 0.15764649793836805 Test Loss: 0.41223570421006944\n",
      "Epoch: 5910 Training Loss: 0.15666769409179687 Test Loss: 0.41147322591145835\n",
      "Epoch: 5911 Training Loss: 0.1556858401828342 Test Loss: 0.4114363335503472\n",
      "Epoch: 5912 Training Loss: 0.15479053582085503 Test Loss: 0.4113295627170139\n",
      "Epoch: 5913 Training Loss: 0.1540026109483507 Test Loss: 0.4114429796006944\n",
      "Epoch: 5914 Training Loss: 0.15333260260687934 Test Loss: 0.41155485026041666\n",
      "Epoch: 5915 Training Loss: 0.1527631072998047 Test Loss: 0.41156342230902776\n",
      "Epoch: 5916 Training Loss: 0.1522788848876953 Test Loss: 0.41159190538194446\n",
      "Epoch: 5917 Training Loss: 0.15187063598632813 Test Loss: 0.41168006727430556\n",
      "Epoch: 5918 Training Loss: 0.15153299289279515 Test Loss: 0.41174064127604165\n",
      "Epoch: 5919 Training Loss: 0.15125690375434028 Test Loss: 0.41166563585069443\n",
      "Epoch: 5920 Training Loss: 0.15104101901584202 Test Loss: 0.41163541666666664\n",
      "Epoch: 5921 Training Loss: 0.15086528862847223 Test Loss: 0.4113412814670139\n",
      "Epoch: 5922 Training Loss: 0.150718994140625 Test Loss: 0.4112046169704861\n",
      "Epoch: 5923 Training Loss: 0.15059961785210504 Test Loss: 0.4110540364583333\n",
      "Epoch: 5924 Training Loss: 0.15050566779242622 Test Loss: 0.4108818901909722\n",
      "Epoch: 5925 Training Loss: 0.15043116929796008 Test Loss: 0.4106676161024306\n",
      "Epoch: 5926 Training Loss: 0.15037557813856336 Test Loss: 0.4105244140625\n",
      "Epoch: 5927 Training Loss: 0.15034598117404513 Test Loss: 0.41039195421006947\n",
      "Epoch: 5928 Training Loss: 0.15031469048394097 Test Loss: 0.4102758246527778\n",
      "Epoch: 5929 Training Loss: 0.15030358378092448 Test Loss: 0.4102905002170139\n",
      "Epoch: 5930 Training Loss: 0.1503003929985894 Test Loss: 0.4102092013888889\n",
      "Epoch: 5931 Training Loss: 0.15027684020996093 Test Loss: 0.41022607421875\n",
      "Epoch: 5932 Training Loss: 0.15024164157443576 Test Loss: 0.4103103298611111\n",
      "Epoch: 5933 Training Loss: 0.1501941901312934 Test Loss: 0.4102048068576389\n",
      "Epoch: 5934 Training Loss: 0.15015535651312933 Test Loss: 0.41044930013020836\n",
      "Epoch: 5935 Training Loss: 0.15012415228949652 Test Loss: 0.41064301215277776\n",
      "Epoch: 5936 Training Loss: 0.1501057400173611 Test Loss: 0.41089713541666667\n",
      "Epoch: 5937 Training Loss: 0.1500955556233724 Test Loss: 0.41115842013888887\n",
      "Epoch: 5938 Training Loss: 0.15009596082899307 Test Loss: 0.41157628038194444\n",
      "Epoch: 5939 Training Loss: 0.1501015167236328 Test Loss: 0.41193372938368056\n",
      "Epoch: 5940 Training Loss: 0.15010917324490017 Test Loss: 0.4124055447048611\n",
      "Epoch: 5941 Training Loss: 0.1501157430013021 Test Loss: 0.412693359375\n",
      "Epoch: 5942 Training Loss: 0.15013506571451823 Test Loss: 0.41304722764756946\n",
      "Epoch: 5943 Training Loss: 0.15015943060980902 Test Loss: 0.41358387586805556\n",
      "Epoch: 5944 Training Loss: 0.1501844736735026 Test Loss: 0.41398133680555554\n",
      "Epoch: 5945 Training Loss: 0.15021262953016493 Test Loss: 0.4141845160590278\n",
      "Epoch: 5946 Training Loss: 0.15025041029188368 Test Loss: 0.41472564019097224\n",
      "Epoch: 5947 Training Loss: 0.15028954399956598 Test Loss: 0.41507801649305553\n",
      "Epoch: 5948 Training Loss: 0.15033383009168838 Test Loss: 0.4153904079861111\n",
      "Epoch: 5949 Training Loss: 0.1503785163031684 Test Loss: 0.4157670627170139\n",
      "Epoch: 5950 Training Loss: 0.15042693922254774 Test Loss: 0.4158742947048611\n",
      "Epoch: 5951 Training Loss: 0.1504815165201823 Test Loss: 0.4160146484375\n",
      "Epoch: 5952 Training Loss: 0.15052903069390192 Test Loss: 0.4157836371527778\n",
      "Epoch: 5953 Training Loss: 0.15057623630099826 Test Loss: 0.41568768988715277\n",
      "Epoch: 5954 Training Loss: 0.15063284301757812 Test Loss: 0.41551280381944444\n",
      "Epoch: 5955 Training Loss: 0.15067949422200522 Test Loss: 0.4153637424045139\n",
      "Epoch: 5956 Training Loss: 0.15074144490559896 Test Loss: 0.41527281358506946\n",
      "Epoch: 5957 Training Loss: 0.15080017598470052 Test Loss: 0.4153250054253472\n",
      "Epoch: 5958 Training Loss: 0.1508654344346788 Test Loss: 0.41526817491319445\n",
      "Epoch: 5959 Training Loss: 0.15096539815266927 Test Loss: 0.41539447699652776\n",
      "Epoch: 5960 Training Loss: 0.1510763380262587 Test Loss: 0.41579739040798613\n",
      "Epoch: 5961 Training Loss: 0.15119601270887587 Test Loss: 0.4162005208333333\n",
      "Epoch: 5962 Training Loss: 0.15131946987575956 Test Loss: 0.4166365559895833\n",
      "Epoch: 5963 Training Loss: 0.1514406958685981 Test Loss: 0.4172015245225694\n",
      "Epoch: 5964 Training Loss: 0.1515649634467231 Test Loss: 0.4178663736979167\n",
      "Epoch: 5965 Training Loss: 0.15168738640679252 Test Loss: 0.4185057508680556\n",
      "Epoch: 5966 Training Loss: 0.15182358805338542 Test Loss: 0.4191794162326389\n",
      "Epoch: 5967 Training Loss: 0.15197337341308595 Test Loss: 0.4199194878472222\n",
      "Epoch: 5968 Training Loss: 0.15213739861382378 Test Loss: 0.42057674153645835\n",
      "Epoch: 5969 Training Loss: 0.15230865987141928 Test Loss: 0.42148551432291664\n",
      "Epoch: 5970 Training Loss: 0.15250105455186633 Test Loss: 0.4221837565104167\n",
      "Epoch: 5971 Training Loss: 0.15273279486762154 Test Loss: 0.42253583441840276\n",
      "Epoch: 5972 Training Loss: 0.1529908972846137 Test Loss: 0.4228149685329861\n",
      "Epoch: 5973 Training Loss: 0.15328460693359375 Test Loss: 0.4231211480034722\n",
      "Epoch: 5974 Training Loss: 0.15361773681640625 Test Loss: 0.42314274088541665\n",
      "Epoch: 5975 Training Loss: 0.15400584920247395 Test Loss: 0.42308544921875\n",
      "Epoch: 5976 Training Loss: 0.15447524346245659 Test Loss: 0.4228214518229167\n",
      "Epoch: 5977 Training Loss: 0.1550153554280599 Test Loss: 0.42217716471354166\n",
      "Epoch: 5978 Training Loss: 0.15564412604437933 Test Loss: 0.42168565538194447\n",
      "Epoch: 5979 Training Loss: 0.15636451382107205 Test Loss: 0.421068359375\n",
      "Epoch: 5980 Training Loss: 0.15717355516221787 Test Loss: 0.4211760525173611\n",
      "Epoch: 5981 Training Loss: 0.1580705362955729 Test Loss: 0.42202227105034723\n",
      "Epoch: 5982 Training Loss: 0.15904897732204862 Test Loss: 0.4241713324652778\n",
      "Epoch: 5983 Training Loss: 0.16008683268229168 Test Loss: 0.42782166883680556\n",
      "Epoch: 5984 Training Loss: 0.16113035753038193 Test Loss: 0.43200453016493057\n",
      "Epoch: 5985 Training Loss: 0.16228875054253472 Test Loss: 0.43489995659722225\n",
      "Epoch: 5986 Training Loss: 0.1636104278564453 Test Loss: 0.43514409722222225\n",
      "Epoch: 5987 Training Loss: 0.16498851521809896 Test Loss: 0.43176844618055554\n",
      "Epoch: 5988 Training Loss: 0.16622259691026475 Test Loss: 0.4264852430555556\n",
      "Epoch: 5989 Training Loss: 0.16680586920844184 Test Loss: 0.4233337131076389\n",
      "Epoch: 5990 Training Loss: 0.16683800591362846 Test Loss: 0.42023583984375\n",
      "Epoch: 5991 Training Loss: 0.16665079922146267 Test Loss: 0.41596074761284724\n",
      "Epoch: 5992 Training Loss: 0.16620774163140192 Test Loss: 0.4144948459201389\n",
      "Epoch: 5993 Training Loss: 0.16575176832411023 Test Loss: 0.41928550889756944\n",
      "Epoch: 5994 Training Loss: 0.16533702935112848 Test Loss: 0.42807557508680555\n",
      "Epoch: 5995 Training Loss: 0.1649609137641059 Test Loss: 0.43664534505208336\n",
      "Epoch: 5996 Training Loss: 0.16457993910047744 Test Loss: 0.44162890625\n",
      "Epoch: 5997 Training Loss: 0.1643587900797526 Test Loss: 0.4395186089409722\n",
      "Epoch: 5998 Training Loss: 0.16405469767252603 Test Loss: 0.4316044921875\n",
      "Epoch: 5999 Training Loss: 0.164010006374783 Test Loss: 0.433788330078125\n",
      "Epoch: 6000 Training Loss: 0.16453687879774306 Test Loss: 0.43310199652777776\n",
      "Epoch: 6001 Training Loss: 0.16495722283257377 Test Loss: 0.4277228732638889\n",
      "Epoch: 6002 Training Loss: 0.1647939707438151 Test Loss: 0.42867106119791665\n",
      "Epoch: 6003 Training Loss: 0.1634705556233724 Test Loss: 0.427491455078125\n",
      "Epoch: 6004 Training Loss: 0.16092469957139757 Test Loss: 0.4263379448784722\n",
      "Epoch: 6005 Training Loss: 0.15857534620496963 Test Loss: 0.42469097222222224\n",
      "Epoch: 6006 Training Loss: 0.15745574442545573 Test Loss: 0.41972957356770835\n",
      "Epoch: 6007 Training Loss: 0.15705464680989584 Test Loss: 0.4150232204861111\n",
      "Epoch: 6008 Training Loss: 0.15700701226128472 Test Loss: 0.4137628580729167\n",
      "Epoch: 6009 Training Loss: 0.15709919569227432 Test Loss: 0.4139992133246528\n",
      "Epoch: 6010 Training Loss: 0.1571992950439453 Test Loss: 0.4152459581163194\n",
      "Epoch: 6011 Training Loss: 0.15726817999945747 Test Loss: 0.4171770833333333\n",
      "Epoch: 6012 Training Loss: 0.15732813856336805 Test Loss: 0.41878515625\n",
      "Epoch: 6013 Training Loss: 0.15738557264539932 Test Loss: 0.4190173611111111\n",
      "Epoch: 6014 Training Loss: 0.15745811123318143 Test Loss: 0.41845052083333334\n",
      "Epoch: 6015 Training Loss: 0.15757936096191405 Test Loss: 0.4182275390625\n",
      "Epoch: 6016 Training Loss: 0.15773917643229166 Test Loss: 0.41770692274305554\n",
      "Epoch: 6017 Training Loss: 0.15793033854166666 Test Loss: 0.41731212022569447\n",
      "Epoch: 6018 Training Loss: 0.15811876085069446 Test Loss: 0.41701725260416667\n",
      "Epoch: 6019 Training Loss: 0.15827145216200086 Test Loss: 0.4169501681857639\n",
      "Epoch: 6020 Training Loss: 0.15835517883300781 Test Loss: 0.4169847547743056\n",
      "Epoch: 6021 Training Loss: 0.15838065083821615 Test Loss: 0.4172259114583333\n",
      "Epoch: 6022 Training Loss: 0.15832159084743924 Test Loss: 0.4176875271267361\n",
      "Epoch: 6023 Training Loss: 0.15820387776692707 Test Loss: 0.4181868218315972\n",
      "Epoch: 6024 Training Loss: 0.1580768517388238 Test Loss: 0.418331298828125\n",
      "Epoch: 6025 Training Loss: 0.1579933403862847 Test Loss: 0.41772564019097225\n",
      "Epoch: 6026 Training Loss: 0.1579849090576172 Test Loss: 0.41689876302083334\n",
      "Epoch: 6027 Training Loss: 0.15805724080403646 Test Loss: 0.4157015245225694\n",
      "Epoch: 6028 Training Loss: 0.15825278896755643 Test Loss: 0.41425431315104166\n",
      "Epoch: 6029 Training Loss: 0.15856615363226997 Test Loss: 0.41276104058159724\n",
      "Epoch: 6030 Training Loss: 0.1589933793809679 Test Loss: 0.4111303168402778\n",
      "Epoch: 6031 Training Loss: 0.15952883741590712 Test Loss: 0.4098541666666667\n",
      "Epoch: 6032 Training Loss: 0.16014422437879774 Test Loss: 0.4087272135416667\n",
      "Epoch: 6033 Training Loss: 0.16083393859863282 Test Loss: 0.40843267144097223\n",
      "Epoch: 6034 Training Loss: 0.16149317084418402 Test Loss: 0.4094728461371528\n",
      "Epoch: 6035 Training Loss: 0.16199305046929252 Test Loss: 0.41171348741319447\n",
      "Epoch: 6036 Training Loss: 0.16232564798990887 Test Loss: 0.41404975043402775\n",
      "Epoch: 6037 Training Loss: 0.16249462381998697 Test Loss: 0.41588191731770835\n",
      "Epoch: 6038 Training Loss: 0.16247149149576823 Test Loss: 0.4169472113715278\n",
      "Epoch: 6039 Training Loss: 0.16223703002929687 Test Loss: 0.41672721354166664\n",
      "Epoch: 6040 Training Loss: 0.16169788445366753 Test Loss: 0.41522154405381945\n",
      "Epoch: 6041 Training Loss: 0.16082942199707032 Test Loss: 0.4132899848090278\n",
      "Epoch: 6042 Training Loss: 0.15973389858669704 Test Loss: 0.41161317274305553\n",
      "Epoch: 6043 Training Loss: 0.1585141313340929 Test Loss: 0.41061634657118057\n",
      "Epoch: 6044 Training Loss: 0.15727864413791232 Test Loss: 0.41041213650173614\n",
      "Epoch: 6045 Training Loss: 0.1561683570014106 Test Loss: 0.4105188802083333\n",
      "Epoch: 6046 Training Loss: 0.15521595255533854 Test Loss: 0.41050368923611114\n",
      "Epoch: 6047 Training Loss: 0.15442357381184896 Test Loss: 0.41053721788194447\n",
      "Epoch: 6048 Training Loss: 0.1537895982530382 Test Loss: 0.4104291178385417\n",
      "Epoch: 6049 Training Loss: 0.1532818383110894 Test Loss: 0.4103867730034722\n",
      "Epoch: 6050 Training Loss: 0.15288674926757811 Test Loss: 0.4100914713541667\n",
      "Epoch: 6051 Training Loss: 0.15258238220214843 Test Loss: 0.40980967881944447\n",
      "Epoch: 6052 Training Loss: 0.15236661783854166 Test Loss: 0.40971017795138887\n",
      "Epoch: 6053 Training Loss: 0.15222251722547744 Test Loss: 0.4095358615451389\n",
      "Epoch: 6054 Training Loss: 0.15213197835286457 Test Loss: 0.409434326171875\n",
      "Epoch: 6055 Training Loss: 0.15208140224880642 Test Loss: 0.4093376193576389\n",
      "Epoch: 6056 Training Loss: 0.15206114366319445 Test Loss: 0.4093132595486111\n",
      "Epoch: 6057 Training Loss: 0.15207801649305555 Test Loss: 0.40930767144097224\n",
      "Epoch: 6058 Training Loss: 0.15212244839138456 Test Loss: 0.409155029296875\n",
      "Epoch: 6059 Training Loss: 0.15219898308648003 Test Loss: 0.4092245551215278\n",
      "Epoch: 6060 Training Loss: 0.15230683051215277 Test Loss: 0.40926258680555555\n",
      "Epoch: 6061 Training Loss: 0.1524362352159288 Test Loss: 0.4094027777777778\n",
      "Epoch: 6062 Training Loss: 0.15260450575086806 Test Loss: 0.409519287109375\n",
      "Epoch: 6063 Training Loss: 0.1528272230360243 Test Loss: 0.40987999131944447\n",
      "Epoch: 6064 Training Loss: 0.15308333163791232 Test Loss: 0.4102334255642361\n",
      "Epoch: 6065 Training Loss: 0.15336471557617187 Test Loss: 0.41091248914930556\n",
      "Epoch: 6066 Training Loss: 0.1537101762559679 Test Loss: 0.4117248263888889\n",
      "Epoch: 6067 Training Loss: 0.15412217712402343 Test Loss: 0.4129843478732639\n",
      "Epoch: 6068 Training Loss: 0.15459286329481337 Test Loss: 0.41460660807291666\n",
      "Epoch: 6069 Training Loss: 0.15512095472547743 Test Loss: 0.41631239149305554\n",
      "Epoch: 6070 Training Loss: 0.1557379896375868 Test Loss: 0.41861827256944445\n",
      "Epoch: 6071 Training Loss: 0.15644786919487846 Test Loss: 0.4210322808159722\n",
      "Epoch: 6072 Training Loss: 0.15722649468315972 Test Loss: 0.42337662760416667\n",
      "Epoch: 6073 Training Loss: 0.15806890530056425 Test Loss: 0.4249013129340278\n",
      "Epoch: 6074 Training Loss: 0.15894178263346354 Test Loss: 0.4249330512152778\n",
      "Epoch: 6075 Training Loss: 0.159775143093533 Test Loss: 0.42319471571180556\n",
      "Epoch: 6076 Training Loss: 0.16048947991265192 Test Loss: 0.4203193630642361\n",
      "Epoch: 6077 Training Loss: 0.16092738511827256 Test Loss: 0.4162228732638889\n",
      "Epoch: 6078 Training Loss: 0.16095100741916232 Test Loss: 0.4121693522135417\n",
      "Epoch: 6079 Training Loss: 0.16056334771050348 Test Loss: 0.4095453559027778\n",
      "Epoch: 6080 Training Loss: 0.15985704210069446 Test Loss: 0.40802235243055557\n",
      "Epoch: 6081 Training Loss: 0.15892334832085503 Test Loss: 0.4077552354600694\n",
      "Epoch: 6082 Training Loss: 0.15791500176323786 Test Loss: 0.40793901909722224\n",
      "Epoch: 6083 Training Loss: 0.15692725626627604 Test Loss: 0.4081150173611111\n",
      "Epoch: 6084 Training Loss: 0.15603692966037327 Test Loss: 0.4083938530815972\n",
      "Epoch: 6085 Training Loss: 0.1552764638264974 Test Loss: 0.40864141167534723\n",
      "Epoch: 6086 Training Loss: 0.1546550818549262 Test Loss: 0.408957763671875\n",
      "Epoch: 6087 Training Loss: 0.15418251037597655 Test Loss: 0.40930712890625\n",
      "Epoch: 6088 Training Loss: 0.15384137471516926 Test Loss: 0.40971077473958334\n",
      "Epoch: 6089 Training Loss: 0.1536114044189453 Test Loss: 0.4102304958767361\n",
      "Epoch: 6090 Training Loss: 0.15348101976182726 Test Loss: 0.41075935872395836\n",
      "Epoch: 6091 Training Loss: 0.15345222981770834 Test Loss: 0.41144533962673613\n",
      "Epoch: 6092 Training Loss: 0.15350662740071613 Test Loss: 0.4121203884548611\n",
      "Epoch: 6093 Training Loss: 0.15363321261935764 Test Loss: 0.41285557725694444\n",
      "Epoch: 6094 Training Loss: 0.15383537122938368 Test Loss: 0.4136006401909722\n",
      "Epoch: 6095 Training Loss: 0.1540903065999349 Test Loss: 0.414167724609375\n",
      "Epoch: 6096 Training Loss: 0.15438670179578992 Test Loss: 0.4147638075086806\n",
      "Epoch: 6097 Training Loss: 0.1547222883436415 Test Loss: 0.415310791015625\n",
      "Epoch: 6098 Training Loss: 0.15509772576226127 Test Loss: 0.41590234375\n",
      "Epoch: 6099 Training Loss: 0.15545784674750435 Test Loss: 0.4164469943576389\n",
      "Epoch: 6100 Training Loss: 0.1558227250840929 Test Loss: 0.41726057942708333\n",
      "Epoch: 6101 Training Loss: 0.15618621487087675 Test Loss: 0.4181322970920139\n",
      "Epoch: 6102 Training Loss: 0.15650921969943576 Test Loss: 0.41876866319444445\n",
      "Epoch: 6103 Training Loss: 0.1567574225531684 Test Loss: 0.41982958984375\n",
      "Epoch: 6104 Training Loss: 0.15697603352864584 Test Loss: 0.4205927191840278\n",
      "Epoch: 6105 Training Loss: 0.1571369934082031 Test Loss: 0.420979736328125\n",
      "Epoch: 6106 Training Loss: 0.15722546047634547 Test Loss: 0.4209473198784722\n",
      "Epoch: 6107 Training Loss: 0.15727722676595052 Test Loss: 0.42076258680555556\n",
      "Epoch: 6108 Training Loss: 0.15726627434624565 Test Loss: 0.4202194281684028\n",
      "Epoch: 6109 Training Loss: 0.1572085689968533 Test Loss: 0.4199292534722222\n",
      "Epoch: 6110 Training Loss: 0.15715303378634982 Test Loss: 0.41959423828125\n",
      "Epoch: 6111 Training Loss: 0.1571161092122396 Test Loss: 0.4192979058159722\n",
      "Epoch: 6112 Training Loss: 0.15710079277886285 Test Loss: 0.4192673611111111\n",
      "Epoch: 6113 Training Loss: 0.15713667975531684 Test Loss: 0.4191585286458333\n",
      "Epoch: 6114 Training Loss: 0.15722641838921442 Test Loss: 0.4189132486979167\n",
      "Epoch: 6115 Training Loss: 0.15735735575358073 Test Loss: 0.4186607259114583\n",
      "Epoch: 6116 Training Loss: 0.1575707295735677 Test Loss: 0.41830029296875\n",
      "Epoch: 6117 Training Loss: 0.15787787712944878 Test Loss: 0.41778681098090276\n",
      "Epoch: 6118 Training Loss: 0.15827657911512588 Test Loss: 0.416900390625\n",
      "Epoch: 6119 Training Loss: 0.15879261440700954 Test Loss: 0.41581046549479167\n",
      "Epoch: 6120 Training Loss: 0.1594301503499349 Test Loss: 0.4145287814670139\n",
      "Epoch: 6121 Training Loss: 0.1601580030653212 Test Loss: 0.4137934299045139\n",
      "Epoch: 6122 Training Loss: 0.16098986307779947 Test Loss: 0.4137917209201389\n",
      "Epoch: 6123 Training Loss: 0.16189915466308594 Test Loss: 0.4147369791666667\n",
      "Epoch: 6124 Training Loss: 0.16278987460666233 Test Loss: 0.4164653049045139\n",
      "Epoch: 6125 Training Loss: 0.16360531277126736 Test Loss: 0.4185111762152778\n",
      "Epoch: 6126 Training Loss: 0.16429065110948352 Test Loss: 0.4205938042534722\n",
      "Epoch: 6127 Training Loss: 0.1647670135498047 Test Loss: 0.4225466037326389\n",
      "Epoch: 6128 Training Loss: 0.16498672993977864 Test Loss: 0.42402237955729166\n",
      "Epoch: 6129 Training Loss: 0.16485250684950087 Test Loss: 0.424544189453125\n",
      "Epoch: 6130 Training Loss: 0.16436741807725694 Test Loss: 0.4239052734375\n",
      "Epoch: 6131 Training Loss: 0.1635157725016276 Test Loss: 0.42247764756944445\n",
      "Epoch: 6132 Training Loss: 0.16233407592773438 Test Loss: 0.4203980848524306\n",
      "Epoch: 6133 Training Loss: 0.16094490390353733 Test Loss: 0.4181145562065972\n",
      "Epoch: 6134 Training Loss: 0.15941778733995227 Test Loss: 0.4160127224392361\n",
      "Epoch: 6135 Training Loss: 0.1578707512749566 Test Loss: 0.41398578559027777\n",
      "Epoch: 6136 Training Loss: 0.1563798116048177 Test Loss: 0.4120136990017361\n",
      "Epoch: 6137 Training Loss: 0.15498931715223524 Test Loss: 0.4100915256076389\n",
      "Epoch: 6138 Training Loss: 0.1537551778157552 Test Loss: 0.40832722981770836\n",
      "Epoch: 6139 Training Loss: 0.1526904517279731 Test Loss: 0.40696620008680556\n",
      "Epoch: 6140 Training Loss: 0.15178426954481336 Test Loss: 0.4058772243923611\n",
      "Epoch: 6141 Training Loss: 0.15101634046766493 Test Loss: 0.4049578450520833\n",
      "Epoch: 6142 Training Loss: 0.15035147942437066 Test Loss: 0.404218505859375\n",
      "Epoch: 6143 Training Loss: 0.14978482903374565 Test Loss: 0.4036547580295139\n",
      "Epoch: 6144 Training Loss: 0.1493034430609809 Test Loss: 0.40321742078993056\n",
      "Epoch: 6145 Training Loss: 0.1488881564670139 Test Loss: 0.4028979220920139\n",
      "Epoch: 6146 Training Loss: 0.1485360378689236 Test Loss: 0.40264545355902776\n",
      "Epoch: 6147 Training Loss: 0.148234861585829 Test Loss: 0.40246937391493054\n",
      "Epoch: 6148 Training Loss: 0.14797726270887587 Test Loss: 0.4023639865451389\n",
      "Epoch: 6149 Training Loss: 0.14775441657172309 Test Loss: 0.40227286783854166\n",
      "Epoch: 6150 Training Loss: 0.1475620303683811 Test Loss: 0.40233219401041664\n",
      "Epoch: 6151 Training Loss: 0.14739160664876302 Test Loss: 0.40234912109375\n",
      "Epoch: 6152 Training Loss: 0.1472393120659722 Test Loss: 0.4024308539496528\n",
      "Epoch: 6153 Training Loss: 0.14710660976833767 Test Loss: 0.4024233669704861\n",
      "Epoch: 6154 Training Loss: 0.14698900349934896 Test Loss: 0.4025250651041667\n",
      "Epoch: 6155 Training Loss: 0.14688683234320746 Test Loss: 0.40264854600694444\n",
      "Epoch: 6156 Training Loss: 0.14679340277777778 Test Loss: 0.4027175564236111\n",
      "Epoch: 6157 Training Loss: 0.14670977952745226 Test Loss: 0.40283992513020833\n",
      "Epoch: 6158 Training Loss: 0.14663349405924478 Test Loss: 0.4029319118923611\n",
      "Epoch: 6159 Training Loss: 0.14656361558702258 Test Loss: 0.4030785590277778\n",
      "Epoch: 6160 Training Loss: 0.14650214301215278 Test Loss: 0.4031282009548611\n",
      "Epoch: 6161 Training Loss: 0.14644684516059028 Test Loss: 0.4032929416232639\n",
      "Epoch: 6162 Training Loss: 0.1463954332139757 Test Loss: 0.4034038357204861\n",
      "Epoch: 6163 Training Loss: 0.1463476325141059 Test Loss: 0.403513671875\n",
      "Epoch: 6164 Training Loss: 0.14630409579806858 Test Loss: 0.40372205946180556\n",
      "Epoch: 6165 Training Loss: 0.14626336330837675 Test Loss: 0.40387025282118055\n",
      "Epoch: 6166 Training Loss: 0.1462236565483941 Test Loss: 0.403997802734375\n",
      "Epoch: 6167 Training Loss: 0.14619066874186198 Test Loss: 0.4041210666232639\n",
      "Epoch: 6168 Training Loss: 0.14615750461154514 Test Loss: 0.4042674967447917\n",
      "Epoch: 6169 Training Loss: 0.14613160705566405 Test Loss: 0.40442141384548613\n",
      "Epoch: 6170 Training Loss: 0.14610568406846788 Test Loss: 0.40449739583333333\n",
      "Epoch: 6171 Training Loss: 0.14607972208658854 Test Loss: 0.404629638671875\n",
      "Epoch: 6172 Training Loss: 0.14605233764648437 Test Loss: 0.4048195529513889\n",
      "Epoch: 6173 Training Loss: 0.1460275624593099 Test Loss: 0.404939697265625\n",
      "Epoch: 6174 Training Loss: 0.14600443183051215 Test Loss: 0.40506586371527775\n",
      "Epoch: 6175 Training Loss: 0.14598473273383247 Test Loss: 0.40522705078125\n",
      "Epoch: 6176 Training Loss: 0.14596538628472222 Test Loss: 0.40532134331597225\n",
      "Epoch: 6177 Training Loss: 0.14594765896267362 Test Loss: 0.40544742838541664\n",
      "Epoch: 6178 Training Loss: 0.1459327596028646 Test Loss: 0.40559716796875\n",
      "Epoch: 6179 Training Loss: 0.14591571723090277 Test Loss: 0.40568880208333336\n",
      "Epoch: 6180 Training Loss: 0.14590077039930555 Test Loss: 0.4058364529079861\n",
      "Epoch: 6181 Training Loss: 0.14588483513726128 Test Loss: 0.40600214301215276\n",
      "Epoch: 6182 Training Loss: 0.14587217712402345 Test Loss: 0.40608780924479165\n",
      "Epoch: 6183 Training Loss: 0.14586048210991753 Test Loss: 0.4062389322916667\n",
      "Epoch: 6184 Training Loss: 0.1458505588107639 Test Loss: 0.40639634874131947\n",
      "Epoch: 6185 Training Loss: 0.14583961147732205 Test Loss: 0.406462890625\n",
      "Epoch: 6186 Training Loss: 0.145825442843967 Test Loss: 0.40659114583333333\n",
      "Epoch: 6187 Training Loss: 0.14581829833984375 Test Loss: 0.40672444661458335\n",
      "Epoch: 6188 Training Loss: 0.14580691867404513 Test Loss: 0.40684876844618056\n",
      "Epoch: 6189 Training Loss: 0.14579573398166232 Test Loss: 0.40699601236979166\n",
      "Epoch: 6190 Training Loss: 0.14578431871202258 Test Loss: 0.4071526150173611\n",
      "Epoch: 6191 Training Loss: 0.14577466837565103 Test Loss: 0.40728369140625\n",
      "Epoch: 6192 Training Loss: 0.14576175944010417 Test Loss: 0.4074064398871528\n",
      "Epoch: 6193 Training Loss: 0.14575150044759114 Test Loss: 0.40749658203125\n",
      "Epoch: 6194 Training Loss: 0.14574055480957032 Test Loss: 0.407619873046875\n",
      "Epoch: 6195 Training Loss: 0.14573192172580296 Test Loss: 0.4077384982638889\n",
      "Epoch: 6196 Training Loss: 0.14572389899359808 Test Loss: 0.407802490234375\n",
      "Epoch: 6197 Training Loss: 0.14571636793348525 Test Loss: 0.40788774956597224\n",
      "Epoch: 6198 Training Loss: 0.1457073754204644 Test Loss: 0.4080009223090278\n",
      "Epoch: 6199 Training Loss: 0.14569747077094183 Test Loss: 0.4081642795138889\n",
      "Epoch: 6200 Training Loss: 0.14569031439887153 Test Loss: 0.40824045138888887\n",
      "Epoch: 6201 Training Loss: 0.14568358696831596 Test Loss: 0.40842320421006945\n",
      "Epoch: 6202 Training Loss: 0.145676274617513 Test Loss: 0.408513916015625\n",
      "Epoch: 6203 Training Loss: 0.1456696065266927 Test Loss: 0.40872355143229167\n",
      "Epoch: 6204 Training Loss: 0.14566573248969183 Test Loss: 0.40876936848958334\n",
      "Epoch: 6205 Training Loss: 0.14565945265028213 Test Loss: 0.4087999131944444\n",
      "Epoch: 6206 Training Loss: 0.14565234714084202 Test Loss: 0.408951904296875\n",
      "Epoch: 6207 Training Loss: 0.14564629279242622 Test Loss: 0.4090602756076389\n",
      "Epoch: 6208 Training Loss: 0.14564588589138455 Test Loss: 0.40920274522569444\n",
      "Epoch: 6209 Training Loss: 0.14563760715060764 Test Loss: 0.4091903483072917\n",
      "Epoch: 6210 Training Loss: 0.14562967088487414 Test Loss: 0.40936083984375\n",
      "Epoch: 6211 Training Loss: 0.145622075398763 Test Loss: 0.40949986436631947\n",
      "Epoch: 6212 Training Loss: 0.14561597188313802 Test Loss: 0.40960970052083334\n",
      "Epoch: 6213 Training Loss: 0.14560974629720053 Test Loss: 0.40963278537326386\n",
      "Epoch: 6214 Training Loss: 0.1456073676215278 Test Loss: 0.40980406358506943\n",
      "Epoch: 6215 Training Loss: 0.14559949408637152 Test Loss: 0.4098808051215278\n",
      "Epoch: 6216 Training Loss: 0.14559628634982638 Test Loss: 0.40996923828125\n",
      "Epoch: 6217 Training Loss: 0.14560087246365017 Test Loss: 0.4100453559027778\n",
      "Epoch: 6218 Training Loss: 0.14559856669108073 Test Loss: 0.4101174587673611\n",
      "Epoch: 6219 Training Loss: 0.14559229193793402 Test Loss: 0.4102887912326389\n",
      "Epoch: 6220 Training Loss: 0.14558911641438801 Test Loss: 0.4103546549479167\n",
      "Epoch: 6221 Training Loss: 0.1455888909233941 Test Loss: 0.4104409450954861\n",
      "Epoch: 6222 Training Loss: 0.1455929667154948 Test Loss: 0.4105144856770833\n",
      "Epoch: 6223 Training Loss: 0.14559068298339845 Test Loss: 0.41059988064236114\n",
      "Epoch: 6224 Training Loss: 0.1455834248860677 Test Loss: 0.41087144639756945\n",
      "Epoch: 6225 Training Loss: 0.14558697001139323 Test Loss: 0.41093416341145833\n",
      "Epoch: 6226 Training Loss: 0.14558733622233072 Test Loss: 0.4110691189236111\n",
      "Epoch: 6227 Training Loss: 0.14559133911132813 Test Loss: 0.41108970811631945\n",
      "Epoch: 6228 Training Loss: 0.14559178331163194 Test Loss: 0.4111101345486111\n",
      "Epoch: 6229 Training Loss: 0.14559044053819445 Test Loss: 0.41131464301215276\n",
      "Epoch: 6230 Training Loss: 0.14559137810601128 Test Loss: 0.41121636284722224\n",
      "Epoch: 6231 Training Loss: 0.1456030985514323 Test Loss: 0.41141541883680555\n",
      "Epoch: 6232 Training Loss: 0.14561497158474393 Test Loss: 0.4115798611111111\n",
      "Epoch: 6233 Training Loss: 0.1456211632622613 Test Loss: 0.41149864366319444\n",
      "Epoch: 6234 Training Loss: 0.14561778937445746 Test Loss: 0.41203963216145834\n",
      "Epoch: 6235 Training Loss: 0.14561736382378473 Test Loss: 0.4121037326388889\n",
      "Epoch: 6236 Training Loss: 0.14562640211317274 Test Loss: 0.41211241319444447\n",
      "Epoch: 6237 Training Loss: 0.14564269341362848 Test Loss: 0.41240101453993055\n",
      "Epoch: 6238 Training Loss: 0.1456527591281467 Test Loss: 0.4120963541666667\n",
      "Epoch: 6239 Training Loss: 0.14565640767415364 Test Loss: 0.41246468098958333\n",
      "Epoch: 6240 Training Loss: 0.14565929836697047 Test Loss: 0.41277132161458335\n",
      "Epoch: 6241 Training Loss: 0.14567502170138888 Test Loss: 0.412478515625\n",
      "Epoch: 6242 Training Loss: 0.14570001559787327 Test Loss: 0.41289781358506944\n",
      "Epoch: 6243 Training Loss: 0.14572515699598523 Test Loss: 0.41267911783854166\n",
      "Epoch: 6244 Training Loss: 0.1457341986762153 Test Loss: 0.4125042588975694\n",
      "Epoch: 6245 Training Loss: 0.1457385474310981 Test Loss: 0.4133898654513889\n",
      "Epoch: 6246 Training Loss: 0.145752688937717 Test Loss: 0.4131344672309028\n",
      "Epoch: 6247 Training Loss: 0.1457786153157552 Test Loss: 0.41325884331597224\n",
      "Epoch: 6248 Training Loss: 0.1458072221544054 Test Loss: 0.41329695638020836\n",
      "Epoch: 6249 Training Loss: 0.1458233167860243 Test Loss: 0.4127794596354167\n",
      "Epoch: 6250 Training Loss: 0.14584408060709636 Test Loss: 0.41354112413194444\n",
      "Epoch: 6251 Training Loss: 0.1458822479248047 Test Loss: 0.41340380859375\n",
      "Epoch: 6252 Training Loss: 0.1459188978407118 Test Loss: 0.4127459038628472\n",
      "Epoch: 6253 Training Loss: 0.14596987575954862 Test Loss: 0.41280447048611113\n",
      "Epoch: 6254 Training Loss: 0.14601339213053385 Test Loss: 0.4116902126736111\n",
      "Epoch: 6255 Training Loss: 0.14604695298936632 Test Loss: 0.41175835503472225\n",
      "Epoch: 6256 Training Loss: 0.1461301032172309 Test Loss: 0.41185462782118054\n",
      "Epoch: 6257 Training Loss: 0.1462207743326823 Test Loss: 0.4112544487847222\n",
      "Epoch: 6258 Training Loss: 0.14628961520724826 Test Loss: 0.41131141493055556\n",
      "Epoch: 6259 Training Loss: 0.1463650410970052 Test Loss: 0.4112966037326389\n",
      "Epoch: 6260 Training Loss: 0.14645023600260418 Test Loss: 0.41276771375868054\n",
      "Epoch: 6261 Training Loss: 0.14655901421440973 Test Loss: 0.41369032118055554\n",
      "Epoch: 6262 Training Loss: 0.14665082295735676 Test Loss: 0.4132179904513889\n",
      "Epoch: 6263 Training Loss: 0.14671333991156685 Test Loss: 0.41339048936631945\n",
      "Epoch: 6264 Training Loss: 0.1467620883517795 Test Loss: 0.4136006130642361\n",
      "Epoch: 6265 Training Loss: 0.14676836649576822 Test Loss: 0.4141300184461806\n",
      "Epoch: 6266 Training Loss: 0.14676663377549914 Test Loss: 0.41416449652777776\n",
      "Epoch: 6267 Training Loss: 0.14677736070421007 Test Loss: 0.41409882269965276\n",
      "Epoch: 6268 Training Loss: 0.14679632737901477 Test Loss: 0.41409529622395835\n",
      "Epoch: 6269 Training Loss: 0.1468235321044922 Test Loss: 0.41418400065104166\n",
      "Epoch: 6270 Training Loss: 0.14684483506944446 Test Loss: 0.4144814453125\n",
      "Epoch: 6271 Training Loss: 0.1468817121717665 Test Loss: 0.41487147352430553\n",
      "Epoch: 6272 Training Loss: 0.14694225735134547 Test Loss: 0.41459014214409723\n",
      "Epoch: 6273 Training Loss: 0.1470182834201389 Test Loss: 0.4146445855034722\n",
      "Epoch: 6274 Training Loss: 0.1471347927517361 Test Loss: 0.4144620768229167\n",
      "Epoch: 6275 Training Loss: 0.14722949896918403 Test Loss: 0.4148757052951389\n",
      "Epoch: 6276 Training Loss: 0.14731072828504774 Test Loss: 0.4155597873263889\n",
      "Epoch: 6277 Training Loss: 0.14742809041341146 Test Loss: 0.41510530598958334\n",
      "Epoch: 6278 Training Loss: 0.1475586920844184 Test Loss: 0.4151557074652778\n",
      "Epoch: 6279 Training Loss: 0.14771438937717013 Test Loss: 0.41491861979166667\n",
      "Epoch: 6280 Training Loss: 0.14787903001573352 Test Loss: 0.4149289279513889\n",
      "Epoch: 6281 Training Loss: 0.14802463107638889 Test Loss: 0.41578651258680555\n",
      "Epoch: 6282 Training Loss: 0.14825196160210505 Test Loss: 0.41563123914930555\n",
      "Epoch: 6283 Training Loss: 0.1485066172281901 Test Loss: 0.4150311957465278\n",
      "Epoch: 6284 Training Loss: 0.14879831271701388 Test Loss: 0.4147380642361111\n",
      "Epoch: 6285 Training Loss: 0.14912923685709636 Test Loss: 0.4153083224826389\n",
      "Epoch: 6286 Training Loss: 0.14953914727105036 Test Loss: 0.4158684353298611\n",
      "Epoch: 6287 Training Loss: 0.1500448523627387 Test Loss: 0.4153576117621528\n",
      "Epoch: 6288 Training Loss: 0.15066393195258246 Test Loss: 0.41540738932291665\n",
      "Epoch: 6289 Training Loss: 0.1514373999701606 Test Loss: 0.4165161675347222\n",
      "Epoch: 6290 Training Loss: 0.152399658203125 Test Loss: 0.4183569878472222\n",
      "Epoch: 6291 Training Loss: 0.15369405280219184 Test Loss: 0.4192375759548611\n",
      "Epoch: 6292 Training Loss: 0.15538785807291666 Test Loss: 0.42149788411458333\n",
      "Epoch: 6293 Training Loss: 0.15768085394965278 Test Loss: 0.4257035590277778\n",
      "Epoch: 6294 Training Loss: 0.1608113521999783 Test Loss: 0.43228271484375\n",
      "Epoch: 6295 Training Loss: 0.1650743950737847 Test Loss: 0.4409169921875\n",
      "Epoch: 6296 Training Loss: 0.17079301961263021 Test Loss: 0.44437456597222225\n",
      "Epoch: 6297 Training Loss: 0.17808316548665365 Test Loss: 0.43050775824652776\n",
      "Epoch: 6298 Training Loss: 0.18384708489312065 Test Loss: 0.4196321614583333\n",
      "Epoch: 6299 Training Loss: 0.1829696773952908 Test Loss: 0.42434608289930553\n",
      "Epoch: 6300 Training Loss: 0.17635958184136286 Test Loss: 0.4223091362847222\n",
      "Epoch: 6301 Training Loss: 0.16988490634494358 Test Loss: 0.41385753038194445\n",
      "Epoch: 6302 Training Loss: 0.16495581902398004 Test Loss: 0.40935663519965276\n",
      "Epoch: 6303 Training Loss: 0.16175419786241318 Test Loss: 0.4045730794270833\n",
      "Epoch: 6304 Training Loss: 0.15963733588324652 Test Loss: 0.40317941623263887\n",
      "Epoch: 6305 Training Loss: 0.15809878370496963 Test Loss: 0.40344240993923614\n",
      "Epoch: 6306 Training Loss: 0.15680581665039062 Test Loss: 0.4046044921875\n",
      "Epoch: 6307 Training Loss: 0.15563153923882378 Test Loss: 0.40655137803819447\n",
      "Epoch: 6308 Training Loss: 0.15458707682291667 Test Loss: 0.40824560546875\n",
      "Epoch: 6309 Training Loss: 0.15375160556369358 Test Loss: 0.40984271918402776\n",
      "Epoch: 6310 Training Loss: 0.15313834296332465 Test Loss: 0.41162456597222224\n",
      "Epoch: 6311 Training Loss: 0.15276133558485244 Test Loss: 0.4131604817708333\n",
      "Epoch: 6312 Training Loss: 0.1526186777750651 Test Loss: 0.4148319769965278\n",
      "Epoch: 6313 Training Loss: 0.15270485432942707 Test Loss: 0.41597786458333336\n",
      "Epoch: 6314 Training Loss: 0.15299246046278211 Test Loss: 0.41716585286458335\n",
      "Epoch: 6315 Training Loss: 0.15347950914171007 Test Loss: 0.4176875542534722\n",
      "Epoch: 6316 Training Loss: 0.15419007025824652 Test Loss: 0.4178259548611111\n",
      "Epoch: 6317 Training Loss: 0.15510991583930123 Test Loss: 0.41663368055555555\n",
      "Epoch: 6318 Training Loss: 0.15621274142795138 Test Loss: 0.4146045464409722\n",
      "Epoch: 6319 Training Loss: 0.15738768344455295 Test Loss: 0.41182505967881944\n",
      "Epoch: 6320 Training Loss: 0.15847982788085938 Test Loss: 0.40920323350694443\n",
      "Epoch: 6321 Training Loss: 0.15924376763237846 Test Loss: 0.40849156358506944\n",
      "Epoch: 6322 Training Loss: 0.1594027557373047 Test Loss: 0.4091614854600694\n",
      "Epoch: 6323 Training Loss: 0.15891570027669272 Test Loss: 0.4107890625\n",
      "Epoch: 6324 Training Loss: 0.1579287651909722 Test Loss: 0.4111018337673611\n",
      "Epoch: 6325 Training Loss: 0.15669933234320746 Test Loss: 0.4105724826388889\n",
      "Epoch: 6326 Training Loss: 0.15537699551052517 Test Loss: 0.4099634331597222\n",
      "Epoch: 6327 Training Loss: 0.15412713962131078 Test Loss: 0.4117079535590278\n",
      "Epoch: 6328 Training Loss: 0.15297867499457465 Test Loss: 0.41386048719618057\n",
      "Epoch: 6329 Training Loss: 0.15197340901692707 Test Loss: 0.41630013020833334\n",
      "Epoch: 6330 Training Loss: 0.1511371070014106 Test Loss: 0.4188210991753472\n",
      "Epoch: 6331 Training Loss: 0.1504937947591146 Test Loss: 0.42024348958333335\n",
      "Epoch: 6332 Training Loss: 0.14999237738715276 Test Loss: 0.42076795789930554\n",
      "Epoch: 6333 Training Loss: 0.14961810133192274 Test Loss: 0.4215303276909722\n",
      "Epoch: 6334 Training Loss: 0.14936446634928385 Test Loss: 0.4231789822048611\n",
      "Epoch: 6335 Training Loss: 0.14919532775878908 Test Loss: 0.42515294053819447\n",
      "Epoch: 6336 Training Loss: 0.14913909912109374 Test Loss: 0.42567621527777777\n",
      "Epoch: 6337 Training Loss: 0.14914187452528213 Test Loss: 0.4245167643229167\n",
      "Epoch: 6338 Training Loss: 0.1491421407063802 Test Loss: 0.42339567057291666\n",
      "Epoch: 6339 Training Loss: 0.1491392584906684 Test Loss: 0.42485153537326387\n",
      "Epoch: 6340 Training Loss: 0.14912233140733508 Test Loss: 0.42440852864583334\n",
      "Epoch: 6341 Training Loss: 0.14904796685112848 Test Loss: 0.41867632378472225\n",
      "Epoch: 6342 Training Loss: 0.1488667195638021 Test Loss: 0.4141924370659722\n",
      "Epoch: 6343 Training Loss: 0.14864412943522134 Test Loss: 0.41394029405381944\n",
      "Epoch: 6344 Training Loss: 0.14848364596896702 Test Loss: 0.4151218532986111\n",
      "Epoch: 6345 Training Loss: 0.14845715162489148 Test Loss: 0.41593907335069447\n",
      "Epoch: 6346 Training Loss: 0.148556640625 Test Loss: 0.4175593532986111\n",
      "Epoch: 6347 Training Loss: 0.14878638034396702 Test Loss: 0.41976025390625\n",
      "Epoch: 6348 Training Loss: 0.1491332261827257 Test Loss: 0.42201038953993053\n",
      "Epoch: 6349 Training Loss: 0.14957881842719184 Test Loss: 0.42448301866319443\n",
      "Epoch: 6350 Training Loss: 0.15023725891113282 Test Loss: 0.4305695258246528\n",
      "Epoch: 6351 Training Loss: 0.1511989288330078 Test Loss: 0.44363715277777777\n",
      "Epoch: 6352 Training Loss: 0.15272737799750433 Test Loss: 0.4567359483506944\n",
      "Epoch: 6353 Training Loss: 0.15485764736599392 Test Loss: 0.4452400987413194\n",
      "Epoch: 6354 Training Loss: 0.15587317742241755 Test Loss: 0.41837651909722223\n",
      "Epoch: 6355 Training Loss: 0.15486328125 Test Loss: 0.4192421875\n",
      "Epoch: 6356 Training Loss: 0.15353950839572483 Test Loss: 0.41685563151041666\n",
      "Epoch: 6357 Training Loss: 0.1520504862467448 Test Loss: 0.41320708550347224\n",
      "Epoch: 6358 Training Loss: 0.15135586717393662 Test Loss: 0.41539027235243053\n",
      "Epoch: 6359 Training Loss: 0.15148215569390192 Test Loss: 0.4172319878472222\n",
      "Epoch: 6360 Training Loss: 0.15188876003689236 Test Loss: 0.4164097222222222\n",
      "Epoch: 6361 Training Loss: 0.15226952785915798 Test Loss: 0.41396158854166665\n",
      "Epoch: 6362 Training Loss: 0.15257306416829428 Test Loss: 0.4117861328125\n",
      "Epoch: 6363 Training Loss: 0.15278784349229602 Test Loss: 0.4109267578125\n",
      "Epoch: 6364 Training Loss: 0.1529684549967448 Test Loss: 0.41129779730902777\n",
      "Epoch: 6365 Training Loss: 0.1531566111246745 Test Loss: 0.4119141167534722\n",
      "Epoch: 6366 Training Loss: 0.15335449727376302 Test Loss: 0.41217192925347224\n",
      "Epoch: 6367 Training Loss: 0.15356836785210504 Test Loss: 0.4121673177083333\n",
      "Epoch: 6368 Training Loss: 0.15384890577528212 Test Loss: 0.4120368109809028\n",
      "Epoch: 6369 Training Loss: 0.1541762203640408 Test Loss: 0.41201654730902776\n",
      "Epoch: 6370 Training Loss: 0.15456616380479601 Test Loss: 0.4124109700520833\n",
      "Epoch: 6371 Training Loss: 0.15502477518717447 Test Loss: 0.41335902235243055\n",
      "Epoch: 6372 Training Loss: 0.15555589803059897 Test Loss: 0.4148930121527778\n",
      "Epoch: 6373 Training Loss: 0.156178220960829 Test Loss: 0.41694683159722223\n",
      "Epoch: 6374 Training Loss: 0.15691473388671875 Test Loss: 0.41940494791666666\n",
      "Epoch: 6375 Training Loss: 0.15773109605577257 Test Loss: 0.42177983940972225\n",
      "Epoch: 6376 Training Loss: 0.15862196011013455 Test Loss: 0.4237373046875\n",
      "Epoch: 6377 Training Loss: 0.15953179423014324 Test Loss: 0.42463547092013887\n",
      "Epoch: 6378 Training Loss: 0.16040858289930557 Test Loss: 0.4239939236111111\n",
      "Epoch: 6379 Training Loss: 0.16117733256022135 Test Loss: 0.4211984592013889\n",
      "Epoch: 6380 Training Loss: 0.1616845228407118 Test Loss: 0.41637448459201387\n",
      "Epoch: 6381 Training Loss: 0.16176405334472657 Test Loss: 0.41131022135416667\n",
      "Epoch: 6382 Training Loss: 0.16135389200846353 Test Loss: 0.40741297743055555\n",
      "Epoch: 6383 Training Loss: 0.16052098761664496 Test Loss: 0.4055947536892361\n",
      "Epoch: 6384 Training Loss: 0.15942377387152779 Test Loss: 0.40560267469618055\n",
      "Epoch: 6385 Training Loss: 0.15825313652886286 Test Loss: 0.40678529188368057\n",
      "Epoch: 6386 Training Loss: 0.1571141153971354 Test Loss: 0.40825086805555555\n",
      "Epoch: 6387 Training Loss: 0.15607882012261284 Test Loss: 0.40930262586805555\n",
      "Epoch: 6388 Training Loss: 0.15521126302083332 Test Loss: 0.4099921332465278\n",
      "Epoch: 6389 Training Loss: 0.15453893195258248 Test Loss: 0.41049549696180554\n",
      "Epoch: 6390 Training Loss: 0.15408796861436633 Test Loss: 0.41147265625\n",
      "Epoch: 6391 Training Loss: 0.1539078403049045 Test Loss: 0.41282166883680554\n",
      "Epoch: 6392 Training Loss: 0.15394801669650607 Test Loss: 0.41461946614583334\n",
      "Epoch: 6393 Training Loss: 0.15420718214246962 Test Loss: 0.41672797309027776\n",
      "Epoch: 6394 Training Loss: 0.1546754930284288 Test Loss: 0.41838883463541665\n",
      "Epoch: 6395 Training Loss: 0.1553375973171658 Test Loss: 0.419705810546875\n",
      "Epoch: 6396 Training Loss: 0.15618412441677518 Test Loss: 0.42057199435763887\n",
      "Epoch: 6397 Training Loss: 0.15721371120876737 Test Loss: 0.42018728298611113\n",
      "Epoch: 6398 Training Loss: 0.15832425604926215 Test Loss: 0.41827937825520833\n",
      "Epoch: 6399 Training Loss: 0.15945823330349393 Test Loss: 0.41527745225694446\n",
      "Epoch: 6400 Training Loss: 0.16046167500813802 Test Loss: 0.4120967610677083\n",
      "Epoch: 6401 Training Loss: 0.16109576076931423 Test Loss: 0.40975591362847225\n",
      "Epoch: 6402 Training Loss: 0.16114464653862848 Test Loss: 0.4097679036458333\n",
      "Epoch: 6403 Training Loss: 0.1606400400797526 Test Loss: 0.41206239149305557\n",
      "Epoch: 6404 Training Loss: 0.1598227759467231 Test Loss: 0.4152774251302083\n",
      "Epoch: 6405 Training Loss: 0.1588846215142144 Test Loss: 0.41739561631944444\n",
      "Epoch: 6406 Training Loss: 0.1579977281358507 Test Loss: 0.417650634765625\n",
      "Epoch: 6407 Training Loss: 0.15727042812771266 Test Loss: 0.41617138671875\n",
      "Epoch: 6408 Training Loss: 0.15671181911892362 Test Loss: 0.41360441080729166\n",
      "Epoch: 6409 Training Loss: 0.15635213555230035 Test Loss: 0.4110590549045139\n",
      "Epoch: 6410 Training Loss: 0.15614466688368056 Test Loss: 0.40880088975694445\n",
      "Epoch: 6411 Training Loss: 0.1560707787407769 Test Loss: 0.4071859809027778\n",
      "Epoch: 6412 Training Loss: 0.1560975528293186 Test Loss: 0.4058308376736111\n",
      "Epoch: 6413 Training Loss: 0.15616827901204428 Test Loss: 0.40481141493055556\n",
      "Epoch: 6414 Training Loss: 0.15620337422688801 Test Loss: 0.4036717122395833\n",
      "Epoch: 6415 Training Loss: 0.15619640096028645 Test Loss: 0.40289854600694447\n",
      "Epoch: 6416 Training Loss: 0.15611583794487846 Test Loss: 0.40203995768229167\n",
      "Epoch: 6417 Training Loss: 0.1559174363878038 Test Loss: 0.40139366319444447\n",
      "Epoch: 6418 Training Loss: 0.15561525132921006 Test Loss: 0.40063392469618053\n",
      "Epoch: 6419 Training Loss: 0.15522415669759115 Test Loss: 0.40023177083333333\n",
      "Epoch: 6420 Training Loss: 0.1547397969563802 Test Loss: 0.39991666666666664\n",
      "Epoch: 6421 Training Loss: 0.15419219122992622 Test Loss: 0.3996906467013889\n",
      "Epoch: 6422 Training Loss: 0.15359934319390192 Test Loss: 0.39950572374131943\n",
      "Epoch: 6423 Training Loss: 0.1529895748562283 Test Loss: 0.39968844943576387\n",
      "Epoch: 6424 Training Loss: 0.1523847859700521 Test Loss: 0.39966775173611113\n",
      "Epoch: 6425 Training Loss: 0.15176932949490018 Test Loss: 0.39968421766493056\n",
      "Epoch: 6426 Training Loss: 0.15117042371961806 Test Loss: 0.399593017578125\n",
      "Epoch: 6427 Training Loss: 0.15058931477864584 Test Loss: 0.39954603407118056\n",
      "Epoch: 6428 Training Loss: 0.15003334214952257 Test Loss: 0.3994491644965278\n",
      "Epoch: 6429 Training Loss: 0.1495146213107639 Test Loss: 0.3994507649739583\n",
      "Epoch: 6430 Training Loss: 0.14902389695909288 Test Loss: 0.39950575086805556\n",
      "Epoch: 6431 Training Loss: 0.14857587687174478 Test Loss: 0.3996094292534722\n",
      "Epoch: 6432 Training Loss: 0.14816617329915366 Test Loss: 0.39979185655381944\n",
      "Epoch: 6433 Training Loss: 0.14779239230685765 Test Loss: 0.39991802300347223\n",
      "Epoch: 6434 Training Loss: 0.14745735677083333 Test Loss: 0.4002013888888889\n",
      "Epoch: 6435 Training Loss: 0.1471610327826606 Test Loss: 0.40039423285590275\n",
      "Epoch: 6436 Training Loss: 0.14688760375976562 Test Loss: 0.4006015625\n",
      "Epoch: 6437 Training Loss: 0.14664869350857204 Test Loss: 0.4008629557291667\n",
      "Epoch: 6438 Training Loss: 0.14643477206759983 Test Loss: 0.4011958550347222\n",
      "Epoch: 6439 Training Loss: 0.14623617892795138 Test Loss: 0.4014053819444444\n",
      "Epoch: 6440 Training Loss: 0.14605999755859375 Test Loss: 0.4016570095486111\n",
      "Epoch: 6441 Training Loss: 0.1459046359592014 Test Loss: 0.401942138671875\n",
      "Epoch: 6442 Training Loss: 0.14576902601453992 Test Loss: 0.4021119249131944\n",
      "Epoch: 6443 Training Loss: 0.14564517381456163 Test Loss: 0.40226633029513886\n",
      "Epoch: 6444 Training Loss: 0.14553330654568142 Test Loss: 0.4023967556423611\n",
      "Epoch: 6445 Training Loss: 0.1454349839952257 Test Loss: 0.40259944661458336\n",
      "Epoch: 6446 Training Loss: 0.14535028076171874 Test Loss: 0.4026211480034722\n",
      "Epoch: 6447 Training Loss: 0.14527884080674913 Test Loss: 0.4027609592013889\n",
      "Epoch: 6448 Training Loss: 0.14521490817599828 Test Loss: 0.40287828233506945\n",
      "Epoch: 6449 Training Loss: 0.14515764533148873 Test Loss: 0.40290570746527776\n",
      "Epoch: 6450 Training Loss: 0.1451052992078993 Test Loss: 0.4030933159722222\n",
      "Epoch: 6451 Training Loss: 0.14506139119466147 Test Loss: 0.403186767578125\n",
      "Epoch: 6452 Training Loss: 0.1450323486328125 Test Loss: 0.40324058702256943\n",
      "Epoch: 6453 Training Loss: 0.14501033359103732 Test Loss: 0.40329052734375\n",
      "Epoch: 6454 Training Loss: 0.14499878946940103 Test Loss: 0.40341069878472224\n",
      "Epoch: 6455 Training Loss: 0.14499397616916232 Test Loss: 0.4035459526909722\n",
      "Epoch: 6456 Training Loss: 0.14500416056315105 Test Loss: 0.40363045247395835\n",
      "Epoch: 6457 Training Loss: 0.14501511976453993 Test Loss: 0.40370372178819447\n",
      "Epoch: 6458 Training Loss: 0.14502523125542535 Test Loss: 0.4038019748263889\n",
      "Epoch: 6459 Training Loss: 0.14504615614149305 Test Loss: 0.40398589409722224\n",
      "Epoch: 6460 Training Loss: 0.1450676015218099 Test Loss: 0.4042268337673611\n",
      "Epoch: 6461 Training Loss: 0.14509021165635852 Test Loss: 0.4043807508680556\n",
      "Epoch: 6462 Training Loss: 0.14511101447211372 Test Loss: 0.4046155598958333\n",
      "Epoch: 6463 Training Loss: 0.1451404266357422 Test Loss: 0.4048481174045139\n",
      "Epoch: 6464 Training Loss: 0.14518222215440538 Test Loss: 0.40510609266493053\n",
      "Epoch: 6465 Training Loss: 0.14522638787163628 Test Loss: 0.4053112521701389\n",
      "Epoch: 6466 Training Loss: 0.14527886793348524 Test Loss: 0.4056156141493056\n",
      "Epoch: 6467 Training Loss: 0.1453317599826389 Test Loss: 0.40580862087673614\n",
      "Epoch: 6468 Training Loss: 0.1453861829969618 Test Loss: 0.4060689019097222\n",
      "Epoch: 6469 Training Loss: 0.14545760938856336 Test Loss: 0.40628917100694445\n",
      "Epoch: 6470 Training Loss: 0.14553731960720487 Test Loss: 0.40662201605902776\n",
      "Epoch: 6471 Training Loss: 0.14562587483723957 Test Loss: 0.4069358995225694\n",
      "Epoch: 6472 Training Loss: 0.1457157965766059 Test Loss: 0.40720692274305553\n",
      "Epoch: 6473 Training Loss: 0.14583121236165364 Test Loss: 0.4077084689670139\n",
      "Epoch: 6474 Training Loss: 0.14595752800835504 Test Loss: 0.4081316731770833\n",
      "Epoch: 6475 Training Loss: 0.14609606085883248 Test Loss: 0.40874723307291666\n",
      "Epoch: 6476 Training Loss: 0.14625427754720052 Test Loss: 0.40932166883680554\n",
      "Epoch: 6477 Training Loss: 0.14643423461914062 Test Loss: 0.41003510199652776\n",
      "Epoch: 6478 Training Loss: 0.1466302185058594 Test Loss: 0.4107351888020833\n",
      "Epoch: 6479 Training Loss: 0.14684214952256944 Test Loss: 0.41145467122395835\n",
      "Epoch: 6480 Training Loss: 0.14708543226453993 Test Loss: 0.4122753363715278\n",
      "Epoch: 6481 Training Loss: 0.14735124376085068 Test Loss: 0.4128642578125\n",
      "Epoch: 6482 Training Loss: 0.14764540100097656 Test Loss: 0.41348719618055557\n",
      "Epoch: 6483 Training Loss: 0.14795779418945312 Test Loss: 0.4140774197048611\n",
      "Epoch: 6484 Training Loss: 0.14828537156846788 Test Loss: 0.4147658420138889\n",
      "Epoch: 6485 Training Loss: 0.14863349236382378 Test Loss: 0.4160129665798611\n",
      "Epoch: 6486 Training Loss: 0.14901049296061197 Test Loss: 0.41793123372395835\n",
      "Epoch: 6487 Training Loss: 0.14944511583116318 Test Loss: 0.42055088975694443\n",
      "Epoch: 6488 Training Loss: 0.14997803412543403 Test Loss: 0.4226123046875\n",
      "Epoch: 6489 Training Loss: 0.15058392503526477 Test Loss: 0.42397509765625\n",
      "Epoch: 6490 Training Loss: 0.1512470482720269 Test Loss: 0.42347900390625\n",
      "Epoch: 6491 Training Loss: 0.15193180677625867 Test Loss: 0.4221315104166667\n",
      "Epoch: 6492 Training Loss: 0.15265686882866752 Test Loss: 0.4214867892795139\n",
      "Epoch: 6493 Training Loss: 0.15339775763617622 Test Loss: 0.4226006944444444\n",
      "Epoch: 6494 Training Loss: 0.1541702202690972 Test Loss: 0.42490554470486114\n",
      "Epoch: 6495 Training Loss: 0.15501968722873263 Test Loss: 0.4264419216579861\n",
      "Epoch: 6496 Training Loss: 0.15591966417100694 Test Loss: 0.42613202582465276\n",
      "Epoch: 6497 Training Loss: 0.15683618333604601 Test Loss: 0.4254472113715278\n",
      "Epoch: 6498 Training Loss: 0.1578357883029514 Test Loss: 0.4237209743923611\n",
      "Epoch: 6499 Training Loss: 0.15887569003634983 Test Loss: 0.4202660590277778\n",
      "Epoch: 6500 Training Loss: 0.15984239535861544 Test Loss: 0.41564637586805553\n",
      "Epoch: 6501 Training Loss: 0.16054485405815971 Test Loss: 0.4111802842881944\n",
      "Epoch: 6502 Training Loss: 0.16075835503472222 Test Loss: 0.40878955078125\n",
      "Epoch: 6503 Training Loss: 0.16032430691189237 Test Loss: 0.40843565538194443\n",
      "Epoch: 6504 Training Loss: 0.1592051018608941 Test Loss: 0.40920453559027775\n",
      "Epoch: 6505 Training Loss: 0.15765144517686633 Test Loss: 0.40921261935763886\n",
      "Epoch: 6506 Training Loss: 0.15590899319118923 Test Loss: 0.40878472222222223\n",
      "Epoch: 6507 Training Loss: 0.1541842041015625 Test Loss: 0.4071904296875\n",
      "Epoch: 6508 Training Loss: 0.15255164591471354 Test Loss: 0.40533951822916664\n",
      "Epoch: 6509 Training Loss: 0.15106246439615886 Test Loss: 0.4035062662760417\n",
      "Epoch: 6510 Training Loss: 0.1497540774875217 Test Loss: 0.4022057291666667\n",
      "Epoch: 6511 Training Loss: 0.14863009304470487 Test Loss: 0.4012082790798611\n",
      "Epoch: 6512 Training Loss: 0.14771381123860677 Test Loss: 0.4006955837673611\n",
      "Epoch: 6513 Training Loss: 0.1469609188503689 Test Loss: 0.40027091471354165\n",
      "Epoch: 6514 Training Loss: 0.14635743882921007 Test Loss: 0.40006363932291666\n",
      "Epoch: 6515 Training Loss: 0.14588553534613716 Test Loss: 0.3998984375\n",
      "Epoch: 6516 Training Loss: 0.14551012844509548 Test Loss: 0.39980523003472224\n",
      "Epoch: 6517 Training Loss: 0.14521153089735242 Test Loss: 0.3998449978298611\n",
      "Epoch: 6518 Training Loss: 0.14497641499837238 Test Loss: 0.3999196506076389\n",
      "Epoch: 6519 Training Loss: 0.14478220452202692 Test Loss: 0.40005490451388886\n",
      "Epoch: 6520 Training Loss: 0.1446304711235894 Test Loss: 0.40023665364583333\n",
      "Epoch: 6521 Training Loss: 0.14451251898871528 Test Loss: 0.40042686631944446\n",
      "Epoch: 6522 Training Loss: 0.14441976759168837 Test Loss: 0.4006846788194444\n",
      "Epoch: 6523 Training Loss: 0.14435110473632812 Test Loss: 0.4010217013888889\n",
      "Epoch: 6524 Training Loss: 0.14428816731770833 Test Loss: 0.4011928168402778\n",
      "Epoch: 6525 Training Loss: 0.14423605685763888 Test Loss: 0.4014823676215278\n",
      "Epoch: 6526 Training Loss: 0.14419033983018664 Test Loss: 0.4015093044704861\n",
      "Epoch: 6527 Training Loss: 0.14415189107259116 Test Loss: 0.4017384711371528\n",
      "Epoch: 6528 Training Loss: 0.1441152835422092 Test Loss: 0.4019672580295139\n",
      "Epoch: 6529 Training Loss: 0.14408311631944445 Test Loss: 0.402080078125\n",
      "Epoch: 6530 Training Loss: 0.1440528089735243 Test Loss: 0.40208167860243055\n",
      "Epoch: 6531 Training Loss: 0.14402698940700956 Test Loss: 0.4021002061631944\n",
      "Epoch: 6532 Training Loss: 0.1440076887342665 Test Loss: 0.4021118706597222\n",
      "Epoch: 6533 Training Loss: 0.14398994954427083 Test Loss: 0.40218180338541665\n",
      "Epoch: 6534 Training Loss: 0.1439785885281033 Test Loss: 0.402280029296875\n",
      "Epoch: 6535 Training Loss: 0.14396384853786892 Test Loss: 0.402255859375\n",
      "Epoch: 6536 Training Loss: 0.14395087009006077 Test Loss: 0.40227430555555554\n",
      "Epoch: 6537 Training Loss: 0.1439395005967882 Test Loss: 0.4023164333767361\n",
      "Epoch: 6538 Training Loss: 0.14393290371365017 Test Loss: 0.40233509657118055\n",
      "Epoch: 6539 Training Loss: 0.14393184238009982 Test Loss: 0.4025151909722222\n",
      "Epoch: 6540 Training Loss: 0.14393378702799478 Test Loss: 0.40250618489583334\n",
      "Epoch: 6541 Training Loss: 0.1439510006374783 Test Loss: 0.402642578125\n",
      "Epoch: 6542 Training Loss: 0.14396653917100694 Test Loss: 0.4027884928385417\n",
      "Epoch: 6543 Training Loss: 0.14398314921061198 Test Loss: 0.40301760525173613\n",
      "Epoch: 6544 Training Loss: 0.14400884840223524 Test Loss: 0.40319227430555554\n",
      "Epoch: 6545 Training Loss: 0.14403412373860677 Test Loss: 0.4035896267361111\n",
      "Epoch: 6546 Training Loss: 0.14406930372450086 Test Loss: 0.4038357747395833\n",
      "Epoch: 6547 Training Loss: 0.14411533610026042 Test Loss: 0.40414027235243055\n",
      "Epoch: 6548 Training Loss: 0.14416822984483507 Test Loss: 0.40422111002604166\n",
      "Epoch: 6549 Training Loss: 0.14423012118869358 Test Loss: 0.40456217447916665\n",
      "Epoch: 6550 Training Loss: 0.14429876369900174 Test Loss: 0.4048613009982639\n",
      "Epoch: 6551 Training Loss: 0.14438007269965278 Test Loss: 0.40525472005208335\n",
      "Epoch: 6552 Training Loss: 0.1444685567220052 Test Loss: 0.40547862413194447\n",
      "Epoch: 6553 Training Loss: 0.14456509568956163 Test Loss: 0.4058012152777778\n",
      "Epoch: 6554 Training Loss: 0.14467611355251736 Test Loss: 0.4060596245659722\n",
      "Epoch: 6555 Training Loss: 0.14479733276367188 Test Loss: 0.4062970648871528\n",
      "Epoch: 6556 Training Loss: 0.14493656582302517 Test Loss: 0.4067548828125\n",
      "Epoch: 6557 Training Loss: 0.14509476216634115 Test Loss: 0.4069898003472222\n",
      "Epoch: 6558 Training Loss: 0.1452808583577474 Test Loss: 0.4070703125\n",
      "Epoch: 6559 Training Loss: 0.14548528374565972 Test Loss: 0.40695925564236113\n",
      "Epoch: 6560 Training Loss: 0.145708497789171 Test Loss: 0.4068640950520833\n",
      "Epoch: 6561 Training Loss: 0.14595575290256077 Test Loss: 0.40676513671875\n",
      "Epoch: 6562 Training Loss: 0.14623258972167968 Test Loss: 0.4064573296440972\n",
      "Epoch: 6563 Training Loss: 0.14654413350423176 Test Loss: 0.40596988932291667\n",
      "Epoch: 6564 Training Loss: 0.14689738294813368 Test Loss: 0.4056210394965278\n",
      "Epoch: 6565 Training Loss: 0.1472830861409505 Test Loss: 0.4052433810763889\n",
      "Epoch: 6566 Training Loss: 0.1477139638264974 Test Loss: 0.4047937825520833\n",
      "Epoch: 6567 Training Loss: 0.14817045254177516 Test Loss: 0.40451600477430555\n",
      "Epoch: 6568 Training Loss: 0.14865833706325954 Test Loss: 0.4045705837673611\n",
      "Epoch: 6569 Training Loss: 0.14918491617838542 Test Loss: 0.40504947916666667\n",
      "Epoch: 6570 Training Loss: 0.14972367350260415 Test Loss: 0.40629918077256943\n",
      "Epoch: 6571 Training Loss: 0.150283450656467 Test Loss: 0.4079827202690972\n",
      "Epoch: 6572 Training Loss: 0.15085093688964843 Test Loss: 0.4106899956597222\n",
      "Epoch: 6573 Training Loss: 0.1514253913031684 Test Loss: 0.4137506781684028\n",
      "Epoch: 6574 Training Loss: 0.15197279696994356 Test Loss: 0.416364013671875\n",
      "Epoch: 6575 Training Loss: 0.1524398922390408 Test Loss: 0.41778716362847224\n",
      "Epoch: 6576 Training Loss: 0.1527706536187066 Test Loss: 0.41763321940104164\n",
      "Epoch: 6577 Training Loss: 0.1528873291015625 Test Loss: 0.41538319227430553\n",
      "Epoch: 6578 Training Loss: 0.1527356177435981 Test Loss: 0.4121279839409722\n",
      "Epoch: 6579 Training Loss: 0.15231744893391927 Test Loss: 0.40881629774305556\n",
      "Epoch: 6580 Training Loss: 0.15165468173556856 Test Loss: 0.40587906901041665\n",
      "Epoch: 6581 Training Loss: 0.15086539204915364 Test Loss: 0.40447447374131945\n",
      "Epoch: 6582 Training Loss: 0.15008387247721355 Test Loss: 0.4038376736111111\n",
      "Epoch: 6583 Training Loss: 0.14936385769314237 Test Loss: 0.40434312608506945\n",
      "Epoch: 6584 Training Loss: 0.14878446112738716 Test Loss: 0.40511366102430557\n",
      "Epoch: 6585 Training Loss: 0.1483763156467014 Test Loss: 0.40610069444444447\n",
      "Epoch: 6586 Training Loss: 0.14811199612087675 Test Loss: 0.4066484646267361\n",
      "Epoch: 6587 Training Loss: 0.1479646182590061 Test Loss: 0.4070876193576389\n",
      "Epoch: 6588 Training Loss: 0.1479327155219184 Test Loss: 0.4069699435763889\n",
      "Epoch: 6589 Training Loss: 0.14798082817925348 Test Loss: 0.4068729654947917\n",
      "Epoch: 6590 Training Loss: 0.14809192572699653 Test Loss: 0.40647347005208334\n",
      "Epoch: 6591 Training Loss: 0.14824857754177517 Test Loss: 0.4059254286024306\n",
      "Epoch: 6592 Training Loss: 0.14844894409179688 Test Loss: 0.405559326171875\n",
      "Epoch: 6593 Training Loss: 0.1486798146565755 Test Loss: 0.4053917643229167\n",
      "Epoch: 6594 Training Loss: 0.14894283548990886 Test Loss: 0.40487830946180553\n",
      "Epoch: 6595 Training Loss: 0.14923531256781683 Test Loss: 0.40421332465277776\n",
      "Epoch: 6596 Training Loss: 0.14954767354329426 Test Loss: 0.40362353515625\n",
      "Epoch: 6597 Training Loss: 0.14985766262478298 Test Loss: 0.4027434353298611\n",
      "Epoch: 6598 Training Loss: 0.150166263156467 Test Loss: 0.40201524522569443\n",
      "Epoch: 6599 Training Loss: 0.1504300096299913 Test Loss: 0.40170464409722223\n",
      "Epoch: 6600 Training Loss: 0.15064295450846354 Test Loss: 0.40236490885416665\n",
      "Epoch: 6601 Training Loss: 0.15081316121419272 Test Loss: 0.40369292534722223\n",
      "Epoch: 6602 Training Loss: 0.1509076453314887 Test Loss: 0.405458984375\n",
      "Epoch: 6603 Training Loss: 0.15092250569661458 Test Loss: 0.4074361979166667\n",
      "Epoch: 6604 Training Loss: 0.15086535135904947 Test Loss: 0.4091591525607639\n",
      "Epoch: 6605 Training Loss: 0.15072407701280383 Test Loss: 0.41063427734375\n",
      "Epoch: 6606 Training Loss: 0.1505364702012804 Test Loss: 0.41111653645833335\n",
      "Epoch: 6607 Training Loss: 0.1503198750813802 Test Loss: 0.41078762478298614\n",
      "Epoch: 6608 Training Loss: 0.1500312228732639 Test Loss: 0.40966129557291664\n",
      "Epoch: 6609 Training Loss: 0.14968292914496528 Test Loss: 0.40840131293402776\n",
      "Epoch: 6610 Training Loss: 0.149289057413737 Test Loss: 0.40714990234375\n",
      "Epoch: 6611 Training Loss: 0.1488673061794705 Test Loss: 0.4065586208767361\n",
      "Epoch: 6612 Training Loss: 0.148431884765625 Test Loss: 0.4063950737847222\n",
      "Epoch: 6613 Training Loss: 0.14800381639268664 Test Loss: 0.4063986002604167\n",
      "Epoch: 6614 Training Loss: 0.14762137688530816 Test Loss: 0.4066541069878472\n",
      "Epoch: 6615 Training Loss: 0.14729252794053818 Test Loss: 0.4065383029513889\n",
      "Epoch: 6616 Training Loss: 0.14698272026909723 Test Loss: 0.4063801540798611\n",
      "Epoch: 6617 Training Loss: 0.14672918701171875 Test Loss: 0.4064631618923611\n",
      "Epoch: 6618 Training Loss: 0.14649083285861544 Test Loss: 0.4066697591145833\n",
      "Epoch: 6619 Training Loss: 0.14627137247721353 Test Loss: 0.4069697265625\n",
      "Epoch: 6620 Training Loss: 0.14607304212782118 Test Loss: 0.4072246636284722\n",
      "Epoch: 6621 Training Loss: 0.14589333767361112 Test Loss: 0.4073661838107639\n",
      "Epoch: 6622 Training Loss: 0.14573660447862413 Test Loss: 0.4074071723090278\n",
      "Epoch: 6623 Training Loss: 0.14560166761610244 Test Loss: 0.40742469618055555\n",
      "Epoch: 6624 Training Loss: 0.1454893781873915 Test Loss: 0.407231689453125\n",
      "Epoch: 6625 Training Loss: 0.14539205932617189 Test Loss: 0.40694954427083335\n",
      "Epoch: 6626 Training Loss: 0.14530440606011286 Test Loss: 0.40657638888888886\n",
      "Epoch: 6627 Training Loss: 0.14522289360894097 Test Loss: 0.40616205512152775\n",
      "Epoch: 6628 Training Loss: 0.14515994432237414 Test Loss: 0.40571321614583333\n",
      "Epoch: 6629 Training Loss: 0.14511971367730034 Test Loss: 0.40526722547743055\n",
      "Epoch: 6630 Training Loss: 0.14509110514322918 Test Loss: 0.4048589138454861\n",
      "Epoch: 6631 Training Loss: 0.1450701649983724 Test Loss: 0.4043708224826389\n",
      "Epoch: 6632 Training Loss: 0.14506629604763455 Test Loss: 0.40405189344618053\n",
      "Epoch: 6633 Training Loss: 0.1450821058485243 Test Loss: 0.4038340386284722\n",
      "Epoch: 6634 Training Loss: 0.14511276584201388 Test Loss: 0.40373253038194445\n",
      "Epoch: 6635 Training Loss: 0.14515446302625867 Test Loss: 0.4036796603732639\n",
      "Epoch: 6636 Training Loss: 0.1452117445203993 Test Loss: 0.40382413736979167\n",
      "Epoch: 6637 Training Loss: 0.14529166836208768 Test Loss: 0.4039419759114583\n",
      "Epoch: 6638 Training Loss: 0.1453927730984158 Test Loss: 0.40441438802083335\n",
      "Epoch: 6639 Training Loss: 0.14551443820529514 Test Loss: 0.40485080295138887\n",
      "Epoch: 6640 Training Loss: 0.14566262817382813 Test Loss: 0.40531076388888887\n",
      "Epoch: 6641 Training Loss: 0.1458299594455295 Test Loss: 0.4057579752604167\n",
      "Epoch: 6642 Training Loss: 0.14602538723415798 Test Loss: 0.4061494140625\n",
      "Epoch: 6643 Training Loss: 0.14624610392252604 Test Loss: 0.40665223524305555\n",
      "Epoch: 6644 Training Loss: 0.14648472595214843 Test Loss: 0.40687727864583334\n",
      "Epoch: 6645 Training Loss: 0.14675828382703993 Test Loss: 0.40718435329861113\n",
      "Epoch: 6646 Training Loss: 0.1470611555311415 Test Loss: 0.40737120225694445\n",
      "Epoch: 6647 Training Loss: 0.14738114081488715 Test Loss: 0.4077307400173611\n",
      "Epoch: 6648 Training Loss: 0.14770900302463108 Test Loss: 0.40829649522569444\n",
      "Epoch: 6649 Training Loss: 0.1480449981689453 Test Loss: 0.40886192491319445\n",
      "Epoch: 6650 Training Loss: 0.1483965572781033 Test Loss: 0.4097644585503472\n",
      "Epoch: 6651 Training Loss: 0.14878156365288628 Test Loss: 0.4106145290798611\n",
      "Epoch: 6652 Training Loss: 0.14918537055121528 Test Loss: 0.4111252712673611\n",
      "Epoch: 6653 Training Loss: 0.14960369873046875 Test Loss: 0.41202487521701386\n",
      "Epoch: 6654 Training Loss: 0.15004210069444446 Test Loss: 0.4126375054253472\n",
      "Epoch: 6655 Training Loss: 0.15050330437554255 Test Loss: 0.4130398220486111\n",
      "Epoch: 6656 Training Loss: 0.15095835876464844 Test Loss: 0.41338416883680557\n",
      "Epoch: 6657 Training Loss: 0.15139971923828124 Test Loss: 0.4133343098958333\n",
      "Epoch: 6658 Training Loss: 0.15180926513671875 Test Loss: 0.413046142578125\n",
      "Epoch: 6659 Training Loss: 0.1521753421359592 Test Loss: 0.41300691731770833\n",
      "Epoch: 6660 Training Loss: 0.152448243882921 Test Loss: 0.4127135416666667\n",
      "Epoch: 6661 Training Loss: 0.15263776143391927 Test Loss: 0.4128748101128472\n",
      "Epoch: 6662 Training Loss: 0.15271907721625433 Test Loss: 0.4131689453125\n",
      "Epoch: 6663 Training Loss: 0.1527293667263455 Test Loss: 0.4143774956597222\n",
      "Epoch: 6664 Training Loss: 0.15271385701497395 Test Loss: 0.41627175564236113\n",
      "Epoch: 6665 Training Loss: 0.15268746948242187 Test Loss: 0.41864594184027776\n",
      "Epoch: 6666 Training Loss: 0.15263786146375868 Test Loss: 0.4209391818576389\n",
      "Epoch: 6667 Training Loss: 0.1525481194390191 Test Loss: 0.4227635091145833\n",
      "Epoch: 6668 Training Loss: 0.15240263875325522 Test Loss: 0.4239519314236111\n",
      "Epoch: 6669 Training Loss: 0.1521958228217231 Test Loss: 0.4243242458767361\n",
      "Epoch: 6670 Training Loss: 0.15194992404513888 Test Loss: 0.42373643663194444\n",
      "Epoch: 6671 Training Loss: 0.15162681240505643 Test Loss: 0.4227555881076389\n",
      "Epoch: 6672 Training Loss: 0.15121168687608508 Test Loss: 0.4213324110243056\n",
      "Epoch: 6673 Training Loss: 0.15073170979817707 Test Loss: 0.4198191189236111\n",
      "Epoch: 6674 Training Loss: 0.15024764675564237 Test Loss: 0.41901917860243054\n",
      "Epoch: 6675 Training Loss: 0.14978965081108941 Test Loss: 0.4189299587673611\n",
      "Epoch: 6676 Training Loss: 0.14935840521918403 Test Loss: 0.41855425347222225\n",
      "Epoch: 6677 Training Loss: 0.14898147413465712 Test Loss: 0.4184050021701389\n",
      "Epoch: 6678 Training Loss: 0.14866622416178385 Test Loss: 0.4184121636284722\n",
      "Epoch: 6679 Training Loss: 0.14842638312445747 Test Loss: 0.41848763020833335\n",
      "Epoch: 6680 Training Loss: 0.1482506832546658 Test Loss: 0.4184246148003472\n",
      "Epoch: 6681 Training Loss: 0.1481444346110026 Test Loss: 0.41826652018229166\n",
      "Epoch: 6682 Training Loss: 0.14812791273328993 Test Loss: 0.41796888563368056\n",
      "Epoch: 6683 Training Loss: 0.14814705234103734 Test Loss: 0.4177966579861111\n",
      "Epoch: 6684 Training Loss: 0.14821417744954427 Test Loss: 0.41795855034722224\n",
      "Epoch: 6685 Training Loss: 0.1483318820529514 Test Loss: 0.4178460828993056\n",
      "Epoch: 6686 Training Loss: 0.1484751976860894 Test Loss: 0.4179053276909722\n",
      "Epoch: 6687 Training Loss: 0.14864809163411458 Test Loss: 0.4181604817708333\n",
      "Epoch: 6688 Training Loss: 0.14884346347384983 Test Loss: 0.41810270182291664\n",
      "Epoch: 6689 Training Loss: 0.14901632520887587 Test Loss: 0.4174153103298611\n",
      "Epoch: 6690 Training Loss: 0.14918067593044704 Test Loss: 0.41636360677083334\n",
      "Epoch: 6691 Training Loss: 0.14934061347113714 Test Loss: 0.4152260199652778\n",
      "Epoch: 6692 Training Loss: 0.1494912346733941 Test Loss: 0.4135230577256944\n",
      "Epoch: 6693 Training Loss: 0.14960960896809897 Test Loss: 0.41160205078125\n",
      "Epoch: 6694 Training Loss: 0.1497139180501302 Test Loss: 0.40978184678819446\n",
      "Epoch: 6695 Training Loss: 0.1497993672688802 Test Loss: 0.40800390625\n",
      "Epoch: 6696 Training Loss: 0.149855222913954 Test Loss: 0.4064608832465278\n",
      "Epoch: 6697 Training Loss: 0.1498929697672526 Test Loss: 0.405345947265625\n",
      "Epoch: 6698 Training Loss: 0.14991788567437067 Test Loss: 0.40498866102430553\n",
      "Epoch: 6699 Training Loss: 0.14989418707953558 Test Loss: 0.4049041341145833\n",
      "Epoch: 6700 Training Loss: 0.1498152058919271 Test Loss: 0.40545366753472223\n",
      "Epoch: 6701 Training Loss: 0.14968481106228299 Test Loss: 0.40634608289930557\n",
      "Epoch: 6702 Training Loss: 0.14953335910373264 Test Loss: 0.40758824327256943\n",
      "Epoch: 6703 Training Loss: 0.14933888075086807 Test Loss: 0.40852967664930556\n",
      "Epoch: 6704 Training Loss: 0.14910492282443577 Test Loss: 0.40956212022569444\n",
      "Epoch: 6705 Training Loss: 0.14886560736762153 Test Loss: 0.4098644476996528\n",
      "Epoch: 6706 Training Loss: 0.14861945597330728 Test Loss: 0.4095254448784722\n",
      "Epoch: 6707 Training Loss: 0.14836172824435764 Test Loss: 0.40875718858506943\n",
      "Epoch: 6708 Training Loss: 0.1481092800564236 Test Loss: 0.4076308322482639\n",
      "Epoch: 6709 Training Loss: 0.1478490753173828 Test Loss: 0.40631608072916664\n",
      "Epoch: 6710 Training Loss: 0.14760147433810764 Test Loss: 0.4051748589409722\n",
      "Epoch: 6711 Training Loss: 0.14737006971571182 Test Loss: 0.4041636284722222\n",
      "Epoch: 6712 Training Loss: 0.14717232937282987 Test Loss: 0.40383924696180556\n",
      "Epoch: 6713 Training Loss: 0.14700301954481337 Test Loss: 0.40385750325520836\n",
      "Epoch: 6714 Training Loss: 0.14687973022460937 Test Loss: 0.4039452311197917\n",
      "Epoch: 6715 Training Loss: 0.1467749515109592 Test Loss: 0.4046337619357639\n",
      "Epoch: 6716 Training Loss: 0.14673298136393229 Test Loss: 0.4054906955295139\n",
      "Epoch: 6717 Training Loss: 0.14673821682400173 Test Loss: 0.4066111111111111\n",
      "Epoch: 6718 Training Loss: 0.14678055148654515 Test Loss: 0.40760465494791664\n",
      "Epoch: 6719 Training Loss: 0.14685761515299478 Test Loss: 0.40852880859375\n",
      "Epoch: 6720 Training Loss: 0.14697015380859374 Test Loss: 0.4092368706597222\n",
      "Epoch: 6721 Training Loss: 0.14711053466796875 Test Loss: 0.40940606011284725\n",
      "Epoch: 6722 Training Loss: 0.1472934332953559 Test Loss: 0.4089218207465278\n",
      "Epoch: 6723 Training Loss: 0.14750066630045572 Test Loss: 0.4081330837673611\n",
      "Epoch: 6724 Training Loss: 0.14775676812065971 Test Loss: 0.40715733506944446\n",
      "Epoch: 6725 Training Loss: 0.14804032389322916 Test Loss: 0.4061692708333333\n",
      "Epoch: 6726 Training Loss: 0.14836851331922743 Test Loss: 0.40543104383680556\n",
      "Epoch: 6727 Training Loss: 0.14872650824652778 Test Loss: 0.40521761067708334\n",
      "Epoch: 6728 Training Loss: 0.14912532721625435 Test Loss: 0.405715576171875\n",
      "Epoch: 6729 Training Loss: 0.14955157131618924 Test Loss: 0.4071299099392361\n",
      "Epoch: 6730 Training Loss: 0.1500279998779297 Test Loss: 0.41004454210069446\n",
      "Epoch: 6731 Training Loss: 0.15054596794976127 Test Loss: 0.4138098415798611\n",
      "Epoch: 6732 Training Loss: 0.15107801310221355 Test Loss: 0.4157980143229167\n",
      "Epoch: 6733 Training Loss: 0.15154153611924914 Test Loss: 0.4144992133246528\n",
      "Epoch: 6734 Training Loss: 0.1517432081434462 Test Loss: 0.41261509874131946\n",
      "Epoch: 6735 Training Loss: 0.15163286844889323 Test Loss: 0.4140188259548611\n",
      "Epoch: 6736 Training Loss: 0.15125371636284723 Test Loss: 0.4160262044270833\n",
      "Epoch: 6737 Training Loss: 0.15071232096354167 Test Loss: 0.4118632269965278\n",
      "Epoch: 6738 Training Loss: 0.15004729885525173 Test Loss: 0.40726443142361113\n",
      "Epoch: 6739 Training Loss: 0.14936493428548178 Test Loss: 0.40776180013020835\n",
      "Epoch: 6740 Training Loss: 0.14877774895562065 Test Loss: 0.4083239474826389\n",
      "Epoch: 6741 Training Loss: 0.1482788560655382 Test Loss: 0.40684602864583336\n",
      "Epoch: 6742 Training Loss: 0.14784670681423612 Test Loss: 0.4053416069878472\n",
      "Epoch: 6743 Training Loss: 0.14747613694932726 Test Loss: 0.40403065321180553\n",
      "Epoch: 6744 Training Loss: 0.14716264173719618 Test Loss: 0.4031194118923611\n",
      "Epoch: 6745 Training Loss: 0.14688671366373698 Test Loss: 0.4022659233940972\n",
      "Epoch: 6746 Training Loss: 0.14665208943684896 Test Loss: 0.4015070529513889\n",
      "Epoch: 6747 Training Loss: 0.1464713592529297 Test Loss: 0.40082820638020833\n",
      "Epoch: 6748 Training Loss: 0.14634449937608507 Test Loss: 0.4003503689236111\n",
      "Epoch: 6749 Training Loss: 0.14626080830891927 Test Loss: 0.3998532443576389\n",
      "Epoch: 6750 Training Loss: 0.14622992621527778 Test Loss: 0.3996669650607639\n",
      "Epoch: 6751 Training Loss: 0.1462369367811415 Test Loss: 0.39954117838541664\n",
      "Epoch: 6752 Training Loss: 0.14627586873372395 Test Loss: 0.39960915798611113\n",
      "Epoch: 6753 Training Loss: 0.14635948011610242 Test Loss: 0.39978038194444443\n",
      "Epoch: 6754 Training Loss: 0.14648980034722223 Test Loss: 0.39987673611111113\n",
      "Epoch: 6755 Training Loss: 0.14665326097276477 Test Loss: 0.40015662977430555\n",
      "Epoch: 6756 Training Loss: 0.1468702884250217 Test Loss: 0.4003993598090278\n",
      "Epoch: 6757 Training Loss: 0.1471210411919488 Test Loss: 0.4006641438802083\n",
      "Epoch: 6758 Training Loss: 0.14741381496853298 Test Loss: 0.40116031901041665\n",
      "Epoch: 6759 Training Loss: 0.14775336541069878 Test Loss: 0.40167274305555556\n",
      "Epoch: 6760 Training Loss: 0.14813003540039063 Test Loss: 0.4024599609375\n",
      "Epoch: 6761 Training Loss: 0.1485111846923828 Test Loss: 0.40325244140625\n",
      "Epoch: 6762 Training Loss: 0.148892579820421 Test Loss: 0.4043395724826389\n",
      "Epoch: 6763 Training Loss: 0.14927271016438803 Test Loss: 0.4053879123263889\n",
      "Epoch: 6764 Training Loss: 0.14965450710720485 Test Loss: 0.40620201280381946\n",
      "Epoch: 6765 Training Loss: 0.1500177968343099 Test Loss: 0.40679280598958334\n",
      "Epoch: 6766 Training Loss: 0.15036533949110242 Test Loss: 0.40713612196180554\n",
      "Epoch: 6767 Training Loss: 0.15068145073784722 Test Loss: 0.4070804036458333\n",
      "Epoch: 6768 Training Loss: 0.15099484252929687 Test Loss: 0.4067169867621528\n",
      "Epoch: 6769 Training Loss: 0.1512633022732205 Test Loss: 0.40601226128472223\n",
      "Epoch: 6770 Training Loss: 0.1514446004231771 Test Loss: 0.40537109375\n",
      "Epoch: 6771 Training Loss: 0.15152323744032117 Test Loss: 0.40498763020833334\n",
      "Epoch: 6772 Training Loss: 0.15152949863009982 Test Loss: 0.40461165364583335\n",
      "Epoch: 6773 Training Loss: 0.15149316575792102 Test Loss: 0.40453241644965277\n",
      "Epoch: 6774 Training Loss: 0.1514257066514757 Test Loss: 0.4048932562934028\n",
      "Epoch: 6775 Training Loss: 0.15132918972439235 Test Loss: 0.405740478515625\n",
      "Epoch: 6776 Training Loss: 0.15123831854926215 Test Loss: 0.40655699327256944\n",
      "Epoch: 6777 Training Loss: 0.15114463636610243 Test Loss: 0.40697054036458336\n",
      "Epoch: 6778 Training Loss: 0.15102618916829427 Test Loss: 0.4070478515625\n",
      "Epoch: 6779 Training Loss: 0.15084821065266926 Test Loss: 0.40638671875\n",
      "Epoch: 6780 Training Loss: 0.15059021335177952 Test Loss: 0.40560926649305556\n",
      "Epoch: 6781 Training Loss: 0.15025659688313803 Test Loss: 0.4047509494357639\n",
      "Epoch: 6782 Training Loss: 0.14985271708170572 Test Loss: 0.40417711046006943\n",
      "Epoch: 6783 Training Loss: 0.14939713711208769 Test Loss: 0.40397835286458333\n",
      "Epoch: 6784 Training Loss: 0.14892165798611112 Test Loss: 0.40398708767361113\n",
      "Epoch: 6785 Training Loss: 0.14841404554578994 Test Loss: 0.40389363606770834\n",
      "Epoch: 6786 Training Loss: 0.14788807678222657 Test Loss: 0.4039386393229167\n",
      "Epoch: 6787 Training Loss: 0.1473846723768446 Test Loss: 0.40370285373263887\n",
      "Epoch: 6788 Training Loss: 0.14691242133246527 Test Loss: 0.40311105685763887\n",
      "Epoch: 6789 Training Loss: 0.14647383626302082 Test Loss: 0.40234388563368056\n",
      "Epoch: 6790 Training Loss: 0.14608056301540798 Test Loss: 0.40141867404513887\n",
      "Epoch: 6791 Training Loss: 0.14572740851508245 Test Loss: 0.4005909830729167\n",
      "Epoch: 6792 Training Loss: 0.14539932929144966 Test Loss: 0.3998812934027778\n",
      "Epoch: 6793 Training Loss: 0.145093020968967 Test Loss: 0.39924837239583333\n",
      "Epoch: 6794 Training Loss: 0.1448009796142578 Test Loss: 0.39868638780381943\n",
      "Epoch: 6795 Training Loss: 0.14453644476996527 Test Loss: 0.398334228515625\n",
      "Epoch: 6796 Training Loss: 0.14427206081814237 Test Loss: 0.3982112087673611\n",
      "Epoch: 6797 Training Loss: 0.14402025180392794 Test Loss: 0.39810904947916664\n",
      "Epoch: 6798 Training Loss: 0.1437759297688802 Test Loss: 0.3983072916666667\n",
      "Epoch: 6799 Training Loss: 0.14353621758355034 Test Loss: 0.3985703396267361\n",
      "Epoch: 6800 Training Loss: 0.14331661309136284 Test Loss: 0.398925048828125\n",
      "Epoch: 6801 Training Loss: 0.14311607699924045 Test Loss: 0.399291259765625\n",
      "Epoch: 6802 Training Loss: 0.14293223995632595 Test Loss: 0.3996682671440972\n",
      "Epoch: 6803 Training Loss: 0.14276907433403863 Test Loss: 0.4001708713107639\n",
      "Epoch: 6804 Training Loss: 0.14262029181586372 Test Loss: 0.40069775390625\n",
      "Epoch: 6805 Training Loss: 0.14248982577853733 Test Loss: 0.40108946397569445\n",
      "Epoch: 6806 Training Loss: 0.14237747107611762 Test Loss: 0.40154248046875\n",
      "Epoch: 6807 Training Loss: 0.14227339935302735 Test Loss: 0.40188446723090276\n",
      "Epoch: 6808 Training Loss: 0.14218315124511718 Test Loss: 0.4023098958333333\n",
      "Epoch: 6809 Training Loss: 0.14210756429036459 Test Loss: 0.40266905381944446\n",
      "Epoch: 6810 Training Loss: 0.14205177307128905 Test Loss: 0.40295372178819444\n",
      "Epoch: 6811 Training Loss: 0.1419928207397461 Test Loss: 0.40317876519097223\n",
      "Epoch: 6812 Training Loss: 0.14195617336697047 Test Loss: 0.40337809244791667\n",
      "Epoch: 6813 Training Loss: 0.14193024020724826 Test Loss: 0.40353171115451386\n",
      "Epoch: 6814 Training Loss: 0.14190962473551433 Test Loss: 0.4035920138888889\n",
      "Epoch: 6815 Training Loss: 0.1419045910305447 Test Loss: 0.4035912272135417\n",
      "Epoch: 6816 Training Loss: 0.14190762498643664 Test Loss: 0.4035663519965278\n",
      "Epoch: 6817 Training Loss: 0.14191649542914497 Test Loss: 0.40358607313368056\n",
      "Epoch: 6818 Training Loss: 0.1419438942803277 Test Loss: 0.4036368001302083\n",
      "Epoch: 6819 Training Loss: 0.14198799302842882 Test Loss: 0.40351866319444446\n",
      "Epoch: 6820 Training Loss: 0.14203559112548828 Test Loss: 0.4034931640625\n",
      "Epoch: 6821 Training Loss: 0.14210055033365884 Test Loss: 0.4035526801215278\n",
      "Epoch: 6822 Training Loss: 0.14218284606933593 Test Loss: 0.40362630208333333\n",
      "Epoch: 6823 Training Loss: 0.14227825419108073 Test Loss: 0.4037222222222222\n",
      "Epoch: 6824 Training Loss: 0.14238976457383898 Test Loss: 0.40381624348958334\n",
      "Epoch: 6825 Training Loss: 0.14252799140082464 Test Loss: 0.4040157877604167\n",
      "Epoch: 6826 Training Loss: 0.14268291897243923 Test Loss: 0.40433995225694447\n",
      "Epoch: 6827 Training Loss: 0.14285325961642795 Test Loss: 0.4047319607204861\n",
      "Epoch: 6828 Training Loss: 0.14305016157362196 Test Loss: 0.40546031358506945\n",
      "Epoch: 6829 Training Loss: 0.14327979532877605 Test Loss: 0.4061030002170139\n",
      "Epoch: 6830 Training Loss: 0.14355495198567708 Test Loss: 0.4072030707465278\n",
      "Epoch: 6831 Training Loss: 0.14388226911756727 Test Loss: 0.4088637424045139\n",
      "Epoch: 6832 Training Loss: 0.14425543721516926 Test Loss: 0.4109228515625\n",
      "Epoch: 6833 Training Loss: 0.14466760592990452 Test Loss: 0.41338026258680555\n",
      "Epoch: 6834 Training Loss: 0.14513820393880209 Test Loss: 0.4167751193576389\n",
      "Epoch: 6835 Training Loss: 0.14565342712402343 Test Loss: 0.42112833658854165\n",
      "Epoch: 6836 Training Loss: 0.14620196194118923 Test Loss: 0.42508854166666665\n",
      "Epoch: 6837 Training Loss: 0.1467605506049262 Test Loss: 0.4264887966579861\n",
      "Epoch: 6838 Training Loss: 0.147100343492296 Test Loss: 0.42502870008680554\n",
      "Epoch: 6839 Training Loss: 0.14716953870985244 Test Loss: 0.42468250868055557\n",
      "Epoch: 6840 Training Loss: 0.1471811828613281 Test Loss: 0.4241480034722222\n",
      "Epoch: 6841 Training Loss: 0.1471203884548611 Test Loss: 0.4177676323784722\n",
      "Epoch: 6842 Training Loss: 0.14688822597927517 Test Loss: 0.41129964192708335\n",
      "Epoch: 6843 Training Loss: 0.1466694064670139 Test Loss: 0.40792442491319447\n",
      "Epoch: 6844 Training Loss: 0.1464969482421875 Test Loss: 0.4064123263888889\n",
      "Epoch: 6845 Training Loss: 0.14635926479763456 Test Loss: 0.4051638454861111\n",
      "Epoch: 6846 Training Loss: 0.14623374769422742 Test Loss: 0.4039431423611111\n",
      "Epoch: 6847 Training Loss: 0.14610345967610677 Test Loss: 0.40285823567708334\n",
      "Epoch: 6848 Training Loss: 0.14596044074164496 Test Loss: 0.40235709635416667\n",
      "Epoch: 6849 Training Loss: 0.14577645704481337 Test Loss: 0.4022334526909722\n",
      "Epoch: 6850 Training Loss: 0.14556788974338108 Test Loss: 0.402564208984375\n",
      "Epoch: 6851 Training Loss: 0.1453336147732205 Test Loss: 0.40332432725694445\n",
      "Epoch: 6852 Training Loss: 0.14509236992730035 Test Loss: 0.4043893229166667\n",
      "Epoch: 6853 Training Loss: 0.1448633304172092 Test Loss: 0.4053474934895833\n",
      "Epoch: 6854 Training Loss: 0.14464918348524305 Test Loss: 0.4059475368923611\n",
      "Epoch: 6855 Training Loss: 0.14444283294677734 Test Loss: 0.40626942274305555\n",
      "Epoch: 6856 Training Loss: 0.1442430699666341 Test Loss: 0.4060966254340278\n",
      "Epoch: 6857 Training Loss: 0.1440396499633789 Test Loss: 0.405353271484375\n",
      "Epoch: 6858 Training Loss: 0.14384212748209635 Test Loss: 0.4043930121527778\n",
      "Epoch: 6859 Training Loss: 0.14364048343234592 Test Loss: 0.4033608127170139\n",
      "Epoch: 6860 Training Loss: 0.14344041951497397 Test Loss: 0.4023511284722222\n",
      "Epoch: 6861 Training Loss: 0.14325605604383682 Test Loss: 0.40129991319444447\n",
      "Epoch: 6862 Training Loss: 0.1430765660603841 Test Loss: 0.40044004991319443\n",
      "Epoch: 6863 Training Loss: 0.14290704006618923 Test Loss: 0.3999602322048611\n",
      "Epoch: 6864 Training Loss: 0.1427589399549696 Test Loss: 0.39992521158854166\n",
      "Epoch: 6865 Training Loss: 0.14263740878634984 Test Loss: 0.4002675509982639\n",
      "Epoch: 6866 Training Loss: 0.1425378901163737 Test Loss: 0.40087489149305555\n",
      "Epoch: 6867 Training Loss: 0.14246126386854382 Test Loss: 0.4015112033420139\n",
      "Epoch: 6868 Training Loss: 0.1424109412299262 Test Loss: 0.40236490885416665\n",
      "Epoch: 6869 Training Loss: 0.14238254547119142 Test Loss: 0.40320277235243057\n",
      "Epoch: 6870 Training Loss: 0.14237327406141492 Test Loss: 0.40400618489583334\n",
      "Epoch: 6871 Training Loss: 0.1423848631117079 Test Loss: 0.40480523003472224\n",
      "Epoch: 6872 Training Loss: 0.14241917673746746 Test Loss: 0.4055007052951389\n",
      "Epoch: 6873 Training Loss: 0.14245928276909722 Test Loss: 0.4061913519965278\n",
      "Epoch: 6874 Training Loss: 0.1424995320638021 Test Loss: 0.40656388346354166\n",
      "Epoch: 6875 Training Loss: 0.1425418217976888 Test Loss: 0.4071259765625\n",
      "Epoch: 6876 Training Loss: 0.14258942074245876 Test Loss: 0.4070426974826389\n",
      "Epoch: 6877 Training Loss: 0.14264232211642794 Test Loss: 0.4069431423611111\n",
      "Epoch: 6878 Training Loss: 0.14269083234998914 Test Loss: 0.40691227213541664\n",
      "Epoch: 6879 Training Loss: 0.14274493662516277 Test Loss: 0.4069601236979167\n",
      "Epoch: 6880 Training Loss: 0.14279723019070095 Test Loss: 0.4068455403645833\n",
      "Epoch: 6881 Training Loss: 0.14284493255615235 Test Loss: 0.4067741427951389\n",
      "Epoch: 6882 Training Loss: 0.14289984215630425 Test Loss: 0.40688175455729164\n",
      "Epoch: 6883 Training Loss: 0.14296366119384765 Test Loss: 0.4068925509982639\n",
      "Epoch: 6884 Training Loss: 0.143025515238444 Test Loss: 0.4069958224826389\n",
      "Epoch: 6885 Training Loss: 0.1431052483452691 Test Loss: 0.4074213324652778\n",
      "Epoch: 6886 Training Loss: 0.1431995798746745 Test Loss: 0.40732492404513887\n",
      "Epoch: 6887 Training Loss: 0.1433060031467014 Test Loss: 0.4075344509548611\n",
      "Epoch: 6888 Training Loss: 0.14342440795898437 Test Loss: 0.40781803385416665\n",
      "Epoch: 6889 Training Loss: 0.1435664537217882 Test Loss: 0.4082185329861111\n",
      "Epoch: 6890 Training Loss: 0.14372794935438368 Test Loss: 0.40845675998263886\n",
      "Epoch: 6891 Training Loss: 0.14390826246473523 Test Loss: 0.40860904947916665\n",
      "Epoch: 6892 Training Loss: 0.14411295064290364 Test Loss: 0.4085651584201389\n",
      "Epoch: 6893 Training Loss: 0.14433856540256076 Test Loss: 0.4086229926215278\n",
      "Epoch: 6894 Training Loss: 0.14459304470486112 Test Loss: 0.40870903862847224\n",
      "Epoch: 6895 Training Loss: 0.1448663380940755 Test Loss: 0.4086218532986111\n",
      "Epoch: 6896 Training Loss: 0.14515010240342882 Test Loss: 0.40876611328125\n",
      "Epoch: 6897 Training Loss: 0.14546216159396702 Test Loss: 0.4089206271701389\n",
      "Epoch: 6898 Training Loss: 0.14579405890570746 Test Loss: 0.409260986328125\n",
      "Epoch: 6899 Training Loss: 0.1461584235297309 Test Loss: 0.4093724229600694\n",
      "Epoch: 6900 Training Loss: 0.1465601077609592 Test Loss: 0.4092753363715278\n",
      "Epoch: 6901 Training Loss: 0.1469728783501519 Test Loss: 0.40883430989583336\n",
      "Epoch: 6902 Training Loss: 0.14738829379611545 Test Loss: 0.40795675998263886\n",
      "Epoch: 6903 Training Loss: 0.14780905490451388 Test Loss: 0.40662272135416666\n",
      "Epoch: 6904 Training Loss: 0.14818637763129341 Test Loss: 0.4051834309895833\n",
      "Epoch: 6905 Training Loss: 0.14846580844455295 Test Loss: 0.4040051540798611\n",
      "Epoch: 6906 Training Loss: 0.14862150404188368 Test Loss: 0.4035510796440972\n",
      "Epoch: 6907 Training Loss: 0.14862430487738715 Test Loss: 0.40374549696180556\n",
      "Epoch: 6908 Training Loss: 0.14849010891384548 Test Loss: 0.4051496310763889\n",
      "Epoch: 6909 Training Loss: 0.1482675543891059 Test Loss: 0.4071103515625\n",
      "Epoch: 6910 Training Loss: 0.14799965752495658 Test Loss: 0.40892616102430557\n",
      "Epoch: 6911 Training Loss: 0.1477003631591797 Test Loss: 0.4100384657118056\n",
      "Epoch: 6912 Training Loss: 0.1474469960530599 Test Loss: 0.410354736328125\n",
      "Epoch: 6913 Training Loss: 0.14721293640136718 Test Loss: 0.4093856879340278\n",
      "Epoch: 6914 Training Loss: 0.1469866739908854 Test Loss: 0.40799601236979166\n",
      "Epoch: 6915 Training Loss: 0.14673055013020833 Test Loss: 0.4054308810763889\n",
      "Epoch: 6916 Training Loss: 0.14646920776367187 Test Loss: 0.40323366970486113\n",
      "Epoch: 6917 Training Loss: 0.14620980834960937 Test Loss: 0.40158940972222223\n",
      "Epoch: 6918 Training Loss: 0.1460276557074653 Test Loss: 0.40133436414930557\n",
      "Epoch: 6919 Training Loss: 0.1459009246826172 Test Loss: 0.402193359375\n",
      "Epoch: 6920 Training Loss: 0.145850830078125 Test Loss: 0.403637939453125\n",
      "Epoch: 6921 Training Loss: 0.14589046054416233 Test Loss: 0.4048861762152778\n",
      "Epoch: 6922 Training Loss: 0.14599880133734808 Test Loss: 0.4045637478298611\n",
      "Epoch: 6923 Training Loss: 0.1461524895562066 Test Loss: 0.4032919379340278\n",
      "Epoch: 6924 Training Loss: 0.14638272433810764 Test Loss: 0.40259773763020834\n",
      "Epoch: 6925 Training Loss: 0.14670863511827256 Test Loss: 0.40288226996527776\n",
      "Epoch: 6926 Training Loss: 0.14711506822374132 Test Loss: 0.4030478515625\n",
      "Epoch: 6927 Training Loss: 0.1475545179578993 Test Loss: 0.4023870442708333\n",
      "Epoch: 6928 Training Loss: 0.14801896158854166 Test Loss: 0.4020212131076389\n",
      "Epoch: 6929 Training Loss: 0.1485150638156467 Test Loss: 0.40209228515625\n",
      "Epoch: 6930 Training Loss: 0.14906705050998265 Test Loss: 0.4028777126736111\n",
      "Epoch: 6931 Training Loss: 0.14965482415093315 Test Loss: 0.4042316080729167\n",
      "Epoch: 6932 Training Loss: 0.15025003051757813 Test Loss: 0.4054650336371528\n",
      "Epoch: 6933 Training Loss: 0.15085919358995226 Test Loss: 0.4066046820746528\n",
      "Epoch: 6934 Training Loss: 0.15140947638617622 Test Loss: 0.40673963758680554\n",
      "Epoch: 6935 Training Loss: 0.15180591328938803 Test Loss: 0.40551985677083335\n",
      "Epoch: 6936 Training Loss: 0.15197830539279514 Test Loss: 0.40308544921875\n",
      "Epoch: 6937 Training Loss: 0.15179798041449652 Test Loss: 0.40111724175347224\n",
      "Epoch: 6938 Training Loss: 0.15128267923990885 Test Loss: 0.4002326388888889\n",
      "Epoch: 6939 Training Loss: 0.15053745863172743 Test Loss: 0.4005961642795139\n",
      "Epoch: 6940 Training Loss: 0.1496794179280599 Test Loss: 0.401806884765625\n",
      "Epoch: 6941 Training Loss: 0.1488387722439236 Test Loss: 0.4030048828125\n",
      "Epoch: 6942 Training Loss: 0.14804720052083334 Test Loss: 0.40360088433159724\n",
      "Epoch: 6943 Training Loss: 0.1473361324734158 Test Loss: 0.40367936197916665\n",
      "Epoch: 6944 Training Loss: 0.14670799424913195 Test Loss: 0.4031702202690972\n",
      "Epoch: 6945 Training Loss: 0.1461460706922743 Test Loss: 0.40245806206597223\n",
      "Epoch: 6946 Training Loss: 0.14562210252549912 Test Loss: 0.40174034288194443\n",
      "Epoch: 6947 Training Loss: 0.14515569220648872 Test Loss: 0.4011489800347222\n",
      "Epoch: 6948 Training Loss: 0.1447450663248698 Test Loss: 0.4007238226996528\n",
      "Epoch: 6949 Training Loss: 0.1443890584309896 Test Loss: 0.40069007703993054\n",
      "Epoch: 6950 Training Loss: 0.14410933430989584 Test Loss: 0.4009376898871528\n",
      "Epoch: 6951 Training Loss: 0.14391124979654948 Test Loss: 0.4012771809895833\n",
      "Epoch: 6952 Training Loss: 0.14379070197211372 Test Loss: 0.4015825466579861\n",
      "Epoch: 6953 Training Loss: 0.14372364807128907 Test Loss: 0.4020613606770833\n",
      "Epoch: 6954 Training Loss: 0.1437115995619032 Test Loss: 0.40256268988715277\n",
      "Epoch: 6955 Training Loss: 0.1437461658053928 Test Loss: 0.4029689670138889\n",
      "Epoch: 6956 Training Loss: 0.1438307885064019 Test Loss: 0.4034072808159722\n",
      "Epoch: 6957 Training Loss: 0.14394543541802302 Test Loss: 0.40383794487847224\n",
      "Epoch: 6958 Training Loss: 0.1440997034708659 Test Loss: 0.4041008572048611\n",
      "Epoch: 6959 Training Loss: 0.14427231174045138 Test Loss: 0.4042616644965278\n",
      "Epoch: 6960 Training Loss: 0.14445675489637586 Test Loss: 0.4044676106770833\n",
      "Epoch: 6961 Training Loss: 0.14464153883192274 Test Loss: 0.4045426703559028\n",
      "Epoch: 6962 Training Loss: 0.144835206773546 Test Loss: 0.40465592447916665\n",
      "Epoch: 6963 Training Loss: 0.14502738952636718 Test Loss: 0.40478293185763886\n",
      "Epoch: 6964 Training Loss: 0.14520944213867187 Test Loss: 0.40492789713541666\n",
      "Epoch: 6965 Training Loss: 0.14537917073567708 Test Loss: 0.40509944661458336\n",
      "Epoch: 6966 Training Loss: 0.14554819064670138 Test Loss: 0.4054345160590278\n",
      "Epoch: 6967 Training Loss: 0.14570874701605901 Test Loss: 0.4058200412326389\n",
      "Epoch: 6968 Training Loss: 0.14587003241644966 Test Loss: 0.4060786675347222\n",
      "Epoch: 6969 Training Loss: 0.14602598063151043 Test Loss: 0.40638077799479166\n",
      "Epoch: 6970 Training Loss: 0.14618493991427953 Test Loss: 0.40660302734375\n",
      "Epoch: 6971 Training Loss: 0.146357177734375 Test Loss: 0.40655919053819445\n",
      "Epoch: 6972 Training Loss: 0.14654167005750868 Test Loss: 0.4060136447482639\n",
      "Epoch: 6973 Training Loss: 0.1467327643500434 Test Loss: 0.4052538791232639\n",
      "Epoch: 6974 Training Loss: 0.14694524637858072 Test Loss: 0.40409879557291667\n",
      "Epoch: 6975 Training Loss: 0.14717699856228297 Test Loss: 0.4029274631076389\n",
      "Epoch: 6976 Training Loss: 0.14742318386501735 Test Loss: 0.40169482421875\n",
      "Epoch: 6977 Training Loss: 0.14769017706976997 Test Loss: 0.4006086697048611\n",
      "Epoch: 6978 Training Loss: 0.14795634290907117 Test Loss: 0.39983653428819443\n",
      "Epoch: 6979 Training Loss: 0.14820072428385417 Test Loss: 0.3995083550347222\n",
      "Epoch: 6980 Training Loss: 0.14837646145290798 Test Loss: 0.3996674262152778\n",
      "Epoch: 6981 Training Loss: 0.14850387912326388 Test Loss: 0.4001974826388889\n",
      "Epoch: 6982 Training Loss: 0.14857285902235243 Test Loss: 0.40132367621527776\n",
      "Epoch: 6983 Training Loss: 0.14855523342556423 Test Loss: 0.4024632703993056\n",
      "Epoch: 6984 Training Loss: 0.14843713209364148 Test Loss: 0.40337193467881943\n",
      "Epoch: 6985 Training Loss: 0.1482540961371528 Test Loss: 0.4038009982638889\n",
      "Epoch: 6986 Training Loss: 0.14801446872287327 Test Loss: 0.4039478081597222\n",
      "Epoch: 6987 Training Loss: 0.14772152201334635 Test Loss: 0.4037881401909722\n",
      "Epoch: 6988 Training Loss: 0.14740886773003473 Test Loss: 0.4034358181423611\n",
      "Epoch: 6989 Training Loss: 0.14707870822482638 Test Loss: 0.40305056423611113\n",
      "Epoch: 6990 Training Loss: 0.1467333984375 Test Loss: 0.40242735460069445\n",
      "Epoch: 6991 Training Loss: 0.14639317321777343 Test Loss: 0.40219976128472223\n",
      "Epoch: 6992 Training Loss: 0.1460710669623481 Test Loss: 0.40181849500868055\n",
      "Epoch: 6993 Training Loss: 0.14577083333333332 Test Loss: 0.4016975911458333\n",
      "Epoch: 6994 Training Loss: 0.14550724792480468 Test Loss: 0.40171934678819443\n",
      "Epoch: 6995 Training Loss: 0.1452856733534071 Test Loss: 0.4019097764756944\n",
      "Epoch: 6996 Training Loss: 0.14511713494194878 Test Loss: 0.4022313910590278\n",
      "Epoch: 6997 Training Loss: 0.1450034450954861 Test Loss: 0.40301188151041667\n",
      "Epoch: 6998 Training Loss: 0.144964111328125 Test Loss: 0.40363962131076386\n",
      "Epoch: 6999 Training Loss: 0.14498685709635417 Test Loss: 0.40468359375\n",
      "Epoch: 7000 Training Loss: 0.1450742950439453 Test Loss: 0.40557996961805554\n",
      "Epoch: 7001 Training Loss: 0.14524354722764757 Test Loss: 0.40691851128472223\n",
      "Epoch: 7002 Training Loss: 0.14546115451388889 Test Loss: 0.40848280164930556\n",
      "Epoch: 7003 Training Loss: 0.14572898525661893 Test Loss: 0.41014501953125\n",
      "Epoch: 7004 Training Loss: 0.14603886583116318 Test Loss: 0.4118893771701389\n",
      "Epoch: 7005 Training Loss: 0.14640914916992187 Test Loss: 0.4131067165798611\n",
      "Epoch: 7006 Training Loss: 0.14681129286024305 Test Loss: 0.4135137261284722\n",
      "Epoch: 7007 Training Loss: 0.14719947475857204 Test Loss: 0.41299755859375\n",
      "Epoch: 7008 Training Loss: 0.1475686798095703 Test Loss: 0.41091183810763887\n",
      "Epoch: 7009 Training Loss: 0.1478563469780816 Test Loss: 0.40791208224826386\n",
      "Epoch: 7010 Training Loss: 0.14796828206380208 Test Loss: 0.4046180555555556\n",
      "Epoch: 7011 Training Loss: 0.1478637457953559 Test Loss: 0.40177625868055555\n",
      "Epoch: 7012 Training Loss: 0.1475247853597005 Test Loss: 0.39996275499131945\n",
      "Epoch: 7013 Training Loss: 0.14694594489203558 Test Loss: 0.3989789496527778\n",
      "Epoch: 7014 Training Loss: 0.1461842498779297 Test Loss: 0.3984392903645833\n",
      "Epoch: 7015 Training Loss: 0.1453242696126302 Test Loss: 0.3983833279079861\n",
      "Epoch: 7016 Training Loss: 0.14444927639431424 Test Loss: 0.39875770399305555\n",
      "Epoch: 7017 Training Loss: 0.14361333889431424 Test Loss: 0.39891278754340276\n",
      "Epoch: 7018 Training Loss: 0.14283004252115886 Test Loss: 0.39911279296875\n",
      "Epoch: 7019 Training Loss: 0.14211678483751086 Test Loss: 0.3990746256510417\n",
      "Epoch: 7020 Training Loss: 0.14148089769151476 Test Loss: 0.39884147135416664\n",
      "Epoch: 7021 Training Loss: 0.1409184341430664 Test Loss: 0.3985100368923611\n",
      "Epoch: 7022 Training Loss: 0.14043631744384766 Test Loss: 0.39808756510416665\n",
      "Epoch: 7023 Training Loss: 0.14002708095974392 Test Loss: 0.39769715711805553\n",
      "Epoch: 7024 Training Loss: 0.13968281979031033 Test Loss: 0.3972347005208333\n",
      "Epoch: 7025 Training Loss: 0.13939193725585938 Test Loss: 0.3968900282118056\n",
      "Epoch: 7026 Training Loss: 0.13915582021077474 Test Loss: 0.3965302734375\n",
      "Epoch: 7027 Training Loss: 0.13895562828911676 Test Loss: 0.39623567708333335\n",
      "Epoch: 7028 Training Loss: 0.13879117329915364 Test Loss: 0.3960794270833333\n",
      "Epoch: 7029 Training Loss: 0.13865645684136285 Test Loss: 0.39592879231770833\n",
      "Epoch: 7030 Training Loss: 0.13854469807942707 Test Loss: 0.39587730577256947\n",
      "Epoch: 7031 Training Loss: 0.13845375400119359 Test Loss: 0.39581480577256944\n",
      "Epoch: 7032 Training Loss: 0.1383748050265842 Test Loss: 0.39579652235243057\n",
      "Epoch: 7033 Training Loss: 0.13831048923068576 Test Loss: 0.395818359375\n",
      "Epoch: 7034 Training Loss: 0.13826294708251954 Test Loss: 0.3957595486111111\n",
      "Epoch: 7035 Training Loss: 0.1382229258219401 Test Loss: 0.39584491644965275\n",
      "Epoch: 7036 Training Loss: 0.13818595801459418 Test Loss: 0.39583669704861113\n",
      "Epoch: 7037 Training Loss: 0.13815818362765842 Test Loss: 0.39596394856770833\n",
      "Epoch: 7038 Training Loss: 0.13813513692220053 Test Loss: 0.39597488064236114\n",
      "Epoch: 7039 Training Loss: 0.1381185523139106 Test Loss: 0.39606857638888887\n",
      "Epoch: 7040 Training Loss: 0.1381020957099067 Test Loss: 0.3960812174479167\n",
      "Epoch: 7041 Training Loss: 0.13809104580349393 Test Loss: 0.3961329752604167\n",
      "Epoch: 7042 Training Loss: 0.1380851847330729 Test Loss: 0.39617892795138887\n",
      "Epoch: 7043 Training Loss: 0.13808152770996093 Test Loss: 0.3962506510416667\n",
      "Epoch: 7044 Training Loss: 0.13807692125108506 Test Loss: 0.39625062391493054\n",
      "Epoch: 7045 Training Loss: 0.13807299041748047 Test Loss: 0.39633534071180554\n",
      "Epoch: 7046 Training Loss: 0.1380713916354709 Test Loss: 0.3963041720920139\n",
      "Epoch: 7047 Training Loss: 0.138070064968533 Test Loss: 0.3962596299913194\n",
      "Epoch: 7048 Training Loss: 0.13807198418511285 Test Loss: 0.39629481336805555\n",
      "Epoch: 7049 Training Loss: 0.13807506646050346 Test Loss: 0.39634486219618054\n",
      "Epoch: 7050 Training Loss: 0.1380791024102105 Test Loss: 0.396349853515625\n",
      "Epoch: 7051 Training Loss: 0.13808887990315755 Test Loss: 0.39636089409722225\n",
      "Epoch: 7052 Training Loss: 0.13810056983100044 Test Loss: 0.39633805338541667\n",
      "Epoch: 7053 Training Loss: 0.13811155954996746 Test Loss: 0.3964152018229167\n",
      "Epoch: 7054 Training Loss: 0.13812679121229385 Test Loss: 0.3964052734375\n",
      "Epoch: 7055 Training Loss: 0.13814757622612847 Test Loss: 0.3963673773871528\n",
      "Epoch: 7056 Training Loss: 0.13817213609483506 Test Loss: 0.39644216579861113\n",
      "Epoch: 7057 Training Loss: 0.13819827609592014 Test Loss: 0.3964646267361111\n",
      "Epoch: 7058 Training Loss: 0.13822854868570963 Test Loss: 0.39655216471354165\n",
      "Epoch: 7059 Training Loss: 0.13826138475206162 Test Loss: 0.396486328125\n",
      "Epoch: 7060 Training Loss: 0.13829922485351562 Test Loss: 0.3965988498263889\n",
      "Epoch: 7061 Training Loss: 0.13833609178331163 Test Loss: 0.3966145290798611\n",
      "Epoch: 7062 Training Loss: 0.1383733664618598 Test Loss: 0.39671009657118056\n",
      "Epoch: 7063 Training Loss: 0.13841655137803818 Test Loss: 0.396882568359375\n",
      "Epoch: 7064 Training Loss: 0.13846354590521917 Test Loss: 0.3969620768229167\n",
      "Epoch: 7065 Training Loss: 0.13851380072699654 Test Loss: 0.397046875\n",
      "Epoch: 7066 Training Loss: 0.13856666310628254 Test Loss: 0.39717919921875\n",
      "Epoch: 7067 Training Loss: 0.13862314605712892 Test Loss: 0.39743104383680555\n",
      "Epoch: 7068 Training Loss: 0.1386903796725803 Test Loss: 0.39763926866319443\n",
      "Epoch: 7069 Training Loss: 0.1387542190551758 Test Loss: 0.3977229817708333\n",
      "Epoch: 7070 Training Loss: 0.1388261913723416 Test Loss: 0.3980570746527778\n",
      "Epoch: 7071 Training Loss: 0.13890307108561198 Test Loss: 0.3981884765625\n",
      "Epoch: 7072 Training Loss: 0.13898775906032987 Test Loss: 0.3983788519965278\n",
      "Epoch: 7073 Training Loss: 0.1390900141398112 Test Loss: 0.39858083767361113\n",
      "Epoch: 7074 Training Loss: 0.13919740295410157 Test Loss: 0.3988774685329861\n",
      "Epoch: 7075 Training Loss: 0.1393184568617079 Test Loss: 0.39917572699652776\n",
      "Epoch: 7076 Training Loss: 0.13945112948947483 Test Loss: 0.3995552300347222\n",
      "Epoch: 7077 Training Loss: 0.13958893330891928 Test Loss: 0.39996625434027777\n",
      "Epoch: 7078 Training Loss: 0.13974584367540147 Test Loss: 0.4004331597222222\n",
      "Epoch: 7079 Training Loss: 0.13991683705647787 Test Loss: 0.4008205295138889\n",
      "Epoch: 7080 Training Loss: 0.14009767489963107 Test Loss: 0.4013024631076389\n",
      "Epoch: 7081 Training Loss: 0.14030518256293403 Test Loss: 0.4018097330729167\n",
      "Epoch: 7082 Training Loss: 0.14053174336751303 Test Loss: 0.4024386393229167\n",
      "Epoch: 7083 Training Loss: 0.14078339216444227 Test Loss: 0.40293513997395836\n",
      "Epoch: 7084 Training Loss: 0.1410574934217665 Test Loss: 0.4032298990885417\n",
      "Epoch: 7085 Training Loss: 0.14135215250651043 Test Loss: 0.4039160427517361\n",
      "Epoch: 7086 Training Loss: 0.14167672559950087 Test Loss: 0.40436607530381946\n",
      "Epoch: 7087 Training Loss: 0.14204957665337456 Test Loss: 0.4046978352864583\n",
      "Epoch: 7088 Training Loss: 0.14245662773980033 Test Loss: 0.40525303819444447\n",
      "Epoch: 7089 Training Loss: 0.14291471438937717 Test Loss: 0.4058621961805556\n",
      "Epoch: 7090 Training Loss: 0.14343277994791667 Test Loss: 0.40664740668402777\n",
      "Epoch: 7091 Training Loss: 0.14400885518391926 Test Loss: 0.40747059461805557\n",
      "Epoch: 7092 Training Loss: 0.14464408704969617 Test Loss: 0.40781694878472224\n",
      "Epoch: 7093 Training Loss: 0.14527354939778644 Test Loss: 0.40723280164930553\n",
      "Epoch: 7094 Training Loss: 0.14594226752387152 Test Loss: 0.40671988932291664\n",
      "Epoch: 7095 Training Loss: 0.14670870632595487 Test Loss: 0.40660069444444447\n",
      "Epoch: 7096 Training Loss: 0.1475265858968099 Test Loss: 0.4066229383680556\n",
      "Epoch: 7097 Training Loss: 0.148391603257921 Test Loss: 0.4072801106770833\n",
      "Epoch: 7098 Training Loss: 0.1493027886284722 Test Loss: 0.4087890353732639\n",
      "Epoch: 7099 Training Loss: 0.15022638448079428 Test Loss: 0.41042274305555554\n",
      "Epoch: 7100 Training Loss: 0.15117172410753038 Test Loss: 0.41316813151041665\n",
      "Epoch: 7101 Training Loss: 0.1521049279106988 Test Loss: 0.41605110677083335\n",
      "Epoch: 7102 Training Loss: 0.15294580586751302 Test Loss: 0.41844677734375\n",
      "Epoch: 7103 Training Loss: 0.15365562947591146 Test Loss: 0.4174391818576389\n",
      "Epoch: 7104 Training Loss: 0.15402784729003907 Test Loss: 0.41536699761284723\n",
      "Epoch: 7105 Training Loss: 0.15398570251464844 Test Loss: 0.4133281792534722\n",
      "Epoch: 7106 Training Loss: 0.15366611056857638 Test Loss: 0.4119225802951389\n",
      "Epoch: 7107 Training Loss: 0.1531152581108941 Test Loss: 0.41091373697916667\n",
      "Epoch: 7108 Training Loss: 0.15231334431966145 Test Loss: 0.4105583767361111\n",
      "Epoch: 7109 Training Loss: 0.15132879299587673 Test Loss: 0.41115885416666664\n",
      "Epoch: 7110 Training Loss: 0.1502730950249566 Test Loss: 0.4121553005642361\n",
      "Epoch: 7111 Training Loss: 0.14925782436794705 Test Loss: 0.4111315646701389\n",
      "Epoch: 7112 Training Loss: 0.14824347601996526 Test Loss: 0.4095275607638889\n",
      "Epoch: 7113 Training Loss: 0.14736910671657985 Test Loss: 0.4078294270833333\n",
      "Epoch: 7114 Training Loss: 0.14666021219889322 Test Loss: 0.40664813910590275\n",
      "Epoch: 7115 Training Loss: 0.14610033501519099 Test Loss: 0.4055674370659722\n",
      "Epoch: 7116 Training Loss: 0.14563716125488282 Test Loss: 0.4042111545138889\n",
      "Epoch: 7117 Training Loss: 0.145264400906033 Test Loss: 0.40282411024305553\n",
      "Epoch: 7118 Training Loss: 0.14495236206054687 Test Loss: 0.40146305338541666\n",
      "Epoch: 7119 Training Loss: 0.144680660671658 Test Loss: 0.4003242458767361\n",
      "Epoch: 7120 Training Loss: 0.14440655093722873 Test Loss: 0.3997912326388889\n",
      "Epoch: 7121 Training Loss: 0.1441751929389106 Test Loss: 0.3995100368923611\n",
      "Epoch: 7122 Training Loss: 0.1439739727444119 Test Loss: 0.39951117621527776\n",
      "Epoch: 7123 Training Loss: 0.14380025906032987 Test Loss: 0.39986946614583335\n",
      "Epoch: 7124 Training Loss: 0.14363536071777344 Test Loss: 0.4001734212239583\n",
      "Epoch: 7125 Training Loss: 0.1434921400282118 Test Loss: 0.4008375651041667\n",
      "Epoch: 7126 Training Loss: 0.14338164859347874 Test Loss: 0.40144669596354166\n",
      "Epoch: 7127 Training Loss: 0.14329952748616537 Test Loss: 0.40191172960069443\n",
      "Epoch: 7128 Training Loss: 0.14323253292507596 Test Loss: 0.40234502495659724\n",
      "Epoch: 7129 Training Loss: 0.14317869059244792 Test Loss: 0.40253662109375\n",
      "Epoch: 7130 Training Loss: 0.14313981119791666 Test Loss: 0.402447265625\n",
      "Epoch: 7131 Training Loss: 0.143109740363227 Test Loss: 0.4022518988715278\n",
      "Epoch: 7132 Training Loss: 0.1430818642510308 Test Loss: 0.40188601345486114\n",
      "Epoch: 7133 Training Loss: 0.14307069142659506 Test Loss: 0.4015498589409722\n",
      "Epoch: 7134 Training Loss: 0.14305677456325955 Test Loss: 0.40109814453125\n",
      "Epoch: 7135 Training Loss: 0.14303245459662545 Test Loss: 0.40069618055555556\n",
      "Epoch: 7136 Training Loss: 0.14302239820692275 Test Loss: 0.40034157986111113\n",
      "Epoch: 7137 Training Loss: 0.1430187979804145 Test Loss: 0.40020743815104165\n",
      "Epoch: 7138 Training Loss: 0.14300478108723957 Test Loss: 0.4000054253472222\n",
      "Epoch: 7139 Training Loss: 0.14299435000949437 Test Loss: 0.3999501953125\n",
      "Epoch: 7140 Training Loss: 0.14300067562527127 Test Loss: 0.3999132758246528\n",
      "Epoch: 7141 Training Loss: 0.14301655578613282 Test Loss: 0.39997840711805555\n",
      "Epoch: 7142 Training Loss: 0.14303666263156467 Test Loss: 0.40041910807291664\n",
      "Epoch: 7143 Training Loss: 0.1430813768174913 Test Loss: 0.4008716362847222\n",
      "Epoch: 7144 Training Loss: 0.1431210445827908 Test Loss: 0.4015400390625\n",
      "Epoch: 7145 Training Loss: 0.1431700405544705 Test Loss: 0.4023560112847222\n",
      "Epoch: 7146 Training Loss: 0.14321656968858507 Test Loss: 0.40323193359375\n",
      "Epoch: 7147 Training Loss: 0.14324163648817273 Test Loss: 0.40432522243923613\n",
      "Epoch: 7148 Training Loss: 0.14325623830159506 Test Loss: 0.4050156792534722\n",
      "Epoch: 7149 Training Loss: 0.14323367818196614 Test Loss: 0.4052815212673611\n",
      "Epoch: 7150 Training Loss: 0.1431767764621311 Test Loss: 0.40508056640625\n",
      "Epoch: 7151 Training Loss: 0.14308785671657986 Test Loss: 0.40459209526909723\n",
      "Epoch: 7152 Training Loss: 0.1429787843492296 Test Loss: 0.4037870551215278\n",
      "Epoch: 7153 Training Loss: 0.14286132388644748 Test Loss: 0.40334727647569446\n",
      "Epoch: 7154 Training Loss: 0.14273812781439887 Test Loss: 0.4027597113715278\n",
      "Epoch: 7155 Training Loss: 0.14260772874620226 Test Loss: 0.40206727430555556\n",
      "Epoch: 7156 Training Loss: 0.14248197852240668 Test Loss: 0.40134239366319446\n",
      "Epoch: 7157 Training Loss: 0.14234672546386717 Test Loss: 0.40056092664930554\n",
      "Epoch: 7158 Training Loss: 0.14220831976996529 Test Loss: 0.3999885525173611\n",
      "Epoch: 7159 Training Loss: 0.14209892018636067 Test Loss: 0.39959716796875\n",
      "Epoch: 7160 Training Loss: 0.1420164040459527 Test Loss: 0.3994528537326389\n",
      "Epoch: 7161 Training Loss: 0.14195699310302734 Test Loss: 0.39943901909722224\n",
      "Epoch: 7162 Training Loss: 0.1419151407877604 Test Loss: 0.39960904947916664\n",
      "Epoch: 7163 Training Loss: 0.1418923602634006 Test Loss: 0.40003011067708333\n",
      "Epoch: 7164 Training Loss: 0.14191063774956597 Test Loss: 0.40067564561631946\n",
      "Epoch: 7165 Training Loss: 0.1419512218899197 Test Loss: 0.4012908528645833\n",
      "Epoch: 7166 Training Loss: 0.14201702372233074 Test Loss: 0.4021149631076389\n",
      "Epoch: 7167 Training Loss: 0.14210977851019965 Test Loss: 0.40323592122395835\n",
      "Epoch: 7168 Training Loss: 0.14222864447699654 Test Loss: 0.404490234375\n",
      "Epoch: 7169 Training Loss: 0.14239394039577907 Test Loss: 0.40580517578125\n",
      "Epoch: 7170 Training Loss: 0.14256760576036243 Test Loss: 0.4068555230034722\n",
      "Epoch: 7171 Training Loss: 0.1427513427734375 Test Loss: 0.4081467827690972\n",
      "Epoch: 7172 Training Loss: 0.14296893819173176 Test Loss: 0.4090698784722222\n",
      "Epoch: 7173 Training Loss: 0.14320609876844617 Test Loss: 0.41004833984375\n",
      "Epoch: 7174 Training Loss: 0.14347463989257814 Test Loss: 0.410747314453125\n",
      "Epoch: 7175 Training Loss: 0.1437734069824219 Test Loss: 0.41105284288194444\n",
      "Epoch: 7176 Training Loss: 0.14406182352701824 Test Loss: 0.4112363823784722\n",
      "Epoch: 7177 Training Loss: 0.14433960384792752 Test Loss: 0.41128073459201386\n",
      "Epoch: 7178 Training Loss: 0.1446270497639974 Test Loss: 0.41100303819444445\n",
      "Epoch: 7179 Training Loss: 0.14490169270833334 Test Loss: 0.4106259765625\n",
      "Epoch: 7180 Training Loss: 0.14516646660698784 Test Loss: 0.4103018120659722\n",
      "Epoch: 7181 Training Loss: 0.14540888129340276 Test Loss: 0.41012516276041666\n",
      "Epoch: 7182 Training Loss: 0.14560918680826823 Test Loss: 0.4101471896701389\n",
      "Epoch: 7183 Training Loss: 0.1457841271294488 Test Loss: 0.4107270779079861\n",
      "Epoch: 7184 Training Loss: 0.14588963656955295 Test Loss: 0.4111210394965278\n",
      "Epoch: 7185 Training Loss: 0.14597176615397137 Test Loss: 0.41190410698784724\n",
      "Epoch: 7186 Training Loss: 0.146034181382921 Test Loss: 0.412630615234375\n",
      "Epoch: 7187 Training Loss: 0.146071782430013 Test Loss: 0.41337434895833336\n",
      "Epoch: 7188 Training Loss: 0.14610572984483508 Test Loss: 0.41365288628472224\n",
      "Epoch: 7189 Training Loss: 0.14614236450195311 Test Loss: 0.41339816623263886\n",
      "Epoch: 7190 Training Loss: 0.14617605929904515 Test Loss: 0.41234391276041665\n",
      "Epoch: 7191 Training Loss: 0.14620906066894532 Test Loss: 0.4108180067274306\n",
      "Epoch: 7192 Training Loss: 0.1462522684733073 Test Loss: 0.4092584635416667\n",
      "Epoch: 7193 Training Loss: 0.14632057020399306 Test Loss: 0.4080296495225694\n",
      "Epoch: 7194 Training Loss: 0.14644034491644967 Test Loss: 0.40694276258680556\n",
      "Epoch: 7195 Training Loss: 0.1465497046576606 Test Loss: 0.40593326822916664\n",
      "Epoch: 7196 Training Loss: 0.14664248826768664 Test Loss: 0.40539103190104164\n",
      "Epoch: 7197 Training Loss: 0.1467373267279731 Test Loss: 0.4054388020833333\n",
      "Epoch: 7198 Training Loss: 0.1468072967529297 Test Loss: 0.4060365668402778\n",
      "Epoch: 7199 Training Loss: 0.14686468505859376 Test Loss: 0.40734996202256946\n",
      "Epoch: 7200 Training Loss: 0.14686991034613714 Test Loss: 0.40896419270833334\n",
      "Epoch: 7201 Training Loss: 0.14681917487250434 Test Loss: 0.4106494683159722\n",
      "Epoch: 7202 Training Loss: 0.14672877502441406 Test Loss: 0.41233962673611113\n",
      "Epoch: 7203 Training Loss: 0.14659715949164495 Test Loss: 0.4131426866319444\n",
      "Epoch: 7204 Training Loss: 0.14641547309027778 Test Loss: 0.41311423068576386\n",
      "Epoch: 7205 Training Loss: 0.14618401930067274 Test Loss: 0.411327392578125\n",
      "Epoch: 7206 Training Loss: 0.14589410569932726 Test Loss: 0.4088324924045139\n",
      "Epoch: 7207 Training Loss: 0.1455544196234809 Test Loss: 0.40580078125\n",
      "Epoch: 7208 Training Loss: 0.1451920182969835 Test Loss: 0.4024645724826389\n",
      "Epoch: 7209 Training Loss: 0.1448419664171007 Test Loss: 0.39972998046875\n",
      "Epoch: 7210 Training Loss: 0.1445386216905382 Test Loss: 0.39792919921875\n",
      "Epoch: 7211 Training Loss: 0.14428381856282552 Test Loss: 0.3969337565104167\n",
      "Epoch: 7212 Training Loss: 0.14406266615125868 Test Loss: 0.39623518880208336\n",
      "Epoch: 7213 Training Loss: 0.14391316223144532 Test Loss: 0.3960034722222222\n",
      "Epoch: 7214 Training Loss: 0.14385445827907986 Test Loss: 0.3962971462673611\n",
      "Epoch: 7215 Training Loss: 0.1438841111924913 Test Loss: 0.39679975043402776\n",
      "Epoch: 7216 Training Loss: 0.144014646742079 Test Loss: 0.3972792697482639\n",
      "Epoch: 7217 Training Loss: 0.14418956163194444 Test Loss: 0.39783710394965277\n",
      "Epoch: 7218 Training Loss: 0.14430930582682291 Test Loss: 0.3984466688368056\n",
      "Epoch: 7219 Training Loss: 0.14440609232584636 Test Loss: 0.39784212239583333\n",
      "Epoch: 7220 Training Loss: 0.14452801852756075 Test Loss: 0.39694132486979167\n",
      "Epoch: 7221 Training Loss: 0.14463529629177518 Test Loss: 0.3965787217881944\n",
      "Epoch: 7222 Training Loss: 0.1447467549641927 Test Loss: 0.3966200086805556\n",
      "Epoch: 7223 Training Loss: 0.14482838270399306 Test Loss: 0.3967686089409722\n",
      "Epoch: 7224 Training Loss: 0.14489304097493488 Test Loss: 0.39721891276041665\n",
      "Epoch: 7225 Training Loss: 0.14493642171223958 Test Loss: 0.3982398817274306\n",
      "Epoch: 7226 Training Loss: 0.14495941670735676 Test Loss: 0.399427734375\n",
      "Epoch: 7227 Training Loss: 0.14499376254611546 Test Loss: 0.4010863986545139\n",
      "Epoch: 7228 Training Loss: 0.14505896504720053 Test Loss: 0.4033948025173611\n",
      "Epoch: 7229 Training Loss: 0.14513570658365885 Test Loss: 0.406053466796875\n",
      "Epoch: 7230 Training Loss: 0.14523161994086373 Test Loss: 0.407983154296875\n",
      "Epoch: 7231 Training Loss: 0.1453105943467882 Test Loss: 0.40739002821180553\n",
      "Epoch: 7232 Training Loss: 0.14528565979003907 Test Loss: 0.4036394585503472\n",
      "Epoch: 7233 Training Loss: 0.14509038628472223 Test Loss: 0.39902490234375\n",
      "Epoch: 7234 Training Loss: 0.14466121164957682 Test Loss: 0.3971584201388889\n",
      "Epoch: 7235 Training Loss: 0.14414435407850476 Test Loss: 0.39906803385416667\n",
      "Epoch: 7236 Training Loss: 0.14360404035780164 Test Loss: 0.4014616427951389\n",
      "Epoch: 7237 Training Loss: 0.14301583523220487 Test Loss: 0.4024682074652778\n",
      "Epoch: 7238 Training Loss: 0.14242872111002605 Test Loss: 0.40178184678819445\n",
      "Epoch: 7239 Training Loss: 0.1419012213812934 Test Loss: 0.4002320963541667\n",
      "Epoch: 7240 Training Loss: 0.1414765141805013 Test Loss: 0.39871923828125\n",
      "Epoch: 7241 Training Loss: 0.14112488132052953 Test Loss: 0.3974787868923611\n",
      "Epoch: 7242 Training Loss: 0.14081193288167318 Test Loss: 0.3965768771701389\n",
      "Epoch: 7243 Training Loss: 0.14054329766167534 Test Loss: 0.39604793294270835\n",
      "Epoch: 7244 Training Loss: 0.14029644351535372 Test Loss: 0.395842041015625\n",
      "Epoch: 7245 Training Loss: 0.14004345024956596 Test Loss: 0.39581982421875\n",
      "Epoch: 7246 Training Loss: 0.1398042271931966 Test Loss: 0.39592396375868055\n",
      "Epoch: 7247 Training Loss: 0.1395836885240343 Test Loss: 0.3960893825954861\n",
      "Epoch: 7248 Training Loss: 0.13937498304578994 Test Loss: 0.3961034613715278\n",
      "Epoch: 7249 Training Loss: 0.13918011983235676 Test Loss: 0.3960606282552083\n",
      "Epoch: 7250 Training Loss: 0.13900972154405383 Test Loss: 0.39622395833333335\n",
      "Epoch: 7251 Training Loss: 0.13885605875651041 Test Loss: 0.3964395073784722\n",
      "Epoch: 7252 Training Loss: 0.13872726949055988 Test Loss: 0.39670426432291667\n",
      "Epoch: 7253 Training Loss: 0.13861247846815322 Test Loss: 0.3970608723958333\n",
      "Epoch: 7254 Training Loss: 0.13852003479003908 Test Loss: 0.39748814561631945\n",
      "Epoch: 7255 Training Loss: 0.1384425294664171 Test Loss: 0.3977834743923611\n",
      "Epoch: 7256 Training Loss: 0.13838179524739583 Test Loss: 0.3981375054253472\n",
      "Epoch: 7257 Training Loss: 0.13833863237169053 Test Loss: 0.3985569118923611\n",
      "Epoch: 7258 Training Loss: 0.13830890231662327 Test Loss: 0.39904144965277777\n",
      "Epoch: 7259 Training Loss: 0.13828265296088324 Test Loss: 0.39947173394097224\n",
      "Epoch: 7260 Training Loss: 0.1382706824408637 Test Loss: 0.399723876953125\n",
      "Epoch: 7261 Training Loss: 0.13826900651719834 Test Loss: 0.4000450032552083\n",
      "Epoch: 7262 Training Loss: 0.13827912055121527 Test Loss: 0.40019642469618055\n",
      "Epoch: 7263 Training Loss: 0.13828806898328994 Test Loss: 0.40031212022569446\n",
      "Epoch: 7264 Training Loss: 0.13831075456407335 Test Loss: 0.40026771375868053\n",
      "Epoch: 7265 Training Loss: 0.13835042911105686 Test Loss: 0.40020903862847224\n",
      "Epoch: 7266 Training Loss: 0.1383946762084961 Test Loss: 0.4001181098090278\n",
      "Epoch: 7267 Training Loss: 0.13843623352050782 Test Loss: 0.3998457573784722\n",
      "Epoch: 7268 Training Loss: 0.13849428219265408 Test Loss: 0.39968956163194447\n",
      "Epoch: 7269 Training Loss: 0.13855668640136717 Test Loss: 0.39953545464409723\n",
      "Epoch: 7270 Training Loss: 0.13863128153483073 Test Loss: 0.39925672743055557\n",
      "Epoch: 7271 Training Loss: 0.13871224466959636 Test Loss: 0.39907958984375\n",
      "Epoch: 7272 Training Loss: 0.13880433230929903 Test Loss: 0.3989881184895833\n",
      "Epoch: 7273 Training Loss: 0.13890395609537762 Test Loss: 0.3989216579861111\n",
      "Epoch: 7274 Training Loss: 0.13901401774088543 Test Loss: 0.39895638020833335\n",
      "Epoch: 7275 Training Loss: 0.1391223839653863 Test Loss: 0.39885698784722223\n",
      "Epoch: 7276 Training Loss: 0.13924077267116972 Test Loss: 0.3987764214409722\n",
      "Epoch: 7277 Training Loss: 0.13936232418484157 Test Loss: 0.3988098958333333\n",
      "Epoch: 7278 Training Loss: 0.13949094899495443 Test Loss: 0.3988408474392361\n",
      "Epoch: 7279 Training Loss: 0.13963694932725695 Test Loss: 0.3989617241753472\n",
      "Epoch: 7280 Training Loss: 0.13977675543891058 Test Loss: 0.39914651150173613\n",
      "Epoch: 7281 Training Loss: 0.13993355390760634 Test Loss: 0.39966471354166666\n",
      "Epoch: 7282 Training Loss: 0.1401076185438368 Test Loss: 0.40032074652777777\n",
      "Epoch: 7283 Training Loss: 0.1402920388115777 Test Loss: 0.4011060384114583\n",
      "Epoch: 7284 Training Loss: 0.14048152584499782 Test Loss: 0.4019848090277778\n",
      "Epoch: 7285 Training Loss: 0.14068002743191188 Test Loss: 0.40285774739583335\n",
      "Epoch: 7286 Training Loss: 0.14090646362304687 Test Loss: 0.40389317491319443\n",
      "Epoch: 7287 Training Loss: 0.14114792633056641 Test Loss: 0.40496101888020836\n",
      "Epoch: 7288 Training Loss: 0.14139486609564886 Test Loss: 0.4057112087673611\n",
      "Epoch: 7289 Training Loss: 0.14165079667833116 Test Loss: 0.40609285481770835\n",
      "Epoch: 7290 Training Loss: 0.14193819936116536 Test Loss: 0.4064146050347222\n",
      "Epoch: 7291 Training Loss: 0.14223058319091797 Test Loss: 0.4065043131510417\n",
      "Epoch: 7292 Training Loss: 0.14253077782524956 Test Loss: 0.40642439778645834\n",
      "Epoch: 7293 Training Loss: 0.1428415764702691 Test Loss: 0.40611244032118055\n",
      "Epoch: 7294 Training Loss: 0.14316111755371094 Test Loss: 0.40571223958333336\n",
      "Epoch: 7295 Training Loss: 0.14348618570963542 Test Loss: 0.4055066189236111\n",
      "Epoch: 7296 Training Loss: 0.1438225368923611 Test Loss: 0.40547032335069444\n",
      "Epoch: 7297 Training Loss: 0.14416718037923176 Test Loss: 0.4056799587673611\n",
      "Epoch: 7298 Training Loss: 0.1445055881076389 Test Loss: 0.4065276150173611\n",
      "Epoch: 7299 Training Loss: 0.1448484632703993 Test Loss: 0.4077505967881944\n",
      "Epoch: 7300 Training Loss: 0.14517849731445312 Test Loss: 0.40894954427083335\n",
      "Epoch: 7301 Training Loss: 0.14548759290907118 Test Loss: 0.41021956380208335\n",
      "Epoch: 7302 Training Loss: 0.145767827351888 Test Loss: 0.41083284505208334\n",
      "Epoch: 7303 Training Loss: 0.1460084211561415 Test Loss: 0.4110632595486111\n",
      "Epoch: 7304 Training Loss: 0.1461999715169271 Test Loss: 0.41095543077256946\n",
      "Epoch: 7305 Training Loss: 0.14636759609646266 Test Loss: 0.41046861436631943\n",
      "Epoch: 7306 Training Loss: 0.1465069834391276 Test Loss: 0.40918202039930557\n",
      "Epoch: 7307 Training Loss: 0.14659857008192273 Test Loss: 0.4073876681857639\n",
      "Epoch: 7308 Training Loss: 0.14663869222005207 Test Loss: 0.40540706380208336\n",
      "Epoch: 7309 Training Loss: 0.14658441501193575 Test Loss: 0.4036846788194444\n",
      "Epoch: 7310 Training Loss: 0.14642889234754775 Test Loss: 0.4024083658854167\n",
      "Epoch: 7311 Training Loss: 0.14615538872612846 Test Loss: 0.4017213812934028\n",
      "Epoch: 7312 Training Loss: 0.145777586195204 Test Loss: 0.4016088324652778\n",
      "Epoch: 7313 Training Loss: 0.14533599853515625 Test Loss: 0.40180143229166665\n",
      "Epoch: 7314 Training Loss: 0.14484995693630642 Test Loss: 0.40196351453993057\n",
      "Epoch: 7315 Training Loss: 0.1443513403998481 Test Loss: 0.40188286675347223\n",
      "Epoch: 7316 Training Loss: 0.1438916965060764 Test Loss: 0.4018318413628472\n",
      "Epoch: 7317 Training Loss: 0.1434876946343316 Test Loss: 0.40187782118055554\n",
      "Epoch: 7318 Training Loss: 0.14312543318006726 Test Loss: 0.4017354058159722\n",
      "Epoch: 7319 Training Loss: 0.1428058149549696 Test Loss: 0.40156163194444444\n",
      "Epoch: 7320 Training Loss: 0.14253759765625 Test Loss: 0.4013958062065972\n",
      "Epoch: 7321 Training Loss: 0.14229708353678386 Test Loss: 0.4010294867621528\n",
      "Epoch: 7322 Training Loss: 0.14207613881429038 Test Loss: 0.4007702365451389\n",
      "Epoch: 7323 Training Loss: 0.14187065717909073 Test Loss: 0.40018226453993055\n",
      "Epoch: 7324 Training Loss: 0.1416786397298177 Test Loss: 0.39963492838541664\n",
      "Epoch: 7325 Training Loss: 0.14148815324571398 Test Loss: 0.3991730143229167\n",
      "Epoch: 7326 Training Loss: 0.1412994172837999 Test Loss: 0.3987100694444444\n",
      "Epoch: 7327 Training Loss: 0.14112992519802517 Test Loss: 0.3982506510416667\n",
      "Epoch: 7328 Training Loss: 0.14096485392252603 Test Loss: 0.3978040093315972\n",
      "Epoch: 7329 Training Loss: 0.14081836191813152 Test Loss: 0.3974905056423611\n",
      "Epoch: 7330 Training Loss: 0.1406821594238281 Test Loss: 0.39728409830729167\n",
      "Epoch: 7331 Training Loss: 0.14055191633436415 Test Loss: 0.397074462890625\n",
      "Epoch: 7332 Training Loss: 0.14043629879421657 Test Loss: 0.39694661458333336\n",
      "Epoch: 7333 Training Loss: 0.14033102332221137 Test Loss: 0.3970722927517361\n",
      "Epoch: 7334 Training Loss: 0.14022558254665798 Test Loss: 0.3972856174045139\n",
      "Epoch: 7335 Training Loss: 0.14012086317274305 Test Loss: 0.3975447048611111\n",
      "Epoch: 7336 Training Loss: 0.14002593570285374 Test Loss: 0.397857666015625\n",
      "Epoch: 7337 Training Loss: 0.139962774488661 Test Loss: 0.39825439453125\n",
      "Epoch: 7338 Training Loss: 0.1399096399943034 Test Loss: 0.39873128255208334\n",
      "Epoch: 7339 Training Loss: 0.13986624145507812 Test Loss: 0.39902498372395834\n",
      "Epoch: 7340 Training Loss: 0.1398376736111111 Test Loss: 0.3995231119791667\n",
      "Epoch: 7341 Training Loss: 0.13982247670491538 Test Loss: 0.3999157986111111\n",
      "Epoch: 7342 Training Loss: 0.13980118730333116 Test Loss: 0.40036366102430554\n",
      "Epoch: 7343 Training Loss: 0.13978294457329643 Test Loss: 0.40092784288194444\n",
      "Epoch: 7344 Training Loss: 0.13978861490885416 Test Loss: 0.4012401258680556\n",
      "Epoch: 7345 Training Loss: 0.13980339135064018 Test Loss: 0.4015577799479167\n",
      "Epoch: 7346 Training Loss: 0.1398274205525716 Test Loss: 0.40174180772569446\n",
      "Epoch: 7347 Training Loss: 0.13986584811740452 Test Loss: 0.4018037109375\n",
      "Epoch: 7348 Training Loss: 0.13992736392550997 Test Loss: 0.401849609375\n",
      "Epoch: 7349 Training Loss: 0.14000479973687066 Test Loss: 0.40174150933159725\n",
      "Epoch: 7350 Training Loss: 0.1401169891357422 Test Loss: 0.4014306098090278\n",
      "Epoch: 7351 Training Loss: 0.14026826900906034 Test Loss: 0.4011915961371528\n",
      "Epoch: 7352 Training Loss: 0.1404558842976888 Test Loss: 0.40077674696180554\n",
      "Epoch: 7353 Training Loss: 0.14066890631781684 Test Loss: 0.40023931206597224\n",
      "Epoch: 7354 Training Loss: 0.1409038636949327 Test Loss: 0.3995262586805556\n",
      "Epoch: 7355 Training Loss: 0.1411697531806098 Test Loss: 0.3986752658420139\n",
      "Epoch: 7356 Training Loss: 0.14147240532769098 Test Loss: 0.39741094292534723\n",
      "Epoch: 7357 Training Loss: 0.14181641472710504 Test Loss: 0.39620659722222223\n",
      "Epoch: 7358 Training Loss: 0.1422283231947157 Test Loss: 0.39484209526909725\n",
      "Epoch: 7359 Training Loss: 0.14266857486301 Test Loss: 0.3937612847222222\n",
      "Epoch: 7360 Training Loss: 0.14313168504503038 Test Loss: 0.3927077094184028\n",
      "Epoch: 7361 Training Loss: 0.14362016042073567 Test Loss: 0.39277137586805555\n",
      "Epoch: 7362 Training Loss: 0.1441015353732639 Test Loss: 0.39347249348958335\n",
      "Epoch: 7363 Training Loss: 0.14453182729085287 Test Loss: 0.3947314181857639\n",
      "Epoch: 7364 Training Loss: 0.14490753767225478 Test Loss: 0.39661214192708333\n",
      "Epoch: 7365 Training Loss: 0.1451895226372613 Test Loss: 0.3980544162326389\n",
      "Epoch: 7366 Training Loss: 0.1453550542195638 Test Loss: 0.39870670572916667\n",
      "Epoch: 7367 Training Loss: 0.1453579093085395 Test Loss: 0.3985329318576389\n",
      "Epoch: 7368 Training Loss: 0.1451776902940538 Test Loss: 0.39736935763888886\n",
      "Epoch: 7369 Training Loss: 0.14476356336805554 Test Loss: 0.39617583550347224\n",
      "Epoch: 7370 Training Loss: 0.14414923434787327 Test Loss: 0.39491153971354165\n",
      "Epoch: 7371 Training Loss: 0.14336952718098958 Test Loss: 0.3937302517361111\n",
      "Epoch: 7372 Training Loss: 0.14250579325358073 Test Loss: 0.3928146701388889\n",
      "Epoch: 7373 Training Loss: 0.14163053639729817 Test Loss: 0.392514404296875\n",
      "Epoch: 7374 Training Loss: 0.14080970594618056 Test Loss: 0.39259505208333334\n",
      "Epoch: 7375 Training Loss: 0.14008944193522135 Test Loss: 0.39279310438368054\n",
      "Epoch: 7376 Training Loss: 0.13946233791775173 Test Loss: 0.39290087890625\n",
      "Epoch: 7377 Training Loss: 0.1389228507147895 Test Loss: 0.39297856987847224\n",
      "Epoch: 7378 Training Loss: 0.13849396091037328 Test Loss: 0.39304313151041664\n",
      "Epoch: 7379 Training Loss: 0.13815763515896268 Test Loss: 0.39299278428819445\n",
      "Epoch: 7380 Training Loss: 0.1378950949774848 Test Loss: 0.39303515625\n",
      "Epoch: 7381 Training Loss: 0.1376944088406033 Test Loss: 0.3930602213541667\n",
      "Epoch: 7382 Training Loss: 0.1375391057332357 Test Loss: 0.3931637369791667\n",
      "Epoch: 7383 Training Loss: 0.13743205261230468 Test Loss: 0.3932372504340278\n",
      "Epoch: 7384 Training Loss: 0.13735504489474826 Test Loss: 0.3934415418836806\n",
      "Epoch: 7385 Training Loss: 0.13729165988498263 Test Loss: 0.3936740451388889\n",
      "Epoch: 7386 Training Loss: 0.1372494828965929 Test Loss: 0.393916748046875\n",
      "Epoch: 7387 Training Loss: 0.13722356838650174 Test Loss: 0.394161865234375\n",
      "Epoch: 7388 Training Loss: 0.13721309577094185 Test Loss: 0.39450689019097224\n",
      "Epoch: 7389 Training Loss: 0.13721750725640192 Test Loss: 0.39478477647569443\n",
      "Epoch: 7390 Training Loss: 0.13723611365424263 Test Loss: 0.3950646701388889\n",
      "Epoch: 7391 Training Loss: 0.13727119784884984 Test Loss: 0.39539529079861113\n",
      "Epoch: 7392 Training Loss: 0.13731308322482638 Test Loss: 0.39557524956597223\n",
      "Epoch: 7393 Training Loss: 0.1373663347032335 Test Loss: 0.39570201280381945\n",
      "Epoch: 7394 Training Loss: 0.13742586347791882 Test Loss: 0.39585883246527775\n",
      "Epoch: 7395 Training Loss: 0.13749685753716362 Test Loss: 0.39597900390625\n",
      "Epoch: 7396 Training Loss: 0.13757248942057293 Test Loss: 0.3961068793402778\n",
      "Epoch: 7397 Training Loss: 0.1376498777601454 Test Loss: 0.39638387044270834\n",
      "Epoch: 7398 Training Loss: 0.13773714362250433 Test Loss: 0.396710205078125\n",
      "Epoch: 7399 Training Loss: 0.13783609178331163 Test Loss: 0.39687890625\n",
      "Epoch: 7400 Training Loss: 0.1379426574707031 Test Loss: 0.3970638020833333\n",
      "Epoch: 7401 Training Loss: 0.13805972120496962 Test Loss: 0.39736501736111113\n",
      "Epoch: 7402 Training Loss: 0.13819088236490887 Test Loss: 0.39769596354166664\n",
      "Epoch: 7403 Training Loss: 0.13832320743136936 Test Loss: 0.3980920681423611\n",
      "Epoch: 7404 Training Loss: 0.13845933363172744 Test Loss: 0.39862776692708335\n",
      "Epoch: 7405 Training Loss: 0.13860600958930122 Test Loss: 0.39918221028645834\n",
      "Epoch: 7406 Training Loss: 0.13877667744954428 Test Loss: 0.39976513671875\n",
      "Epoch: 7407 Training Loss: 0.1389578170776367 Test Loss: 0.4004667697482639\n",
      "Epoch: 7408 Training Loss: 0.13914054361979167 Test Loss: 0.40094544813368055\n",
      "Epoch: 7409 Training Loss: 0.1393289091322157 Test Loss: 0.40160164388020836\n",
      "Epoch: 7410 Training Loss: 0.13953610144721137 Test Loss: 0.4021899142795139\n",
      "Epoch: 7411 Training Loss: 0.13977422756618924 Test Loss: 0.4026882595486111\n",
      "Epoch: 7412 Training Loss: 0.14002180396185981 Test Loss: 0.4032202419704861\n",
      "Epoch: 7413 Training Loss: 0.14028431701660157 Test Loss: 0.40372349717881945\n",
      "Epoch: 7414 Training Loss: 0.14055974748399522 Test Loss: 0.4039191623263889\n",
      "Epoch: 7415 Training Loss: 0.1408277130126953 Test Loss: 0.40394349500868054\n",
      "Epoch: 7416 Training Loss: 0.14109427642822264 Test Loss: 0.40387779405381946\n",
      "Epoch: 7417 Training Loss: 0.14138646274142797 Test Loss: 0.40373651801215277\n",
      "Epoch: 7418 Training Loss: 0.14167191229926215 Test Loss: 0.4034526095920139\n",
      "Epoch: 7419 Training Loss: 0.14194171057807076 Test Loss: 0.4030189887152778\n",
      "Epoch: 7420 Training Loss: 0.14221607293023003 Test Loss: 0.40249430338541664\n",
      "Epoch: 7421 Training Loss: 0.14246422492133246 Test Loss: 0.402309326171875\n",
      "Epoch: 7422 Training Loss: 0.14270024787055122 Test Loss: 0.4019980740017361\n",
      "Epoch: 7423 Training Loss: 0.14288852861192491 Test Loss: 0.40203553602430553\n",
      "Epoch: 7424 Training Loss: 0.14305201127794054 Test Loss: 0.4023937717013889\n",
      "Epoch: 7425 Training Loss: 0.14315833197699654 Test Loss: 0.4031088324652778\n",
      "Epoch: 7426 Training Loss: 0.14321140797932944 Test Loss: 0.40436257595486114\n",
      "Epoch: 7427 Training Loss: 0.14321227773030598 Test Loss: 0.40561593967013887\n",
      "Epoch: 7428 Training Loss: 0.14316903771294487 Test Loss: 0.40682516818576386\n",
      "Epoch: 7429 Training Loss: 0.1430692418416341 Test Loss: 0.4073350151909722\n",
      "Epoch: 7430 Training Loss: 0.1428962156507704 Test Loss: 0.40733851453993053\n",
      "Epoch: 7431 Training Loss: 0.142646848042806 Test Loss: 0.4069188910590278\n",
      "Epoch: 7432 Training Loss: 0.14231706237792968 Test Loss: 0.405742431640625\n",
      "Epoch: 7433 Training Loss: 0.14192857615152996 Test Loss: 0.40436740451388886\n",
      "Epoch: 7434 Training Loss: 0.1414990539550781 Test Loss: 0.40277400716145834\n",
      "Epoch: 7435 Training Loss: 0.14103792911105686 Test Loss: 0.401489013671875\n",
      "Epoch: 7436 Training Loss: 0.14058625878228082 Test Loss: 0.4004577365451389\n",
      "Epoch: 7437 Training Loss: 0.14018126254611546 Test Loss: 0.40025667317708336\n",
      "Epoch: 7438 Training Loss: 0.13983085038926865 Test Loss: 0.400378662109375\n",
      "Epoch: 7439 Training Loss: 0.13951543680826822 Test Loss: 0.40081171332465276\n",
      "Epoch: 7440 Training Loss: 0.13922179497612847 Test Loss: 0.40119712999131946\n",
      "Epoch: 7441 Training Loss: 0.13894425286187065 Test Loss: 0.4015489908854167\n",
      "Epoch: 7442 Training Loss: 0.13869379255506728 Test Loss: 0.40190011935763886\n",
      "Epoch: 7443 Training Loss: 0.1384491441514757 Test Loss: 0.4018629557291667\n",
      "Epoch: 7444 Training Loss: 0.1382183142768012 Test Loss: 0.40166883680555554\n",
      "Epoch: 7445 Training Loss: 0.13799300723605687 Test Loss: 0.401283203125\n",
      "Epoch: 7446 Training Loss: 0.13777200571695963 Test Loss: 0.40057090928819444\n",
      "Epoch: 7447 Training Loss: 0.13756397416856553 Test Loss: 0.39999864366319443\n",
      "Epoch: 7448 Training Loss: 0.137358523050944 Test Loss: 0.39913075086805555\n",
      "Epoch: 7449 Training Loss: 0.13716664632161457 Test Loss: 0.39859195963541666\n",
      "Epoch: 7450 Training Loss: 0.13698621029324 Test Loss: 0.39819832356770835\n",
      "Epoch: 7451 Training Loss: 0.1368125508626302 Test Loss: 0.39798790147569446\n",
      "Epoch: 7452 Training Loss: 0.13665827009412979 Test Loss: 0.3976317274305556\n",
      "Epoch: 7453 Training Loss: 0.13652955966525607 Test Loss: 0.39748885091145836\n",
      "Epoch: 7454 Training Loss: 0.13642688412136503 Test Loss: 0.3974461263020833\n",
      "Epoch: 7455 Training Loss: 0.13633544158935545 Test Loss: 0.3976904296875\n",
      "Epoch: 7456 Training Loss: 0.13626462046305338 Test Loss: 0.3978934190538194\n",
      "Epoch: 7457 Training Loss: 0.13621329074435765 Test Loss: 0.3983123372395833\n",
      "Epoch: 7458 Training Loss: 0.1361707017686632 Test Loss: 0.39843787977430556\n",
      "Epoch: 7459 Training Loss: 0.1361435029771593 Test Loss: 0.3987367892795139\n",
      "Epoch: 7460 Training Loss: 0.1361324717203776 Test Loss: 0.3989856770833333\n",
      "Epoch: 7461 Training Loss: 0.13614103529188368 Test Loss: 0.3993222113715278\n",
      "Epoch: 7462 Training Loss: 0.13616092427571613 Test Loss: 0.3995559353298611\n",
      "Epoch: 7463 Training Loss: 0.13619892205132378 Test Loss: 0.3997712131076389\n",
      "Epoch: 7464 Training Loss: 0.1362480731540256 Test Loss: 0.4000399848090278\n",
      "Epoch: 7465 Training Loss: 0.13631211344401042 Test Loss: 0.4001520724826389\n",
      "Epoch: 7466 Training Loss: 0.13639109124077692 Test Loss: 0.4004075249565972\n",
      "Epoch: 7467 Training Loss: 0.13648028649224175 Test Loss: 0.40034293619791667\n",
      "Epoch: 7468 Training Loss: 0.13658009253607856 Test Loss: 0.40034681532118055\n",
      "Epoch: 7469 Training Loss: 0.13668807050916884 Test Loss: 0.4002055121527778\n",
      "Epoch: 7470 Training Loss: 0.13679573482937282 Test Loss: 0.4001279296875\n",
      "Epoch: 7471 Training Loss: 0.13692305925157336 Test Loss: 0.39965977647569445\n",
      "Epoch: 7472 Training Loss: 0.13706124623616536 Test Loss: 0.39955316840277777\n",
      "Epoch: 7473 Training Loss: 0.137200070699056 Test Loss: 0.399501220703125\n",
      "Epoch: 7474 Training Loss: 0.13735098266601561 Test Loss: 0.3992238226996528\n",
      "Epoch: 7475 Training Loss: 0.13751616075303819 Test Loss: 0.39895597330729166\n",
      "Epoch: 7476 Training Loss: 0.13769458940294055 Test Loss: 0.3987719997829861\n",
      "Epoch: 7477 Training Loss: 0.13787237972683378 Test Loss: 0.3984794379340278\n",
      "Epoch: 7478 Training Loss: 0.13805174340142143 Test Loss: 0.39839219835069445\n",
      "Epoch: 7479 Training Loss: 0.1382549116346571 Test Loss: 0.39904310438368057\n",
      "Epoch: 7480 Training Loss: 0.13846805318196614 Test Loss: 0.4000541449652778\n",
      "Epoch: 7481 Training Loss: 0.1386959465874566 Test Loss: 0.40109510633680556\n",
      "Epoch: 7482 Training Loss: 0.13889595540364583 Test Loss: 0.40199018012152776\n",
      "Epoch: 7483 Training Loss: 0.1390984344482422 Test Loss: 0.40291200086805556\n",
      "Epoch: 7484 Training Loss: 0.13930961354573568 Test Loss: 0.4038173285590278\n",
      "Epoch: 7485 Training Loss: 0.1395105717976888 Test Loss: 0.40471945529513886\n",
      "Epoch: 7486 Training Loss: 0.1397087843153212 Test Loss: 0.4052363823784722\n",
      "Epoch: 7487 Training Loss: 0.13990118153889974 Test Loss: 0.40536466471354166\n",
      "Epoch: 7488 Training Loss: 0.140067140367296 Test Loss: 0.4057038302951389\n",
      "Epoch: 7489 Training Loss: 0.1402284647623698 Test Loss: 0.40517542860243055\n",
      "Epoch: 7490 Training Loss: 0.14038982899983724 Test Loss: 0.4046626247829861\n",
      "Epoch: 7491 Training Loss: 0.14052750566270616 Test Loss: 0.4044509548611111\n",
      "Epoch: 7492 Training Loss: 0.1406487316555447 Test Loss: 0.4036655815972222\n",
      "Epoch: 7493 Training Loss: 0.14076637437608508 Test Loss: 0.4031377495659722\n",
      "Epoch: 7494 Training Loss: 0.14087828572591146 Test Loss: 0.4025984157986111\n",
      "Epoch: 7495 Training Loss: 0.14097195265028212 Test Loss: 0.40232014973958335\n",
      "Epoch: 7496 Training Loss: 0.1410660629272461 Test Loss: 0.40243277994791665\n",
      "Epoch: 7497 Training Loss: 0.14114471011691623 Test Loss: 0.4025190700954861\n",
      "Epoch: 7498 Training Loss: 0.14123412153455947 Test Loss: 0.4030247124565972\n",
      "Epoch: 7499 Training Loss: 0.14132434929741752 Test Loss: 0.40358192274305554\n",
      "Epoch: 7500 Training Loss: 0.14140589650472005 Test Loss: 0.40377501085069445\n",
      "Epoch: 7501 Training Loss: 0.14146990966796874 Test Loss: 0.4036618923611111\n",
      "Epoch: 7502 Training Loss: 0.14153769938151042 Test Loss: 0.40306597222222224\n",
      "Epoch: 7503 Training Loss: 0.14158660634358725 Test Loss: 0.40274763997395835\n",
      "Epoch: 7504 Training Loss: 0.14164285702175564 Test Loss: 0.4025028211805556\n",
      "Epoch: 7505 Training Loss: 0.14168425835503473 Test Loss: 0.40244742838541664\n",
      "Epoch: 7506 Training Loss: 0.14170023685031466 Test Loss: 0.4022921820746528\n",
      "Epoch: 7507 Training Loss: 0.1416809531317817 Test Loss: 0.40224509006076387\n",
      "Epoch: 7508 Training Loss: 0.141622556898329 Test Loss: 0.40174799262152777\n",
      "Epoch: 7509 Training Loss: 0.14156876712375216 Test Loss: 0.4011587456597222\n",
      "Epoch: 7510 Training Loss: 0.14150755649142796 Test Loss: 0.4000533854166667\n",
      "Epoch: 7511 Training Loss: 0.1414545652601454 Test Loss: 0.39866951497395836\n",
      "Epoch: 7512 Training Loss: 0.1414320017496745 Test Loss: 0.3978213704427083\n",
      "Epoch: 7513 Training Loss: 0.14141993035210504 Test Loss: 0.3973698459201389\n",
      "Epoch: 7514 Training Loss: 0.14141116333007814 Test Loss: 0.39746869574652777\n",
      "Epoch: 7515 Training Loss: 0.14137346733940973 Test Loss: 0.39752568901909724\n",
      "Epoch: 7516 Training Loss: 0.14129521857367622 Test Loss: 0.3972561848958333\n",
      "Epoch: 7517 Training Loss: 0.1411950488620334 Test Loss: 0.3964397515190972\n",
      "Epoch: 7518 Training Loss: 0.14106911638047961 Test Loss: 0.3954013671875\n",
      "Epoch: 7519 Training Loss: 0.14090105183919271 Test Loss: 0.3944210883246528\n",
      "Epoch: 7520 Training Loss: 0.14071855163574218 Test Loss: 0.39358761935763886\n",
      "Epoch: 7521 Training Loss: 0.14052540588378906 Test Loss: 0.39294981553819447\n",
      "Epoch: 7522 Training Loss: 0.1403299526638455 Test Loss: 0.39278070746527777\n",
      "Epoch: 7523 Training Loss: 0.1401432825724284 Test Loss: 0.39280186631944447\n",
      "Epoch: 7524 Training Loss: 0.14000571695963543 Test Loss: 0.39333110894097223\n",
      "Epoch: 7525 Training Loss: 0.13992156643337675 Test Loss: 0.39453697374131946\n",
      "Epoch: 7526 Training Loss: 0.13988158586290148 Test Loss: 0.39578233506944444\n",
      "Epoch: 7527 Training Loss: 0.1399045681423611 Test Loss: 0.39715782335069444\n",
      "Epoch: 7528 Training Loss: 0.1399813927544488 Test Loss: 0.3984794379340278\n",
      "Epoch: 7529 Training Loss: 0.14011646864149305 Test Loss: 0.39980148654513886\n",
      "Epoch: 7530 Training Loss: 0.1403062506781684 Test Loss: 0.4008896484375\n",
      "Epoch: 7531 Training Loss: 0.14054254235161676 Test Loss: 0.4019884982638889\n",
      "Epoch: 7532 Training Loss: 0.14083787112765841 Test Loss: 0.4027316080729167\n",
      "Epoch: 7533 Training Loss: 0.1411719004313151 Test Loss: 0.40332801649305555\n",
      "Epoch: 7534 Training Loss: 0.14154512786865234 Test Loss: 0.4037079535590278\n",
      "Epoch: 7535 Training Loss: 0.14196950954861112 Test Loss: 0.40404093424479165\n",
      "Epoch: 7536 Training Loss: 0.14243675316704643 Test Loss: 0.40424294704861113\n",
      "Epoch: 7537 Training Loss: 0.1429421903822157 Test Loss: 0.40373806423611114\n",
      "Epoch: 7538 Training Loss: 0.14347147538926866 Test Loss: 0.4030499403211806\n",
      "Epoch: 7539 Training Loss: 0.14401711781819662 Test Loss: 0.4021618381076389\n",
      "Epoch: 7540 Training Loss: 0.14455937872992622 Test Loss: 0.4004949001736111\n",
      "Epoch: 7541 Training Loss: 0.14512639872233074 Test Loss: 0.3990187717013889\n",
      "Epoch: 7542 Training Loss: 0.14565147060818143 Test Loss: 0.39825906032986114\n",
      "Epoch: 7543 Training Loss: 0.14603106350368925 Test Loss: 0.39877880859375\n",
      "Epoch: 7544 Training Loss: 0.14633506944444444 Test Loss: 0.3994954427083333\n",
      "Epoch: 7545 Training Loss: 0.1464486083984375 Test Loss: 0.4003127170138889\n",
      "Epoch: 7546 Training Loss: 0.14645064629448784 Test Loss: 0.40245914713541664\n",
      "Epoch: 7547 Training Loss: 0.1464243435329861 Test Loss: 0.40500547960069444\n",
      "Epoch: 7548 Training Loss: 0.14645470852322048 Test Loss: 0.40672509765625\n",
      "Epoch: 7549 Training Loss: 0.14660823059082032 Test Loss: 0.4070737847222222\n",
      "Epoch: 7550 Training Loss: 0.14692007191975912 Test Loss: 0.40637662760416665\n",
      "Epoch: 7551 Training Loss: 0.14725465477837457 Test Loss: 0.40550767686631944\n",
      "Epoch: 7552 Training Loss: 0.14756988779703775 Test Loss: 0.4050075412326389\n",
      "Epoch: 7553 Training Loss: 0.14786719767252604 Test Loss: 0.40501774088541664\n",
      "Epoch: 7554 Training Loss: 0.14807134331597221 Test Loss: 0.40490863715277775\n",
      "Epoch: 7555 Training Loss: 0.1481791042751736 Test Loss: 0.4053906521267361\n",
      "Epoch: 7556 Training Loss: 0.14816118876139323 Test Loss: 0.4063263888888889\n",
      "Epoch: 7557 Training Loss: 0.1479998745388455 Test Loss: 0.40655322265625\n",
      "Epoch: 7558 Training Loss: 0.14770948791503907 Test Loss: 0.4056508517795139\n",
      "Epoch: 7559 Training Loss: 0.1472935045030382 Test Loss: 0.4042881401909722\n",
      "Epoch: 7560 Training Loss: 0.14681646389431424 Test Loss: 0.4041260850694444\n",
      "Epoch: 7561 Training Loss: 0.1462898966471354 Test Loss: 0.40468424479166665\n",
      "Epoch: 7562 Training Loss: 0.1456853485107422 Test Loss: 0.4059605305989583\n",
      "Epoch: 7563 Training Loss: 0.14498681640625 Test Loss: 0.40741373697916666\n",
      "Epoch: 7564 Training Loss: 0.14417367384168836 Test Loss: 0.4084230685763889\n",
      "Epoch: 7565 Training Loss: 0.14326308017306857 Test Loss: 0.4083592122395833\n",
      "Epoch: 7566 Training Loss: 0.14231148952907985 Test Loss: 0.40776893446180557\n",
      "Epoch: 7567 Training Loss: 0.1413846952650282 Test Loss: 0.4068774142795139\n",
      "Epoch: 7568 Training Loss: 0.1405365702311198 Test Loss: 0.40567713758680557\n",
      "Epoch: 7569 Training Loss: 0.13976739925808376 Test Loss: 0.40448453776041665\n",
      "Epoch: 7570 Training Loss: 0.13905226813422308 Test Loss: 0.40311637369791664\n",
      "Epoch: 7571 Training Loss: 0.13841662004258898 Test Loss: 0.4018694118923611\n",
      "Epoch: 7572 Training Loss: 0.1378578855726454 Test Loss: 0.4007522515190972\n",
      "Epoch: 7573 Training Loss: 0.13738065592447918 Test Loss: 0.3995730794270833\n",
      "Epoch: 7574 Training Loss: 0.13697003343370226 Test Loss: 0.3988169216579861\n",
      "Epoch: 7575 Training Loss: 0.13661353386773004 Test Loss: 0.3979829644097222\n",
      "Epoch: 7576 Training Loss: 0.13630585564507378 Test Loss: 0.39718760850694446\n",
      "Epoch: 7577 Training Loss: 0.1360515128241645 Test Loss: 0.3966330837673611\n",
      "Epoch: 7578 Training Loss: 0.13584068044026693 Test Loss: 0.39608935546875\n",
      "Epoch: 7579 Training Loss: 0.1356619576348199 Test Loss: 0.39565657552083333\n",
      "Epoch: 7580 Training Loss: 0.13551406775580513 Test Loss: 0.39534898546006947\n",
      "Epoch: 7581 Training Loss: 0.13539595455593534 Test Loss: 0.39510926649305556\n",
      "Epoch: 7582 Training Loss: 0.13530269114176433 Test Loss: 0.3949441460503472\n",
      "Epoch: 7583 Training Loss: 0.1352299796210395 Test Loss: 0.3947701551649306\n",
      "Epoch: 7584 Training Loss: 0.13517833794487846 Test Loss: 0.3947577311197917\n",
      "Epoch: 7585 Training Loss: 0.1351538314819336 Test Loss: 0.3946969943576389\n",
      "Epoch: 7586 Training Loss: 0.13515514628092448 Test Loss: 0.3948444552951389\n",
      "Epoch: 7587 Training Loss: 0.1351794153849284 Test Loss: 0.3950239529079861\n",
      "Epoch: 7588 Training Loss: 0.1352175750732422 Test Loss: 0.3954324001736111\n",
      "Epoch: 7589 Training Loss: 0.13526197136773002 Test Loss: 0.3958356662326389\n",
      "Epoch: 7590 Training Loss: 0.13530957200792101 Test Loss: 0.39634990776909723\n",
      "Epoch: 7591 Training Loss: 0.1353710979885525 Test Loss: 0.39714811197916666\n",
      "Epoch: 7592 Training Loss: 0.13545351325141058 Test Loss: 0.39804752604166665\n",
      "Epoch: 7593 Training Loss: 0.13554738277859157 Test Loss: 0.39905360243055554\n",
      "Epoch: 7594 Training Loss: 0.13565984937879774 Test Loss: 0.40008167860243055\n",
      "Epoch: 7595 Training Loss: 0.13580035146077474 Test Loss: 0.4010667046440972\n",
      "Epoch: 7596 Training Loss: 0.13596481068929037 Test Loss: 0.401366455078125\n",
      "Epoch: 7597 Training Loss: 0.1361185582478841 Test Loss: 0.40096365017361113\n",
      "Epoch: 7598 Training Loss: 0.13629862043592664 Test Loss: 0.4001153700086806\n",
      "Epoch: 7599 Training Loss: 0.13658292304144964 Test Loss: 0.4001599392361111\n",
      "Epoch: 7600 Training Loss: 0.1369578128390842 Test Loss: 0.3988422037760417\n",
      "Epoch: 7601 Training Loss: 0.13728020985921224 Test Loss: 0.39606925455729164\n",
      "Epoch: 7602 Training Loss: 0.13752556610107422 Test Loss: 0.3947156032986111\n",
      "Epoch: 7603 Training Loss: 0.13770274183485243 Test Loss: 0.3966755099826389\n",
      "Epoch: 7604 Training Loss: 0.1378497280544705 Test Loss: 0.3992156575520833\n",
      "Epoch: 7605 Training Loss: 0.13789036814371744 Test Loss: 0.39826307508680553\n",
      "Epoch: 7606 Training Loss: 0.13779026879204645 Test Loss: 0.3950304904513889\n",
      "Epoch: 7607 Training Loss: 0.13764872402615017 Test Loss: 0.3930729437934028\n",
      "Epoch: 7608 Training Loss: 0.1375377426147461 Test Loss: 0.39281559244791664\n",
      "Epoch: 7609 Training Loss: 0.13755658382839628 Test Loss: 0.39273795572916664\n",
      "Epoch: 7610 Training Loss: 0.13768387264675563 Test Loss: 0.3926302354600694\n",
      "Epoch: 7611 Training Loss: 0.13790636528862849 Test Loss: 0.39277821180555555\n",
      "Epoch: 7612 Training Loss: 0.1381758550008138 Test Loss: 0.39323763020833336\n",
      "Epoch: 7613 Training Loss: 0.13848645952012803 Test Loss: 0.39398499891493055\n",
      "Epoch: 7614 Training Loss: 0.13884866417778863 Test Loss: 0.39491246202256947\n",
      "Epoch: 7615 Training Loss: 0.1392414016723633 Test Loss: 0.3958740776909722\n",
      "Epoch: 7616 Training Loss: 0.13967659420437284 Test Loss: 0.39682421875\n",
      "Epoch: 7617 Training Loss: 0.140130130343967 Test Loss: 0.3975424262152778\n",
      "Epoch: 7618 Training Loss: 0.14061467742919923 Test Loss: 0.3979145779079861\n",
      "Epoch: 7619 Training Loss: 0.1411254433525933 Test Loss: 0.39791978624131946\n",
      "Epoch: 7620 Training Loss: 0.14160997856987848 Test Loss: 0.39722401258680556\n",
      "Epoch: 7621 Training Loss: 0.14206073082817924 Test Loss: 0.39631624348958333\n",
      "Epoch: 7622 Training Loss: 0.1424395980834961 Test Loss: 0.39560118272569444\n",
      "Epoch: 7623 Training Loss: 0.14265765211317274 Test Loss: 0.3960960286458333\n",
      "Epoch: 7624 Training Loss: 0.14270008595784506 Test Loss: 0.39785026041666666\n",
      "Epoch: 7625 Training Loss: 0.14257005225287545 Test Loss: 0.4006228298611111\n",
      "Epoch: 7626 Training Loss: 0.14230135175916883 Test Loss: 0.4040478515625\n",
      "Epoch: 7627 Training Loss: 0.14195777045355903 Test Loss: 0.4064447699652778\n",
      "Epoch: 7628 Training Loss: 0.14156378428141275 Test Loss: 0.40717198350694445\n",
      "Epoch: 7629 Training Loss: 0.14112976667616103 Test Loss: 0.4061210394965278\n",
      "Epoch: 7630 Training Loss: 0.14062173461914063 Test Loss: 0.4038275824652778\n",
      "Epoch: 7631 Training Loss: 0.14003306579589844 Test Loss: 0.4007715386284722\n",
      "Epoch: 7632 Training Loss: 0.1393633312649197 Test Loss: 0.39765299479166666\n",
      "Epoch: 7633 Training Loss: 0.1386617414686415 Test Loss: 0.3951912163628472\n",
      "Epoch: 7634 Training Loss: 0.13796415201822917 Test Loss: 0.39325927734375\n",
      "Epoch: 7635 Training Loss: 0.13729666222466363 Test Loss: 0.39224560546875\n",
      "Epoch: 7636 Training Loss: 0.13668987782796224 Test Loss: 0.39210536024305553\n",
      "Epoch: 7637 Training Loss: 0.13615487586127387 Test Loss: 0.39240557183159724\n",
      "Epoch: 7638 Training Loss: 0.13569393581814237 Test Loss: 0.3927625325520833\n",
      "Epoch: 7639 Training Loss: 0.13530132717556423 Test Loss: 0.39317268880208334\n",
      "Epoch: 7640 Training Loss: 0.13498355695936415 Test Loss: 0.3936841905381944\n",
      "Epoch: 7641 Training Loss: 0.1347286181979709 Test Loss: 0.39406136067708336\n",
      "Epoch: 7642 Training Loss: 0.13452130466037326 Test Loss: 0.3944086371527778\n",
      "Epoch: 7643 Training Loss: 0.13434952969021266 Test Loss: 0.39474050564236113\n",
      "Epoch: 7644 Training Loss: 0.13420394219292534 Test Loss: 0.39486905924479165\n",
      "Epoch: 7645 Training Loss: 0.1340860349867079 Test Loss: 0.3949517686631944\n",
      "Epoch: 7646 Training Loss: 0.13398196411132812 Test Loss: 0.39492900933159725\n",
      "Epoch: 7647 Training Loss: 0.13387716166178384 Test Loss: 0.3947018771701389\n",
      "Epoch: 7648 Training Loss: 0.13377935960557727 Test Loss: 0.39454313151041664\n",
      "Epoch: 7649 Training Loss: 0.13369011857774524 Test Loss: 0.394248779296875\n",
      "Epoch: 7650 Training Loss: 0.13360858493381075 Test Loss: 0.3939294162326389\n",
      "Epoch: 7651 Training Loss: 0.13353721110026043 Test Loss: 0.3936539713541667\n",
      "Epoch: 7652 Training Loss: 0.13347114902072482 Test Loss: 0.39330528428819445\n",
      "Epoch: 7653 Training Loss: 0.1334137700398763 Test Loss: 0.3929656846788194\n",
      "Epoch: 7654 Training Loss: 0.13336143408881293 Test Loss: 0.39261604817708334\n",
      "Epoch: 7655 Training Loss: 0.13332048712836372 Test Loss: 0.39234602864583334\n",
      "Epoch: 7656 Training Loss: 0.13329107242160373 Test Loss: 0.39202256944444447\n",
      "Epoch: 7657 Training Loss: 0.1332637693617079 Test Loss: 0.39185850694444446\n",
      "Epoch: 7658 Training Loss: 0.13324279191758898 Test Loss: 0.3916270073784722\n",
      "Epoch: 7659 Training Loss: 0.1332279569837782 Test Loss: 0.3915706108940972\n",
      "Epoch: 7660 Training Loss: 0.1332205318874783 Test Loss: 0.39152159288194444\n",
      "Epoch: 7661 Training Loss: 0.13321392483181424 Test Loss: 0.3915082194010417\n",
      "Epoch: 7662 Training Loss: 0.1332152345445421 Test Loss: 0.39152872721354165\n",
      "Epoch: 7663 Training Loss: 0.13322571902804906 Test Loss: 0.39153746202256945\n",
      "Epoch: 7664 Training Loss: 0.1332435302734375 Test Loss: 0.39157101779513886\n",
      "Epoch: 7665 Training Loss: 0.13327485741509332 Test Loss: 0.39164708116319447\n",
      "Epoch: 7666 Training Loss: 0.13330340915256075 Test Loss: 0.3917024468315972\n",
      "Epoch: 7667 Training Loss: 0.13333849080403645 Test Loss: 0.39189189995659723\n",
      "Epoch: 7668 Training Loss: 0.13338653479682075 Test Loss: 0.3919844835069444\n",
      "Epoch: 7669 Training Loss: 0.13344267951117622 Test Loss: 0.39215668402777776\n",
      "Epoch: 7670 Training Loss: 0.13350113677978515 Test Loss: 0.3922431369357639\n",
      "Epoch: 7671 Training Loss: 0.13356563991970485 Test Loss: 0.3925219184027778\n",
      "Epoch: 7672 Training Loss: 0.13364498392740887 Test Loss: 0.39265907118055554\n",
      "Epoch: 7673 Training Loss: 0.13372623019748264 Test Loss: 0.39276261393229167\n",
      "Epoch: 7674 Training Loss: 0.13381224144829645 Test Loss: 0.3928967013888889\n",
      "Epoch: 7675 Training Loss: 0.13392420111762152 Test Loss: 0.39291693793402777\n",
      "Epoch: 7676 Training Loss: 0.13404790327284072 Test Loss: 0.39294300672743054\n",
      "Epoch: 7677 Training Loss: 0.1341858698527018 Test Loss: 0.39301014539930557\n",
      "Epoch: 7678 Training Loss: 0.1343503689236111 Test Loss: 0.3930048828125\n",
      "Epoch: 7679 Training Loss: 0.13453934987386068 Test Loss: 0.3930442708333333\n",
      "Epoch: 7680 Training Loss: 0.13473419019911023 Test Loss: 0.3930213216145833\n",
      "Epoch: 7681 Training Loss: 0.13495692189534506 Test Loss: 0.3930812174479167\n",
      "Epoch: 7682 Training Loss: 0.13520419311523438 Test Loss: 0.39301212565104165\n",
      "Epoch: 7683 Training Loss: 0.13546902465820312 Test Loss: 0.39330579969618057\n",
      "Epoch: 7684 Training Loss: 0.13577526007758248 Test Loss: 0.39352821180555553\n",
      "Epoch: 7685 Training Loss: 0.13610302310519748 Test Loss: 0.393815673828125\n",
      "Epoch: 7686 Training Loss: 0.13645336151123047 Test Loss: 0.39455181206597223\n",
      "Epoch: 7687 Training Loss: 0.13683472781711153 Test Loss: 0.3955251736111111\n",
      "Epoch: 7688 Training Loss: 0.13724522060818142 Test Loss: 0.39701185438368053\n",
      "Epoch: 7689 Training Loss: 0.13770623779296876 Test Loss: 0.3988681369357639\n",
      "Epoch: 7690 Training Loss: 0.13817669932047527 Test Loss: 0.4004035915798611\n",
      "Epoch: 7691 Training Loss: 0.13861629994710287 Test Loss: 0.40165776909722223\n",
      "Epoch: 7692 Training Loss: 0.13900116305881077 Test Loss: 0.4021328396267361\n",
      "Epoch: 7693 Training Loss: 0.13932202657063802 Test Loss: 0.40209510633680556\n",
      "Epoch: 7694 Training Loss: 0.13958435736762154 Test Loss: 0.4015416666666667\n",
      "Epoch: 7695 Training Loss: 0.1397853791978624 Test Loss: 0.400359375\n",
      "Epoch: 7696 Training Loss: 0.13992144944932725 Test Loss: 0.3990189887152778\n",
      "Epoch: 7697 Training Loss: 0.1399552485148112 Test Loss: 0.39798689778645835\n",
      "Epoch: 7698 Training Loss: 0.1399126451280382 Test Loss: 0.3971877712673611\n",
      "Epoch: 7699 Training Loss: 0.1398228980170356 Test Loss: 0.3967654079861111\n",
      "Epoch: 7700 Training Loss: 0.13973985290527344 Test Loss: 0.39723234049479167\n",
      "Epoch: 7701 Training Loss: 0.1396913333468967 Test Loss: 0.3981543240017361\n",
      "Epoch: 7702 Training Loss: 0.1396780310736762 Test Loss: 0.39936952039930557\n",
      "Epoch: 7703 Training Loss: 0.13973039245605468 Test Loss: 0.40050569661458335\n",
      "Epoch: 7704 Training Loss: 0.13983981408013238 Test Loss: 0.4021227484809028\n",
      "Epoch: 7705 Training Loss: 0.14000382656521268 Test Loss: 0.4041292317708333\n",
      "Epoch: 7706 Training Loss: 0.1402794969346788 Test Loss: 0.406135986328125\n",
      "Epoch: 7707 Training Loss: 0.14064964972601995 Test Loss: 0.4078327365451389\n",
      "Epoch: 7708 Training Loss: 0.1411510492960612 Test Loss: 0.40908555772569444\n",
      "Epoch: 7709 Training Loss: 0.14173842112223306 Test Loss: 0.4089626736111111\n",
      "Epoch: 7710 Training Loss: 0.1423519795735677 Test Loss: 0.40751063368055557\n",
      "Epoch: 7711 Training Loss: 0.14289110649956596 Test Loss: 0.40545979817708333\n",
      "Epoch: 7712 Training Loss: 0.14320650227864584 Test Loss: 0.4031547037760417\n",
      "Epoch: 7713 Training Loss: 0.14315492163764107 Test Loss: 0.4017121039496528\n",
      "Epoch: 7714 Training Loss: 0.14265601687961155 Test Loss: 0.4006896701388889\n",
      "Epoch: 7715 Training Loss: 0.14180155860053167 Test Loss: 0.40018147786458336\n",
      "Epoch: 7716 Training Loss: 0.14076245456271702 Test Loss: 0.4000946723090278\n",
      "Epoch: 7717 Training Loss: 0.13967956119113498 Test Loss: 0.39960427517361113\n",
      "Epoch: 7718 Training Loss: 0.13865799289279515 Test Loss: 0.39864740668402776\n",
      "Epoch: 7719 Training Loss: 0.13775449371337892 Test Loss: 0.39708791775173613\n",
      "Epoch: 7720 Training Loss: 0.13699126942952475 Test Loss: 0.39527916124131945\n",
      "Epoch: 7721 Training Loss: 0.13635379028320313 Test Loss: 0.39374709743923614\n",
      "Epoch: 7722 Training Loss: 0.135822386847602 Test Loss: 0.39282823350694446\n",
      "Epoch: 7723 Training Loss: 0.13541015031602646 Test Loss: 0.3928728298611111\n",
      "Epoch: 7724 Training Loss: 0.1350840759277344 Test Loss: 0.39342811414930556\n",
      "Epoch: 7725 Training Loss: 0.13485133785671657 Test Loss: 0.39403271484375\n",
      "Epoch: 7726 Training Loss: 0.1347088572184245 Test Loss: 0.39465966796875\n",
      "Epoch: 7727 Training Loss: 0.13463595411512586 Test Loss: 0.3950465494791667\n",
      "Epoch: 7728 Training Loss: 0.13463280063205296 Test Loss: 0.3948449164496528\n",
      "Epoch: 7729 Training Loss: 0.13464512634277342 Test Loss: 0.39319173177083333\n",
      "Epoch: 7730 Training Loss: 0.1346115714179145 Test Loss: 0.3915117458767361\n",
      "Epoch: 7731 Training Loss: 0.1346466776529948 Test Loss: 0.39137462022569447\n",
      "Epoch: 7732 Training Loss: 0.134718874613444 Test Loss: 0.39049896918402777\n",
      "Epoch: 7733 Training Loss: 0.1347801996866862 Test Loss: 0.3900583224826389\n",
      "Epoch: 7734 Training Loss: 0.13483419715033637 Test Loss: 0.39168177625868056\n",
      "Epoch: 7735 Training Loss: 0.13489344787597657 Test Loss: 0.3954468315972222\n",
      "Epoch: 7736 Training Loss: 0.13494758775499133 Test Loss: 0.3993604058159722\n",
      "Epoch: 7737 Training Loss: 0.13495635477701823 Test Loss: 0.40136328125\n",
      "Epoch: 7738 Training Loss: 0.13491370307074652 Test Loss: 0.40051386176215276\n",
      "Epoch: 7739 Training Loss: 0.13485370127360027 Test Loss: 0.39771541341145833\n",
      "Epoch: 7740 Training Loss: 0.1347827631632487 Test Loss: 0.3954709743923611\n",
      "Epoch: 7741 Training Loss: 0.13473346116807725 Test Loss: 0.39471723090277777\n",
      "Epoch: 7742 Training Loss: 0.13471294148763022 Test Loss: 0.39549690755208333\n",
      "Epoch: 7743 Training Loss: 0.1347378412882487 Test Loss: 0.396618896484375\n",
      "Epoch: 7744 Training Loss: 0.13478988308376735 Test Loss: 0.3969001193576389\n",
      "Epoch: 7745 Training Loss: 0.13486329989963108 Test Loss: 0.3960706380208333\n",
      "Epoch: 7746 Training Loss: 0.13493159823947481 Test Loss: 0.3946186794704861\n",
      "Epoch: 7747 Training Loss: 0.13498573387993706 Test Loss: 0.3934932996961806\n",
      "Epoch: 7748 Training Loss: 0.1350438969930013 Test Loss: 0.39329638671875\n",
      "Epoch: 7749 Training Loss: 0.13512657250298393 Test Loss: 0.3938873969184028\n",
      "Epoch: 7750 Training Loss: 0.1352590094672309 Test Loss: 0.39495138888888887\n",
      "Epoch: 7751 Training Loss: 0.13542996131049262 Test Loss: 0.39580916341145833\n",
      "Epoch: 7752 Training Loss: 0.13563658057318792 Test Loss: 0.3960183919270833\n",
      "Epoch: 7753 Training Loss: 0.13584173583984374 Test Loss: 0.3957455783420139\n",
      "Epoch: 7754 Training Loss: 0.13605822160508899 Test Loss: 0.3953765190972222\n",
      "Epoch: 7755 Training Loss: 0.13628128475613063 Test Loss: 0.39509703233506943\n",
      "Epoch: 7756 Training Loss: 0.13651581658257378 Test Loss: 0.3953001302083333\n",
      "Epoch: 7757 Training Loss: 0.13676942782931858 Test Loss: 0.3957938910590278\n",
      "Epoch: 7758 Training Loss: 0.13701557159423827 Test Loss: 0.39642076280381944\n",
      "Epoch: 7759 Training Loss: 0.13725849829779732 Test Loss: 0.3970073784722222\n",
      "Epoch: 7760 Training Loss: 0.13751060824924044 Test Loss: 0.3974412434895833\n",
      "Epoch: 7761 Training Loss: 0.13778739929199219 Test Loss: 0.39757763671875\n",
      "Epoch: 7762 Training Loss: 0.13807869974772136 Test Loss: 0.39728927951388887\n",
      "Epoch: 7763 Training Loss: 0.1383919177585178 Test Loss: 0.39680962456597224\n",
      "Epoch: 7764 Training Loss: 0.13873234727647568 Test Loss: 0.3961223958333333\n",
      "Epoch: 7765 Training Loss: 0.1390708999633789 Test Loss: 0.39551236979166665\n",
      "Epoch: 7766 Training Loss: 0.13938092549641928 Test Loss: 0.3948633083767361\n",
      "Epoch: 7767 Training Loss: 0.13965663401285808 Test Loss: 0.39458317057291664\n",
      "Epoch: 7768 Training Loss: 0.13989649878607857 Test Loss: 0.3947920464409722\n",
      "Epoch: 7769 Training Loss: 0.14011249711778428 Test Loss: 0.39584966362847224\n",
      "Epoch: 7770 Training Loss: 0.14033907741970486 Test Loss: 0.3972162814670139\n",
      "Epoch: 7771 Training Loss: 0.14058757697211371 Test Loss: 0.3989399956597222\n",
      "Epoch: 7772 Training Loss: 0.14089158376057942 Test Loss: 0.4004015842013889\n",
      "Epoch: 7773 Training Loss: 0.14127990044487848 Test Loss: 0.4013688693576389\n",
      "Epoch: 7774 Training Loss: 0.14173588562011719 Test Loss: 0.4019430881076389\n",
      "Epoch: 7775 Training Loss: 0.14218906826443142 Test Loss: 0.40212467447916667\n",
      "Epoch: 7776 Training Loss: 0.1425660892062717 Test Loss: 0.4021100802951389\n",
      "Epoch: 7777 Training Loss: 0.14275100792778864 Test Loss: 0.4027292209201389\n",
      "Epoch: 7778 Training Loss: 0.1427492938571506 Test Loss: 0.4040602213541667\n",
      "Epoch: 7779 Training Loss: 0.1425381893581814 Test Loss: 0.4057609592013889\n",
      "Epoch: 7780 Training Loss: 0.14216876390245226 Test Loss: 0.40706792534722225\n",
      "Epoch: 7781 Training Loss: 0.1416848848130968 Test Loss: 0.4074506564670139\n",
      "Epoch: 7782 Training Loss: 0.14108262210422093 Test Loss: 0.4065251193576389\n",
      "Epoch: 7783 Training Loss: 0.14034905582004123 Test Loss: 0.40455745442708335\n",
      "Epoch: 7784 Training Loss: 0.13951924726698134 Test Loss: 0.40193505859375\n",
      "Epoch: 7785 Training Loss: 0.13862932671440972 Test Loss: 0.39932177734375\n",
      "Epoch: 7786 Training Loss: 0.13773441823323568 Test Loss: 0.3968861762152778\n",
      "Epoch: 7787 Training Loss: 0.13687995062934027 Test Loss: 0.3948962673611111\n",
      "Epoch: 7788 Training Loss: 0.1360888197157118 Test Loss: 0.39339605034722225\n",
      "Epoch: 7789 Training Loss: 0.13538090176052517 Test Loss: 0.39227091471354164\n",
      "Epoch: 7790 Training Loss: 0.1347672856648763 Test Loss: 0.3917536349826389\n",
      "Epoch: 7791 Training Loss: 0.13423282284206814 Test Loss: 0.39154893663194446\n",
      "Epoch: 7792 Training Loss: 0.13376320224338106 Test Loss: 0.3914722222222222\n",
      "Epoch: 7793 Training Loss: 0.13336488342285155 Test Loss: 0.39161669921875\n",
      "Epoch: 7794 Training Loss: 0.13302817874484593 Test Loss: 0.39169004991319445\n",
      "Epoch: 7795 Training Loss: 0.1327450705634223 Test Loss: 0.3919035101996528\n",
      "Epoch: 7796 Training Loss: 0.132507204691569 Test Loss: 0.3921558159722222\n",
      "Epoch: 7797 Training Loss: 0.13230518849690756 Test Loss: 0.3923555772569444\n",
      "Epoch: 7798 Training Loss: 0.13213440534803603 Test Loss: 0.39262746853298613\n",
      "Epoch: 7799 Training Loss: 0.1319899664984809 Test Loss: 0.3925907389322917\n",
      "Epoch: 7800 Training Loss: 0.13186603715684678 Test Loss: 0.3926348741319444\n",
      "Epoch: 7801 Training Loss: 0.1317565451727973 Test Loss: 0.39256689453125\n",
      "Epoch: 7802 Training Loss: 0.1316596739027235 Test Loss: 0.392560302734375\n",
      "Epoch: 7803 Training Loss: 0.1315733337402344 Test Loss: 0.3924760470920139\n",
      "Epoch: 7804 Training Loss: 0.1314962158203125 Test Loss: 0.39237874348958335\n",
      "Epoch: 7805 Training Loss: 0.1314240934583876 Test Loss: 0.3920931803385417\n",
      "Epoch: 7806 Training Loss: 0.13135846625434028 Test Loss: 0.3918977322048611\n",
      "Epoch: 7807 Training Loss: 0.13130147637261286 Test Loss: 0.39177690972222223\n",
      "Epoch: 7808 Training Loss: 0.13125426567925347 Test Loss: 0.3915654025607639\n",
      "Epoch: 7809 Training Loss: 0.1312101567586263 Test Loss: 0.39141056315104167\n",
      "Epoch: 7810 Training Loss: 0.13117041354709202 Test Loss: 0.39140253363715277\n",
      "Epoch: 7811 Training Loss: 0.13113614230685763 Test Loss: 0.39138004557291667\n",
      "Epoch: 7812 Training Loss: 0.1311066682603624 Test Loss: 0.391334716796875\n",
      "Epoch: 7813 Training Loss: 0.13108081648084852 Test Loss: 0.39126345486111114\n",
      "Epoch: 7814 Training Loss: 0.13106046973334418 Test Loss: 0.3912699652777778\n",
      "Epoch: 7815 Training Loss: 0.13104785580105252 Test Loss: 0.3912786458333333\n",
      "Epoch: 7816 Training Loss: 0.13104576789008246 Test Loss: 0.3913161349826389\n",
      "Epoch: 7817 Training Loss: 0.13105010477701823 Test Loss: 0.39137548828125\n",
      "Epoch: 7818 Training Loss: 0.13105696360270183 Test Loss: 0.39160468207465277\n",
      "Epoch: 7819 Training Loss: 0.13107036929660373 Test Loss: 0.39171402994791665\n",
      "Epoch: 7820 Training Loss: 0.13109149424235025 Test Loss: 0.39182948133680556\n",
      "Epoch: 7821 Training Loss: 0.1311161863538954 Test Loss: 0.39193693033854166\n",
      "Epoch: 7822 Training Loss: 0.13114149051242405 Test Loss: 0.39211927625868054\n",
      "Epoch: 7823 Training Loss: 0.13117264811197918 Test Loss: 0.3922728949652778\n",
      "Epoch: 7824 Training Loss: 0.13120696428087023 Test Loss: 0.39246695963541667\n",
      "Epoch: 7825 Training Loss: 0.13124810451931423 Test Loss: 0.39265074327256944\n",
      "Epoch: 7826 Training Loss: 0.1312834726969401 Test Loss: 0.3927427842881944\n",
      "Epoch: 7827 Training Loss: 0.1313118650648329 Test Loss: 0.39280867513020834\n",
      "Epoch: 7828 Training Loss: 0.1313490727742513 Test Loss: 0.39294715711805556\n",
      "Epoch: 7829 Training Loss: 0.13138392893473308 Test Loss: 0.39311271158854166\n",
      "Epoch: 7830 Training Loss: 0.131429564581977 Test Loss: 0.39329172092013887\n",
      "Epoch: 7831 Training Loss: 0.13148088836669922 Test Loss: 0.39341734483506946\n",
      "Epoch: 7832 Training Loss: 0.13154046546088324 Test Loss: 0.3935725368923611\n",
      "Epoch: 7833 Training Loss: 0.13160306379530165 Test Loss: 0.3935866970486111\n",
      "Epoch: 7834 Training Loss: 0.13166312069363065 Test Loss: 0.3936700303819444\n",
      "Epoch: 7835 Training Loss: 0.13173218536376954 Test Loss: 0.3936827799479167\n",
      "Epoch: 7836 Training Loss: 0.13179842715793186 Test Loss: 0.393555908203125\n",
      "Epoch: 7837 Training Loss: 0.13187579939100477 Test Loss: 0.39343245442708336\n",
      "Epoch: 7838 Training Loss: 0.131968265109592 Test Loss: 0.39341986762152775\n",
      "Epoch: 7839 Training Loss: 0.13206834581163193 Test Loss: 0.393305908203125\n",
      "Epoch: 7840 Training Loss: 0.1321827341715495 Test Loss: 0.3933581271701389\n",
      "Epoch: 7841 Training Loss: 0.13230722130669487 Test Loss: 0.39348350694444445\n",
      "Epoch: 7842 Training Loss: 0.132448605855306 Test Loss: 0.39361599392361113\n",
      "Epoch: 7843 Training Loss: 0.13259782324896918 Test Loss: 0.3937478298611111\n",
      "Epoch: 7844 Training Loss: 0.13277263556586372 Test Loss: 0.3941762424045139\n",
      "Epoch: 7845 Training Loss: 0.1329583740234375 Test Loss: 0.3946001247829861\n",
      "Epoch: 7846 Training Loss: 0.13315960778130426 Test Loss: 0.3951719292534722\n",
      "Epoch: 7847 Training Loss: 0.13337923177083333 Test Loss: 0.39561968315972224\n",
      "Epoch: 7848 Training Loss: 0.1336165313720703 Test Loss: 0.39642230902777775\n",
      "Epoch: 7849 Training Loss: 0.13387580362955728 Test Loss: 0.39705995008680556\n",
      "Epoch: 7850 Training Loss: 0.13415523020426431 Test Loss: 0.3979207899305556\n",
      "Epoch: 7851 Training Loss: 0.1344711235894097 Test Loss: 0.39852454969618056\n",
      "Epoch: 7852 Training Loss: 0.13482608032226562 Test Loss: 0.3991448025173611\n",
      "Epoch: 7853 Training Loss: 0.13520874871148003 Test Loss: 0.3998213704427083\n",
      "Epoch: 7854 Training Loss: 0.13562505340576173 Test Loss: 0.40042909071180555\n",
      "Epoch: 7855 Training Loss: 0.13608711666531034 Test Loss: 0.40086583116319446\n",
      "Epoch: 7856 Training Loss: 0.13659958224826388 Test Loss: 0.40139594184027777\n",
      "Epoch: 7857 Training Loss: 0.13717955017089845 Test Loss: 0.4015144856770833\n",
      "Epoch: 7858 Training Loss: 0.13783375803629558 Test Loss: 0.40133121744791667\n",
      "Epoch: 7859 Training Loss: 0.1385466995239258 Test Loss: 0.4005483669704861\n",
      "Epoch: 7860 Training Loss: 0.13927565511067708 Test Loss: 0.3990601671006944\n",
      "Epoch: 7861 Training Loss: 0.13998136732313368 Test Loss: 0.3977603352864583\n",
      "Epoch: 7862 Training Loss: 0.14060895453559027 Test Loss: 0.3973798828125\n",
      "Epoch: 7863 Training Loss: 0.14103095075819227 Test Loss: 0.39792485894097224\n",
      "Epoch: 7864 Training Loss: 0.1412644246419271 Test Loss: 0.3999613715277778\n",
      "Epoch: 7865 Training Loss: 0.14132176632351345 Test Loss: 0.40297705078125\n",
      "Epoch: 7866 Training Loss: 0.14131118774414062 Test Loss: 0.4056037326388889\n",
      "Epoch: 7867 Training Loss: 0.14127438184950086 Test Loss: 0.4067431098090278\n",
      "Epoch: 7868 Training Loss: 0.14118204243977864 Test Loss: 0.4058700629340278\n",
      "Epoch: 7869 Training Loss: 0.14098192426893447 Test Loss: 0.4037503526475694\n",
      "Epoch: 7870 Training Loss: 0.14063771480984158 Test Loss: 0.4013425564236111\n",
      "Epoch: 7871 Training Loss: 0.14013941531711155 Test Loss: 0.39994127061631946\n",
      "Epoch: 7872 Training Loss: 0.13954356299506293 Test Loss: 0.40012565104166664\n",
      "Epoch: 7873 Training Loss: 0.13903984239366318 Test Loss: 0.40161854383680556\n",
      "Epoch: 7874 Training Loss: 0.1387398223876953 Test Loss: 0.40372273763020833\n",
      "Epoch: 7875 Training Loss: 0.1386303482055664 Test Loss: 0.40594973415798613\n",
      "Epoch: 7876 Training Loss: 0.1385617887708876 Test Loss: 0.4074820149739583\n",
      "Epoch: 7877 Training Loss: 0.138572267320421 Test Loss: 0.4086379123263889\n",
      "Epoch: 7878 Training Loss: 0.13864915381537543 Test Loss: 0.40955457899305553\n",
      "Epoch: 7879 Training Loss: 0.13880714162190755 Test Loss: 0.41009280056423614\n",
      "Epoch: 7880 Training Loss: 0.13901143137613933 Test Loss: 0.41045930989583335\n",
      "Epoch: 7881 Training Loss: 0.13924218241373698 Test Loss: 0.4100736762152778\n",
      "Epoch: 7882 Training Loss: 0.13949218241373698 Test Loss: 0.40913533528645835\n",
      "Epoch: 7883 Training Loss: 0.13968856726752388 Test Loss: 0.4076726616753472\n",
      "Epoch: 7884 Training Loss: 0.13979305860731336 Test Loss: 0.40588623046875\n",
      "Epoch: 7885 Training Loss: 0.13976813167995877 Test Loss: 0.4042685004340278\n",
      "Epoch: 7886 Training Loss: 0.13963013712565103 Test Loss: 0.4028808051215278\n",
      "Epoch: 7887 Training Loss: 0.1394061999850803 Test Loss: 0.4022270236545139\n",
      "Epoch: 7888 Training Loss: 0.1391362838745117 Test Loss: 0.4016532660590278\n",
      "Epoch: 7889 Training Loss: 0.1388629853990343 Test Loss: 0.4014599609375\n",
      "Epoch: 7890 Training Loss: 0.13858230506049263 Test Loss: 0.4008476019965278\n",
      "Epoch: 7891 Training Loss: 0.1382915996975369 Test Loss: 0.40005506727430556\n",
      "Epoch: 7892 Training Loss: 0.1380050786336263 Test Loss: 0.3997032877604167\n",
      "Epoch: 7893 Training Loss: 0.13770407104492188 Test Loss: 0.3991965060763889\n",
      "Epoch: 7894 Training Loss: 0.13737876722547743 Test Loss: 0.39887822808159723\n",
      "Epoch: 7895 Training Loss: 0.13705822330050998 Test Loss: 0.3986998969184028\n",
      "Epoch: 7896 Training Loss: 0.13674542236328124 Test Loss: 0.39890931532118057\n",
      "Epoch: 7897 Training Loss: 0.13648254903157553 Test Loss: 0.3996055501302083\n",
      "Epoch: 7898 Training Loss: 0.13627038065592448 Test Loss: 0.40042998589409723\n",
      "Epoch: 7899 Training Loss: 0.13608066982693143 Test Loss: 0.4013253038194444\n",
      "Epoch: 7900 Training Loss: 0.1359173346625434 Test Loss: 0.4016767849392361\n",
      "Epoch: 7901 Training Loss: 0.1357956551445855 Test Loss: 0.40183360460069445\n",
      "Epoch: 7902 Training Loss: 0.13572562323676216 Test Loss: 0.4018793402777778\n",
      "Epoch: 7903 Training Loss: 0.13572027333577474 Test Loss: 0.40187863498263887\n",
      "Epoch: 7904 Training Loss: 0.13573412068684895 Test Loss: 0.4018220486111111\n",
      "Epoch: 7905 Training Loss: 0.13574213070339627 Test Loss: 0.4011686740451389\n",
      "Epoch: 7906 Training Loss: 0.1357796613905165 Test Loss: 0.40051502821180557\n",
      "Epoch: 7907 Training Loss: 0.13585950385199652 Test Loss: 0.400159912109375\n",
      "Epoch: 7908 Training Loss: 0.1359960013495551 Test Loss: 0.4005233561197917\n",
      "Epoch: 7909 Training Loss: 0.1361906500922309 Test Loss: 0.40168560112847224\n",
      "Epoch: 7910 Training Loss: 0.13645812140570746 Test Loss: 0.40351641167534724\n",
      "Epoch: 7911 Training Loss: 0.13682427554660373 Test Loss: 0.4057538519965278\n",
      "Epoch: 7912 Training Loss: 0.13727134195963542 Test Loss: 0.40761244032118055\n",
      "Epoch: 7913 Training Loss: 0.13778384568956162 Test Loss: 0.4081748589409722\n",
      "Epoch: 7914 Training Loss: 0.13833079783121746 Test Loss: 0.4081329210069444\n",
      "Epoch: 7915 Training Loss: 0.13890006340874567 Test Loss: 0.4085593532986111\n",
      "Epoch: 7916 Training Loss: 0.1394767328898112 Test Loss: 0.411572265625\n",
      "Epoch: 7917 Training Loss: 0.140036744011773 Test Loss: 0.41644664171006945\n",
      "Epoch: 7918 Training Loss: 0.14055765787760416 Test Loss: 0.4190851236979167\n",
      "Epoch: 7919 Training Loss: 0.14106066046820748 Test Loss: 0.41665635850694444\n",
      "Epoch: 7920 Training Loss: 0.1414868901570638 Test Loss: 0.41160601128472224\n",
      "Epoch: 7921 Training Loss: 0.14191654883490668 Test Loss: 0.4080626085069444\n",
      "Epoch: 7922 Training Loss: 0.14241015031602647 Test Loss: 0.406689453125\n",
      "Epoch: 7923 Training Loss: 0.14294256083170573 Test Loss: 0.4066882595486111\n",
      "Epoch: 7924 Training Loss: 0.14346908654106988 Test Loss: 0.4075698784722222\n",
      "Epoch: 7925 Training Loss: 0.1439237501356337 Test Loss: 0.40956187608506944\n",
      "Epoch: 7926 Training Loss: 0.14428208753797744 Test Loss: 0.41135400390625\n",
      "Epoch: 7927 Training Loss: 0.14448116217719184 Test Loss: 0.41226633029513887\n",
      "Epoch: 7928 Training Loss: 0.1444538116455078 Test Loss: 0.41234803602430553\n",
      "Epoch: 7929 Training Loss: 0.14425413682725693 Test Loss: 0.41193695746527775\n",
      "Epoch: 7930 Training Loss: 0.14388636016845704 Test Loss: 0.4099060872395833\n",
      "Epoch: 7931 Training Loss: 0.14344017706976997 Test Loss: 0.40640397135416667\n",
      "Epoch: 7932 Training Loss: 0.1429346686469184 Test Loss: 0.4032246365017361\n",
      "Epoch: 7933 Training Loss: 0.14244283379448786 Test Loss: 0.4011699761284722\n",
      "Epoch: 7934 Training Loss: 0.14201784939236112 Test Loss: 0.40009819878472225\n",
      "Epoch: 7935 Training Loss: 0.14167693074544271 Test Loss: 0.3995793185763889\n",
      "Epoch: 7936 Training Loss: 0.14143467542860244 Test Loss: 0.399588623046875\n",
      "Epoch: 7937 Training Loss: 0.1412874035305447 Test Loss: 0.4002595757378472\n",
      "Epoch: 7938 Training Loss: 0.14117623138427735 Test Loss: 0.4008102756076389\n",
      "Epoch: 7939 Training Loss: 0.14111557854546442 Test Loss: 0.4015576714409722\n",
      "Epoch: 7940 Training Loss: 0.14116947597927518 Test Loss: 0.40255669487847223\n",
      "Epoch: 7941 Training Loss: 0.14136949496799045 Test Loss: 0.4034721137152778\n",
      "Epoch: 7942 Training Loss: 0.14165377044677735 Test Loss: 0.40408311631944444\n",
      "Epoch: 7943 Training Loss: 0.14204466162787543 Test Loss: 0.40474544270833335\n",
      "Epoch: 7944 Training Loss: 0.14256165991889105 Test Loss: 0.40576171875\n",
      "Epoch: 7945 Training Loss: 0.14327219729953342 Test Loss: 0.4077900390625\n",
      "Epoch: 7946 Training Loss: 0.14421397399902344 Test Loss: 0.41081266276041667\n",
      "Epoch: 7947 Training Loss: 0.14531776173909505 Test Loss: 0.4144810112847222\n",
      "Epoch: 7948 Training Loss: 0.14666496107313368 Test Loss: 0.41792041015625\n",
      "Epoch: 7949 Training Loss: 0.1482442372639974 Test Loss: 0.41878917100694446\n",
      "Epoch: 7950 Training Loss: 0.14988530137803818 Test Loss: 0.4153039822048611\n",
      "Epoch: 7951 Training Loss: 0.1512082299126519 Test Loss: 0.40748833550347224\n",
      "Epoch: 7952 Training Loss: 0.1518490770128038 Test Loss: 0.39853670247395834\n",
      "Epoch: 7953 Training Loss: 0.15134116448296442 Test Loss: 0.3922101779513889\n",
      "Epoch: 7954 Training Loss: 0.14965540228949653 Test Loss: 0.3901866319444444\n",
      "Epoch: 7955 Training Loss: 0.1472704620361328 Test Loss: 0.3907131618923611\n",
      "Epoch: 7956 Training Loss: 0.14470543246799045 Test Loss: 0.39175526258680554\n",
      "Epoch: 7957 Training Loss: 0.14233121914333768 Test Loss: 0.39229302300347224\n",
      "Epoch: 7958 Training Loss: 0.14031751844618057 Test Loss: 0.39195024956597224\n",
      "Epoch: 7959 Training Loss: 0.138750981648763 Test Loss: 0.3913062065972222\n",
      "Epoch: 7960 Training Loss: 0.13762002309163412 Test Loss: 0.39072691514756946\n",
      "Epoch: 7961 Training Loss: 0.13683558570014107 Test Loss: 0.3903240559895833\n",
      "Epoch: 7962 Training Loss: 0.1363052707248264 Test Loss: 0.390011474609375\n",
      "Epoch: 7963 Training Loss: 0.13593896145290799 Test Loss: 0.38999609375\n",
      "Epoch: 7964 Training Loss: 0.13569017367892794 Test Loss: 0.3899853515625\n",
      "Epoch: 7965 Training Loss: 0.13551769680447048 Test Loss: 0.3899091796875\n",
      "Epoch: 7966 Training Loss: 0.13539657931857638 Test Loss: 0.3898069118923611\n",
      "Epoch: 7967 Training Loss: 0.1353215111626519 Test Loss: 0.3898186848958333\n",
      "Epoch: 7968 Training Loss: 0.13526981268988716 Test Loss: 0.3899367404513889\n",
      "Epoch: 7969 Training Loss: 0.13523004404703776 Test Loss: 0.39014599609375\n",
      "Epoch: 7970 Training Loss: 0.13521808793809678 Test Loss: 0.39025542534722224\n",
      "Epoch: 7971 Training Loss: 0.13521515485975477 Test Loss: 0.39022279188368053\n",
      "Epoch: 7972 Training Loss: 0.13522079467773437 Test Loss: 0.39035389539930554\n",
      "Epoch: 7973 Training Loss: 0.1352257537841797 Test Loss: 0.3901130913628472\n",
      "Epoch: 7974 Training Loss: 0.13523396470811633 Test Loss: 0.38996381293402776\n",
      "Epoch: 7975 Training Loss: 0.13525124867757163 Test Loss: 0.38977137586805555\n",
      "Epoch: 7976 Training Loss: 0.13527706909179688 Test Loss: 0.38943942599826387\n",
      "Epoch: 7977 Training Loss: 0.1353075714111328 Test Loss: 0.38900528971354165\n",
      "Epoch: 7978 Training Loss: 0.13533492533365885 Test Loss: 0.388623779296875\n",
      "Epoch: 7979 Training Loss: 0.1353817138671875 Test Loss: 0.3880407172309028\n",
      "Epoch: 7980 Training Loss: 0.13543775431315105 Test Loss: 0.3874984809027778\n",
      "Epoch: 7981 Training Loss: 0.1354975085788303 Test Loss: 0.38688004557291666\n",
      "Epoch: 7982 Training Loss: 0.13556565941704643 Test Loss: 0.38644745551215276\n",
      "Epoch: 7983 Training Loss: 0.1356560567220052 Test Loss: 0.3858789605034722\n",
      "Epoch: 7984 Training Loss: 0.1357526668972439 Test Loss: 0.3854684244791667\n",
      "Epoch: 7985 Training Loss: 0.1358457302517361 Test Loss: 0.38513026258680555\n",
      "Epoch: 7986 Training Loss: 0.135940549214681 Test Loss: 0.3849419487847222\n",
      "Epoch: 7987 Training Loss: 0.1360304506089952 Test Loss: 0.3850392252604167\n",
      "Epoch: 7988 Training Loss: 0.1360983403523763 Test Loss: 0.38533561197916666\n",
      "Epoch: 7989 Training Loss: 0.13616573333740234 Test Loss: 0.3859365776909722\n",
      "Epoch: 7990 Training Loss: 0.13623065694173178 Test Loss: 0.3866118435329861\n",
      "Epoch: 7991 Training Loss: 0.13629302554660372 Test Loss: 0.3877517632378472\n",
      "Epoch: 7992 Training Loss: 0.136359617445204 Test Loss: 0.38883921983506947\n",
      "Epoch: 7993 Training Loss: 0.1364217546251085 Test Loss: 0.3898557400173611\n",
      "Epoch: 7994 Training Loss: 0.1364599829779731 Test Loss: 0.3908076171875\n",
      "Epoch: 7995 Training Loss: 0.13646261342366536 Test Loss: 0.3914167751736111\n",
      "Epoch: 7996 Training Loss: 0.13643888261583118 Test Loss: 0.3916232096354167\n",
      "Epoch: 7997 Training Loss: 0.13639136335584853 Test Loss: 0.3913296169704861\n",
      "Epoch: 7998 Training Loss: 0.1362995113796658 Test Loss: 0.3906525336371528\n",
      "Epoch: 7999 Training Loss: 0.13616177537706164 Test Loss: 0.3896221245659722\n",
      "Epoch: 8000 Training Loss: 0.1359784206814236 Test Loss: 0.38835649956597224\n",
      "Epoch: 8001 Training Loss: 0.1357587187025282 Test Loss: 0.3871009657118056\n",
      "Epoch: 8002 Training Loss: 0.13550931294759114 Test Loss: 0.38591663953993055\n",
      "Epoch: 8003 Training Loss: 0.13522015974256726 Test Loss: 0.3851382649739583\n",
      "Epoch: 8004 Training Loss: 0.1349217342800564 Test Loss: 0.38492008463541666\n",
      "Epoch: 8005 Training Loss: 0.1345988286336263 Test Loss: 0.3849208984375\n",
      "Epoch: 8006 Training Loss: 0.1342734163072374 Test Loss: 0.38536876085069444\n",
      "Epoch: 8007 Training Loss: 0.13395323011610244 Test Loss: 0.38614615885416664\n",
      "Epoch: 8008 Training Loss: 0.1336476542154948 Test Loss: 0.386984619140625\n",
      "Epoch: 8009 Training Loss: 0.13335843658447266 Test Loss: 0.3878081597222222\n",
      "Epoch: 8010 Training Loss: 0.13309240129258898 Test Loss: 0.3885719943576389\n",
      "Epoch: 8011 Training Loss: 0.13285668606228299 Test Loss: 0.38925577799479166\n",
      "Epoch: 8012 Training Loss: 0.1326533423529731 Test Loss: 0.3895955403645833\n",
      "Epoch: 8013 Training Loss: 0.13246573808458117 Test Loss: 0.3899231499565972\n",
      "Epoch: 8014 Training Loss: 0.1323027826944987 Test Loss: 0.3898645562065972\n",
      "Epoch: 8015 Training Loss: 0.1321560541788737 Test Loss: 0.3899134657118056\n",
      "Epoch: 8016 Training Loss: 0.13202888912624783 Test Loss: 0.38977940538194444\n",
      "Epoch: 8017 Training Loss: 0.13190240139431422 Test Loss: 0.3893846028645833\n",
      "Epoch: 8018 Training Loss: 0.1317930213080512 Test Loss: 0.3890411783854167\n",
      "Epoch: 8019 Training Loss: 0.13169725799560547 Test Loss: 0.38863170030381944\n",
      "Epoch: 8020 Training Loss: 0.1316185523139106 Test Loss: 0.38837006293402776\n",
      "Epoch: 8021 Training Loss: 0.13155337693956162 Test Loss: 0.3880774197048611\n",
      "Epoch: 8022 Training Loss: 0.13149374559190538 Test Loss: 0.3878543294270833\n",
      "Epoch: 8023 Training Loss: 0.13145001898871528 Test Loss: 0.387684814453125\n",
      "Epoch: 8024 Training Loss: 0.13141834259033203 Test Loss: 0.38771085611979167\n",
      "Epoch: 8025 Training Loss: 0.131398195054796 Test Loss: 0.3878961588541667\n",
      "Epoch: 8026 Training Loss: 0.13138009219699437 Test Loss: 0.38794769965277776\n",
      "Epoch: 8027 Training Loss: 0.13135818481445313 Test Loss: 0.388247802734375\n",
      "Epoch: 8028 Training Loss: 0.13134186130099826 Test Loss: 0.3884270833333333\n",
      "Epoch: 8029 Training Loss: 0.1313308283487956 Test Loss: 0.38870228407118057\n",
      "Epoch: 8030 Training Loss: 0.1313241466946072 Test Loss: 0.38905680338541665\n",
      "Epoch: 8031 Training Loss: 0.13131979031032986 Test Loss: 0.389357666015625\n",
      "Epoch: 8032 Training Loss: 0.13131876034206813 Test Loss: 0.3895289171006944\n",
      "Epoch: 8033 Training Loss: 0.131314456515842 Test Loss: 0.3897624782986111\n",
      "Epoch: 8034 Training Loss: 0.131312134636773 Test Loss: 0.38988023546006945\n",
      "Epoch: 8035 Training Loss: 0.13131502024332684 Test Loss: 0.38991468641493054\n",
      "Epoch: 8036 Training Loss: 0.1313185814751519 Test Loss: 0.3899585503472222\n",
      "Epoch: 8037 Training Loss: 0.1313284640842014 Test Loss: 0.389690673828125\n",
      "Epoch: 8038 Training Loss: 0.1313403065999349 Test Loss: 0.3894641927083333\n",
      "Epoch: 8039 Training Loss: 0.13135725911458335 Test Loss: 0.38905162217881945\n",
      "Epoch: 8040 Training Loss: 0.13137210930718315 Test Loss: 0.38854210069444445\n",
      "Epoch: 8041 Training Loss: 0.1313893780178494 Test Loss: 0.3880382758246528\n",
      "Epoch: 8042 Training Loss: 0.13141217719184028 Test Loss: 0.3875691189236111\n",
      "Epoch: 8043 Training Loss: 0.13145025041368272 Test Loss: 0.38708851453993054\n",
      "Epoch: 8044 Training Loss: 0.13149639638264973 Test Loss: 0.38664382595486113\n",
      "Epoch: 8045 Training Loss: 0.13155193413628471 Test Loss: 0.3863003200954861\n",
      "Epoch: 8046 Training Loss: 0.13161690775553386 Test Loss: 0.3860763888888889\n",
      "Epoch: 8047 Training Loss: 0.13170379299587673 Test Loss: 0.3860930989583333\n",
      "Epoch: 8048 Training Loss: 0.13180250634087456 Test Loss: 0.3862440863715278\n",
      "Epoch: 8049 Training Loss: 0.13191236792670355 Test Loss: 0.3866142306857639\n",
      "Epoch: 8050 Training Loss: 0.13204459889729817 Test Loss: 0.38730438910590276\n",
      "Epoch: 8051 Training Loss: 0.13220061916775175 Test Loss: 0.38824066840277777\n",
      "Epoch: 8052 Training Loss: 0.13236666700575087 Test Loss: 0.3893442111545139\n",
      "Epoch: 8053 Training Loss: 0.13255711534288195 Test Loss: 0.3906810980902778\n",
      "Epoch: 8054 Training Loss: 0.13276561567518447 Test Loss: 0.3922594401041667\n",
      "Epoch: 8055 Training Loss: 0.13298902384440103 Test Loss: 0.39393443467881945\n",
      "Epoch: 8056 Training Loss: 0.13324121941460504 Test Loss: 0.3956220703125\n",
      "Epoch: 8057 Training Loss: 0.13350660451253255 Test Loss: 0.3974770236545139\n",
      "Epoch: 8058 Training Loss: 0.13378170691596136 Test Loss: 0.39920540364583335\n",
      "Epoch: 8059 Training Loss: 0.13407286156548395 Test Loss: 0.40071687825520835\n",
      "Epoch: 8060 Training Loss: 0.13435366990831163 Test Loss: 0.4020710720486111\n",
      "Epoch: 8061 Training Loss: 0.1346357642279731 Test Loss: 0.4026365017361111\n",
      "Epoch: 8062 Training Loss: 0.13489238823784722 Test Loss: 0.40256241861979164\n",
      "Epoch: 8063 Training Loss: 0.13513189273410373 Test Loss: 0.40206749131944447\n",
      "Epoch: 8064 Training Loss: 0.13533054945203993 Test Loss: 0.40120464409722223\n",
      "Epoch: 8065 Training Loss: 0.13549054294162327 Test Loss: 0.4003282877604167\n",
      "Epoch: 8066 Training Loss: 0.1356431613498264 Test Loss: 0.3998502875434028\n",
      "Epoch: 8067 Training Loss: 0.13579988521999783 Test Loss: 0.3995754665798611\n",
      "Epoch: 8068 Training Loss: 0.13594793446858724 Test Loss: 0.4002903103298611\n",
      "Epoch: 8069 Training Loss: 0.13612312316894531 Test Loss: 0.4015787217881944\n",
      "Epoch: 8070 Training Loss: 0.13632408396402995 Test Loss: 0.4032876519097222\n",
      "Epoch: 8071 Training Loss: 0.1365493401421441 Test Loss: 0.4053721245659722\n",
      "Epoch: 8072 Training Loss: 0.13679074859619142 Test Loss: 0.4063889431423611\n",
      "Epoch: 8073 Training Loss: 0.13704386478000216 Test Loss: 0.4062802734375\n",
      "Epoch: 8074 Training Loss: 0.13733509741889105 Test Loss: 0.40466899956597224\n",
      "Epoch: 8075 Training Loss: 0.13766901312934027 Test Loss: 0.40219422743055555\n",
      "Epoch: 8076 Training Loss: 0.13800172593858506 Test Loss: 0.39891609700520836\n",
      "Epoch: 8077 Training Loss: 0.1382542995876736 Test Loss: 0.39607340494791665\n",
      "Epoch: 8078 Training Loss: 0.1384250013563368 Test Loss: 0.39390869140625\n",
      "Epoch: 8079 Training Loss: 0.1385034900241428 Test Loss: 0.39299527994791666\n",
      "Epoch: 8080 Training Loss: 0.13852098253038195 Test Loss: 0.39247935655381944\n",
      "Epoch: 8081 Training Loss: 0.13845773484971788 Test Loss: 0.393421875\n",
      "Epoch: 8082 Training Loss: 0.13839077419704862 Test Loss: 0.39459144422743053\n",
      "Epoch: 8083 Training Loss: 0.13834360673692492 Test Loss: 0.3961354166666667\n",
      "Epoch: 8084 Training Loss: 0.13829847971598308 Test Loss: 0.39735972764756944\n",
      "Epoch: 8085 Training Loss: 0.13823722500271268 Test Loss: 0.39769363064236113\n",
      "Epoch: 8086 Training Loss: 0.13819024149576822 Test Loss: 0.3974677191840278\n",
      "Epoch: 8087 Training Loss: 0.13811294047037762 Test Loss: 0.39677935112847224\n",
      "Epoch: 8088 Training Loss: 0.1379918017917209 Test Loss: 0.3959505208333333\n",
      "Epoch: 8089 Training Loss: 0.13789501190185546 Test Loss: 0.39497154405381946\n",
      "Epoch: 8090 Training Loss: 0.13782355075412325 Test Loss: 0.3935374348958333\n",
      "Epoch: 8091 Training Loss: 0.1377522193060981 Test Loss: 0.3919088812934028\n",
      "Epoch: 8092 Training Loss: 0.13771205478244358 Test Loss: 0.3904579806857639\n",
      "Epoch: 8093 Training Loss: 0.1376965815226237 Test Loss: 0.3896003146701389\n",
      "Epoch: 8094 Training Loss: 0.13774001651340062 Test Loss: 0.3890053168402778\n",
      "Epoch: 8095 Training Loss: 0.1378618392944336 Test Loss: 0.3886231553819444\n",
      "Epoch: 8096 Training Loss: 0.1380621761745877 Test Loss: 0.38848893229166664\n",
      "Epoch: 8097 Training Loss: 0.13830856323242188 Test Loss: 0.38850211588541667\n",
      "Epoch: 8098 Training Loss: 0.13850508032904732 Test Loss: 0.389189453125\n",
      "Epoch: 8099 Training Loss: 0.13862002648247612 Test Loss: 0.3896729329427083\n",
      "Epoch: 8100 Training Loss: 0.13863575744628906 Test Loss: 0.3900166015625\n",
      "Epoch: 8101 Training Loss: 0.13857561577690972 Test Loss: 0.38967347547743053\n",
      "Epoch: 8102 Training Loss: 0.13845108964708117 Test Loss: 0.38921484375\n",
      "Epoch: 8103 Training Loss: 0.1382242219712999 Test Loss: 0.3884235297309028\n",
      "Epoch: 8104 Training Loss: 0.1378924560546875 Test Loss: 0.3875726453993056\n",
      "Epoch: 8105 Training Loss: 0.13751180097791885 Test Loss: 0.38716015625\n",
      "Epoch: 8106 Training Loss: 0.13710873921712238 Test Loss: 0.3867848578559028\n",
      "Epoch: 8107 Training Loss: 0.13672035302056207 Test Loss: 0.38692236328125\n",
      "Epoch: 8108 Training Loss: 0.13635702599419489 Test Loss: 0.3877066786024306\n",
      "Epoch: 8109 Training Loss: 0.13601808251274958 Test Loss: 0.38891121419270835\n",
      "Epoch: 8110 Training Loss: 0.1357265345255534 Test Loss: 0.3903630642361111\n",
      "Epoch: 8111 Training Loss: 0.13546914672851562 Test Loss: 0.39162266710069443\n",
      "Epoch: 8112 Training Loss: 0.13523665449354383 Test Loss: 0.3924769422743056\n",
      "Epoch: 8113 Training Loss: 0.13501897854275174 Test Loss: 0.3927822265625\n",
      "Epoch: 8114 Training Loss: 0.13479710557725694 Test Loss: 0.39273876953125\n",
      "Epoch: 8115 Training Loss: 0.13457079145643447 Test Loss: 0.39217458767361113\n",
      "Epoch: 8116 Training Loss: 0.134341672261556 Test Loss: 0.39180859375\n",
      "Epoch: 8117 Training Loss: 0.13412715827094185 Test Loss: 0.3916550835503472\n",
      "Epoch: 8118 Training Loss: 0.1339414537217882 Test Loss: 0.3912212456597222\n",
      "Epoch: 8119 Training Loss: 0.13377801174587672 Test Loss: 0.3910633680555556\n",
      "Epoch: 8120 Training Loss: 0.1336448957655165 Test Loss: 0.39083799913194445\n",
      "Epoch: 8121 Training Loss: 0.13353370327419706 Test Loss: 0.39067184787326387\n",
      "Epoch: 8122 Training Loss: 0.13344840494791665 Test Loss: 0.39048551432291667\n",
      "Epoch: 8123 Training Loss: 0.13338819207085503 Test Loss: 0.39041883680555556\n",
      "Epoch: 8124 Training Loss: 0.13334447140163846 Test Loss: 0.39041710069444446\n",
      "Epoch: 8125 Training Loss: 0.13332022179497613 Test Loss: 0.3904651421440972\n",
      "Epoch: 8126 Training Loss: 0.13330481804741753 Test Loss: 0.39047998046875\n",
      "Epoch: 8127 Training Loss: 0.1333049799601237 Test Loss: 0.39050580512152777\n",
      "Epoch: 8128 Training Loss: 0.1333220757378472 Test Loss: 0.3905537923177083\n",
      "Epoch: 8129 Training Loss: 0.13334577263726127 Test Loss: 0.3905879448784722\n",
      "Epoch: 8130 Training Loss: 0.13337678866916233 Test Loss: 0.3908394097222222\n",
      "Epoch: 8131 Training Loss: 0.1334164827134874 Test Loss: 0.3912160915798611\n",
      "Epoch: 8132 Training Loss: 0.13347747039794922 Test Loss: 0.3914625379774306\n",
      "Epoch: 8133 Training Loss: 0.13355196719699436 Test Loss: 0.39201261393229164\n",
      "Epoch: 8134 Training Loss: 0.1336447321573893 Test Loss: 0.3924203559027778\n",
      "Epoch: 8135 Training Loss: 0.1337433590359158 Test Loss: 0.3928376193576389\n",
      "Epoch: 8136 Training Loss: 0.13386331939697266 Test Loss: 0.39331193033854167\n",
      "Epoch: 8137 Training Loss: 0.1340049786037869 Test Loss: 0.3936697319878472\n",
      "Epoch: 8138 Training Loss: 0.1341523929172092 Test Loss: 0.3940889485677083\n",
      "Epoch: 8139 Training Loss: 0.13431737942165797 Test Loss: 0.39437649197048613\n",
      "Epoch: 8140 Training Loss: 0.13450339084201388 Test Loss: 0.39453043619791667\n",
      "Epoch: 8141 Training Loss: 0.13470856136745876 Test Loss: 0.3945998263888889\n",
      "Epoch: 8142 Training Loss: 0.1349351772732205 Test Loss: 0.3944365234375\n",
      "Epoch: 8143 Training Loss: 0.13516503228081597 Test Loss: 0.3939693196614583\n",
      "Epoch: 8144 Training Loss: 0.13540707482231987 Test Loss: 0.39315115017361113\n",
      "Epoch: 8145 Training Loss: 0.13564214833577473 Test Loss: 0.39232693142361114\n",
      "Epoch: 8146 Training Loss: 0.1358707521226671 Test Loss: 0.39128065321180555\n",
      "Epoch: 8147 Training Loss: 0.13607251654730904 Test Loss: 0.3907584364149306\n",
      "Epoch: 8148 Training Loss: 0.13624060567220053 Test Loss: 0.39057649739583333\n",
      "Epoch: 8149 Training Loss: 0.13637240346272786 Test Loss: 0.39058997938368056\n",
      "Epoch: 8150 Training Loss: 0.13648290506998698 Test Loss: 0.3906070963541667\n",
      "Epoch: 8151 Training Loss: 0.13651628706190322 Test Loss: 0.3907299262152778\n",
      "Epoch: 8152 Training Loss: 0.13647794172498914 Test Loss: 0.39024479166666665\n",
      "Epoch: 8153 Training Loss: 0.13638336351182725 Test Loss: 0.38990809461805553\n",
      "Epoch: 8154 Training Loss: 0.13620817226833767 Test Loss: 0.3889363064236111\n",
      "Epoch: 8155 Training Loss: 0.13592375183105468 Test Loss: 0.38744173177083335\n",
      "Epoch: 8156 Training Loss: 0.13556262546115452 Test Loss: 0.3860136176215278\n",
      "Epoch: 8157 Training Loss: 0.13514858161078558 Test Loss: 0.3846342230902778\n",
      "Epoch: 8158 Training Loss: 0.13468108707004123 Test Loss: 0.3836847873263889\n",
      "Epoch: 8159 Training Loss: 0.13418908521864148 Test Loss: 0.3831455349392361\n",
      "Epoch: 8160 Training Loss: 0.1336898447672526 Test Loss: 0.38304983181423613\n",
      "Epoch: 8161 Training Loss: 0.1332066921657986 Test Loss: 0.3833351779513889\n",
      "Epoch: 8162 Training Loss: 0.13276668802897135 Test Loss: 0.3838662651909722\n",
      "Epoch: 8163 Training Loss: 0.1323639390733507 Test Loss: 0.3846268988715278\n",
      "Epoch: 8164 Training Loss: 0.13200361972384983 Test Loss: 0.3852604437934028\n",
      "Epoch: 8165 Training Loss: 0.1316935789320204 Test Loss: 0.3859012044270833\n",
      "Epoch: 8166 Training Loss: 0.13142906612820096 Test Loss: 0.38634573025173613\n",
      "Epoch: 8167 Training Loss: 0.13118707699245877 Test Loss: 0.38664794921875\n",
      "Epoch: 8168 Training Loss: 0.13096688079833985 Test Loss: 0.3866232638888889\n",
      "Epoch: 8169 Training Loss: 0.13077568901909722 Test Loss: 0.38661789279513886\n",
      "Epoch: 8170 Training Loss: 0.1306156285603841 Test Loss: 0.38638685438368053\n",
      "Epoch: 8171 Training Loss: 0.13046659427218968 Test Loss: 0.3862587890625\n",
      "Epoch: 8172 Training Loss: 0.1303285420735677 Test Loss: 0.3858617078993056\n",
      "Epoch: 8173 Training Loss: 0.1301998748779297 Test Loss: 0.3854114854600694\n",
      "Epoch: 8174 Training Loss: 0.13007914394802517 Test Loss: 0.3850477973090278\n",
      "Epoch: 8175 Training Loss: 0.12997120157877604 Test Loss: 0.3847073296440972\n",
      "Epoch: 8176 Training Loss: 0.12987274424235026 Test Loss: 0.384396484375\n",
      "Epoch: 8177 Training Loss: 0.12978453826904296 Test Loss: 0.38416262478298613\n",
      "Epoch: 8178 Training Loss: 0.12969974348280164 Test Loss: 0.3841307779947917\n",
      "Epoch: 8179 Training Loss: 0.1296220245361328 Test Loss: 0.38423912217881945\n",
      "Epoch: 8180 Training Loss: 0.1295517077975803 Test Loss: 0.38429541015625\n",
      "Epoch: 8181 Training Loss: 0.12948796929253473 Test Loss: 0.38449300130208336\n",
      "Epoch: 8182 Training Loss: 0.1294364013671875 Test Loss: 0.3847323676215278\n",
      "Epoch: 8183 Training Loss: 0.12938686370849609 Test Loss: 0.3849309353298611\n",
      "Epoch: 8184 Training Loss: 0.1293502671983507 Test Loss: 0.3851935763888889\n",
      "Epoch: 8185 Training Loss: 0.1293193333943685 Test Loss: 0.38548320855034723\n",
      "Epoch: 8186 Training Loss: 0.12929586622450087 Test Loss: 0.3857814670138889\n",
      "Epoch: 8187 Training Loss: 0.1292821502685547 Test Loss: 0.38595751953125\n",
      "Epoch: 8188 Training Loss: 0.1292752193874783 Test Loss: 0.38622954644097224\n",
      "Epoch: 8189 Training Loss: 0.12927904171413845 Test Loss: 0.3863122829861111\n",
      "Epoch: 8190 Training Loss: 0.12929423692491318 Test Loss: 0.38637754991319445\n",
      "Epoch: 8191 Training Loss: 0.1293156009250217 Test Loss: 0.3865300021701389\n",
      "Epoch: 8192 Training Loss: 0.12933818562825522 Test Loss: 0.3865263671875\n",
      "Epoch: 8193 Training Loss: 0.12935147094726562 Test Loss: 0.38643240017361113\n",
      "Epoch: 8194 Training Loss: 0.1293661371866862 Test Loss: 0.3863012152777778\n",
      "Epoch: 8195 Training Loss: 0.12937958611382377 Test Loss: 0.38617236328125\n",
      "Epoch: 8196 Training Loss: 0.12939031897650824 Test Loss: 0.385855224609375\n",
      "Epoch: 8197 Training Loss: 0.12940452490912543 Test Loss: 0.3855645073784722\n",
      "Epoch: 8198 Training Loss: 0.1294226523505317 Test Loss: 0.385436279296875\n",
      "Epoch: 8199 Training Loss: 0.1294413791232639 Test Loss: 0.38515351019965277\n",
      "Epoch: 8200 Training Loss: 0.1294695341322157 Test Loss: 0.3849444986979167\n",
      "Epoch: 8201 Training Loss: 0.12950860341389975 Test Loss: 0.3849975857204861\n",
      "Epoch: 8202 Training Loss: 0.12955496385362414 Test Loss: 0.3849531792534722\n",
      "Epoch: 8203 Training Loss: 0.1296111263699002 Test Loss: 0.3850129665798611\n",
      "Epoch: 8204 Training Loss: 0.12968561808268228 Test Loss: 0.38519759114583335\n",
      "Epoch: 8205 Training Loss: 0.1297635243733724 Test Loss: 0.3852859700520833\n",
      "Epoch: 8206 Training Loss: 0.12985620880126952 Test Loss: 0.38561705186631945\n",
      "Epoch: 8207 Training Loss: 0.12996131642659506 Test Loss: 0.38588753255208336\n",
      "Epoch: 8208 Training Loss: 0.13008206346299914 Test Loss: 0.38599720594618053\n",
      "Epoch: 8209 Training Loss: 0.13021761067708335 Test Loss: 0.38608094618055555\n",
      "Epoch: 8210 Training Loss: 0.13036923048231336 Test Loss: 0.3861303439670139\n",
      "Epoch: 8211 Training Loss: 0.130548466152615 Test Loss: 0.38637736002604167\n",
      "Epoch: 8212 Training Loss: 0.13074566819932726 Test Loss: 0.3863955078125\n",
      "Epoch: 8213 Training Loss: 0.1309484592013889 Test Loss: 0.3863518880208333\n",
      "Epoch: 8214 Training Loss: 0.13115520562065971 Test Loss: 0.38631629774305554\n",
      "Epoch: 8215 Training Loss: 0.1313613772922092 Test Loss: 0.38639274088541664\n",
      "Epoch: 8216 Training Loss: 0.13157645331488715 Test Loss: 0.3864693739149306\n",
      "Epoch: 8217 Training Loss: 0.1318066660563151 Test Loss: 0.38673906792534724\n",
      "Epoch: 8218 Training Loss: 0.1320512958102756 Test Loss: 0.38701014539930556\n",
      "Epoch: 8219 Training Loss: 0.13232269541422526 Test Loss: 0.38753347439236113\n",
      "Epoch: 8220 Training Loss: 0.13261346859402126 Test Loss: 0.3881581488715278\n",
      "Epoch: 8221 Training Loss: 0.13293851216634114 Test Loss: 0.38886669921875\n",
      "Epoch: 8222 Training Loss: 0.1333197292751736 Test Loss: 0.3893464084201389\n",
      "Epoch: 8223 Training Loss: 0.1337433336046007 Test Loss: 0.3904404296875\n",
      "Epoch: 8224 Training Loss: 0.13422730594211155 Test Loss: 0.3919627007378472\n",
      "Epoch: 8225 Training Loss: 0.13474298604329427 Test Loss: 0.39396240234375\n",
      "Epoch: 8226 Training Loss: 0.13529869503445097 Test Loss: 0.39581437174479167\n",
      "Epoch: 8227 Training Loss: 0.13588282097710502 Test Loss: 0.3971095377604167\n",
      "Epoch: 8228 Training Loss: 0.13646934339735242 Test Loss: 0.39780137803819443\n",
      "Epoch: 8229 Training Loss: 0.1370438232421875 Test Loss: 0.3968961588541667\n",
      "Epoch: 8230 Training Loss: 0.137503779941135 Test Loss: 0.39575132921006945\n",
      "Epoch: 8231 Training Loss: 0.13778561231825087 Test Loss: 0.39482628038194445\n",
      "Epoch: 8232 Training Loss: 0.1379700453016493 Test Loss: 0.39399701605902776\n",
      "Epoch: 8233 Training Loss: 0.13807093302408854 Test Loss: 0.3937795138888889\n",
      "Epoch: 8234 Training Loss: 0.13806743537055122 Test Loss: 0.39205029296875\n",
      "Epoch: 8235 Training Loss: 0.13791014862060547 Test Loss: 0.39019189453125\n",
      "Epoch: 8236 Training Loss: 0.13763104248046876 Test Loss: 0.38902012803819447\n",
      "Epoch: 8237 Training Loss: 0.13732804701063367 Test Loss: 0.3877392849392361\n",
      "Epoch: 8238 Training Loss: 0.13704710218641494 Test Loss: 0.38713194444444443\n",
      "Epoch: 8239 Training Loss: 0.1368469738430447 Test Loss: 0.3867038031684028\n",
      "Epoch: 8240 Training Loss: 0.1367659149169922 Test Loss: 0.3866182725694444\n",
      "Epoch: 8241 Training Loss: 0.13683573913574218 Test Loss: 0.3869622395833333\n",
      "Epoch: 8242 Training Loss: 0.13705276404486763 Test Loss: 0.3876698676215278\n",
      "Epoch: 8243 Training Loss: 0.13743870713975695 Test Loss: 0.38830810546875\n",
      "Epoch: 8244 Training Loss: 0.13797828759087458 Test Loss: 0.38914143880208335\n",
      "Epoch: 8245 Training Loss: 0.13857882097032334 Test Loss: 0.39014111328125\n",
      "Epoch: 8246 Training Loss: 0.13915718841552735 Test Loss: 0.3917958984375\n",
      "Epoch: 8247 Training Loss: 0.1396138161553277 Test Loss: 0.39347325303819447\n",
      "Epoch: 8248 Training Loss: 0.13991436343722874 Test Loss: 0.39482099066840276\n",
      "Epoch: 8249 Training Loss: 0.14000496927897135 Test Loss: 0.39522509765625\n",
      "Epoch: 8250 Training Loss: 0.13983123101128472 Test Loss: 0.39486219618055557\n",
      "Epoch: 8251 Training Loss: 0.1393742421468099 Test Loss: 0.3933492838541667\n",
      "Epoch: 8252 Training Loss: 0.1386382090250651 Test Loss: 0.391197509765625\n",
      "Epoch: 8253 Training Loss: 0.13764667002360026 Test Loss: 0.38867328559027775\n",
      "Epoch: 8254 Training Loss: 0.1365166736178928 Test Loss: 0.38596337890625\n",
      "Epoch: 8255 Training Loss: 0.1352939410739475 Test Loss: 0.38362434895833336\n",
      "Epoch: 8256 Training Loss: 0.13408809577094183 Test Loss: 0.3820945095486111\n",
      "Epoch: 8257 Training Loss: 0.13299832831488714 Test Loss: 0.3815925564236111\n",
      "Epoch: 8258 Training Loss: 0.1320579325358073 Test Loss: 0.3814788953993056\n",
      "Epoch: 8259 Training Loss: 0.13127442423502605 Test Loss: 0.3817485894097222\n",
      "Epoch: 8260 Training Loss: 0.13064806535508897 Test Loss: 0.382169921875\n",
      "Epoch: 8261 Training Loss: 0.13012403361002603 Test Loss: 0.382545654296875\n",
      "Epoch: 8262 Training Loss: 0.12969134182400174 Test Loss: 0.3828913845486111\n",
      "Epoch: 8263 Training Loss: 0.1293373268975152 Test Loss: 0.3830903049045139\n",
      "Epoch: 8264 Training Loss: 0.12904684448242187 Test Loss: 0.3834431966145833\n",
      "Epoch: 8265 Training Loss: 0.12881226348876954 Test Loss: 0.38362779405381947\n",
      "Epoch: 8266 Training Loss: 0.12863467661539713 Test Loss: 0.3839025607638889\n",
      "Epoch: 8267 Training Loss: 0.1284836196899414 Test Loss: 0.3841656901041667\n",
      "Epoch: 8268 Training Loss: 0.12836398400200738 Test Loss: 0.38452061631944445\n",
      "Epoch: 8269 Training Loss: 0.128276003519694 Test Loss: 0.3847088216145833\n",
      "Epoch: 8270 Training Loss: 0.12819746568467882 Test Loss: 0.384907958984375\n",
      "Epoch: 8271 Training Loss: 0.12814202711317274 Test Loss: 0.3850362955729167\n",
      "Epoch: 8272 Training Loss: 0.12809753926595052 Test Loss: 0.38517881944444443\n",
      "Epoch: 8273 Training Loss: 0.12807142893473308 Test Loss: 0.38532066514756946\n",
      "Epoch: 8274 Training Loss: 0.12805537753634982 Test Loss: 0.38541002061631946\n",
      "Epoch: 8275 Training Loss: 0.12804647827148438 Test Loss: 0.38559716796875\n",
      "Epoch: 8276 Training Loss: 0.1280460442437066 Test Loss: 0.38561360677083334\n",
      "Epoch: 8277 Training Loss: 0.12804884423149956 Test Loss: 0.3858161349826389\n",
      "Epoch: 8278 Training Loss: 0.12806124793158638 Test Loss: 0.3858252495659722\n",
      "Epoch: 8279 Training Loss: 0.1280922631157769 Test Loss: 0.3861117350260417\n",
      "Epoch: 8280 Training Loss: 0.1281277321709527 Test Loss: 0.38634114583333334\n",
      "Epoch: 8281 Training Loss: 0.12817096879747178 Test Loss: 0.38664447699652776\n",
      "Epoch: 8282 Training Loss: 0.12821581522623698 Test Loss: 0.3868330078125\n",
      "Epoch: 8283 Training Loss: 0.12827504136827256 Test Loss: 0.38727490234375\n",
      "Epoch: 8284 Training Loss: 0.12833992852105033 Test Loss: 0.38776350911458335\n",
      "Epoch: 8285 Training Loss: 0.12841314358181424 Test Loss: 0.38829410807291664\n",
      "Epoch: 8286 Training Loss: 0.12849433135986327 Test Loss: 0.3887107747395833\n",
      "Epoch: 8287 Training Loss: 0.12858511945936416 Test Loss: 0.38911097547743057\n",
      "Epoch: 8288 Training Loss: 0.12868218570285372 Test Loss: 0.38964539930555553\n",
      "Epoch: 8289 Training Loss: 0.12878766547309028 Test Loss: 0.3900425347222222\n",
      "Epoch: 8290 Training Loss: 0.12891451941596138 Test Loss: 0.3903900282118056\n",
      "Epoch: 8291 Training Loss: 0.12905355495876736 Test Loss: 0.3904028862847222\n",
      "Epoch: 8292 Training Loss: 0.12919010501437717 Test Loss: 0.3905263129340278\n",
      "Epoch: 8293 Training Loss: 0.12932094913058811 Test Loss: 0.390659423828125\n",
      "Epoch: 8294 Training Loss: 0.12946260155571832 Test Loss: 0.39070741102430556\n",
      "Epoch: 8295 Training Loss: 0.12961637115478516 Test Loss: 0.39068055555555553\n",
      "Epoch: 8296 Training Loss: 0.12978285641140408 Test Loss: 0.3906922200520833\n",
      "Epoch: 8297 Training Loss: 0.12997614118787978 Test Loss: 0.39072610134548613\n",
      "Epoch: 8298 Training Loss: 0.1301844999525282 Test Loss: 0.3905145941840278\n",
      "Epoch: 8299 Training Loss: 0.13042187839084202 Test Loss: 0.39058875868055554\n",
      "Epoch: 8300 Training Loss: 0.13068082851833768 Test Loss: 0.39056298828125\n",
      "Epoch: 8301 Training Loss: 0.13096171654595268 Test Loss: 0.39053377278645834\n",
      "Epoch: 8302 Training Loss: 0.1312439414130317 Test Loss: 0.3903988986545139\n",
      "Epoch: 8303 Training Loss: 0.13152782016330294 Test Loss: 0.3905047743055556\n",
      "Epoch: 8304 Training Loss: 0.13179781002468532 Test Loss: 0.390865234375\n",
      "Epoch: 8305 Training Loss: 0.1320715094672309 Test Loss: 0.3912155490451389\n",
      "Epoch: 8306 Training Loss: 0.13234124755859375 Test Loss: 0.3917092013888889\n",
      "Epoch: 8307 Training Loss: 0.13261756303575303 Test Loss: 0.3923990071614583\n",
      "Epoch: 8308 Training Loss: 0.13289982774522568 Test Loss: 0.39263628472222223\n",
      "Epoch: 8309 Training Loss: 0.1331756574842665 Test Loss: 0.39256797960069445\n",
      "Epoch: 8310 Training Loss: 0.13344894324408638 Test Loss: 0.3919129231770833\n",
      "Epoch: 8311 Training Loss: 0.13373203786214194 Test Loss: 0.39077625868055554\n",
      "Epoch: 8312 Training Loss: 0.13398093838161892 Test Loss: 0.38985194227430553\n",
      "Epoch: 8313 Training Loss: 0.13422332339816623 Test Loss: 0.38933924696180555\n",
      "Epoch: 8314 Training Loss: 0.13448727671305338 Test Loss: 0.3902945963541667\n",
      "Epoch: 8315 Training Loss: 0.13473463185628254 Test Loss: 0.3913055555555556\n",
      "Epoch: 8316 Training Loss: 0.13486707390679253 Test Loss: 0.39228209092881944\n",
      "Epoch: 8317 Training Loss: 0.1349075173272027 Test Loss: 0.39304448784722223\n",
      "Epoch: 8318 Training Loss: 0.13484216901991103 Test Loss: 0.3939244791666667\n",
      "Epoch: 8319 Training Loss: 0.13475590006510416 Test Loss: 0.39403526475694445\n",
      "Epoch: 8320 Training Loss: 0.13466300455729166 Test Loss: 0.39403586154513887\n",
      "Epoch: 8321 Training Loss: 0.13460056644015841 Test Loss: 0.3938305935329861\n",
      "Epoch: 8322 Training Loss: 0.13456486341688367 Test Loss: 0.39292225477430553\n",
      "Epoch: 8323 Training Loss: 0.13455699412027994 Test Loss: 0.3919642740885417\n",
      "Epoch: 8324 Training Loss: 0.13452650197347005 Test Loss: 0.3913636067708333\n",
      "Epoch: 8325 Training Loss: 0.1344879904852973 Test Loss: 0.39163525390625\n",
      "Epoch: 8326 Training Loss: 0.13445183224148222 Test Loss: 0.39237046983506946\n",
      "Epoch: 8327 Training Loss: 0.1343834974500868 Test Loss: 0.39339637586805554\n",
      "Epoch: 8328 Training Loss: 0.13433339945475262 Test Loss: 0.39467816840277775\n",
      "Epoch: 8329 Training Loss: 0.1343123007880317 Test Loss: 0.39591259765625\n",
      "Epoch: 8330 Training Loss: 0.1342882546318902 Test Loss: 0.3963778754340278\n",
      "Epoch: 8331 Training Loss: 0.13425687747531467 Test Loss: 0.3965017361111111\n",
      "Epoch: 8332 Training Loss: 0.13416118621826173 Test Loss: 0.3955575629340278\n",
      "Epoch: 8333 Training Loss: 0.13399702962239585 Test Loss: 0.3940287543402778\n",
      "Epoch: 8334 Training Loss: 0.1337223442925347 Test Loss: 0.39236146375868053\n",
      "Epoch: 8335 Training Loss: 0.1333714311387804 Test Loss: 0.39045732964409724\n",
      "Epoch: 8336 Training Loss: 0.1329569574991862 Test Loss: 0.3893900824652778\n",
      "Epoch: 8337 Training Loss: 0.13254794226752387 Test Loss: 0.3887643771701389\n",
      "Epoch: 8338 Training Loss: 0.1321842753092448 Test Loss: 0.3891568196614583\n",
      "Epoch: 8339 Training Loss: 0.1318770463731554 Test Loss: 0.38988932291666667\n",
      "Epoch: 8340 Training Loss: 0.1316228535970052 Test Loss: 0.3907127007378472\n",
      "Epoch: 8341 Training Loss: 0.1314352272881402 Test Loss: 0.3912824164496528\n",
      "Epoch: 8342 Training Loss: 0.13130704752604166 Test Loss: 0.39183051215277775\n",
      "Epoch: 8343 Training Loss: 0.1312241473727756 Test Loss: 0.3918661566840278\n",
      "Epoch: 8344 Training Loss: 0.13118194749620227 Test Loss: 0.3916389702690972\n",
      "Epoch: 8345 Training Loss: 0.13115348137749566 Test Loss: 0.39118126085069443\n",
      "Epoch: 8346 Training Loss: 0.1311416507297092 Test Loss: 0.39069544813368057\n",
      "Epoch: 8347 Training Loss: 0.13117046610514324 Test Loss: 0.39032383897569445\n",
      "Epoch: 8348 Training Loss: 0.13121022288004558 Test Loss: 0.3897725151909722\n",
      "Epoch: 8349 Training Loss: 0.13125406053331162 Test Loss: 0.3893321397569444\n",
      "Epoch: 8350 Training Loss: 0.13132017856174044 Test Loss: 0.3888498806423611\n",
      "Epoch: 8351 Training Loss: 0.1314091330634223 Test Loss: 0.3885724555121528\n",
      "Epoch: 8352 Training Loss: 0.13153331078423394 Test Loss: 0.38847273763020834\n",
      "Epoch: 8353 Training Loss: 0.13168419901529949 Test Loss: 0.3889401584201389\n",
      "Epoch: 8354 Training Loss: 0.13188922458224828 Test Loss: 0.3894479709201389\n",
      "Epoch: 8355 Training Loss: 0.1321576639811198 Test Loss: 0.3900820583767361\n",
      "Epoch: 8356 Training Loss: 0.13250002966986762 Test Loss: 0.3909361165364583\n",
      "Epoch: 8357 Training Loss: 0.13292914666069877 Test Loss: 0.3916735568576389\n",
      "Epoch: 8358 Training Loss: 0.13342437744140626 Test Loss: 0.3923522677951389\n",
      "Epoch: 8359 Training Loss: 0.1340198449028863 Test Loss: 0.3930342339409722\n",
      "Epoch: 8360 Training Loss: 0.13474401007758247 Test Loss: 0.39334434678819447\n",
      "Epoch: 8361 Training Loss: 0.13559107801649306 Test Loss: 0.39322216796875\n",
      "Epoch: 8362 Training Loss: 0.13658582899305555 Test Loss: 0.39266042751736113\n",
      "Epoch: 8363 Training Loss: 0.13776735093858508 Test Loss: 0.3916926540798611\n",
      "Epoch: 8364 Training Loss: 0.13915154774983723 Test Loss: 0.3905531412760417\n",
      "Epoch: 8365 Training Loss: 0.14060823822021484 Test Loss: 0.39001337348090276\n",
      "Epoch: 8366 Training Loss: 0.14191612074110244 Test Loss: 0.39178672960069444\n",
      "Epoch: 8367 Training Loss: 0.14284078894721136 Test Loss: 0.3969315592447917\n",
      "Epoch: 8368 Training Loss: 0.14314176856146918 Test Loss: 0.4024196506076389\n",
      "Epoch: 8369 Training Loss: 0.1427872314453125 Test Loss: 0.4045157335069444\n",
      "Epoch: 8370 Training Loss: 0.14182772233751084 Test Loss: 0.40257435438368055\n",
      "Epoch: 8371 Training Loss: 0.14043345896402995 Test Loss: 0.39880154079861113\n",
      "Epoch: 8372 Training Loss: 0.13868046230740017 Test Loss: 0.39537559678819445\n",
      "Epoch: 8373 Training Loss: 0.1368227793375651 Test Loss: 0.39246381293402777\n",
      "Epoch: 8374 Training Loss: 0.13506879001193575 Test Loss: 0.3902155490451389\n",
      "Epoch: 8375 Training Loss: 0.13353374565972223 Test Loss: 0.38863186306423614\n",
      "Epoch: 8376 Training Loss: 0.13227122328016494 Test Loss: 0.38734543185763887\n",
      "Epoch: 8377 Training Loss: 0.13125104692247178 Test Loss: 0.3865209418402778\n",
      "Epoch: 8378 Training Loss: 0.1304652353922526 Test Loss: 0.3858629557291667\n",
      "Epoch: 8379 Training Loss: 0.1298695805867513 Test Loss: 0.3855862630208333\n",
      "Epoch: 8380 Training Loss: 0.12941663106282553 Test Loss: 0.3854309895833333\n",
      "Epoch: 8381 Training Loss: 0.1290787607828776 Test Loss: 0.3851789008246528\n",
      "Epoch: 8382 Training Loss: 0.1288111860487196 Test Loss: 0.38515543619791665\n",
      "Epoch: 8383 Training Loss: 0.1286102057562934 Test Loss: 0.3850061848958333\n",
      "Epoch: 8384 Training Loss: 0.1284674546983507 Test Loss: 0.3852003580729167\n",
      "Epoch: 8385 Training Loss: 0.1283637669881185 Test Loss: 0.3853167860243056\n",
      "Epoch: 8386 Training Loss: 0.12828695254855685 Test Loss: 0.3854727647569444\n",
      "Epoch: 8387 Training Loss: 0.1282439888848199 Test Loss: 0.38576700846354167\n",
      "Epoch: 8388 Training Loss: 0.12821856774224175 Test Loss: 0.3861060112847222\n",
      "Epoch: 8389 Training Loss: 0.1282146970960829 Test Loss: 0.3864586588541667\n",
      "Epoch: 8390 Training Loss: 0.1282219712999132 Test Loss: 0.38676790364583336\n",
      "Epoch: 8391 Training Loss: 0.12825005594889322 Test Loss: 0.38716791449652777\n",
      "Epoch: 8392 Training Loss: 0.12829136827256946 Test Loss: 0.3874981011284722\n",
      "Epoch: 8393 Training Loss: 0.12834874047173395 Test Loss: 0.38807725694444445\n",
      "Epoch: 8394 Training Loss: 0.12841372850206162 Test Loss: 0.38862483723958335\n",
      "Epoch: 8395 Training Loss: 0.12849094814724393 Test Loss: 0.38918831380208335\n",
      "Epoch: 8396 Training Loss: 0.12857914310031468 Test Loss: 0.3897333441840278\n",
      "Epoch: 8397 Training Loss: 0.12867716386583117 Test Loss: 0.39025770399305554\n",
      "Epoch: 8398 Training Loss: 0.12878795878092447 Test Loss: 0.39057826063368056\n",
      "Epoch: 8399 Training Loss: 0.12889620463053386 Test Loss: 0.3908929307725694\n",
      "Epoch: 8400 Training Loss: 0.12901442294650609 Test Loss: 0.39086452907986113\n",
      "Epoch: 8401 Training Loss: 0.1291305652194553 Test Loss: 0.39074310980902777\n",
      "Epoch: 8402 Training Loss: 0.12925801340738932 Test Loss: 0.39048209635416664\n",
      "Epoch: 8403 Training Loss: 0.12940184529622395 Test Loss: 0.39034288194444444\n",
      "Epoch: 8404 Training Loss: 0.1295764414469401 Test Loss: 0.390199462890625\n",
      "Epoch: 8405 Training Loss: 0.1297676298353407 Test Loss: 0.3902290581597222\n",
      "Epoch: 8406 Training Loss: 0.12995162285698786 Test Loss: 0.39036827256944445\n",
      "Epoch: 8407 Training Loss: 0.13015651194254557 Test Loss: 0.39045619032118056\n",
      "Epoch: 8408 Training Loss: 0.13037696753607855 Test Loss: 0.3909918077256944\n",
      "Epoch: 8409 Training Loss: 0.13060379706488714 Test Loss: 0.3913828396267361\n",
      "Epoch: 8410 Training Loss: 0.13085740492078993 Test Loss: 0.39241956922743054\n",
      "Epoch: 8411 Training Loss: 0.13115459442138672 Test Loss: 0.39327398003472225\n",
      "Epoch: 8412 Training Loss: 0.13147428131103517 Test Loss: 0.3948068033854167\n",
      "Epoch: 8413 Training Loss: 0.13183335537380642 Test Loss: 0.3966982964409722\n",
      "Epoch: 8414 Training Loss: 0.13224683210584853 Test Loss: 0.3990938313802083\n",
      "Epoch: 8415 Training Loss: 0.13273090532090928 Test Loss: 0.4019210883246528\n",
      "Epoch: 8416 Training Loss: 0.13330768076578775 Test Loss: 0.4045336371527778\n",
      "Epoch: 8417 Training Loss: 0.13395126597086587 Test Loss: 0.40713682725694444\n",
      "Epoch: 8418 Training Loss: 0.13468900129530165 Test Loss: 0.40857763671875\n",
      "Epoch: 8419 Training Loss: 0.13553287929958768 Test Loss: 0.4084745551215278\n",
      "Epoch: 8420 Training Loss: 0.13646341790093316 Test Loss: 0.406576171875\n",
      "Epoch: 8421 Training Loss: 0.13745917256673176 Test Loss: 0.40358881293402776\n",
      "Epoch: 8422 Training Loss: 0.13844854651557076 Test Loss: 0.40026519097222224\n",
      "Epoch: 8423 Training Loss: 0.13935801781548393 Test Loss: 0.39851605902777776\n",
      "Epoch: 8424 Training Loss: 0.14010348934597439 Test Loss: 0.3987587890625\n",
      "Epoch: 8425 Training Loss: 0.14057396782769097 Test Loss: 0.400098388671875\n",
      "Epoch: 8426 Training Loss: 0.14073518964979384 Test Loss: 0.4024841037326389\n",
      "Epoch: 8427 Training Loss: 0.14064651574028864 Test Loss: 0.40421232096354165\n",
      "Epoch: 8428 Training Loss: 0.14042151387532553 Test Loss: 0.40406751844618055\n",
      "Epoch: 8429 Training Loss: 0.1400798848470052 Test Loss: 0.4013455946180556\n",
      "Epoch: 8430 Training Loss: 0.1396513866848416 Test Loss: 0.39695377604166665\n",
      "Epoch: 8431 Training Loss: 0.13910071987575956 Test Loss: 0.3926388075086806\n",
      "Epoch: 8432 Training Loss: 0.13849130164252388 Test Loss: 0.3904362521701389\n",
      "Epoch: 8433 Training Loss: 0.13792322964138454 Test Loss: 0.39113888888888887\n",
      "Epoch: 8434 Training Loss: 0.13744068315294053 Test Loss: 0.39346666124131946\n",
      "Epoch: 8435 Training Loss: 0.1370269020928277 Test Loss: 0.3966626247829861\n",
      "Epoch: 8436 Training Loss: 0.13657869211832682 Test Loss: 0.3998654513888889\n",
      "Epoch: 8437 Training Loss: 0.13609867519802518 Test Loss: 0.4033464898003472\n",
      "Epoch: 8438 Training Loss: 0.1356388956705729 Test Loss: 0.4060986056857639\n",
      "Epoch: 8439 Training Loss: 0.13519935862223306 Test Loss: 0.40728702799479166\n",
      "Epoch: 8440 Training Loss: 0.13469112481011286 Test Loss: 0.40491810438368053\n",
      "Epoch: 8441 Training Loss: 0.13413892194959853 Test Loss: 0.3994202473958333\n",
      "Epoch: 8442 Training Loss: 0.13357546742757162 Test Loss: 0.394064453125\n",
      "Epoch: 8443 Training Loss: 0.13306243048773872 Test Loss: 0.3903998480902778\n",
      "Epoch: 8444 Training Loss: 0.13261420271131727 Test Loss: 0.3884282769097222\n",
      "Epoch: 8445 Training Loss: 0.1322569825914171 Test Loss: 0.38734817165798613\n",
      "Epoch: 8446 Training Loss: 0.13200169711642795 Test Loss: 0.38656903754340277\n",
      "Epoch: 8447 Training Loss: 0.13185102759467232 Test Loss: 0.38654489474826387\n",
      "Epoch: 8448 Training Loss: 0.13180266486273873 Test Loss: 0.386725341796875\n",
      "Epoch: 8449 Training Loss: 0.13183325449625652 Test Loss: 0.3870954318576389\n",
      "Epoch: 8450 Training Loss: 0.13193999565972223 Test Loss: 0.3878704427083333\n",
      "Epoch: 8451 Training Loss: 0.13210409461127387 Test Loss: 0.38853553602430557\n",
      "Epoch: 8452 Training Loss: 0.13232530551486546 Test Loss: 0.38952650282118056\n",
      "Epoch: 8453 Training Loss: 0.13258170572916667 Test Loss: 0.39040540907118054\n",
      "Epoch: 8454 Training Loss: 0.1328711412217882 Test Loss: 0.3913536783854167\n",
      "Epoch: 8455 Training Loss: 0.1331875 Test Loss: 0.3919845920138889\n",
      "Epoch: 8456 Training Loss: 0.1335120078192817 Test Loss: 0.392488525390625\n",
      "Epoch: 8457 Training Loss: 0.1338512920803494 Test Loss: 0.39301551649305555\n",
      "Epoch: 8458 Training Loss: 0.13422523159450955 Test Loss: 0.39371563042534724\n",
      "Epoch: 8459 Training Loss: 0.13467266845703124 Test Loss: 0.39537190755208335\n",
      "Epoch: 8460 Training Loss: 0.13520809173583984 Test Loss: 0.39784882269965277\n",
      "Epoch: 8461 Training Loss: 0.13582792239718966 Test Loss: 0.4010890299479167\n",
      "Epoch: 8462 Training Loss: 0.13654571109347874 Test Loss: 0.4031913519965278\n",
      "Epoch: 8463 Training Loss: 0.1373010737101237 Test Loss: 0.4034067654079861\n",
      "Epoch: 8464 Training Loss: 0.1379428931342231 Test Loss: 0.40266484917534723\n",
      "Epoch: 8465 Training Loss: 0.13832855478922526 Test Loss: 0.4036196017795139\n",
      "Epoch: 8466 Training Loss: 0.138478396097819 Test Loss: 0.4051223415798611\n",
      "Epoch: 8467 Training Loss: 0.1383622512817383 Test Loss: 0.40757161458333335\n",
      "Epoch: 8468 Training Loss: 0.13800156063503688 Test Loss: 0.40708482530381945\n",
      "Epoch: 8469 Training Loss: 0.1374222149319119 Test Loss: 0.40267740885416664\n",
      "Epoch: 8470 Training Loss: 0.1367410430908203 Test Loss: 0.39737342664930553\n",
      "Epoch: 8471 Training Loss: 0.1361714358859592 Test Loss: 0.39289643012152775\n",
      "Epoch: 8472 Training Loss: 0.13570801120334203 Test Loss: 0.3899382595486111\n",
      "Epoch: 8473 Training Loss: 0.13528833940294055 Test Loss: 0.38821142578125\n",
      "Epoch: 8474 Training Loss: 0.13487178802490235 Test Loss: 0.3871654459635417\n",
      "Epoch: 8475 Training Loss: 0.134463382297092 Test Loss: 0.3866688368055556\n",
      "Epoch: 8476 Training Loss: 0.13408420817057293 Test Loss: 0.3868857693142361\n",
      "Epoch: 8477 Training Loss: 0.13374139234754776 Test Loss: 0.38730457899305554\n",
      "Epoch: 8478 Training Loss: 0.13343075222439235 Test Loss: 0.38753108723958335\n",
      "Epoch: 8479 Training Loss: 0.13316077338324653 Test Loss: 0.38775496419270833\n",
      "Epoch: 8480 Training Loss: 0.1329488754272461 Test Loss: 0.38765066189236114\n",
      "Epoch: 8481 Training Loss: 0.13278792232937284 Test Loss: 0.3871650119357639\n",
      "Epoch: 8482 Training Loss: 0.13265636698404948 Test Loss: 0.38657052951388887\n",
      "Epoch: 8483 Training Loss: 0.13255440266927082 Test Loss: 0.38578499348958334\n",
      "Epoch: 8484 Training Loss: 0.1324668494330512 Test Loss: 0.38490272352430555\n",
      "Epoch: 8485 Training Loss: 0.13240729522705078 Test Loss: 0.3841094021267361\n",
      "Epoch: 8486 Training Loss: 0.13238231658935548 Test Loss: 0.3837756618923611\n",
      "Epoch: 8487 Training Loss: 0.13238490125868055 Test Loss: 0.38362014431423613\n",
      "Epoch: 8488 Training Loss: 0.13239415232340496 Test Loss: 0.38384418402777776\n",
      "Epoch: 8489 Training Loss: 0.13240286509195964 Test Loss: 0.384458984375\n",
      "Epoch: 8490 Training Loss: 0.13241673872205947 Test Loss: 0.3852605251736111\n",
      "Epoch: 8491 Training Loss: 0.1324404508802626 Test Loss: 0.38626915147569446\n",
      "Epoch: 8492 Training Loss: 0.13247519514295789 Test Loss: 0.38727989366319443\n",
      "Epoch: 8493 Training Loss: 0.13252015346950954 Test Loss: 0.3881783582899306\n",
      "Epoch: 8494 Training Loss: 0.13257650332980686 Test Loss: 0.3888279622395833\n",
      "Epoch: 8495 Training Loss: 0.13259620073106554 Test Loss: 0.3888012152777778\n",
      "Epoch: 8496 Training Loss: 0.13258960469563802 Test Loss: 0.3887420789930556\n",
      "Epoch: 8497 Training Loss: 0.13255936262342666 Test Loss: 0.3883178439670139\n",
      "Epoch: 8498 Training Loss: 0.13249481370713975 Test Loss: 0.3875368923611111\n",
      "Epoch: 8499 Training Loss: 0.13235791609022352 Test Loss: 0.38643397352430553\n",
      "Epoch: 8500 Training Loss: 0.13218340301513673 Test Loss: 0.38536341688368053\n",
      "Epoch: 8501 Training Loss: 0.1319940405951606 Test Loss: 0.38434114583333334\n",
      "Epoch: 8502 Training Loss: 0.13179691569010415 Test Loss: 0.38380181206597225\n",
      "Epoch: 8503 Training Loss: 0.13161851925320095 Test Loss: 0.3834638943142361\n",
      "Epoch: 8504 Training Loss: 0.13146810404459636 Test Loss: 0.38373627387152776\n",
      "Epoch: 8505 Training Loss: 0.13133006625705296 Test Loss: 0.38444612630208336\n",
      "Epoch: 8506 Training Loss: 0.1312197291056315 Test Loss: 0.3855225423177083\n",
      "Epoch: 8507 Training Loss: 0.13111999935574 Test Loss: 0.3867920464409722\n",
      "Epoch: 8508 Training Loss: 0.13102625698513454 Test Loss: 0.38795716688368054\n",
      "Epoch: 8509 Training Loss: 0.13094156053331163 Test Loss: 0.38887977430555554\n",
      "Epoch: 8510 Training Loss: 0.1308512208726671 Test Loss: 0.389189697265625\n",
      "Epoch: 8511 Training Loss: 0.13077174377441406 Test Loss: 0.3890437282986111\n",
      "Epoch: 8512 Training Loss: 0.13068338182237413 Test Loss: 0.3884304741753472\n",
      "Epoch: 8513 Training Loss: 0.13059560987684463 Test Loss: 0.387404296875\n",
      "Epoch: 8514 Training Loss: 0.13050868648952907 Test Loss: 0.3860625\n",
      "Epoch: 8515 Training Loss: 0.13043436177571616 Test Loss: 0.38467979600694446\n",
      "Epoch: 8516 Training Loss: 0.13035286882188585 Test Loss: 0.3833199327256944\n",
      "Epoch: 8517 Training Loss: 0.1302565943400065 Test Loss: 0.38211930338541666\n",
      "Epoch: 8518 Training Loss: 0.13016426934136285 Test Loss: 0.3813976779513889\n",
      "Epoch: 8519 Training Loss: 0.13007458665635852 Test Loss: 0.3810124240451389\n",
      "Epoch: 8520 Training Loss: 0.12999112701416016 Test Loss: 0.3813528103298611\n",
      "Epoch: 8521 Training Loss: 0.12991377258300782 Test Loss: 0.38221099175347223\n",
      "Epoch: 8522 Training Loss: 0.12984265984429252 Test Loss: 0.38343299696180555\n",
      "Epoch: 8523 Training Loss: 0.12978290642632379 Test Loss: 0.38501264105902777\n",
      "Epoch: 8524 Training Loss: 0.1297321039835612 Test Loss: 0.38663175455729165\n",
      "Epoch: 8525 Training Loss: 0.12970271301269531 Test Loss: 0.38793060980902777\n",
      "Epoch: 8526 Training Loss: 0.12967588297526042 Test Loss: 0.3890885416666667\n",
      "Epoch: 8527 Training Loss: 0.1296451153225369 Test Loss: 0.38959939236111113\n",
      "Epoch: 8528 Training Loss: 0.1296140323215061 Test Loss: 0.38995030381944445\n",
      "Epoch: 8529 Training Loss: 0.12957105678982206 Test Loss: 0.3901467013888889\n",
      "Epoch: 8530 Training Loss: 0.12953450690375434 Test Loss: 0.39042217339409724\n",
      "Epoch: 8531 Training Loss: 0.12949930233425563 Test Loss: 0.3908998752170139\n",
      "Epoch: 8532 Training Loss: 0.1294713855319553 Test Loss: 0.39129172092013886\n",
      "Epoch: 8533 Training Loss: 0.1294475623236762 Test Loss: 0.3917158203125\n",
      "Epoch: 8534 Training Loss: 0.1294169430202908 Test Loss: 0.3917465549045139\n",
      "Epoch: 8535 Training Loss: 0.1293874299791124 Test Loss: 0.391856201171875\n",
      "Epoch: 8536 Training Loss: 0.12936271328396268 Test Loss: 0.39181678602430553\n",
      "Epoch: 8537 Training Loss: 0.1293254894680447 Test Loss: 0.3913953721788194\n",
      "Epoch: 8538 Training Loss: 0.1292796096801758 Test Loss: 0.39098046875\n",
      "Epoch: 8539 Training Loss: 0.12923567877875433 Test Loss: 0.39034488932291667\n",
      "Epoch: 8540 Training Loss: 0.12922066582573785 Test Loss: 0.38967963324652777\n",
      "Epoch: 8541 Training Loss: 0.12922311147054036 Test Loss: 0.3889532606336806\n",
      "Epoch: 8542 Training Loss: 0.12924859873453776 Test Loss: 0.38811943901909723\n",
      "Epoch: 8543 Training Loss: 0.1293114496866862 Test Loss: 0.38728724500868056\n",
      "Epoch: 8544 Training Loss: 0.12942503696017796 Test Loss: 0.38671421983506943\n",
      "Epoch: 8545 Training Loss: 0.1295823254055447 Test Loss: 0.3860386284722222\n",
      "Epoch: 8546 Training Loss: 0.1297857411702474 Test Loss: 0.3854778103298611\n",
      "Epoch: 8547 Training Loss: 0.13004149288601347 Test Loss: 0.38520939127604165\n",
      "Epoch: 8548 Training Loss: 0.13032737647162543 Test Loss: 0.38508582899305555\n",
      "Epoch: 8549 Training Loss: 0.13063759952121312 Test Loss: 0.38496375868055555\n",
      "Epoch: 8550 Training Loss: 0.1309654066297743 Test Loss: 0.3848680013020833\n",
      "Epoch: 8551 Training Loss: 0.13130751376681857 Test Loss: 0.3849407552083333\n",
      "Epoch: 8552 Training Loss: 0.13163735029432508 Test Loss: 0.3849728190104167\n",
      "Epoch: 8553 Training Loss: 0.1319380849202474 Test Loss: 0.3852052951388889\n",
      "Epoch: 8554 Training Loss: 0.13219368998209635 Test Loss: 0.38597808159722224\n",
      "Epoch: 8555 Training Loss: 0.1324127638075087 Test Loss: 0.38691829427083335\n",
      "Epoch: 8556 Training Loss: 0.13258256191677517 Test Loss: 0.38787201605902777\n",
      "Epoch: 8557 Training Loss: 0.1327173368665907 Test Loss: 0.3889594997829861\n",
      "Epoch: 8558 Training Loss: 0.13279071214463975 Test Loss: 0.3899610460069444\n",
      "Epoch: 8559 Training Loss: 0.13280179087320965 Test Loss: 0.39087120225694444\n",
      "Epoch: 8560 Training Loss: 0.13272700246175131 Test Loss: 0.3917349989149306\n",
      "Epoch: 8561 Training Loss: 0.13256731923421225 Test Loss: 0.3921933051215278\n",
      "Epoch: 8562 Training Loss: 0.13236780124240452 Test Loss: 0.39251850043402775\n",
      "Epoch: 8563 Training Loss: 0.1321505330403646 Test Loss: 0.39285139973958333\n",
      "Epoch: 8564 Training Loss: 0.13192623901367187 Test Loss: 0.3928050130208333\n",
      "Epoch: 8565 Training Loss: 0.13170010375976562 Test Loss: 0.3926152886284722\n",
      "Epoch: 8566 Training Loss: 0.1314740685356988 Test Loss: 0.39198130967881945\n",
      "Epoch: 8567 Training Loss: 0.13125194888644748 Test Loss: 0.3914882541232639\n",
      "Epoch: 8568 Training Loss: 0.1310263917711046 Test Loss: 0.3908265516493056\n",
      "Epoch: 8569 Training Loss: 0.13079713779025606 Test Loss: 0.3905070529513889\n",
      "Epoch: 8570 Training Loss: 0.13057155439588758 Test Loss: 0.39044086371527775\n",
      "Epoch: 8571 Training Loss: 0.1303463406032986 Test Loss: 0.3902916937934028\n",
      "Epoch: 8572 Training Loss: 0.13012517717149522 Test Loss: 0.39042450629340275\n",
      "Epoch: 8573 Training Loss: 0.1299159706963433 Test Loss: 0.3903030056423611\n",
      "Epoch: 8574 Training Loss: 0.12970970492892794 Test Loss: 0.39026942274305554\n",
      "Epoch: 8575 Training Loss: 0.12951105414496528 Test Loss: 0.3902788899739583\n",
      "Epoch: 8576 Training Loss: 0.12932244957817926 Test Loss: 0.390201416015625\n",
      "Epoch: 8577 Training Loss: 0.1291595925225152 Test Loss: 0.390316650390625\n",
      "Epoch: 8578 Training Loss: 0.12901759931776258 Test Loss: 0.39032687717013886\n",
      "Epoch: 8579 Training Loss: 0.12890597958034938 Test Loss: 0.39033192274305556\n",
      "Epoch: 8580 Training Loss: 0.12883476003011068 Test Loss: 0.39039320203993055\n",
      "Epoch: 8581 Training Loss: 0.12880007002088759 Test Loss: 0.39039393446180554\n",
      "Epoch: 8582 Training Loss: 0.12879554833306206 Test Loss: 0.39057231987847224\n",
      "Epoch: 8583 Training Loss: 0.1288179677327474 Test Loss: 0.3907836642795139\n",
      "Epoch: 8584 Training Loss: 0.1288581085205078 Test Loss: 0.391205322265625\n",
      "Epoch: 8585 Training Loss: 0.1289052463107639 Test Loss: 0.39123697916666667\n",
      "Epoch: 8586 Training Loss: 0.12897334798177082 Test Loss: 0.3913876953125\n",
      "Epoch: 8587 Training Loss: 0.12904515075683592 Test Loss: 0.3914614529079861\n",
      "Epoch: 8588 Training Loss: 0.1291283476087782 Test Loss: 0.3914141167534722\n",
      "Epoch: 8589 Training Loss: 0.12920634206136067 Test Loss: 0.3911653374565972\n",
      "Epoch: 8590 Training Loss: 0.12927450476752386 Test Loss: 0.3908609212239583\n",
      "Epoch: 8591 Training Loss: 0.12934830220540364 Test Loss: 0.3906553276909722\n",
      "Epoch: 8592 Training Loss: 0.12943470848931207 Test Loss: 0.39046620008680555\n",
      "Epoch: 8593 Training Loss: 0.1295323486328125 Test Loss: 0.3902511935763889\n",
      "Epoch: 8594 Training Loss: 0.12964905717637804 Test Loss: 0.3900200737847222\n",
      "Epoch: 8595 Training Loss: 0.12978402540418837 Test Loss: 0.38985400390625\n",
      "Epoch: 8596 Training Loss: 0.12993048689100478 Test Loss: 0.3898246527777778\n",
      "Epoch: 8597 Training Loss: 0.13009351264105903 Test Loss: 0.38976318359375\n",
      "Epoch: 8598 Training Loss: 0.13025833808051215 Test Loss: 0.3898197699652778\n",
      "Epoch: 8599 Training Loss: 0.13044212341308595 Test Loss: 0.39002308485243053\n",
      "Epoch: 8600 Training Loss: 0.13063883463541667 Test Loss: 0.39025442165798613\n",
      "Epoch: 8601 Training Loss: 0.13083580440945095 Test Loss: 0.3907878689236111\n",
      "Epoch: 8602 Training Loss: 0.13105762905544704 Test Loss: 0.39126123046875\n",
      "Epoch: 8603 Training Loss: 0.13128761545817058 Test Loss: 0.3917910427517361\n",
      "Epoch: 8604 Training Loss: 0.13152849578857423 Test Loss: 0.3922687174479167\n",
      "Epoch: 8605 Training Loss: 0.13179564666748048 Test Loss: 0.3929098849826389\n",
      "Epoch: 8606 Training Loss: 0.13209054565429687 Test Loss: 0.3935601128472222\n",
      "Epoch: 8607 Training Loss: 0.13240992821587458 Test Loss: 0.3942792697482639\n",
      "Epoch: 8608 Training Loss: 0.1327277094523112 Test Loss: 0.39460682508680556\n",
      "Epoch: 8609 Training Loss: 0.1330489264594184 Test Loss: 0.3946725802951389\n",
      "Epoch: 8610 Training Loss: 0.1333672637939453 Test Loss: 0.39430702039930554\n",
      "Epoch: 8611 Training Loss: 0.13369764794243708 Test Loss: 0.39362217881944445\n",
      "Epoch: 8612 Training Loss: 0.13399967617458766 Test Loss: 0.3926220703125\n",
      "Epoch: 8613 Training Loss: 0.13426603613959417 Test Loss: 0.39178070746527777\n",
      "Epoch: 8614 Training Loss: 0.1344771194458008 Test Loss: 0.3913241916232639\n",
      "Epoch: 8615 Training Loss: 0.13464247809516058 Test Loss: 0.39124951171875\n",
      "Epoch: 8616 Training Loss: 0.13472649892171223 Test Loss: 0.39142401801215276\n",
      "Epoch: 8617 Training Loss: 0.134745970831977 Test Loss: 0.39241710069444447\n",
      "Epoch: 8618 Training Loss: 0.1346913596259223 Test Loss: 0.3931412489149306\n",
      "Epoch: 8619 Training Loss: 0.13459820048014323 Test Loss: 0.39338037109375\n",
      "Epoch: 8620 Training Loss: 0.1344333267211914 Test Loss: 0.39287041558159724\n",
      "Epoch: 8621 Training Loss: 0.13423892550998265 Test Loss: 0.3918028157552083\n",
      "Epoch: 8622 Training Loss: 0.13400709533691407 Test Loss: 0.390123046875\n",
      "Epoch: 8623 Training Loss: 0.13374817827012803 Test Loss: 0.3882012532552083\n",
      "Epoch: 8624 Training Loss: 0.13346321360270183 Test Loss: 0.3862799750434028\n",
      "Epoch: 8625 Training Loss: 0.13317209031846788 Test Loss: 0.3843334418402778\n",
      "Epoch: 8626 Training Loss: 0.13287928856743705 Test Loss: 0.38269178602430554\n",
      "Epoch: 8627 Training Loss: 0.13260259501139324 Test Loss: 0.38160758463541666\n",
      "Epoch: 8628 Training Loss: 0.13234388648139106 Test Loss: 0.3812484809027778\n",
      "Epoch: 8629 Training Loss: 0.13214997948540583 Test Loss: 0.3817534450954861\n",
      "Epoch: 8630 Training Loss: 0.13202968088785808 Test Loss: 0.3828166232638889\n",
      "Epoch: 8631 Training Loss: 0.13196735636393228 Test Loss: 0.384146484375\n",
      "Epoch: 8632 Training Loss: 0.13194411129421657 Test Loss: 0.3854044053819444\n",
      "Epoch: 8633 Training Loss: 0.1319351806640625 Test Loss: 0.386736328125\n",
      "Epoch: 8634 Training Loss: 0.13191865370008682 Test Loss: 0.3878981119791667\n",
      "Epoch: 8635 Training Loss: 0.13191184319390192 Test Loss: 0.388740234375\n",
      "Epoch: 8636 Training Loss: 0.13189455498589409 Test Loss: 0.38933759223090275\n",
      "Epoch: 8637 Training Loss: 0.1318643595377604 Test Loss: 0.38946839735243055\n",
      "Epoch: 8638 Training Loss: 0.1318080817328559 Test Loss: 0.3895154079861111\n",
      "Epoch: 8639 Training Loss: 0.13173823208279078 Test Loss: 0.38890402560763887\n",
      "Epoch: 8640 Training Loss: 0.13165125528971355 Test Loss: 0.3884806315104167\n",
      "Epoch: 8641 Training Loss: 0.1315390133327908 Test Loss: 0.3876552734375\n",
      "Epoch: 8642 Training Loss: 0.1313719991048177 Test Loss: 0.38637353515625\n",
      "Epoch: 8643 Training Loss: 0.131121578640408 Test Loss: 0.3852512478298611\n",
      "Epoch: 8644 Training Loss: 0.13079596116807726 Test Loss: 0.384662841796875\n",
      "Epoch: 8645 Training Loss: 0.13040683746337892 Test Loss: 0.384614013671875\n",
      "Epoch: 8646 Training Loss: 0.1300067876180013 Test Loss: 0.3854526095920139\n",
      "Epoch: 8647 Training Loss: 0.12960564507378472 Test Loss: 0.3863174370659722\n",
      "Epoch: 8648 Training Loss: 0.12920101759168837 Test Loss: 0.3872057834201389\n",
      "Epoch: 8649 Training Loss: 0.12880341000027126 Test Loss: 0.3878550347222222\n",
      "Epoch: 8650 Training Loss: 0.12842533111572266 Test Loss: 0.3881056857638889\n",
      "Epoch: 8651 Training Loss: 0.12807704416910806 Test Loss: 0.38780723741319445\n",
      "Epoch: 8652 Training Loss: 0.12776524437798395 Test Loss: 0.3872358127170139\n",
      "Epoch: 8653 Training Loss: 0.12749322001139324 Test Loss: 0.3866320529513889\n",
      "Epoch: 8654 Training Loss: 0.127255736456977 Test Loss: 0.38565559895833335\n",
      "Epoch: 8655 Training Loss: 0.12706422085232205 Test Loss: 0.3848567165798611\n",
      "Epoch: 8656 Training Loss: 0.12691522894965276 Test Loss: 0.3841738552517361\n",
      "Epoch: 8657 Training Loss: 0.12680276489257813 Test Loss: 0.38364192708333333\n",
      "Epoch: 8658 Training Loss: 0.12672341749403213 Test Loss: 0.3831674262152778\n",
      "Epoch: 8659 Training Loss: 0.12666992780897351 Test Loss: 0.3830104709201389\n",
      "Epoch: 8660 Training Loss: 0.12662520260281032 Test Loss: 0.38275233289930555\n",
      "Epoch: 8661 Training Loss: 0.12658230675591362 Test Loss: 0.38266807725694446\n",
      "Epoch: 8662 Training Loss: 0.12653809695773655 Test Loss: 0.38266552734375\n",
      "Epoch: 8663 Training Loss: 0.12650587887234158 Test Loss: 0.38276505533854166\n",
      "Epoch: 8664 Training Loss: 0.1264832017686632 Test Loss: 0.3827654351128472\n",
      "Epoch: 8665 Training Loss: 0.12646800062391492 Test Loss: 0.3828544379340278\n",
      "Epoch: 8666 Training Loss: 0.12645147620307073 Test Loss: 0.3830632595486111\n",
      "Epoch: 8667 Training Loss: 0.12644484286838106 Test Loss: 0.3832026909722222\n",
      "Epoch: 8668 Training Loss: 0.1264530012342665 Test Loss: 0.38342252604166666\n",
      "Epoch: 8669 Training Loss: 0.12647120327419706 Test Loss: 0.383747314453125\n",
      "Epoch: 8670 Training Loss: 0.12649869961208768 Test Loss: 0.38400254991319444\n",
      "Epoch: 8671 Training Loss: 0.1265354504055447 Test Loss: 0.38436767578125\n",
      "Epoch: 8672 Training Loss: 0.1265748079087999 Test Loss: 0.3847046440972222\n",
      "Epoch: 8673 Training Loss: 0.1266294411553277 Test Loss: 0.38487250434027775\n",
      "Epoch: 8674 Training Loss: 0.12669511328803168 Test Loss: 0.3850303548177083\n",
      "Epoch: 8675 Training Loss: 0.12675864749484592 Test Loss: 0.38509437391493057\n",
      "Epoch: 8676 Training Loss: 0.12682628207736546 Test Loss: 0.38523472764756944\n",
      "Epoch: 8677 Training Loss: 0.12690262603759767 Test Loss: 0.38526340060763886\n",
      "Epoch: 8678 Training Loss: 0.12697533840603298 Test Loss: 0.38527750651041665\n",
      "Epoch: 8679 Training Loss: 0.1270644302368164 Test Loss: 0.38524169921875\n",
      "Epoch: 8680 Training Loss: 0.12714942677815755 Test Loss: 0.38531990559895835\n",
      "Epoch: 8681 Training Loss: 0.12723622470431858 Test Loss: 0.3854215494791667\n",
      "Epoch: 8682 Training Loss: 0.12732818433973525 Test Loss: 0.385462646484375\n",
      "Epoch: 8683 Training Loss: 0.12743440839979384 Test Loss: 0.38532161458333336\n",
      "Epoch: 8684 Training Loss: 0.12755287339952256 Test Loss: 0.3853643120659722\n",
      "Epoch: 8685 Training Loss: 0.1276750013563368 Test Loss: 0.38550374348958333\n",
      "Epoch: 8686 Training Loss: 0.12781827121310763 Test Loss: 0.3858025716145833\n",
      "Epoch: 8687 Training Loss: 0.1279876692030165 Test Loss: 0.38604329427083334\n",
      "Epoch: 8688 Training Loss: 0.1281860131157769 Test Loss: 0.3864061414930556\n",
      "Epoch: 8689 Training Loss: 0.12841659969753688 Test Loss: 0.38676030815972223\n",
      "Epoch: 8690 Training Loss: 0.128687255859375 Test Loss: 0.38718961588541667\n",
      "Epoch: 8691 Training Loss: 0.12898639424641928 Test Loss: 0.3876891276041667\n",
      "Epoch: 8692 Training Loss: 0.12932147386338977 Test Loss: 0.3882646755642361\n",
      "Epoch: 8693 Training Loss: 0.12969442579481336 Test Loss: 0.38875865342881943\n",
      "Epoch: 8694 Training Loss: 0.130112060546875 Test Loss: 0.3895195041232639\n",
      "Epoch: 8695 Training Loss: 0.13056590016682942 Test Loss: 0.39005224609375\n",
      "Epoch: 8696 Training Loss: 0.1310586174858941 Test Loss: 0.39107845052083334\n",
      "Epoch: 8697 Training Loss: 0.13155645497639973 Test Loss: 0.3917976888020833\n",
      "Epoch: 8698 Training Loss: 0.13204165225558812 Test Loss: 0.3927724609375\n",
      "Epoch: 8699 Training Loss: 0.13251439836290146 Test Loss: 0.39337131076388887\n",
      "Epoch: 8700 Training Loss: 0.13293970828586155 Test Loss: 0.39449427625868055\n",
      "Epoch: 8701 Training Loss: 0.1332813669840495 Test Loss: 0.39588905164930555\n",
      "Epoch: 8702 Training Loss: 0.13358091396755642 Test Loss: 0.39711528862847223\n",
      "Epoch: 8703 Training Loss: 0.13383738793267144 Test Loss: 0.39813384331597224\n",
      "Epoch: 8704 Training Loss: 0.13407144165039062 Test Loss: 0.39896234809027775\n",
      "Epoch: 8705 Training Loss: 0.13426164245605468 Test Loss: 0.3989951171875\n",
      "Epoch: 8706 Training Loss: 0.1343932418823242 Test Loss: 0.39825661892361114\n",
      "Epoch: 8707 Training Loss: 0.13441136847601998 Test Loss: 0.39668709309895833\n",
      "Epoch: 8708 Training Loss: 0.13431970723470052 Test Loss: 0.39519788953993057\n",
      "Epoch: 8709 Training Loss: 0.13413412221272786 Test Loss: 0.3933550347222222\n",
      "Epoch: 8710 Training Loss: 0.13385756089952258 Test Loss: 0.39182118055555554\n",
      "Epoch: 8711 Training Loss: 0.1335383741590712 Test Loss: 0.39087613932291665\n",
      "Epoch: 8712 Training Loss: 0.13322783999972873 Test Loss: 0.3909362521701389\n",
      "Epoch: 8713 Training Loss: 0.13290855407714844 Test Loss: 0.3917597384982639\n",
      "Epoch: 8714 Training Loss: 0.1326540815565321 Test Loss: 0.39277552625868056\n",
      "Epoch: 8715 Training Loss: 0.13242874315049913 Test Loss: 0.39378000217013887\n",
      "Epoch: 8716 Training Loss: 0.13215980868869356 Test Loss: 0.3942536892361111\n",
      "Epoch: 8717 Training Loss: 0.13184522501627605 Test Loss: 0.39403076171875\n",
      "Epoch: 8718 Training Loss: 0.13150050777859157 Test Loss: 0.3931013454861111\n",
      "Epoch: 8719 Training Loss: 0.1311635988023546 Test Loss: 0.39178873697916666\n",
      "Epoch: 8720 Training Loss: 0.13085385470920138 Test Loss: 0.39056092664930553\n",
      "Epoch: 8721 Training Loss: 0.13057406700981988 Test Loss: 0.3892461208767361\n",
      "Epoch: 8722 Training Loss: 0.1303164859347873 Test Loss: 0.38791357421875\n",
      "Epoch: 8723 Training Loss: 0.1300649710761176 Test Loss: 0.38697856987847223\n",
      "Epoch: 8724 Training Loss: 0.12983478122287326 Test Loss: 0.3861430935329861\n",
      "Epoch: 8725 Training Loss: 0.12964234076605902 Test Loss: 0.38554329427083334\n",
      "Epoch: 8726 Training Loss: 0.12948567623562282 Test Loss: 0.38520122612847224\n",
      "Epoch: 8727 Training Loss: 0.12934557172987196 Test Loss: 0.38519677734375\n",
      "Epoch: 8728 Training Loss: 0.12922156016031902 Test Loss: 0.38525406901041664\n",
      "Epoch: 8729 Training Loss: 0.12911194017198352 Test Loss: 0.385381103515625\n",
      "Epoch: 8730 Training Loss: 0.12900867970784505 Test Loss: 0.38538064236111114\n",
      "Epoch: 8731 Training Loss: 0.12891165754530165 Test Loss: 0.3856499837239583\n",
      "Epoch: 8732 Training Loss: 0.12881866285536026 Test Loss: 0.3857857259114583\n",
      "Epoch: 8733 Training Loss: 0.12873040432400173 Test Loss: 0.38612217881944444\n",
      "Epoch: 8734 Training Loss: 0.12866438717312284 Test Loss: 0.3863058539496528\n",
      "Epoch: 8735 Training Loss: 0.12859258524576822 Test Loss: 0.3863372395833333\n",
      "Epoch: 8736 Training Loss: 0.1285315907796224 Test Loss: 0.3859831271701389\n",
      "Epoch: 8737 Training Loss: 0.12849163818359374 Test Loss: 0.38557535807291665\n",
      "Epoch: 8738 Training Loss: 0.12848071712917752 Test Loss: 0.38499424913194447\n",
      "Epoch: 8739 Training Loss: 0.1285031509399414 Test Loss: 0.38453607855902777\n",
      "Epoch: 8740 Training Loss: 0.12855159166124133 Test Loss: 0.3838681098090278\n",
      "Epoch: 8741 Training Loss: 0.12861036682128907 Test Loss: 0.3833505045572917\n",
      "Epoch: 8742 Training Loss: 0.12867750549316406 Test Loss: 0.38296636284722224\n",
      "Epoch: 8743 Training Loss: 0.12876497141520182 Test Loss: 0.382753173828125\n",
      "Epoch: 8744 Training Loss: 0.12884794108072917 Test Loss: 0.3827604709201389\n",
      "Epoch: 8745 Training Loss: 0.12894946119520398 Test Loss: 0.3829729546440972\n",
      "Epoch: 8746 Training Loss: 0.12904738023546006 Test Loss: 0.3833567437065972\n",
      "Epoch: 8747 Training Loss: 0.12915209706624348 Test Loss: 0.38412841796875\n",
      "Epoch: 8748 Training Loss: 0.12925645955403645 Test Loss: 0.38516029188368056\n",
      "Epoch: 8749 Training Loss: 0.12936185963948568 Test Loss: 0.38648101128472223\n",
      "Epoch: 8750 Training Loss: 0.12946495395236546 Test Loss: 0.38794661458333335\n",
      "Epoch: 8751 Training Loss: 0.12958007049560546 Test Loss: 0.3893888888888889\n",
      "Epoch: 8752 Training Loss: 0.12967887030707465 Test Loss: 0.39039545355902777\n",
      "Epoch: 8753 Training Loss: 0.12979345109727647 Test Loss: 0.3909384494357639\n",
      "Epoch: 8754 Training Loss: 0.12992362043592665 Test Loss: 0.3908646918402778\n",
      "Epoch: 8755 Training Loss: 0.13005245123969184 Test Loss: 0.39017499457465277\n",
      "Epoch: 8756 Training Loss: 0.13016208733452692 Test Loss: 0.38866156684027775\n",
      "Epoch: 8757 Training Loss: 0.1302401089138455 Test Loss: 0.3866174045138889\n",
      "Epoch: 8758 Training Loss: 0.1302602072821723 Test Loss: 0.3842956814236111\n",
      "Epoch: 8759 Training Loss: 0.1301677449544271 Test Loss: 0.38219276258680557\n",
      "Epoch: 8760 Training Loss: 0.12993452962239582 Test Loss: 0.38045613606770834\n",
      "Epoch: 8761 Training Loss: 0.12959041341145833 Test Loss: 0.37945947265625\n",
      "Epoch: 8762 Training Loss: 0.12915878380669488 Test Loss: 0.37899818250868056\n",
      "Epoch: 8763 Training Loss: 0.1286633071899414 Test Loss: 0.37907801649305556\n",
      "Epoch: 8764 Training Loss: 0.12814720577663846 Test Loss: 0.37988880750868054\n",
      "Epoch: 8765 Training Loss: 0.127645873175727 Test Loss: 0.3808557942708333\n",
      "Epoch: 8766 Training Loss: 0.12719306182861329 Test Loss: 0.38175089518229166\n",
      "Epoch: 8767 Training Loss: 0.12679082573784722 Test Loss: 0.38241539171006944\n",
      "Epoch: 8768 Training Loss: 0.12644734530978732 Test Loss: 0.3830637749565972\n",
      "Epoch: 8769 Training Loss: 0.12614823320176866 Test Loss: 0.3835075412326389\n",
      "Epoch: 8770 Training Loss: 0.12588468000623915 Test Loss: 0.3836794704861111\n",
      "Epoch: 8771 Training Loss: 0.12565471903483072 Test Loss: 0.38388552517361113\n",
      "Epoch: 8772 Training Loss: 0.12545119900173612 Test Loss: 0.38384185112847224\n",
      "Epoch: 8773 Training Loss: 0.1252710393269857 Test Loss: 0.3835642632378472\n",
      "Epoch: 8774 Training Loss: 0.12511699591742623 Test Loss: 0.38335929361979165\n",
      "Epoch: 8775 Training Loss: 0.12498016526963976 Test Loss: 0.3829072808159722\n",
      "Epoch: 8776 Training Loss: 0.12487180752224393 Test Loss: 0.38253309461805557\n",
      "Epoch: 8777 Training Loss: 0.12478160349527995 Test Loss: 0.38226207139756946\n",
      "Epoch: 8778 Training Loss: 0.12471191236707899 Test Loss: 0.3818987087673611\n",
      "Epoch: 8779 Training Loss: 0.12465904151068793 Test Loss: 0.3816638454861111\n",
      "Epoch: 8780 Training Loss: 0.12461159345838758 Test Loss: 0.381483642578125\n",
      "Epoch: 8781 Training Loss: 0.12457669152153864 Test Loss: 0.3813682996961806\n",
      "Epoch: 8782 Training Loss: 0.12454508209228515 Test Loss: 0.3813642035590278\n",
      "Epoch: 8783 Training Loss: 0.1245164286295573 Test Loss: 0.3813926866319444\n",
      "Epoch: 8784 Training Loss: 0.124497681511773 Test Loss: 0.38153946940104166\n",
      "Epoch: 8785 Training Loss: 0.12448519727918837 Test Loss: 0.3817853732638889\n",
      "Epoch: 8786 Training Loss: 0.12448642137315538 Test Loss: 0.382034423828125\n",
      "Epoch: 8787 Training Loss: 0.12449192725287543 Test Loss: 0.38230094401041664\n",
      "Epoch: 8788 Training Loss: 0.12451860724555122 Test Loss: 0.38271234809027777\n",
      "Epoch: 8789 Training Loss: 0.12456183624267578 Test Loss: 0.3831289605034722\n",
      "Epoch: 8790 Training Loss: 0.12461617279052735 Test Loss: 0.38356363932291665\n",
      "Epoch: 8791 Training Loss: 0.12467547692192925 Test Loss: 0.3840275607638889\n",
      "Epoch: 8792 Training Loss: 0.12474396514892579 Test Loss: 0.3844722222222222\n",
      "Epoch: 8793 Training Loss: 0.12482194349500868 Test Loss: 0.3848500434027778\n",
      "Epoch: 8794 Training Loss: 0.12490671963161892 Test Loss: 0.38517643229166665\n",
      "Epoch: 8795 Training Loss: 0.12498472679985895 Test Loss: 0.38526798502604165\n",
      "Epoch: 8796 Training Loss: 0.12506844160291883 Test Loss: 0.38526011827256945\n",
      "Epoch: 8797 Training Loss: 0.125158940633138 Test Loss: 0.38537158203125\n",
      "Epoch: 8798 Training Loss: 0.12524644978841146 Test Loss: 0.38515760633680557\n",
      "Epoch: 8799 Training Loss: 0.1253495322333442 Test Loss: 0.38514632161458334\n",
      "Epoch: 8800 Training Loss: 0.12545629119873047 Test Loss: 0.38508219401041666\n",
      "Epoch: 8801 Training Loss: 0.12557062615288628 Test Loss: 0.3847461208767361\n",
      "Epoch: 8802 Training Loss: 0.12568589782714842 Test Loss: 0.3845569932725694\n",
      "Epoch: 8803 Training Loss: 0.12580536058213976 Test Loss: 0.38446533203125\n",
      "Epoch: 8804 Training Loss: 0.1259219250149197 Test Loss: 0.38444959852430555\n",
      "Epoch: 8805 Training Loss: 0.1260393286810981 Test Loss: 0.38468690321180554\n",
      "Epoch: 8806 Training Loss: 0.12616168128119576 Test Loss: 0.38494306098090275\n",
      "Epoch: 8807 Training Loss: 0.1262788552178277 Test Loss: 0.38537201605902777\n",
      "Epoch: 8808 Training Loss: 0.12641588338216145 Test Loss: 0.38590277777777776\n",
      "Epoch: 8809 Training Loss: 0.12656502702501085 Test Loss: 0.3866972113715278\n",
      "Epoch: 8810 Training Loss: 0.12672979058159722 Test Loss: 0.3874758572048611\n",
      "Epoch: 8811 Training Loss: 0.12691341484917534 Test Loss: 0.388415283203125\n",
      "Epoch: 8812 Training Loss: 0.12712259928385417 Test Loss: 0.389349365234375\n",
      "Epoch: 8813 Training Loss: 0.12735248650444878 Test Loss: 0.390232421875\n",
      "Epoch: 8814 Training Loss: 0.1276144290500217 Test Loss: 0.3910727810329861\n",
      "Epoch: 8815 Training Loss: 0.12791849517822265 Test Loss: 0.39184971788194445\n",
      "Epoch: 8816 Training Loss: 0.1282544869316949 Test Loss: 0.3924311252170139\n",
      "Epoch: 8817 Training Loss: 0.12859757063123914 Test Loss: 0.39288018120659723\n",
      "Epoch: 8818 Training Loss: 0.12894342973497178 Test Loss: 0.3928576117621528\n",
      "Epoch: 8819 Training Loss: 0.1292573216756185 Test Loss: 0.39249232313368054\n",
      "Epoch: 8820 Training Loss: 0.12955418565538193 Test Loss: 0.3927336154513889\n",
      "Epoch: 8821 Training Loss: 0.12981414455837673 Test Loss: 0.39272580295138887\n",
      "Epoch: 8822 Training Loss: 0.13002805921766494 Test Loss: 0.39287022569444446\n",
      "Epoch: 8823 Training Loss: 0.130202389187283 Test Loss: 0.39318739149305554\n",
      "Epoch: 8824 Training Loss: 0.13031883663601346 Test Loss: 0.39328304036458334\n",
      "Epoch: 8825 Training Loss: 0.13041334109836156 Test Loss: 0.39339035373263886\n",
      "Epoch: 8826 Training Loss: 0.13051185353597006 Test Loss: 0.39317350260416667\n",
      "Epoch: 8827 Training Loss: 0.13056949106852214 Test Loss: 0.3931888834635417\n",
      "Epoch: 8828 Training Loss: 0.13062218475341797 Test Loss: 0.3930526801215278\n",
      "Epoch: 8829 Training Loss: 0.1306754379272461 Test Loss: 0.3930976291232639\n",
      "Epoch: 8830 Training Loss: 0.13072087860107423 Test Loss: 0.39305615234375\n",
      "Epoch: 8831 Training Loss: 0.1307123565673828 Test Loss: 0.3936067979600694\n",
      "Epoch: 8832 Training Loss: 0.1306433580186632 Test Loss: 0.39492529296875\n",
      "Epoch: 8833 Training Loss: 0.13054906378851996 Test Loss: 0.3970284016927083\n",
      "Epoch: 8834 Training Loss: 0.13049298350016275 Test Loss: 0.39951117621527776\n",
      "Epoch: 8835 Training Loss: 0.13050347052680122 Test Loss: 0.4019089626736111\n",
      "Epoch: 8836 Training Loss: 0.13063714430067275 Test Loss: 0.4032249077690972\n",
      "Epoch: 8837 Training Loss: 0.13090895080566406 Test Loss: 0.40337122938368053\n",
      "Epoch: 8838 Training Loss: 0.13126741790771485 Test Loss: 0.4025177951388889\n",
      "Epoch: 8839 Training Loss: 0.13169794633653428 Test Loss: 0.4009677734375\n",
      "Epoch: 8840 Training Loss: 0.13208089870876735 Test Loss: 0.39864488389756947\n",
      "Epoch: 8841 Training Loss: 0.13236102973090277 Test Loss: 0.3959109700520833\n",
      "Epoch: 8842 Training Loss: 0.132511472913954 Test Loss: 0.39351548936631947\n",
      "Epoch: 8843 Training Loss: 0.13256581285264757 Test Loss: 0.3930959743923611\n",
      "Epoch: 8844 Training Loss: 0.13261123572455513 Test Loss: 0.39425179036458335\n",
      "Epoch: 8845 Training Loss: 0.13262355126274958 Test Loss: 0.3946506618923611\n",
      "Epoch: 8846 Training Loss: 0.1326413786146376 Test Loss: 0.3943208550347222\n",
      "Epoch: 8847 Training Loss: 0.13268802981906466 Test Loss: 0.39481095377604164\n",
      "Epoch: 8848 Training Loss: 0.1328041763305664 Test Loss: 0.39622035047743054\n",
      "Epoch: 8849 Training Loss: 0.13290900336371528 Test Loss: 0.3975993109809028\n",
      "Epoch: 8850 Training Loss: 0.13300137837727866 Test Loss: 0.39887114800347223\n",
      "Epoch: 8851 Training Loss: 0.13311395433213977 Test Loss: 0.399244140625\n",
      "Epoch: 8852 Training Loss: 0.13322039540608724 Test Loss: 0.3980041232638889\n",
      "Epoch: 8853 Training Loss: 0.13333177863226997 Test Loss: 0.39532004123263886\n",
      "Epoch: 8854 Training Loss: 0.13344066196017795 Test Loss: 0.3927150607638889\n",
      "Epoch: 8855 Training Loss: 0.13355448998345268 Test Loss: 0.3905599772135417\n",
      "Epoch: 8856 Training Loss: 0.13367182074652778 Test Loss: 0.38914027235243054\n",
      "Epoch: 8857 Training Loss: 0.13386729600694444 Test Loss: 0.3889191080729167\n",
      "Epoch: 8858 Training Loss: 0.13412472279866536 Test Loss: 0.38953504774305553\n",
      "Epoch: 8859 Training Loss: 0.13440309058295355 Test Loss: 0.39060004340277776\n",
      "Epoch: 8860 Training Loss: 0.13474042510986328 Test Loss: 0.39173752170138887\n",
      "Epoch: 8861 Training Loss: 0.13513394588894315 Test Loss: 0.3926037868923611\n",
      "Epoch: 8862 Training Loss: 0.13556524912516277 Test Loss: 0.3926934678819444\n",
      "Epoch: 8863 Training Loss: 0.13602202775743272 Test Loss: 0.39183851453993057\n",
      "Epoch: 8864 Training Loss: 0.1364548136393229 Test Loss: 0.39064900716145834\n",
      "Epoch: 8865 Training Loss: 0.13679570600721572 Test Loss: 0.38914854600694443\n",
      "Epoch: 8866 Training Loss: 0.13698104265001085 Test Loss: 0.38876353624131943\n",
      "Epoch: 8867 Training Loss: 0.1371123072306315 Test Loss: 0.39005756293402777\n",
      "Epoch: 8868 Training Loss: 0.1372115461561415 Test Loss: 0.39263774956597225\n",
      "Epoch: 8869 Training Loss: 0.13730907101101344 Test Loss: 0.3966298828125\n",
      "Epoch: 8870 Training Loss: 0.13739126502143012 Test Loss: 0.39959033203125\n",
      "Epoch: 8871 Training Loss: 0.13743781110975478 Test Loss: 0.40145863172743057\n",
      "Epoch: 8872 Training Loss: 0.1374222166273329 Test Loss: 0.4012690972222222\n",
      "Epoch: 8873 Training Loss: 0.13732120090060765 Test Loss: 0.3995978190104167\n",
      "Epoch: 8874 Training Loss: 0.1370177035861545 Test Loss: 0.39601329210069447\n",
      "Epoch: 8875 Training Loss: 0.1364038391113281 Test Loss: 0.39247960069444443\n",
      "Epoch: 8876 Training Loss: 0.13556344095865885 Test Loss: 0.3911171332465278\n",
      "Epoch: 8877 Training Loss: 0.1346010733710395 Test Loss: 0.3908749186197917\n",
      "Epoch: 8878 Training Loss: 0.13356446160210503 Test Loss: 0.39017130533854166\n",
      "Epoch: 8879 Training Loss: 0.13255840131971572 Test Loss: 0.38882918294270835\n",
      "Epoch: 8880 Training Loss: 0.13161271243625217 Test Loss: 0.3876476508246528\n",
      "Epoch: 8881 Training Loss: 0.13077218627929688 Test Loss: 0.3870586480034722\n",
      "Epoch: 8882 Training Loss: 0.1300450651380751 Test Loss: 0.386747802734375\n",
      "Epoch: 8883 Training Loss: 0.12938970947265624 Test Loss: 0.3866318901909722\n",
      "Epoch: 8884 Training Loss: 0.12879918246799046 Test Loss: 0.3865806749131944\n",
      "Epoch: 8885 Training Loss: 0.12825901794433595 Test Loss: 0.38653851996527777\n",
      "Epoch: 8886 Training Loss: 0.12776313103569878 Test Loss: 0.3863697916666667\n",
      "Epoch: 8887 Training Loss: 0.12731759134928386 Test Loss: 0.3862052408854167\n",
      "Epoch: 8888 Training Loss: 0.12691720750596788 Test Loss: 0.38597962782118056\n",
      "Epoch: 8889 Training Loss: 0.12656245761447482 Test Loss: 0.3857934299045139\n",
      "Epoch: 8890 Training Loss: 0.12624726104736328 Test Loss: 0.38558203125\n",
      "Epoch: 8891 Training Loss: 0.12596438768174914 Test Loss: 0.3852987738715278\n",
      "Epoch: 8892 Training Loss: 0.12571476406521268 Test Loss: 0.38507904730902776\n",
      "Epoch: 8893 Training Loss: 0.12549053870307075 Test Loss: 0.3846662868923611\n",
      "Epoch: 8894 Training Loss: 0.12528962961832682 Test Loss: 0.3844893120659722\n",
      "Epoch: 8895 Training Loss: 0.12511548529730904 Test Loss: 0.38427473958333336\n",
      "Epoch: 8896 Training Loss: 0.12496580166286893 Test Loss: 0.3842438693576389\n",
      "Epoch: 8897 Training Loss: 0.12483193037245009 Test Loss: 0.384055908203125\n",
      "Epoch: 8898 Training Loss: 0.12471658833821615 Test Loss: 0.3840196940104167\n",
      "Epoch: 8899 Training Loss: 0.12462622409396701 Test Loss: 0.3839423014322917\n",
      "Epoch: 8900 Training Loss: 0.12455340660942925 Test Loss: 0.3840068359375\n",
      "Epoch: 8901 Training Loss: 0.12449233839246962 Test Loss: 0.38403786892361114\n",
      "Epoch: 8902 Training Loss: 0.12445062510172526 Test Loss: 0.3841625705295139\n",
      "Epoch: 8903 Training Loss: 0.1244235831366645 Test Loss: 0.38440733506944447\n",
      "Epoch: 8904 Training Loss: 0.12440359751383463 Test Loss: 0.3846160210503472\n",
      "Epoch: 8905 Training Loss: 0.12439408535427518 Test Loss: 0.38493513997395834\n",
      "Epoch: 8906 Training Loss: 0.12440122646755643 Test Loss: 0.38520591905381946\n",
      "Epoch: 8907 Training Loss: 0.12441832394070096 Test Loss: 0.3855949978298611\n",
      "Epoch: 8908 Training Loss: 0.12443772210015192 Test Loss: 0.3859935980902778\n",
      "Epoch: 8909 Training Loss: 0.12446342553032769 Test Loss: 0.38607725694444445\n",
      "Epoch: 8910 Training Loss: 0.12449446868896484 Test Loss: 0.386394287109375\n",
      "Epoch: 8911 Training Loss: 0.12452635616726346 Test Loss: 0.3867001410590278\n",
      "Epoch: 8912 Training Loss: 0.12456336042616102 Test Loss: 0.3867663845486111\n",
      "Epoch: 8913 Training Loss: 0.12460950978597005 Test Loss: 0.38682164171006944\n",
      "Epoch: 8914 Training Loss: 0.12465718587239584 Test Loss: 0.3868224826388889\n",
      "Epoch: 8915 Training Loss: 0.12470937093098958 Test Loss: 0.38670838758680554\n",
      "Epoch: 8916 Training Loss: 0.12476334381103515 Test Loss: 0.3866349283854167\n",
      "Epoch: 8917 Training Loss: 0.1248210457695855 Test Loss: 0.38626695421006946\n",
      "Epoch: 8918 Training Loss: 0.12489059193929036 Test Loss: 0.3859308810763889\n",
      "Epoch: 8919 Training Loss: 0.1249609400431315 Test Loss: 0.3855643174913194\n",
      "Epoch: 8920 Training Loss: 0.12503735012478298 Test Loss: 0.3851081814236111\n",
      "Epoch: 8921 Training Loss: 0.1251288587782118 Test Loss: 0.3848298611111111\n",
      "Epoch: 8922 Training Loss: 0.12522804345024957 Test Loss: 0.38432275390625\n",
      "Epoch: 8923 Training Loss: 0.12533004675971138 Test Loss: 0.3839133843315972\n",
      "Epoch: 8924 Training Loss: 0.12544293467203776 Test Loss: 0.383421875\n",
      "Epoch: 8925 Training Loss: 0.12557276492648656 Test Loss: 0.38326779513888887\n",
      "Epoch: 8926 Training Loss: 0.12572598097059462 Test Loss: 0.38310975477430553\n",
      "Epoch: 8927 Training Loss: 0.12589090813530815 Test Loss: 0.3828959689670139\n",
      "Epoch: 8928 Training Loss: 0.12607031504313151 Test Loss: 0.3828025173611111\n",
      "Epoch: 8929 Training Loss: 0.1262652096218533 Test Loss: 0.38280853949652777\n",
      "Epoch: 8930 Training Loss: 0.12646842193603516 Test Loss: 0.3830223795572917\n",
      "Epoch: 8931 Training Loss: 0.12668437957763673 Test Loss: 0.38321050347222224\n",
      "Epoch: 8932 Training Loss: 0.1269177474975586 Test Loss: 0.3836426052517361\n",
      "Epoch: 8933 Training Loss: 0.12716908688015408 Test Loss: 0.38430425347222225\n",
      "Epoch: 8934 Training Loss: 0.12739968702528212 Test Loss: 0.38509033203125\n",
      "Epoch: 8935 Training Loss: 0.12764184231228298 Test Loss: 0.38623228624131944\n",
      "Epoch: 8936 Training Loss: 0.12789967346191405 Test Loss: 0.3877565375434028\n",
      "Epoch: 8937 Training Loss: 0.12815320926242404 Test Loss: 0.3893075086805556\n",
      "Epoch: 8938 Training Loss: 0.12841048516167536 Test Loss: 0.3909902072482639\n",
      "Epoch: 8939 Training Loss: 0.1286784693400065 Test Loss: 0.39258873155381946\n",
      "Epoch: 8940 Training Loss: 0.12892617713080512 Test Loss: 0.39394840494791666\n",
      "Epoch: 8941 Training Loss: 0.129172726949056 Test Loss: 0.3950431857638889\n",
      "Epoch: 8942 Training Loss: 0.12940970442030164 Test Loss: 0.3959450412326389\n",
      "Epoch: 8943 Training Loss: 0.12962911563449436 Test Loss: 0.397160888671875\n",
      "Epoch: 8944 Training Loss: 0.12984175194634331 Test Loss: 0.39794227430555557\n",
      "Epoch: 8945 Training Loss: 0.13005557674831814 Test Loss: 0.3985552300347222\n",
      "Epoch: 8946 Training Loss: 0.13024418131510418 Test Loss: 0.39900721571180553\n",
      "Epoch: 8947 Training Loss: 0.13038998158772785 Test Loss: 0.39863294813368055\n",
      "Epoch: 8948 Training Loss: 0.1304754384358724 Test Loss: 0.39763484700520835\n",
      "Epoch: 8949 Training Loss: 0.13048463524712456 Test Loss: 0.3966755099826389\n",
      "Epoch: 8950 Training Loss: 0.13045077938503688 Test Loss: 0.39519810655381943\n",
      "Epoch: 8951 Training Loss: 0.13043188985188803 Test Loss: 0.3938528374565972\n",
      "Epoch: 8952 Training Loss: 0.13039840274386935 Test Loss: 0.3921091579861111\n",
      "Epoch: 8953 Training Loss: 0.13029701063368054 Test Loss: 0.39067447916666664\n",
      "Epoch: 8954 Training Loss: 0.1301313264634874 Test Loss: 0.38905436197916665\n",
      "Epoch: 8955 Training Loss: 0.12992526075575087 Test Loss: 0.38791853841145835\n",
      "Epoch: 8956 Training Loss: 0.1296489774915907 Test Loss: 0.3868755154079861\n",
      "Epoch: 8957 Training Loss: 0.12932140435112846 Test Loss: 0.3857781304253472\n",
      "Epoch: 8958 Training Loss: 0.12897318183051215 Test Loss: 0.3844728732638889\n",
      "Epoch: 8959 Training Loss: 0.12863339063856336 Test Loss: 0.38255126953125\n",
      "Epoch: 8960 Training Loss: 0.12833943600124784 Test Loss: 0.3819152289496528\n",
      "Epoch: 8961 Training Loss: 0.1281004087660048 Test Loss: 0.38233848741319443\n",
      "Epoch: 8962 Training Loss: 0.12795325469970703 Test Loss: 0.3832589518229167\n",
      "Epoch: 8963 Training Loss: 0.12788165537516277 Test Loss: 0.3845973307291667\n",
      "Epoch: 8964 Training Loss: 0.12787767791748048 Test Loss: 0.3863505316840278\n",
      "Epoch: 8965 Training Loss: 0.1279472368028429 Test Loss: 0.38794620768229165\n",
      "Epoch: 8966 Training Loss: 0.12808405897352432 Test Loss: 0.38900439453125\n",
      "Epoch: 8967 Training Loss: 0.12827625359429254 Test Loss: 0.38910647243923613\n",
      "Epoch: 8968 Training Loss: 0.12848361460367838 Test Loss: 0.388653564453125\n",
      "Epoch: 8969 Training Loss: 0.12869783867730034 Test Loss: 0.38787879774305556\n",
      "Epoch: 8970 Training Loss: 0.12887277052137586 Test Loss: 0.3871312934027778\n",
      "Epoch: 8971 Training Loss: 0.1289584206475152 Test Loss: 0.38660378689236113\n",
      "Epoch: 8972 Training Loss: 0.1289361122979058 Test Loss: 0.38640098741319445\n",
      "Epoch: 8973 Training Loss: 0.12881292978922526 Test Loss: 0.38653868272569447\n",
      "Epoch: 8974 Training Loss: 0.12860086229112414 Test Loss: 0.386802734375\n",
      "Epoch: 8975 Training Loss: 0.1283582229614258 Test Loss: 0.3870320095486111\n",
      "Epoch: 8976 Training Loss: 0.12810267893473307 Test Loss: 0.38694411892361114\n",
      "Epoch: 8977 Training Loss: 0.12786739942762587 Test Loss: 0.3863564453125\n",
      "Epoch: 8978 Training Loss: 0.12767521413167318 Test Loss: 0.38551085069444446\n",
      "Epoch: 8979 Training Loss: 0.12754146575927736 Test Loss: 0.3846424967447917\n",
      "Epoch: 8980 Training Loss: 0.12747991349962023 Test Loss: 0.3839421929253472\n",
      "Epoch: 8981 Training Loss: 0.12747015719943577 Test Loss: 0.3833444552951389\n",
      "Epoch: 8982 Training Loss: 0.12750133005777994 Test Loss: 0.38302525499131945\n",
      "Epoch: 8983 Training Loss: 0.12754952324761284 Test Loss: 0.38306684027777776\n",
      "Epoch: 8984 Training Loss: 0.12760889519585503 Test Loss: 0.3833363715277778\n",
      "Epoch: 8985 Training Loss: 0.1276672829522027 Test Loss: 0.38383132595486114\n",
      "Epoch: 8986 Training Loss: 0.12771215226915147 Test Loss: 0.3842531195746528\n",
      "Epoch: 8987 Training Loss: 0.12776433478461371 Test Loss: 0.38472300889756944\n",
      "Epoch: 8988 Training Loss: 0.12780677541097005 Test Loss: 0.38541064453125\n",
      "Epoch: 8989 Training Loss: 0.12784940677218967 Test Loss: 0.386251953125\n",
      "Epoch: 8990 Training Loss: 0.12789983622233073 Test Loss: 0.3873242730034722\n",
      "Epoch: 8991 Training Loss: 0.127954470316569 Test Loss: 0.3882073296440972\n",
      "Epoch: 8992 Training Loss: 0.1279921171400282 Test Loss: 0.3889826388888889\n",
      "Epoch: 8993 Training Loss: 0.12802351294623482 Test Loss: 0.38959364149305553\n",
      "Epoch: 8994 Training Loss: 0.12804500155978732 Test Loss: 0.3901226128472222\n",
      "Epoch: 8995 Training Loss: 0.12806339009602866 Test Loss: 0.39005113389756946\n",
      "Epoch: 8996 Training Loss: 0.12807074483235678 Test Loss: 0.3895979817708333\n",
      "Epoch: 8997 Training Loss: 0.12806295098198786 Test Loss: 0.38866517469618056\n",
      "Epoch: 8998 Training Loss: 0.12805460442437067 Test Loss: 0.3874655490451389\n",
      "Epoch: 8999 Training Loss: 0.12805271572536892 Test Loss: 0.38614447699652776\n",
      "Epoch: 9000 Training Loss: 0.12806645202636718 Test Loss: 0.38488199869791667\n",
      "Epoch: 9001 Training Loss: 0.12808071136474608 Test Loss: 0.38358582899305554\n",
      "Epoch: 9002 Training Loss: 0.12809893459743923 Test Loss: 0.38265855577256946\n",
      "Epoch: 9003 Training Loss: 0.12810997178819444 Test Loss: 0.3823104926215278\n",
      "Epoch: 9004 Training Loss: 0.12812945726182726 Test Loss: 0.38264371744791664\n",
      "Epoch: 9005 Training Loss: 0.1281586710611979 Test Loss: 0.3834242892795139\n",
      "Epoch: 9006 Training Loss: 0.12817673916286892 Test Loss: 0.3844565700954861\n",
      "Epoch: 9007 Training Loss: 0.1281843524509006 Test Loss: 0.3856282552083333\n",
      "Epoch: 9008 Training Loss: 0.12818113115098742 Test Loss: 0.38666455078125\n",
      "Epoch: 9009 Training Loss: 0.12817742919921876 Test Loss: 0.38756412760416664\n",
      "Epoch: 9010 Training Loss: 0.12819151560465494 Test Loss: 0.38842724609375\n",
      "Epoch: 9011 Training Loss: 0.12821639082166883 Test Loss: 0.38909551323784725\n",
      "Epoch: 9012 Training Loss: 0.1282333026462131 Test Loss: 0.38975935872395834\n",
      "Epoch: 9013 Training Loss: 0.12826099819607206 Test Loss: 0.39056399197048614\n",
      "Epoch: 9014 Training Loss: 0.12830384572347006 Test Loss: 0.3914369032118056\n",
      "Epoch: 9015 Training Loss: 0.12833633083767362 Test Loss: 0.3921174045138889\n",
      "Epoch: 9016 Training Loss: 0.12838721466064454 Test Loss: 0.39251801215277776\n",
      "Epoch: 9017 Training Loss: 0.12849598524305555 Test Loss: 0.3928650173611111\n",
      "Epoch: 9018 Training Loss: 0.12862944963243272 Test Loss: 0.3932207302517361\n",
      "Epoch: 9019 Training Loss: 0.12880870819091797 Test Loss: 0.39325398763020836\n",
      "Epoch: 9020 Training Loss: 0.12904043918185765 Test Loss: 0.3932651638454861\n",
      "Epoch: 9021 Training Loss: 0.12932791985405817 Test Loss: 0.3932248806423611\n",
      "Epoch: 9022 Training Loss: 0.129661616007487 Test Loss: 0.3929163140190972\n",
      "Epoch: 9023 Training Loss: 0.13003173149956598 Test Loss: 0.39233281792534724\n",
      "Epoch: 9024 Training Loss: 0.13042692311604817 Test Loss: 0.39179833984375\n",
      "Epoch: 9025 Training Loss: 0.13087657250298393 Test Loss: 0.3909343804253472\n",
      "Epoch: 9026 Training Loss: 0.13136040581597222 Test Loss: 0.39000265842013887\n",
      "Epoch: 9027 Training Loss: 0.13185555013020833 Test Loss: 0.3887779947916667\n",
      "Epoch: 9028 Training Loss: 0.13235742780897353 Test Loss: 0.3871094563802083\n",
      "Epoch: 9029 Training Loss: 0.13282896253797744 Test Loss: 0.3860078125\n",
      "Epoch: 9030 Training Loss: 0.13319188351101346 Test Loss: 0.38554286024305556\n",
      "Epoch: 9031 Training Loss: 0.13341514248318143 Test Loss: 0.3858984646267361\n",
      "Epoch: 9032 Training Loss: 0.13350730472140843 Test Loss: 0.3868681369357639\n",
      "Epoch: 9033 Training Loss: 0.13349151187472874 Test Loss: 0.3885096571180556\n",
      "Epoch: 9034 Training Loss: 0.1334130350748698 Test Loss: 0.390448974609375\n",
      "Epoch: 9035 Training Loss: 0.13327893235948352 Test Loss: 0.39230894639756947\n",
      "Epoch: 9036 Training Loss: 0.1330510465833876 Test Loss: 0.3931830512152778\n",
      "Epoch: 9037 Training Loss: 0.13267510392930773 Test Loss: 0.39308303493923613\n",
      "Epoch: 9038 Training Loss: 0.13217279730902778 Test Loss: 0.39144656032986114\n",
      "Epoch: 9039 Training Loss: 0.13159771898057726 Test Loss: 0.38964995659722224\n",
      "Epoch: 9040 Training Loss: 0.13096460893419054 Test Loss: 0.38774720594618056\n",
      "Epoch: 9041 Training Loss: 0.13033592817518447 Test Loss: 0.3863949652777778\n",
      "Epoch: 9042 Training Loss: 0.12973995802137586 Test Loss: 0.38531146918402776\n",
      "Epoch: 9043 Training Loss: 0.12918891143798827 Test Loss: 0.3844418402777778\n",
      "Epoch: 9044 Training Loss: 0.12868516964382595 Test Loss: 0.3840491536458333\n",
      "Epoch: 9045 Training Loss: 0.1282319073147244 Test Loss: 0.3835556640625\n",
      "Epoch: 9046 Training Loss: 0.12782970513237848 Test Loss: 0.3831851399739583\n",
      "Epoch: 9047 Training Loss: 0.12746696133083768 Test Loss: 0.3831984049479167\n",
      "Epoch: 9048 Training Loss: 0.12712357076009115 Test Loss: 0.3829499240451389\n",
      "Epoch: 9049 Training Loss: 0.12680404493543837 Test Loss: 0.38288557942708334\n",
      "Epoch: 9050 Training Loss: 0.1265144746568468 Test Loss: 0.38283680555555555\n",
      "Epoch: 9051 Training Loss: 0.1262673060099284 Test Loss: 0.38322789171006943\n",
      "Epoch: 9052 Training Loss: 0.12605364481608072 Test Loss: 0.3837453342013889\n",
      "Epoch: 9053 Training Loss: 0.12586460791693793 Test Loss: 0.3842639431423611\n",
      "Epoch: 9054 Training Loss: 0.1257047627766927 Test Loss: 0.38485511610243056\n",
      "Epoch: 9055 Training Loss: 0.12556436241997612 Test Loss: 0.38525927734375\n",
      "Epoch: 9056 Training Loss: 0.12542809549967449 Test Loss: 0.38566102430555554\n",
      "Epoch: 9057 Training Loss: 0.1253187705145942 Test Loss: 0.3858564724392361\n",
      "Epoch: 9058 Training Loss: 0.1252392866346571 Test Loss: 0.38592716471354166\n",
      "Epoch: 9059 Training Loss: 0.12516245524088543 Test Loss: 0.38578355577256945\n",
      "Epoch: 9060 Training Loss: 0.12509705013699002 Test Loss: 0.3855430772569444\n",
      "Epoch: 9061 Training Loss: 0.1250479719373915 Test Loss: 0.38514756944444445\n",
      "Epoch: 9062 Training Loss: 0.12500262281629773 Test Loss: 0.38466357421875\n",
      "Epoch: 9063 Training Loss: 0.12497008090549046 Test Loss: 0.38411390516493055\n",
      "Epoch: 9064 Training Loss: 0.12494817267523871 Test Loss: 0.3833296169704861\n",
      "Epoch: 9065 Training Loss: 0.12493465677897135 Test Loss: 0.38243353949652775\n",
      "Epoch: 9066 Training Loss: 0.12493464745415582 Test Loss: 0.3816301540798611\n",
      "Epoch: 9067 Training Loss: 0.12493767547607422 Test Loss: 0.3807987467447917\n",
      "Epoch: 9068 Training Loss: 0.1249345440334744 Test Loss: 0.3800243869357639\n",
      "Epoch: 9069 Training Loss: 0.1249367438422309 Test Loss: 0.3792860785590278\n",
      "Epoch: 9070 Training Loss: 0.1249466306898329 Test Loss: 0.3787632378472222\n",
      "Epoch: 9071 Training Loss: 0.12495174238416884 Test Loss: 0.37833162434895834\n",
      "Epoch: 9072 Training Loss: 0.12496567450629341 Test Loss: 0.37808203125\n",
      "Epoch: 9073 Training Loss: 0.12498716227213541 Test Loss: 0.37816414388020836\n",
      "Epoch: 9074 Training Loss: 0.12502295430501303 Test Loss: 0.37818926323784724\n",
      "Epoch: 9075 Training Loss: 0.12507337188720702 Test Loss: 0.3784665798611111\n",
      "Epoch: 9076 Training Loss: 0.12514747789171007 Test Loss: 0.37885750325520834\n",
      "Epoch: 9077 Training Loss: 0.12524529520670574 Test Loss: 0.37941078016493057\n",
      "Epoch: 9078 Training Loss: 0.12537102508544923 Test Loss: 0.3803128526475694\n",
      "Epoch: 9079 Training Loss: 0.1255426025390625 Test Loss: 0.38138872612847224\n",
      "Epoch: 9080 Training Loss: 0.12574114905463324 Test Loss: 0.3826582302517361\n",
      "Epoch: 9081 Training Loss: 0.12597992451985676 Test Loss: 0.38405867513020836\n",
      "Epoch: 9082 Training Loss: 0.12624221801757812 Test Loss: 0.38500271267361114\n",
      "Epoch: 9083 Training Loss: 0.12650286187065973 Test Loss: 0.38566075303819447\n",
      "Epoch: 9084 Training Loss: 0.12676485104031032 Test Loss: 0.38599967447916667\n",
      "Epoch: 9085 Training Loss: 0.12701974487304687 Test Loss: 0.3862126736111111\n",
      "Epoch: 9086 Training Loss: 0.12723228539360895 Test Loss: 0.38665147569444447\n",
      "Epoch: 9087 Training Loss: 0.12741648525661892 Test Loss: 0.3874915635850694\n",
      "Epoch: 9088 Training Loss: 0.12754065873887804 Test Loss: 0.3885613064236111\n",
      "Epoch: 9089 Training Loss: 0.1276041734483507 Test Loss: 0.38915839301215277\n",
      "Epoch: 9090 Training Loss: 0.1276122072007921 Test Loss: 0.3887948947482639\n",
      "Epoch: 9091 Training Loss: 0.1275334252251519 Test Loss: 0.3883227810329861\n",
      "Epoch: 9092 Training Loss: 0.1273872545030382 Test Loss: 0.3881435546875\n",
      "Epoch: 9093 Training Loss: 0.12718619537353515 Test Loss: 0.3889750434027778\n",
      "Epoch: 9094 Training Loss: 0.1269423141479492 Test Loss: 0.389529296875\n",
      "Epoch: 9095 Training Loss: 0.1266771503024631 Test Loss: 0.38998546006944446\n",
      "Epoch: 9096 Training Loss: 0.12638838280571832 Test Loss: 0.38981700303819444\n",
      "Epoch: 9097 Training Loss: 0.12609283023410373 Test Loss: 0.38940825737847223\n",
      "Epoch: 9098 Training Loss: 0.12580653042263454 Test Loss: 0.38867070855034724\n",
      "Epoch: 9099 Training Loss: 0.12555006069607205 Test Loss: 0.38754638671875\n",
      "Epoch: 9100 Training Loss: 0.1253134045071072 Test Loss: 0.38623502604166665\n",
      "Epoch: 9101 Training Loss: 0.12510432773166233 Test Loss: 0.38492247178819444\n",
      "Epoch: 9102 Training Loss: 0.12488915252685547 Test Loss: 0.3835018988715278\n",
      "Epoch: 9103 Training Loss: 0.12465258449978299 Test Loss: 0.38216959635416664\n",
      "Epoch: 9104 Training Loss: 0.12441531287299262 Test Loss: 0.3809149848090278\n",
      "Epoch: 9105 Training Loss: 0.1241890394422743 Test Loss: 0.3798046875\n",
      "Epoch: 9106 Training Loss: 0.12396158854166667 Test Loss: 0.37876784939236113\n",
      "Epoch: 9107 Training Loss: 0.12373734622531467 Test Loss: 0.3781585286458333\n",
      "Epoch: 9108 Training Loss: 0.12353795369466146 Test Loss: 0.377778076171875\n",
      "Epoch: 9109 Training Loss: 0.1233539081149631 Test Loss: 0.37760942925347224\n",
      "Epoch: 9110 Training Loss: 0.12319464280870225 Test Loss: 0.37779562717013887\n",
      "Epoch: 9111 Training Loss: 0.12306038665771485 Test Loss: 0.37813056098090275\n",
      "Epoch: 9112 Training Loss: 0.12294545152452258 Test Loss: 0.37847010633680556\n",
      "Epoch: 9113 Training Loss: 0.12285564931233724 Test Loss: 0.37892811414930555\n",
      "Epoch: 9114 Training Loss: 0.12278822411431206 Test Loss: 0.37943196614583335\n",
      "Epoch: 9115 Training Loss: 0.12272682868109809 Test Loss: 0.37991704644097224\n",
      "Epoch: 9116 Training Loss: 0.12267487674289279 Test Loss: 0.38034423828125\n",
      "Epoch: 9117 Training Loss: 0.12263721296522352 Test Loss: 0.3805669759114583\n",
      "Epoch: 9118 Training Loss: 0.12261283196343316 Test Loss: 0.3807367078993056\n",
      "Epoch: 9119 Training Loss: 0.1226012454562717 Test Loss: 0.38075748697916667\n",
      "Epoch: 9120 Training Loss: 0.12259418148464626 Test Loss: 0.3806501736111111\n",
      "Epoch: 9121 Training Loss: 0.12259406450059679 Test Loss: 0.3805221354166667\n",
      "Epoch: 9122 Training Loss: 0.12260208299424913 Test Loss: 0.38053846571180555\n",
      "Epoch: 9123 Training Loss: 0.12261590915256076 Test Loss: 0.3803668619791667\n",
      "Epoch: 9124 Training Loss: 0.12262571038140191 Test Loss: 0.38026679144965275\n",
      "Epoch: 9125 Training Loss: 0.12263441975911459 Test Loss: 0.380097412109375\n",
      "Epoch: 9126 Training Loss: 0.12265034400092231 Test Loss: 0.38011067708333335\n",
      "Epoch: 9127 Training Loss: 0.12266588507758247 Test Loss: 0.38027159288194445\n",
      "Epoch: 9128 Training Loss: 0.1226899159749349 Test Loss: 0.3804648980034722\n",
      "Epoch: 9129 Training Loss: 0.12272032250298394 Test Loss: 0.3806603732638889\n",
      "Epoch: 9130 Training Loss: 0.12275972493489583 Test Loss: 0.3809593098958333\n",
      "Epoch: 9131 Training Loss: 0.1227986814710829 Test Loss: 0.3814616427951389\n",
      "Epoch: 9132 Training Loss: 0.12284242841932509 Test Loss: 0.3821029188368056\n",
      "Epoch: 9133 Training Loss: 0.12289226955837673 Test Loss: 0.38272786458333335\n",
      "Epoch: 9134 Training Loss: 0.12295574103461372 Test Loss: 0.38342976888020835\n",
      "Epoch: 9135 Training Loss: 0.12302953762478298 Test Loss: 0.38413650173611114\n",
      "Epoch: 9136 Training Loss: 0.12313314988878038 Test Loss: 0.3849182671440972\n",
      "Epoch: 9137 Training Loss: 0.12324932691786024 Test Loss: 0.38553702799479167\n",
      "Epoch: 9138 Training Loss: 0.12338155958387587 Test Loss: 0.38624769422743055\n",
      "Epoch: 9139 Training Loss: 0.12352031707763672 Test Loss: 0.38684882269965276\n",
      "Epoch: 9140 Training Loss: 0.12366754998101129 Test Loss: 0.38722119140625\n",
      "Epoch: 9141 Training Loss: 0.12381093427870009 Test Loss: 0.38741205512152777\n",
      "Epoch: 9142 Training Loss: 0.12395302497016059 Test Loss: 0.38764122178819443\n",
      "Epoch: 9143 Training Loss: 0.12408818138970269 Test Loss: 0.38759993489583333\n",
      "Epoch: 9144 Training Loss: 0.12422576734754774 Test Loss: 0.3873854437934028\n",
      "Epoch: 9145 Training Loss: 0.12437186940511068 Test Loss: 0.3871066623263889\n",
      "Epoch: 9146 Training Loss: 0.12452568817138672 Test Loss: 0.3864919704861111\n",
      "Epoch: 9147 Training Loss: 0.12468242645263672 Test Loss: 0.38594341362847223\n",
      "Epoch: 9148 Training Loss: 0.12482245551215278 Test Loss: 0.38565625\n",
      "Epoch: 9149 Training Loss: 0.12494778611924913 Test Loss: 0.38511219618055553\n",
      "Epoch: 9150 Training Loss: 0.12506474049886068 Test Loss: 0.384779052734375\n",
      "Epoch: 9151 Training Loss: 0.12517063988579644 Test Loss: 0.38492692057291666\n",
      "Epoch: 9152 Training Loss: 0.12527896542019315 Test Loss: 0.38534423828125\n",
      "Epoch: 9153 Training Loss: 0.12538419342041016 Test Loss: 0.38599093967013887\n",
      "Epoch: 9154 Training Loss: 0.1254894994099935 Test Loss: 0.3868576388888889\n",
      "Epoch: 9155 Training Loss: 0.12558675893147786 Test Loss: 0.3879164225260417\n",
      "Epoch: 9156 Training Loss: 0.12569216834174263 Test Loss: 0.3890528971354167\n",
      "Epoch: 9157 Training Loss: 0.12579857381184895 Test Loss: 0.38981027560763887\n",
      "Epoch: 9158 Training Loss: 0.12590901692708334 Test Loss: 0.39014138454861114\n",
      "Epoch: 9159 Training Loss: 0.1259992972479926 Test Loss: 0.3898612738715278\n",
      "Epoch: 9160 Training Loss: 0.12608165910508898 Test Loss: 0.38936341688368054\n",
      "Epoch: 9161 Training Loss: 0.126153683980306 Test Loss: 0.388777099609375\n",
      "Epoch: 9162 Training Loss: 0.1262213329739041 Test Loss: 0.38834629991319447\n",
      "Epoch: 9163 Training Loss: 0.12632088216145834 Test Loss: 0.3881430392795139\n",
      "Epoch: 9164 Training Loss: 0.12645248074001736 Test Loss: 0.38801112196180554\n",
      "Epoch: 9165 Training Loss: 0.12655079057481552 Test Loss: 0.38756005859375\n",
      "Epoch: 9166 Training Loss: 0.1266206503974067 Test Loss: 0.38752300347222224\n",
      "Epoch: 9167 Training Loss: 0.12666863674587672 Test Loss: 0.38781901041666667\n",
      "Epoch: 9168 Training Loss: 0.12670567491319445 Test Loss: 0.3885976291232639\n",
      "Epoch: 9169 Training Loss: 0.12672911071777343 Test Loss: 0.3896922200520833\n",
      "Epoch: 9170 Training Loss: 0.12675541432698567 Test Loss: 0.39079405381944443\n",
      "Epoch: 9171 Training Loss: 0.1267661810980903 Test Loss: 0.39206041124131946\n",
      "Epoch: 9172 Training Loss: 0.12678785281711155 Test Loss: 0.393248046875\n",
      "Epoch: 9173 Training Loss: 0.1268547592163086 Test Loss: 0.3945207790798611\n",
      "Epoch: 9174 Training Loss: 0.126941529168023 Test Loss: 0.39580916341145833\n",
      "Epoch: 9175 Training Loss: 0.127067748175727 Test Loss: 0.39713519965277777\n",
      "Epoch: 9176 Training Loss: 0.12721195220947265 Test Loss: 0.39929730902777777\n",
      "Epoch: 9177 Training Loss: 0.12737977684868707 Test Loss: 0.4019856228298611\n",
      "Epoch: 9178 Training Loss: 0.127536986456977 Test Loss: 0.40502783203125\n",
      "Epoch: 9179 Training Loss: 0.12765533277723523 Test Loss: 0.4069538845486111\n",
      "Epoch: 9180 Training Loss: 0.12772114139133028 Test Loss: 0.40641219075520835\n",
      "Epoch: 9181 Training Loss: 0.12774358791775173 Test Loss: 0.40342719184027775\n",
      "Epoch: 9182 Training Loss: 0.12766664547390408 Test Loss: 0.4006565483940972\n",
      "Epoch: 9183 Training Loss: 0.12749946679009333 Test Loss: 0.3989938422309028\n",
      "Epoch: 9184 Training Loss: 0.12726798756917318 Test Loss: 0.39705086263020833\n",
      "Epoch: 9185 Training Loss: 0.12709931776258682 Test Loss: 0.39402685546875\n",
      "Epoch: 9186 Training Loss: 0.1269519263373481 Test Loss: 0.390889892578125\n",
      "Epoch: 9187 Training Loss: 0.126850830078125 Test Loss: 0.38868614366319443\n",
      "Epoch: 9188 Training Loss: 0.12675313059488932 Test Loss: 0.38764442274305555\n",
      "Epoch: 9189 Training Loss: 0.12667797003851997 Test Loss: 0.38671055772569446\n",
      "Epoch: 9190 Training Loss: 0.12664313337537977 Test Loss: 0.38581749131944443\n",
      "Epoch: 9191 Training Loss: 0.1266403817070855 Test Loss: 0.38551478407118056\n",
      "Epoch: 9192 Training Loss: 0.1267066879272461 Test Loss: 0.38617713758680555\n",
      "Epoch: 9193 Training Loss: 0.12677735731336806 Test Loss: 0.3870948350694444\n",
      "Epoch: 9194 Training Loss: 0.1268556916978624 Test Loss: 0.38755685763888886\n",
      "Epoch: 9195 Training Loss: 0.12697954389784072 Test Loss: 0.3871165364583333\n",
      "Epoch: 9196 Training Loss: 0.12715451219346788 Test Loss: 0.3857690700954861\n",
      "Epoch: 9197 Training Loss: 0.12738565572102864 Test Loss: 0.3846569552951389\n",
      "Epoch: 9198 Training Loss: 0.12766795603434244 Test Loss: 0.3843685438368056\n",
      "Epoch: 9199 Training Loss: 0.12803683132595486 Test Loss: 0.3846652018229167\n",
      "Epoch: 9200 Training Loss: 0.12852527448866102 Test Loss: 0.3858442111545139\n",
      "Epoch: 9201 Training Loss: 0.12915816413031683 Test Loss: 0.3873155924479167\n",
      "Epoch: 9202 Training Loss: 0.1299240230984158 Test Loss: 0.38935780164930556\n",
      "Epoch: 9203 Training Loss: 0.1308232430352105 Test Loss: 0.39219938151041667\n",
      "Epoch: 9204 Training Loss: 0.13181224229600694 Test Loss: 0.3949663628472222\n",
      "Epoch: 9205 Training Loss: 0.13281725480821396 Test Loss: 0.39715288628472223\n",
      "Epoch: 9206 Training Loss: 0.1337021959092882 Test Loss: 0.39802902560763886\n",
      "Epoch: 9207 Training Loss: 0.13428997294108072 Test Loss: 0.3969997829861111\n",
      "Epoch: 9208 Training Loss: 0.13442964850531683 Test Loss: 0.3937956814236111\n",
      "Epoch: 9209 Training Loss: 0.13407106018066406 Test Loss: 0.38995648871527777\n",
      "Epoch: 9210 Training Loss: 0.13324667358398437 Test Loss: 0.38570545789930555\n",
      "Epoch: 9211 Training Loss: 0.13211497073703343 Test Loss: 0.3819069552951389\n",
      "Epoch: 9212 Training Loss: 0.13081726667616103 Test Loss: 0.37955995008680554\n",
      "Epoch: 9213 Training Loss: 0.12942320421006945 Test Loss: 0.3787267252604167\n",
      "Epoch: 9214 Training Loss: 0.12805153910319012 Test Loss: 0.37865999348958335\n",
      "Epoch: 9215 Training Loss: 0.12674390496148003 Test Loss: 0.37878564453125\n",
      "Epoch: 9216 Training Loss: 0.12554288143581815 Test Loss: 0.37911474609375\n",
      "Epoch: 9217 Training Loss: 0.12449262491861979 Test Loss: 0.37928276909722225\n",
      "Epoch: 9218 Training Loss: 0.1236120596991645 Test Loss: 0.3790206976996528\n",
      "Epoch: 9219 Training Loss: 0.12289458804660373 Test Loss: 0.37854839409722224\n",
      "Epoch: 9220 Training Loss: 0.12232361602783202 Test Loss: 0.37799275716145836\n",
      "Epoch: 9221 Training Loss: 0.1218799802992079 Test Loss: 0.37745751953125\n",
      "Epoch: 9222 Training Loss: 0.1215393337673611 Test Loss: 0.3769680447048611\n",
      "Epoch: 9223 Training Loss: 0.1212755618625217 Test Loss: 0.37655333116319445\n",
      "Epoch: 9224 Training Loss: 0.12107207658555773 Test Loss: 0.37632744683159725\n",
      "Epoch: 9225 Training Loss: 0.12091650729709201 Test Loss: 0.37625572374131944\n",
      "Epoch: 9226 Training Loss: 0.12080216810438368 Test Loss: 0.37632359483506944\n",
      "Epoch: 9227 Training Loss: 0.1207188983493381 Test Loss: 0.3763809678819444\n",
      "Epoch: 9228 Training Loss: 0.12065432654486762 Test Loss: 0.376556640625\n",
      "Epoch: 9229 Training Loss: 0.12060555436876085 Test Loss: 0.3768188747829861\n",
      "Epoch: 9230 Training Loss: 0.12057123396131728 Test Loss: 0.3770268825954861\n",
      "Epoch: 9231 Training Loss: 0.12054291364881728 Test Loss: 0.37723008897569443\n",
      "Epoch: 9232 Training Loss: 0.1205246802435981 Test Loss: 0.37745884874131946\n",
      "Epoch: 9233 Training Loss: 0.12052399105495877 Test Loss: 0.3776879340277778\n",
      "Epoch: 9234 Training Loss: 0.12052824147542318 Test Loss: 0.37789889865451387\n",
      "Epoch: 9235 Training Loss: 0.12053586154513889 Test Loss: 0.37805040147569446\n",
      "Epoch: 9236 Training Loss: 0.12054442087809245 Test Loss: 0.3781799045138889\n",
      "Epoch: 9237 Training Loss: 0.12055623881022136 Test Loss: 0.37827303059895834\n",
      "Epoch: 9238 Training Loss: 0.1205705829196506 Test Loss: 0.37832603624131944\n",
      "Epoch: 9239 Training Loss: 0.12059074062771268 Test Loss: 0.37847005208333334\n",
      "Epoch: 9240 Training Loss: 0.12061582353379992 Test Loss: 0.37859071180555554\n",
      "Epoch: 9241 Training Loss: 0.12065135192871093 Test Loss: 0.37865934244791666\n",
      "Epoch: 9242 Training Loss: 0.12068728637695313 Test Loss: 0.3788248697916667\n",
      "Epoch: 9243 Training Loss: 0.12074145083957248 Test Loss: 0.3790740017361111\n",
      "Epoch: 9244 Training Loss: 0.12080101945665148 Test Loss: 0.37926445855034724\n",
      "Epoch: 9245 Training Loss: 0.12085872226291233 Test Loss: 0.3795383572048611\n",
      "Epoch: 9246 Training Loss: 0.12092376285129124 Test Loss: 0.3800107421875\n",
      "Epoch: 9247 Training Loss: 0.1210051040649414 Test Loss: 0.380375244140625\n",
      "Epoch: 9248 Training Loss: 0.12109421793619791 Test Loss: 0.38076898871527776\n",
      "Epoch: 9249 Training Loss: 0.12119315846761068 Test Loss: 0.3811254611545139\n",
      "Epoch: 9250 Training Loss: 0.12129542965359158 Test Loss: 0.38153388129340277\n",
      "Epoch: 9251 Training Loss: 0.12139462025960286 Test Loss: 0.3818711480034722\n",
      "Epoch: 9252 Training Loss: 0.12150589582655165 Test Loss: 0.38228987630208333\n",
      "Epoch: 9253 Training Loss: 0.12161483256022135 Test Loss: 0.3826273871527778\n",
      "Epoch: 9254 Training Loss: 0.12173588562011718 Test Loss: 0.3829025607638889\n",
      "Epoch: 9255 Training Loss: 0.12186397043863932 Test Loss: 0.38331597222222225\n",
      "Epoch: 9256 Training Loss: 0.12200618998209635 Test Loss: 0.3837557508680556\n",
      "Epoch: 9257 Training Loss: 0.12216431935628255 Test Loss: 0.3844278428819444\n",
      "Epoch: 9258 Training Loss: 0.12234201982286241 Test Loss: 0.38497992621527777\n",
      "Epoch: 9259 Training Loss: 0.12254221852620442 Test Loss: 0.38574910481770835\n",
      "Epoch: 9260 Training Loss: 0.12275992245144314 Test Loss: 0.3866350368923611\n",
      "Epoch: 9261 Training Loss: 0.12300782352023655 Test Loss: 0.3875030381944444\n",
      "Epoch: 9262 Training Loss: 0.1232875239054362 Test Loss: 0.3884641384548611\n",
      "Epoch: 9263 Training Loss: 0.12360301208496094 Test Loss: 0.3896661783854167\n",
      "Epoch: 9264 Training Loss: 0.12396845245361328 Test Loss: 0.3908964301215278\n",
      "Epoch: 9265 Training Loss: 0.12439546457926433 Test Loss: 0.3922514105902778\n",
      "Epoch: 9266 Training Loss: 0.12488281758626302 Test Loss: 0.3937712673611111\n",
      "Epoch: 9267 Training Loss: 0.12542157830132378 Test Loss: 0.39527286783854165\n",
      "Epoch: 9268 Training Loss: 0.12599695417616102 Test Loss: 0.39659703233506943\n",
      "Epoch: 9269 Training Loss: 0.12660138024224174 Test Loss: 0.3980527886284722\n",
      "Epoch: 9270 Training Loss: 0.12725861019558377 Test Loss: 0.399917724609375\n",
      "Epoch: 9271 Training Loss: 0.12797595384385851 Test Loss: 0.4023528103298611\n",
      "Epoch: 9272 Training Loss: 0.12871458774142794 Test Loss: 0.40492355685763887\n",
      "Epoch: 9273 Training Loss: 0.12943057929144966 Test Loss: 0.4065748697916667\n",
      "Epoch: 9274 Training Loss: 0.13013272433810763 Test Loss: 0.4063469509548611\n",
      "Epoch: 9275 Training Loss: 0.13078248257107206 Test Loss: 0.4042765842013889\n",
      "Epoch: 9276 Training Loss: 0.13140124172634549 Test Loss: 0.40097412109375\n",
      "Epoch: 9277 Training Loss: 0.13205172220865885 Test Loss: 0.3994873046875\n",
      "Epoch: 9278 Training Loss: 0.13270030297173394 Test Loss: 0.40009765625\n",
      "Epoch: 9279 Training Loss: 0.1332970191107856 Test Loss: 0.40012000868055553\n",
      "Epoch: 9280 Training Loss: 0.13384637535942925 Test Loss: 0.39738826497395835\n",
      "Epoch: 9281 Training Loss: 0.1343375244140625 Test Loss: 0.39312443033854166\n",
      "Epoch: 9282 Training Loss: 0.1346762958102756 Test Loss: 0.3873549262152778\n",
      "Epoch: 9283 Training Loss: 0.1346699464586046 Test Loss: 0.38293131510416667\n",
      "Epoch: 9284 Training Loss: 0.13419647386338976 Test Loss: 0.38139225260416665\n",
      "Epoch: 9285 Training Loss: 0.133377683851454 Test Loss: 0.3822595486111111\n",
      "Epoch: 9286 Training Loss: 0.1323337673611111 Test Loss: 0.38346891276041667\n",
      "Epoch: 9287 Training Loss: 0.1311356413099501 Test Loss: 0.383721435546875\n",
      "Epoch: 9288 Training Loss: 0.12987193044026693 Test Loss: 0.382427734375\n",
      "Epoch: 9289 Training Loss: 0.12860175153944228 Test Loss: 0.3804150390625\n",
      "Epoch: 9290 Training Loss: 0.12735856119791666 Test Loss: 0.3785041775173611\n",
      "Epoch: 9291 Training Loss: 0.12619222683376735 Test Loss: 0.3767635362413194\n",
      "Epoch: 9292 Training Loss: 0.12517136552598743 Test Loss: 0.37552625868055556\n",
      "Epoch: 9293 Training Loss: 0.12433114030626086 Test Loss: 0.3749137912326389\n",
      "Epoch: 9294 Training Loss: 0.12365330420600043 Test Loss: 0.37486214192708334\n",
      "Epoch: 9295 Training Loss: 0.12313067966037326 Test Loss: 0.3753291558159722\n",
      "Epoch: 9296 Training Loss: 0.12272542995876737 Test Loss: 0.37585367838541667\n",
      "Epoch: 9297 Training Loss: 0.12241583251953125 Test Loss: 0.37652728949652775\n",
      "Epoch: 9298 Training Loss: 0.12217838372124566 Test Loss: 0.37721769205729166\n",
      "Epoch: 9299 Training Loss: 0.12200903828938803 Test Loss: 0.37768636067708333\n",
      "Epoch: 9300 Training Loss: 0.12189629787868923 Test Loss: 0.37799476453993053\n",
      "Epoch: 9301 Training Loss: 0.12182859208848741 Test Loss: 0.37820057508680555\n",
      "Epoch: 9302 Training Loss: 0.12179073079427083 Test Loss: 0.37842952473958336\n",
      "Epoch: 9303 Training Loss: 0.12177561357286242 Test Loss: 0.3784355197482639\n",
      "Epoch: 9304 Training Loss: 0.1217823986477322 Test Loss: 0.37844037543402775\n",
      "Epoch: 9305 Training Loss: 0.1218089370727539 Test Loss: 0.3783163791232639\n",
      "Epoch: 9306 Training Loss: 0.1218470204671224 Test Loss: 0.3783447808159722\n",
      "Epoch: 9307 Training Loss: 0.1218989249335395 Test Loss: 0.3782781032986111\n",
      "Epoch: 9308 Training Loss: 0.12195473141140407 Test Loss: 0.37838118489583333\n",
      "Epoch: 9309 Training Loss: 0.12201703389485677 Test Loss: 0.3783082139756944\n",
      "Epoch: 9310 Training Loss: 0.12206961059570312 Test Loss: 0.3785615234375\n",
      "Epoch: 9311 Training Loss: 0.12212580278184679 Test Loss: 0.3787367621527778\n",
      "Epoch: 9312 Training Loss: 0.1221937747531467 Test Loss: 0.3789777018229167\n",
      "Epoch: 9313 Training Loss: 0.12226603274875217 Test Loss: 0.3791931694878472\n",
      "Epoch: 9314 Training Loss: 0.12234660169813368 Test Loss: 0.37943733723958334\n",
      "Epoch: 9315 Training Loss: 0.1224325917561849 Test Loss: 0.37975748697916667\n",
      "Epoch: 9316 Training Loss: 0.12253296915690104 Test Loss: 0.37989276801215277\n",
      "Epoch: 9317 Training Loss: 0.12264410569932725 Test Loss: 0.3797384982638889\n",
      "Epoch: 9318 Training Loss: 0.12277530924479167 Test Loss: 0.37968690321180554\n",
      "Epoch: 9319 Training Loss: 0.12293155924479167 Test Loss: 0.3796305881076389\n",
      "Epoch: 9320 Training Loss: 0.12311063554551867 Test Loss: 0.3792346462673611\n",
      "Epoch: 9321 Training Loss: 0.12331119791666667 Test Loss: 0.37891883680555555\n",
      "Epoch: 9322 Training Loss: 0.12354097493489584 Test Loss: 0.37854749891493056\n",
      "Epoch: 9323 Training Loss: 0.12380038282606337 Test Loss: 0.37828325737847224\n",
      "Epoch: 9324 Training Loss: 0.12409466128879124 Test Loss: 0.3782680392795139\n",
      "Epoch: 9325 Training Loss: 0.12441374037000869 Test Loss: 0.37841536458333336\n",
      "Epoch: 9326 Training Loss: 0.12473922475179036 Test Loss: 0.3789798990885417\n",
      "Epoch: 9327 Training Loss: 0.12508220418294272 Test Loss: 0.37991764322916666\n",
      "Epoch: 9328 Training Loss: 0.1254600626627604 Test Loss: 0.3815227864583333\n",
      "Epoch: 9329 Training Loss: 0.1258600616455078 Test Loss: 0.38363300238715276\n",
      "Epoch: 9330 Training Loss: 0.12625824991861978 Test Loss: 0.38568223741319446\n",
      "Epoch: 9331 Training Loss: 0.12668113962809244 Test Loss: 0.38745713975694446\n",
      "Epoch: 9332 Training Loss: 0.12711917029486763 Test Loss: 0.38821563042534724\n",
      "Epoch: 9333 Training Loss: 0.1275433570014106 Test Loss: 0.3876219618055556\n",
      "Epoch: 9334 Training Loss: 0.12789627075195312 Test Loss: 0.3860753309461806\n",
      "Epoch: 9335 Training Loss: 0.12815154774983725 Test Loss: 0.38381670464409723\n",
      "Epoch: 9336 Training Loss: 0.12828844028049044 Test Loss: 0.3819360622829861\n",
      "Epoch: 9337 Training Loss: 0.1282834726969401 Test Loss: 0.3807916124131944\n",
      "Epoch: 9338 Training Loss: 0.12815499962700738 Test Loss: 0.38086376953125\n",
      "Epoch: 9339 Training Loss: 0.12787603081597224 Test Loss: 0.38216606987847224\n",
      "Epoch: 9340 Training Loss: 0.1274807383219401 Test Loss: 0.3843105740017361\n",
      "Epoch: 9341 Training Loss: 0.12709906429714626 Test Loss: 0.3859243977864583\n",
      "Epoch: 9342 Training Loss: 0.12676122283935548 Test Loss: 0.3865632052951389\n",
      "Epoch: 9343 Training Loss: 0.12646588389078775 Test Loss: 0.38580997721354165\n",
      "Epoch: 9344 Training Loss: 0.12618577660454644 Test Loss: 0.3843833550347222\n",
      "Epoch: 9345 Training Loss: 0.12588948991563584 Test Loss: 0.38272271050347223\n",
      "Epoch: 9346 Training Loss: 0.12556930372450087 Test Loss: 0.38112060546875\n",
      "Epoch: 9347 Training Loss: 0.12526578267415364 Test Loss: 0.3802373589409722\n",
      "Epoch: 9348 Training Loss: 0.1249812003241645 Test Loss: 0.37992626953125\n",
      "Epoch: 9349 Training Loss: 0.12475100538465712 Test Loss: 0.3798654513888889\n",
      "Epoch: 9350 Training Loss: 0.12457921346028646 Test Loss: 0.380103515625\n",
      "Epoch: 9351 Training Loss: 0.12447477383083767 Test Loss: 0.3804516330295139\n",
      "Epoch: 9352 Training Loss: 0.12443922339545356 Test Loss: 0.3806782769097222\n",
      "Epoch: 9353 Training Loss: 0.1244518076578776 Test Loss: 0.3805516493055556\n",
      "Epoch: 9354 Training Loss: 0.12453659311930339 Test Loss: 0.38052300347222223\n",
      "Epoch: 9355 Training Loss: 0.12469803025987414 Test Loss: 0.38027197265625\n",
      "Epoch: 9356 Training Loss: 0.12490115017361111 Test Loss: 0.3799289008246528\n",
      "Epoch: 9357 Training Loss: 0.1251479229397244 Test Loss: 0.37977487521701386\n",
      "Epoch: 9358 Training Loss: 0.12543318091498482 Test Loss: 0.37993223741319443\n",
      "Epoch: 9359 Training Loss: 0.12579076470269096 Test Loss: 0.38011317274305556\n",
      "Epoch: 9360 Training Loss: 0.12619128926595052 Test Loss: 0.38076898871527776\n",
      "Epoch: 9361 Training Loss: 0.12659575737847223 Test Loss: 0.3812074381510417\n",
      "Epoch: 9362 Training Loss: 0.1269770253499349 Test Loss: 0.3820749240451389\n",
      "Epoch: 9363 Training Loss: 0.12738256242540147 Test Loss: 0.3828621148003472\n",
      "Epoch: 9364 Training Loss: 0.12779151407877604 Test Loss: 0.38377750651041664\n",
      "Epoch: 9365 Training Loss: 0.12820079294840495 Test Loss: 0.3849345160590278\n",
      "Epoch: 9366 Training Loss: 0.12856727261013454 Test Loss: 0.38610948350694446\n",
      "Epoch: 9367 Training Loss: 0.12885110982259115 Test Loss: 0.38747184244791666\n",
      "Epoch: 9368 Training Loss: 0.1290255889892578 Test Loss: 0.38852254231770833\n",
      "Epoch: 9369 Training Loss: 0.12911228942871095 Test Loss: 0.38902555338541667\n",
      "Epoch: 9370 Training Loss: 0.1291485332912869 Test Loss: 0.3884428982204861\n",
      "Epoch: 9371 Training Loss: 0.12910094451904297 Test Loss: 0.38730517578125\n",
      "Epoch: 9372 Training Loss: 0.12895371246337892 Test Loss: 0.38643937174479165\n",
      "Epoch: 9373 Training Loss: 0.1287197265625 Test Loss: 0.3862311740451389\n",
      "Epoch: 9374 Training Loss: 0.1283881115383572 Test Loss: 0.38701752387152777\n",
      "Epoch: 9375 Training Loss: 0.12796016184488931 Test Loss: 0.38850420464409724\n",
      "Epoch: 9376 Training Loss: 0.1274551535712348 Test Loss: 0.38995865885416664\n",
      "Epoch: 9377 Training Loss: 0.1269228812323676 Test Loss: 0.39157508680555553\n",
      "Epoch: 9378 Training Loss: 0.12642034912109376 Test Loss: 0.39304280598958335\n",
      "Epoch: 9379 Training Loss: 0.12598128594292535 Test Loss: 0.39356868489583335\n",
      "Epoch: 9380 Training Loss: 0.1256007554796007 Test Loss: 0.393713134765625\n",
      "Epoch: 9381 Training Loss: 0.12529661729600694 Test Loss: 0.39374818250868054\n",
      "Epoch: 9382 Training Loss: 0.1250399627685547 Test Loss: 0.39323697916666667\n",
      "Epoch: 9383 Training Loss: 0.12481869761149089 Test Loss: 0.3929701605902778\n",
      "Epoch: 9384 Training Loss: 0.12464564090304904 Test Loss: 0.39254912651909724\n",
      "Epoch: 9385 Training Loss: 0.12449667443169488 Test Loss: 0.3924093967013889\n",
      "Epoch: 9386 Training Loss: 0.1243925543891059 Test Loss: 0.39214276801215275\n",
      "Epoch: 9387 Training Loss: 0.12432862430148654 Test Loss: 0.3919493272569444\n",
      "Epoch: 9388 Training Loss: 0.12429523298475477 Test Loss: 0.39194295247395833\n",
      "Epoch: 9389 Training Loss: 0.12430406358506944 Test Loss: 0.39191954210069446\n",
      "Epoch: 9390 Training Loss: 0.12434654235839844 Test Loss: 0.39205921766493057\n",
      "Epoch: 9391 Training Loss: 0.12442959509955512 Test Loss: 0.39195149739583335\n",
      "Epoch: 9392 Training Loss: 0.12456672837999132 Test Loss: 0.39173057725694443\n",
      "Epoch: 9393 Training Loss: 0.12475657145182292 Test Loss: 0.39109928385416665\n",
      "Epoch: 9394 Training Loss: 0.12502127329508464 Test Loss: 0.390267822265625\n",
      "Epoch: 9395 Training Loss: 0.1253639890882704 Test Loss: 0.38917130533854166\n",
      "Epoch: 9396 Training Loss: 0.1257967529296875 Test Loss: 0.38767464192708334\n",
      "Epoch: 9397 Training Loss: 0.12631564500596787 Test Loss: 0.38601307508680555\n",
      "Epoch: 9398 Training Loss: 0.1269407458835178 Test Loss: 0.38423008897569444\n",
      "Epoch: 9399 Training Loss: 0.1276759982638889 Test Loss: 0.38246869574652775\n",
      "Epoch: 9400 Training Loss: 0.12856004503038193 Test Loss: 0.38148909505208334\n",
      "Epoch: 9401 Training Loss: 0.12957564968532986 Test Loss: 0.3823546820746528\n",
      "Epoch: 9402 Training Loss: 0.1306947775946723 Test Loss: 0.3853896484375\n",
      "Epoch: 9403 Training Loss: 0.13180565643310546 Test Loss: 0.38948809136284723\n",
      "Epoch: 9404 Training Loss: 0.13278439076741536 Test Loss: 0.391846923828125\n",
      "Epoch: 9405 Training Loss: 0.13356107584635415 Test Loss: 0.39285753038194443\n",
      "Epoch: 9406 Training Loss: 0.13402244652642145 Test Loss: 0.3928982476128472\n",
      "Epoch: 9407 Training Loss: 0.13406703609890408 Test Loss: 0.3928710394965278\n",
      "Epoch: 9408 Training Loss: 0.1336451449924045 Test Loss: 0.39233829752604166\n",
      "Epoch: 9409 Training Loss: 0.1328874766031901 Test Loss: 0.39085734049479165\n",
      "Epoch: 9410 Training Loss: 0.13189326985677083 Test Loss: 0.38893294270833334\n",
      "Epoch: 9411 Training Loss: 0.13085909779866536 Test Loss: 0.38711436631944446\n",
      "Epoch: 9412 Training Loss: 0.12990607198079426 Test Loss: 0.3854058973524306\n",
      "Epoch: 9413 Training Loss: 0.12909264204237197 Test Loss: 0.3840912543402778\n",
      "Epoch: 9414 Training Loss: 0.12846444108751084 Test Loss: 0.3835014919704861\n",
      "Epoch: 9415 Training Loss: 0.12796538628472223 Test Loss: 0.38351207139756943\n",
      "Epoch: 9416 Training Loss: 0.12756723022460936 Test Loss: 0.38395952690972224\n",
      "Epoch: 9417 Training Loss: 0.1272662582397461 Test Loss: 0.3847415364583333\n",
      "Epoch: 9418 Training Loss: 0.12702749633789062 Test Loss: 0.3855911458333333\n",
      "Epoch: 9419 Training Loss: 0.12681571112738715 Test Loss: 0.3861094563802083\n",
      "Epoch: 9420 Training Loss: 0.12662705993652343 Test Loss: 0.38635731336805557\n",
      "Epoch: 9421 Training Loss: 0.12646643490261503 Test Loss: 0.3861548665364583\n",
      "Epoch: 9422 Training Loss: 0.12631033494737412 Test Loss: 0.3860647243923611\n",
      "Epoch: 9423 Training Loss: 0.12614580790201824 Test Loss: 0.38590025499131947\n",
      "Epoch: 9424 Training Loss: 0.12598463694254558 Test Loss: 0.385419921875\n",
      "Epoch: 9425 Training Loss: 0.12579440307617187 Test Loss: 0.38518332248263887\n",
      "Epoch: 9426 Training Loss: 0.12562215423583983 Test Loss: 0.385346435546875\n",
      "Epoch: 9427 Training Loss: 0.1254424794514974 Test Loss: 0.3854208713107639\n",
      "Epoch: 9428 Training Loss: 0.12526067691379122 Test Loss: 0.3855834689670139\n",
      "Epoch: 9429 Training Loss: 0.12508322397867838 Test Loss: 0.385578125\n",
      "Epoch: 9430 Training Loss: 0.12492620256212023 Test Loss: 0.38551378038194445\n",
      "Epoch: 9431 Training Loss: 0.124791748046875 Test Loss: 0.3853315972222222\n",
      "Epoch: 9432 Training Loss: 0.12466227298312717 Test Loss: 0.38520616319444445\n",
      "Epoch: 9433 Training Loss: 0.12456343926323785 Test Loss: 0.3850128580729167\n",
      "Epoch: 9434 Training Loss: 0.12446539476182726 Test Loss: 0.3849140625\n",
      "Epoch: 9435 Training Loss: 0.12437482452392579 Test Loss: 0.3846044108072917\n",
      "Epoch: 9436 Training Loss: 0.12431482442220051 Test Loss: 0.3845275065104167\n",
      "Epoch: 9437 Training Loss: 0.12426763916015625 Test Loss: 0.38439708116319443\n",
      "Epoch: 9438 Training Loss: 0.12423313225640191 Test Loss: 0.3841689995659722\n",
      "Epoch: 9439 Training Loss: 0.12420394558376736 Test Loss: 0.3841750759548611\n",
      "Epoch: 9440 Training Loss: 0.12420074547661675 Test Loss: 0.3843336859809028\n",
      "Epoch: 9441 Training Loss: 0.12422301991780599 Test Loss: 0.38475037977430554\n",
      "Epoch: 9442 Training Loss: 0.12426754421657986 Test Loss: 0.3854963107638889\n",
      "Epoch: 9443 Training Loss: 0.12434673902723524 Test Loss: 0.38658631727430554\n",
      "Epoch: 9444 Training Loss: 0.12446143680148654 Test Loss: 0.38787855360243056\n",
      "Epoch: 9445 Training Loss: 0.12460106828477648 Test Loss: 0.38894200303819443\n",
      "Epoch: 9446 Training Loss: 0.12478099992540148 Test Loss: 0.39021912977430556\n",
      "Epoch: 9447 Training Loss: 0.12498821258544922 Test Loss: 0.3912526041666667\n",
      "Epoch: 9448 Training Loss: 0.12523061794704862 Test Loss: 0.39216126844618054\n",
      "Epoch: 9449 Training Loss: 0.12550807105170356 Test Loss: 0.3928612738715278\n",
      "Epoch: 9450 Training Loss: 0.12582607523600262 Test Loss: 0.39347715928819443\n",
      "Epoch: 9451 Training Loss: 0.12619599405924478 Test Loss: 0.39403450520833333\n",
      "Epoch: 9452 Training Loss: 0.12662366655137805 Test Loss: 0.3942860514322917\n",
      "Epoch: 9453 Training Loss: 0.12707251569959852 Test Loss: 0.3943400065104167\n",
      "Epoch: 9454 Training Loss: 0.1275469241672092 Test Loss: 0.39414591471354166\n",
      "Epoch: 9455 Training Loss: 0.1280426762898763 Test Loss: 0.3934528537326389\n",
      "Epoch: 9456 Training Loss: 0.1285657212999132 Test Loss: 0.39268671332465277\n",
      "Epoch: 9457 Training Loss: 0.12910757615831164 Test Loss: 0.3915612250434028\n",
      "Epoch: 9458 Training Loss: 0.1296283704969618 Test Loss: 0.39069742838541666\n",
      "Epoch: 9459 Training Loss: 0.13012843492296006 Test Loss: 0.39014271375868054\n",
      "Epoch: 9460 Training Loss: 0.13062172698974608 Test Loss: 0.39015171983506947\n",
      "Epoch: 9461 Training Loss: 0.131113286336263 Test Loss: 0.39017719184027777\n",
      "Epoch: 9462 Training Loss: 0.13163169013129342 Test Loss: 0.3900605197482639\n",
      "Epoch: 9463 Training Loss: 0.13215052117241755 Test Loss: 0.3896177029079861\n",
      "Epoch: 9464 Training Loss: 0.13265115780300565 Test Loss: 0.38892610677083334\n",
      "Epoch: 9465 Training Loss: 0.13313798268636068 Test Loss: 0.38747233072916665\n",
      "Epoch: 9466 Training Loss: 0.13365616607666014 Test Loss: 0.3855673828125\n",
      "Epoch: 9467 Training Loss: 0.13412010277642145 Test Loss: 0.38419921875\n",
      "Epoch: 9468 Training Loss: 0.13445610809326172 Test Loss: 0.3831798502604167\n",
      "Epoch: 9469 Training Loss: 0.13463590070936415 Test Loss: 0.3838569878472222\n",
      "Epoch: 9470 Training Loss: 0.13470000457763673 Test Loss: 0.3856197645399306\n",
      "Epoch: 9471 Training Loss: 0.13454925791422526 Test Loss: 0.3878467068142361\n",
      "Epoch: 9472 Training Loss: 0.1341802054511176 Test Loss: 0.3897595486111111\n",
      "Epoch: 9473 Training Loss: 0.13359828270806207 Test Loss: 0.39072271050347224\n",
      "Epoch: 9474 Training Loss: 0.1327956263224284 Test Loss: 0.39031504991319443\n",
      "Epoch: 9475 Training Loss: 0.13181568993462456 Test Loss: 0.38818223741319446\n",
      "Epoch: 9476 Training Loss: 0.13066192711724176 Test Loss: 0.3857345920138889\n",
      "Epoch: 9477 Training Loss: 0.12941950310601127 Test Loss: 0.3835607638888889\n",
      "Epoch: 9478 Training Loss: 0.1281780971950955 Test Loss: 0.3821522352430556\n",
      "Epoch: 9479 Training Loss: 0.12703343370225695 Test Loss: 0.3810499945746528\n",
      "Epoch: 9480 Training Loss: 0.12602225833468966 Test Loss: 0.38022002495659724\n",
      "Epoch: 9481 Training Loss: 0.12516217634412977 Test Loss: 0.37962263997395834\n",
      "Epoch: 9482 Training Loss: 0.12441423458523221 Test Loss: 0.37919742838541665\n",
      "Epoch: 9483 Training Loss: 0.12376188574896918 Test Loss: 0.37869140625\n",
      "Epoch: 9484 Training Loss: 0.12317790985107421 Test Loss: 0.3781715766059028\n",
      "Epoch: 9485 Training Loss: 0.12264358096652561 Test Loss: 0.37755571831597223\n",
      "Epoch: 9486 Training Loss: 0.12216529083251954 Test Loss: 0.3768489583333333\n",
      "Epoch: 9487 Training Loss: 0.12175302547878689 Test Loss: 0.3762145182291667\n",
      "Epoch: 9488 Training Loss: 0.12138258361816406 Test Loss: 0.3757741427951389\n",
      "Epoch: 9489 Training Loss: 0.12106143866644965 Test Loss: 0.3752912055121528\n",
      "Epoch: 9490 Training Loss: 0.12078395758734808 Test Loss: 0.3749974500868056\n",
      "Epoch: 9491 Training Loss: 0.12054450903998482 Test Loss: 0.37468104383680556\n",
      "Epoch: 9492 Training Loss: 0.12033855946858724 Test Loss: 0.3745652669270833\n",
      "Epoch: 9493 Training Loss: 0.12016549936930339 Test Loss: 0.3744867892795139\n",
      "Epoch: 9494 Training Loss: 0.12002018313937717 Test Loss: 0.3746129014756944\n",
      "Epoch: 9495 Training Loss: 0.11989382765028211 Test Loss: 0.37475108506944443\n",
      "Epoch: 9496 Training Loss: 0.11978648969862196 Test Loss: 0.37501627604166665\n",
      "Epoch: 9497 Training Loss: 0.11970071834988064 Test Loss: 0.3752736545138889\n",
      "Epoch: 9498 Training Loss: 0.11963038889567057 Test Loss: 0.37555970594618054\n",
      "Epoch: 9499 Training Loss: 0.11957564375135633 Test Loss: 0.37589805772569446\n",
      "Epoch: 9500 Training Loss: 0.11953544108072917 Test Loss: 0.3760809733072917\n",
      "Epoch: 9501 Training Loss: 0.1195076166788737 Test Loss: 0.3762631564670139\n",
      "Epoch: 9502 Training Loss: 0.11948920440673828 Test Loss: 0.3764057345920139\n",
      "Epoch: 9503 Training Loss: 0.11947937181260851 Test Loss: 0.3763875054253472\n",
      "Epoch: 9504 Training Loss: 0.11947618018256294 Test Loss: 0.3765155978732639\n",
      "Epoch: 9505 Training Loss: 0.11948175472683377 Test Loss: 0.3764698350694444\n",
      "Epoch: 9506 Training Loss: 0.11949637095133464 Test Loss: 0.3764520670572917\n",
      "Epoch: 9507 Training Loss: 0.11951697625054253 Test Loss: 0.3763047417534722\n",
      "Epoch: 9508 Training Loss: 0.11954183027479384 Test Loss: 0.37621864149305556\n",
      "Epoch: 9509 Training Loss: 0.11957006751166449 Test Loss: 0.3760400390625\n",
      "Epoch: 9510 Training Loss: 0.11960337151421441 Test Loss: 0.3758484157986111\n",
      "Epoch: 9511 Training Loss: 0.11963631947835286 Test Loss: 0.37576904296875\n",
      "Epoch: 9512 Training Loss: 0.11967452070448134 Test Loss: 0.37565725368923614\n",
      "Epoch: 9513 Training Loss: 0.11971769883897569 Test Loss: 0.3755744900173611\n",
      "Epoch: 9514 Training Loss: 0.1197745590209961 Test Loss: 0.3754496256510417\n",
      "Epoch: 9515 Training Loss: 0.11983845096164279 Test Loss: 0.375504150390625\n",
      "Epoch: 9516 Training Loss: 0.11991026730007595 Test Loss: 0.3756967230902778\n",
      "Epoch: 9517 Training Loss: 0.11998169877794054 Test Loss: 0.3759315863715278\n",
      "Epoch: 9518 Training Loss: 0.12005514526367188 Test Loss: 0.3762400987413194\n",
      "Epoch: 9519 Training Loss: 0.12013200208875868 Test Loss: 0.3766738823784722\n",
      "Epoch: 9520 Training Loss: 0.12020168643527561 Test Loss: 0.3770529513888889\n",
      "Epoch: 9521 Training Loss: 0.12027730984157986 Test Loss: 0.3775332573784722\n",
      "Epoch: 9522 Training Loss: 0.12035327233208551 Test Loss: 0.3780264214409722\n",
      "Epoch: 9523 Training Loss: 0.12043367089165581 Test Loss: 0.37855601671006944\n",
      "Epoch: 9524 Training Loss: 0.12051993476019965 Test Loss: 0.3789936252170139\n",
      "Epoch: 9525 Training Loss: 0.12060496520996093 Test Loss: 0.37945339626736113\n",
      "Epoch: 9526 Training Loss: 0.12069180467393663 Test Loss: 0.3798572319878472\n",
      "Epoch: 9527 Training Loss: 0.12079434712727864 Test Loss: 0.38036029730902776\n",
      "Epoch: 9528 Training Loss: 0.12090233018663195 Test Loss: 0.38070054796006947\n",
      "Epoch: 9529 Training Loss: 0.12102371724446614 Test Loss: 0.38101744249131947\n",
      "Epoch: 9530 Training Loss: 0.12115163421630859 Test Loss: 0.3811670464409722\n",
      "Epoch: 9531 Training Loss: 0.12128906080457899 Test Loss: 0.38134857855902776\n",
      "Epoch: 9532 Training Loss: 0.12144820488823785 Test Loss: 0.3814377170138889\n",
      "Epoch: 9533 Training Loss: 0.12162603505452474 Test Loss: 0.3814247504340278\n",
      "Epoch: 9534 Training Loss: 0.12182283782958984 Test Loss: 0.38135777452256947\n",
      "Epoch: 9535 Training Loss: 0.12203728146023221 Test Loss: 0.38142222764756945\n",
      "Epoch: 9536 Training Loss: 0.12228773074679905 Test Loss: 0.38143001302083335\n",
      "Epoch: 9537 Training Loss: 0.1225649193657769 Test Loss: 0.381451171875\n",
      "Epoch: 9538 Training Loss: 0.12285555267333985 Test Loss: 0.3816956651475694\n",
      "Epoch: 9539 Training Loss: 0.12315371280246311 Test Loss: 0.38179478624131946\n",
      "Epoch: 9540 Training Loss: 0.12345595465766059 Test Loss: 0.3820498589409722\n",
      "Epoch: 9541 Training Loss: 0.12374312252468533 Test Loss: 0.38196685112847223\n",
      "Epoch: 9542 Training Loss: 0.12403923543294271 Test Loss: 0.3821000705295139\n",
      "Epoch: 9543 Training Loss: 0.12438283199734158 Test Loss: 0.3820806749131944\n",
      "Epoch: 9544 Training Loss: 0.12473411305745442 Test Loss: 0.3820234646267361\n",
      "Epoch: 9545 Training Loss: 0.1251101303100586 Test Loss: 0.3823916286892361\n",
      "Epoch: 9546 Training Loss: 0.12548473019070094 Test Loss: 0.38237999131944445\n",
      "Epoch: 9547 Training Loss: 0.1258332307603624 Test Loss: 0.3825963270399306\n",
      "Epoch: 9548 Training Loss: 0.1261850357055664 Test Loss: 0.38230517578125\n",
      "Epoch: 9549 Training Loss: 0.12653702375623915 Test Loss: 0.38174354383680553\n",
      "Epoch: 9550 Training Loss: 0.1268681377834744 Test Loss: 0.38173942057291665\n",
      "Epoch: 9551 Training Loss: 0.12717399173312716 Test Loss: 0.3818584255642361\n",
      "Epoch: 9552 Training Loss: 0.12743937344021267 Test Loss: 0.38229253472222224\n",
      "Epoch: 9553 Training Loss: 0.12767071702745225 Test Loss: 0.3831272515190972\n",
      "Epoch: 9554 Training Loss: 0.12789499070909288 Test Loss: 0.38411526150173614\n",
      "Epoch: 9555 Training Loss: 0.12810113186306424 Test Loss: 0.38528070746527776\n",
      "Epoch: 9556 Training Loss: 0.12830454508463543 Test Loss: 0.38628325737847224\n",
      "Epoch: 9557 Training Loss: 0.12846057467990452 Test Loss: 0.3868154296875\n",
      "Epoch: 9558 Training Loss: 0.12852582634819879 Test Loss: 0.38673958333333336\n",
      "Epoch: 9559 Training Loss: 0.12842591264512804 Test Loss: 0.38619580078125\n",
      "Epoch: 9560 Training Loss: 0.12805191548665365 Test Loss: 0.38671587456597223\n",
      "Epoch: 9561 Training Loss: 0.1274856465657552 Test Loss: 0.3877509223090278\n",
      "Epoch: 9562 Training Loss: 0.1267935519748264 Test Loss: 0.3889294704861111\n",
      "Epoch: 9563 Training Loss: 0.1260851050482856 Test Loss: 0.39021283637152776\n",
      "Epoch: 9564 Training Loss: 0.12539415486653646 Test Loss: 0.3899733072916667\n",
      "Epoch: 9565 Training Loss: 0.124732788933648 Test Loss: 0.3879350043402778\n",
      "Epoch: 9566 Training Loss: 0.12412201351589627 Test Loss: 0.3848540852864583\n",
      "Epoch: 9567 Training Loss: 0.12358665720621745 Test Loss: 0.38264637586805555\n",
      "Epoch: 9568 Training Loss: 0.12320437537299263 Test Loss: 0.38132180447048614\n",
      "Epoch: 9569 Training Loss: 0.12294041866726345 Test Loss: 0.38136064995659724\n",
      "Epoch: 9570 Training Loss: 0.12275484975179037 Test Loss: 0.38280240885416666\n",
      "Epoch: 9571 Training Loss: 0.12261051177978516 Test Loss: 0.3853813205295139\n",
      "Epoch: 9572 Training Loss: 0.122480224609375 Test Loss: 0.3875314670138889\n",
      "Epoch: 9573 Training Loss: 0.12230569034152561 Test Loss: 0.387224609375\n",
      "Epoch: 9574 Training Loss: 0.12209578620062934 Test Loss: 0.38496761067708335\n",
      "Epoch: 9575 Training Loss: 0.12184868367513021 Test Loss: 0.38227696397569444\n",
      "Epoch: 9576 Training Loss: 0.12158457353379991 Test Loss: 0.37978938802083334\n",
      "Epoch: 9577 Training Loss: 0.12133310106065538 Test Loss: 0.37787928602430554\n",
      "Epoch: 9578 Training Loss: 0.121116333855523 Test Loss: 0.37693128797743053\n",
      "Epoch: 9579 Training Loss: 0.12096187252468533 Test Loss: 0.37649365234375\n",
      "Epoch: 9580 Training Loss: 0.1208630362616645 Test Loss: 0.37639973958333334\n",
      "Epoch: 9581 Training Loss: 0.12080311499701606 Test Loss: 0.37634016927083336\n",
      "Epoch: 9582 Training Loss: 0.12076418813069661 Test Loss: 0.3765304633246528\n",
      "Epoch: 9583 Training Loss: 0.12072750430636936 Test Loss: 0.3768557400173611\n",
      "Epoch: 9584 Training Loss: 0.12069539557562935 Test Loss: 0.37740443250868055\n",
      "Epoch: 9585 Training Loss: 0.12066521369086372 Test Loss: 0.3779670681423611\n",
      "Epoch: 9586 Training Loss: 0.12064176771375867 Test Loss: 0.3785847710503472\n",
      "Epoch: 9587 Training Loss: 0.1206351055569119 Test Loss: 0.3794045681423611\n",
      "Epoch: 9588 Training Loss: 0.12064611392550999 Test Loss: 0.38001953125\n",
      "Epoch: 9589 Training Loss: 0.12067233784993489 Test Loss: 0.38086168077256943\n",
      "Epoch: 9590 Training Loss: 0.12072579108344184 Test Loss: 0.3819045952690972\n",
      "Epoch: 9591 Training Loss: 0.12080962965223524 Test Loss: 0.38273280164930557\n",
      "Epoch: 9592 Training Loss: 0.12092245313856337 Test Loss: 0.3835489095052083\n",
      "Epoch: 9593 Training Loss: 0.12105528852674696 Test Loss: 0.38401727973090277\n",
      "Epoch: 9594 Training Loss: 0.12121835581461589 Test Loss: 0.3844109157986111\n",
      "Epoch: 9595 Training Loss: 0.12142014736599392 Test Loss: 0.3848385959201389\n",
      "Epoch: 9596 Training Loss: 0.12164763556586372 Test Loss: 0.3848759765625\n",
      "Epoch: 9597 Training Loss: 0.12187607828776041 Test Loss: 0.3847354600694444\n",
      "Epoch: 9598 Training Loss: 0.12209327952067058 Test Loss: 0.3843622504340278\n",
      "Epoch: 9599 Training Loss: 0.12229451243082683 Test Loss: 0.3839104817708333\n",
      "Epoch: 9600 Training Loss: 0.12247626241048178 Test Loss: 0.3835521918402778\n",
      "Epoch: 9601 Training Loss: 0.1226426247490777 Test Loss: 0.3832800021701389\n",
      "Epoch: 9602 Training Loss: 0.12278900400797527 Test Loss: 0.38339808485243054\n",
      "Epoch: 9603 Training Loss: 0.1229232440524631 Test Loss: 0.38372498914930553\n",
      "Epoch: 9604 Training Loss: 0.12303240627712674 Test Loss: 0.38419449869791666\n",
      "Epoch: 9605 Training Loss: 0.12312689124213325 Test Loss: 0.3846549750434028\n",
      "Epoch: 9606 Training Loss: 0.1232270041571723 Test Loss: 0.3849172634548611\n",
      "Epoch: 9607 Training Loss: 0.12332354736328124 Test Loss: 0.38491650390625\n",
      "Epoch: 9608 Training Loss: 0.12342927042643229 Test Loss: 0.38435785590277777\n",
      "Epoch: 9609 Training Loss: 0.12354767608642578 Test Loss: 0.38371443684895834\n",
      "Epoch: 9610 Training Loss: 0.12366431935628255 Test Loss: 0.38283875868055556\n",
      "Epoch: 9611 Training Loss: 0.12377550337049696 Test Loss: 0.38210582139756943\n",
      "Epoch: 9612 Training Loss: 0.1238650402492947 Test Loss: 0.3810461697048611\n",
      "Epoch: 9613 Training Loss: 0.12390521833631728 Test Loss: 0.3804145779079861\n",
      "Epoch: 9614 Training Loss: 0.12389782884385851 Test Loss: 0.3799215494791667\n",
      "Epoch: 9615 Training Loss: 0.12385047319200304 Test Loss: 0.3795703667534722\n",
      "Epoch: 9616 Training Loss: 0.12380759514702691 Test Loss: 0.37939854600694445\n",
      "Epoch: 9617 Training Loss: 0.12377707333034939 Test Loss: 0.37926809353298613\n",
      "Epoch: 9618 Training Loss: 0.12382086520724826 Test Loss: 0.37942287868923613\n",
      "Epoch: 9619 Training Loss: 0.12390107133653429 Test Loss: 0.37925786675347223\n",
      "Epoch: 9620 Training Loss: 0.12400012800428603 Test Loss: 0.3791896701388889\n",
      "Epoch: 9621 Training Loss: 0.12411345079210069 Test Loss: 0.3792510308159722\n",
      "Epoch: 9622 Training Loss: 0.12422184414333767 Test Loss: 0.37915964084201387\n",
      "Epoch: 9623 Training Loss: 0.12432811652289497 Test Loss: 0.37926649305555554\n",
      "Epoch: 9624 Training Loss: 0.12443162027994792 Test Loss: 0.3792906629774306\n",
      "Epoch: 9625 Training Loss: 0.12453589969211154 Test Loss: 0.3795389811197917\n",
      "Epoch: 9626 Training Loss: 0.12464508480495877 Test Loss: 0.38007777235243057\n",
      "Epoch: 9627 Training Loss: 0.12479918755425347 Test Loss: 0.38076047092013887\n",
      "Epoch: 9628 Training Loss: 0.125020502726237 Test Loss: 0.3816693793402778\n",
      "Epoch: 9629 Training Loss: 0.12530458068847655 Test Loss: 0.3828205837673611\n",
      "Epoch: 9630 Training Loss: 0.12561214277479385 Test Loss: 0.38380061848958336\n",
      "Epoch: 9631 Training Loss: 0.1259590131971571 Test Loss: 0.38401917860243057\n",
      "Epoch: 9632 Training Loss: 0.12629901546902128 Test Loss: 0.3836075032552083\n",
      "Epoch: 9633 Training Loss: 0.12658254835340713 Test Loss: 0.382407470703125\n",
      "Epoch: 9634 Training Loss: 0.12678746626112197 Test Loss: 0.38043025716145834\n",
      "Epoch: 9635 Training Loss: 0.12685458289252388 Test Loss: 0.3784316134982639\n",
      "Epoch: 9636 Training Loss: 0.1267249272664388 Test Loss: 0.37686946614583333\n",
      "Epoch: 9637 Training Loss: 0.12642366451687284 Test Loss: 0.37617949761284725\n",
      "Epoch: 9638 Training Loss: 0.1260166252983941 Test Loss: 0.3762458224826389\n",
      "Epoch: 9639 Training Loss: 0.1255748036702474 Test Loss: 0.3770531684027778\n",
      "Epoch: 9640 Training Loss: 0.1251615473429362 Test Loss: 0.37756998697916666\n",
      "Epoch: 9641 Training Loss: 0.12481375715467664 Test Loss: 0.3784919704861111\n",
      "Epoch: 9642 Training Loss: 0.12452337985568576 Test Loss: 0.37907242838541666\n",
      "Epoch: 9643 Training Loss: 0.12427167510986328 Test Loss: 0.37950260416666665\n",
      "Epoch: 9644 Training Loss: 0.1240675548977322 Test Loss: 0.3798125\n",
      "Epoch: 9645 Training Loss: 0.12391190338134765 Test Loss: 0.3801535373263889\n",
      "Epoch: 9646 Training Loss: 0.12379800923665364 Test Loss: 0.3803073459201389\n",
      "Epoch: 9647 Training Loss: 0.1237354498969184 Test Loss: 0.38047797309027775\n",
      "Epoch: 9648 Training Loss: 0.12370333184136285 Test Loss: 0.3804340277777778\n",
      "Epoch: 9649 Training Loss: 0.12371421898735895 Test Loss: 0.3801750217013889\n",
      "Epoch: 9650 Training Loss: 0.12375637308756511 Test Loss: 0.37985601128472224\n",
      "Epoch: 9651 Training Loss: 0.12384156629774305 Test Loss: 0.3795016818576389\n",
      "Epoch: 9652 Training Loss: 0.12392686716715495 Test Loss: 0.37914227973090275\n",
      "Epoch: 9653 Training Loss: 0.12398915778266059 Test Loss: 0.37874476453993056\n",
      "Epoch: 9654 Training Loss: 0.12406085035536024 Test Loss: 0.3782926703559028\n",
      "Epoch: 9655 Training Loss: 0.12413893381754558 Test Loss: 0.37824156358506944\n",
      "Epoch: 9656 Training Loss: 0.12422833845350477 Test Loss: 0.37830894639756946\n",
      "Epoch: 9657 Training Loss: 0.12433128865559896 Test Loss: 0.37832234700520834\n",
      "Epoch: 9658 Training Loss: 0.12444937557644314 Test Loss: 0.37819235568576387\n",
      "Epoch: 9659 Training Loss: 0.12460481262207031 Test Loss: 0.37813590494791666\n",
      "Epoch: 9660 Training Loss: 0.12479671902126736 Test Loss: 0.3784244520399306\n",
      "Epoch: 9661 Training Loss: 0.12500411393907335 Test Loss: 0.378704833984375\n",
      "Epoch: 9662 Training Loss: 0.12521676042344834 Test Loss: 0.37916452365451386\n",
      "Epoch: 9663 Training Loss: 0.12546380700005424 Test Loss: 0.38041300455729166\n",
      "Epoch: 9664 Training Loss: 0.12572276136610244 Test Loss: 0.38177794053819447\n",
      "Epoch: 9665 Training Loss: 0.12598616706000434 Test Loss: 0.3834302300347222\n",
      "Epoch: 9666 Training Loss: 0.12623585764567058 Test Loss: 0.38507571072048613\n",
      "Epoch: 9667 Training Loss: 0.12647386169433594 Test Loss: 0.3866506890190972\n",
      "Epoch: 9668 Training Loss: 0.12670050472683378 Test Loss: 0.38819883897569446\n",
      "Epoch: 9669 Training Loss: 0.12688638390435114 Test Loss: 0.38921723090277777\n",
      "Epoch: 9670 Training Loss: 0.12701307169596354 Test Loss: 0.3900987684461806\n",
      "Epoch: 9671 Training Loss: 0.12706352742513022 Test Loss: 0.3903515353732639\n",
      "Epoch: 9672 Training Loss: 0.12698807525634764 Test Loss: 0.3901631401909722\n",
      "Epoch: 9673 Training Loss: 0.12675769636366102 Test Loss: 0.3887297634548611\n",
      "Epoch: 9674 Training Loss: 0.12638016764322918 Test Loss: 0.38635389539930554\n",
      "Epoch: 9675 Training Loss: 0.12584393056233725 Test Loss: 0.38395079210069444\n",
      "Epoch: 9676 Training Loss: 0.12518527391221787 Test Loss: 0.38232858615451387\n",
      "Epoch: 9677 Training Loss: 0.12449511210123698 Test Loss: 0.3817230360243056\n",
      "Epoch: 9678 Training Loss: 0.12382811652289497 Test Loss: 0.3816991644965278\n",
      "Epoch: 9679 Training Loss: 0.12320682779947917 Test Loss: 0.38207286241319444\n",
      "Epoch: 9680 Training Loss: 0.12263971371120877 Test Loss: 0.38226595052083334\n",
      "Epoch: 9681 Training Loss: 0.12214435153537326 Test Loss: 0.38219767252604164\n",
      "Epoch: 9682 Training Loss: 0.12170369296603732 Test Loss: 0.38227642144097224\n",
      "Epoch: 9683 Training Loss: 0.12132720438639323 Test Loss: 0.38229611545138886\n",
      "Epoch: 9684 Training Loss: 0.12100242784288194 Test Loss: 0.38221769205729167\n",
      "Epoch: 9685 Training Loss: 0.12074476030137804 Test Loss: 0.38212348090277776\n",
      "Epoch: 9686 Training Loss: 0.1205643073187934 Test Loss: 0.3819333767361111\n",
      "Epoch: 9687 Training Loss: 0.12042622205946181 Test Loss: 0.38191710069444446\n",
      "Epoch: 9688 Training Loss: 0.12034165785047743 Test Loss: 0.3819531792534722\n",
      "Epoch: 9689 Training Loss: 0.12029837629530164 Test Loss: 0.38193744574652777\n",
      "Epoch: 9690 Training Loss: 0.12029698350694444 Test Loss: 0.3821148817274306\n",
      "Epoch: 9691 Training Loss: 0.1203382805718316 Test Loss: 0.3821374240451389\n",
      "Epoch: 9692 Training Loss: 0.12043382602267795 Test Loss: 0.38237193467881947\n",
      "Epoch: 9693 Training Loss: 0.12056773630777995 Test Loss: 0.38277463107638887\n",
      "Epoch: 9694 Training Loss: 0.12074126180013021 Test Loss: 0.3830406358506944\n",
      "Epoch: 9695 Training Loss: 0.1209409942626953 Test Loss: 0.3834339735243056\n",
      "Epoch: 9696 Training Loss: 0.1211440904405382 Test Loss: 0.3838190646701389\n",
      "Epoch: 9697 Training Loss: 0.12135647583007812 Test Loss: 0.38382313368055554\n",
      "Epoch: 9698 Training Loss: 0.12157791561550564 Test Loss: 0.38396142578125\n",
      "Epoch: 9699 Training Loss: 0.12179185316297743 Test Loss: 0.3838611111111111\n",
      "Epoch: 9700 Training Loss: 0.1219867197672526 Test Loss: 0.38357118055555556\n",
      "Epoch: 9701 Training Loss: 0.12217100948757595 Test Loss: 0.38327094184027777\n",
      "Epoch: 9702 Training Loss: 0.12234834374321832 Test Loss: 0.3829136827256944\n",
      "Epoch: 9703 Training Loss: 0.12254363589816623 Test Loss: 0.3826984049479167\n",
      "Epoch: 9704 Training Loss: 0.12275370110405816 Test Loss: 0.38250439453125\n",
      "Epoch: 9705 Training Loss: 0.12298389689127605 Test Loss: 0.38222254774305553\n",
      "Epoch: 9706 Training Loss: 0.12322715504964192 Test Loss: 0.3815505642361111\n",
      "Epoch: 9707 Training Loss: 0.12349664391411676 Test Loss: 0.38088926866319445\n",
      "Epoch: 9708 Training Loss: 0.12380025143093533 Test Loss: 0.3801399197048611\n",
      "Epoch: 9709 Training Loss: 0.12410385555691189 Test Loss: 0.37943424479166665\n",
      "Epoch: 9710 Training Loss: 0.12445018768310546 Test Loss: 0.37899188910590276\n",
      "Epoch: 9711 Training Loss: 0.12482732560899523 Test Loss: 0.37912190755208336\n",
      "Epoch: 9712 Training Loss: 0.12523289489746095 Test Loss: 0.379713623046875\n",
      "Epoch: 9713 Training Loss: 0.12566591389973958 Test Loss: 0.3808047688802083\n",
      "Epoch: 9714 Training Loss: 0.12614918772379557 Test Loss: 0.38252229817708333\n",
      "Epoch: 9715 Training Loss: 0.1266584693060981 Test Loss: 0.38461162651909725\n",
      "Epoch: 9716 Training Loss: 0.12723333909776477 Test Loss: 0.38697737630208334\n",
      "Epoch: 9717 Training Loss: 0.12788248782687717 Test Loss: 0.38965006510416667\n",
      "Epoch: 9718 Training Loss: 0.12856676907009548 Test Loss: 0.3922526041666667\n",
      "Epoch: 9719 Training Loss: 0.12923172166612412 Test Loss: 0.39441837565104165\n",
      "Epoch: 9720 Training Loss: 0.12986742146809896 Test Loss: 0.39630433485243055\n",
      "Epoch: 9721 Training Loss: 0.13045467546251086 Test Loss: 0.3971837836371528\n",
      "Epoch: 9722 Training Loss: 0.13093070644802518 Test Loss: 0.3964981011284722\n",
      "Epoch: 9723 Training Loss: 0.131296752081977 Test Loss: 0.3954406467013889\n",
      "Epoch: 9724 Training Loss: 0.13161765713161894 Test Loss: 0.39417816840277775\n",
      "Epoch: 9725 Training Loss: 0.1318340827094184 Test Loss: 0.39265798611111113\n",
      "Epoch: 9726 Training Loss: 0.1318919898139106 Test Loss: 0.3910897894965278\n",
      "Epoch: 9727 Training Loss: 0.13168899705674914 Test Loss: 0.3895268283420139\n",
      "Epoch: 9728 Training Loss: 0.13120081244574652 Test Loss: 0.3880046657986111\n",
      "Epoch: 9729 Training Loss: 0.1304608637491862 Test Loss: 0.3868840603298611\n",
      "Epoch: 9730 Training Loss: 0.12949795956081814 Test Loss: 0.38550141059027776\n",
      "Epoch: 9731 Training Loss: 0.128430908203125 Test Loss: 0.38406944444444446\n",
      "Epoch: 9732 Training Loss: 0.12732015482584635 Test Loss: 0.382985107421875\n",
      "Epoch: 9733 Training Loss: 0.12622381422254775 Test Loss: 0.38205712890625\n",
      "Epoch: 9734 Training Loss: 0.1251793212890625 Test Loss: 0.38129166666666664\n",
      "Epoch: 9735 Training Loss: 0.12424091000027127 Test Loss: 0.38063536241319446\n",
      "Epoch: 9736 Training Loss: 0.12342330763075086 Test Loss: 0.38023518880208335\n",
      "Epoch: 9737 Training Loss: 0.12272129567464193 Test Loss: 0.37976318359375\n",
      "Epoch: 9738 Training Loss: 0.12212254248725044 Test Loss: 0.37902913411458333\n",
      "Epoch: 9739 Training Loss: 0.12160299258761936 Test Loss: 0.3784819064670139\n",
      "Epoch: 9740 Training Loss: 0.12116302490234375 Test Loss: 0.3779347601996528\n",
      "Epoch: 9741 Training Loss: 0.12079851871066623 Test Loss: 0.3774219563802083\n",
      "Epoch: 9742 Training Loss: 0.12047718302408854 Test Loss: 0.3769656032986111\n",
      "Epoch: 9743 Training Loss: 0.12019886525472005 Test Loss: 0.3766157769097222\n",
      "Epoch: 9744 Training Loss: 0.11996152665879992 Test Loss: 0.37636279296875\n",
      "Epoch: 9745 Training Loss: 0.11976281144883898 Test Loss: 0.37628700086805555\n",
      "Epoch: 9746 Training Loss: 0.11959166632758246 Test Loss: 0.37621793619791666\n",
      "Epoch: 9747 Training Loss: 0.11943941243489584 Test Loss: 0.3763143988715278\n",
      "Epoch: 9748 Training Loss: 0.11931015099419488 Test Loss: 0.37638514539930557\n",
      "Epoch: 9749 Training Loss: 0.1191974826388889 Test Loss: 0.37669590928819446\n",
      "Epoch: 9750 Training Loss: 0.11910451338026258 Test Loss: 0.3771144476996528\n",
      "Epoch: 9751 Training Loss: 0.11903880564371745 Test Loss: 0.37754714626736113\n",
      "Epoch: 9752 Training Loss: 0.11899485948350695 Test Loss: 0.3780851779513889\n",
      "Epoch: 9753 Training Loss: 0.11896288723415799 Test Loss: 0.3782870279947917\n",
      "Epoch: 9754 Training Loss: 0.11894899156358507 Test Loss: 0.3784504123263889\n",
      "Epoch: 9755 Training Loss: 0.11893697102864584 Test Loss: 0.37844281684027775\n",
      "Epoch: 9756 Training Loss: 0.1189228049384223 Test Loss: 0.37823887803819445\n",
      "Epoch: 9757 Training Loss: 0.11891853332519531 Test Loss: 0.37800748697916664\n",
      "Epoch: 9758 Training Loss: 0.1189253200954861 Test Loss: 0.37757047526041665\n",
      "Epoch: 9759 Training Loss: 0.11893110317654079 Test Loss: 0.37710883246527777\n",
      "Epoch: 9760 Training Loss: 0.11893358272976345 Test Loss: 0.3766522894965278\n",
      "Epoch: 9761 Training Loss: 0.11894051530626085 Test Loss: 0.37617635091145835\n",
      "Epoch: 9762 Training Loss: 0.11893807983398437 Test Loss: 0.37564702690972224\n",
      "Epoch: 9763 Training Loss: 0.1189357443915473 Test Loss: 0.37523486328125\n",
      "Epoch: 9764 Training Loss: 0.11893840705023871 Test Loss: 0.37476310221354164\n",
      "Epoch: 9765 Training Loss: 0.11893235778808593 Test Loss: 0.3747052951388889\n",
      "Epoch: 9766 Training Loss: 0.1189307386610243 Test Loss: 0.37469295247395834\n",
      "Epoch: 9767 Training Loss: 0.11893497551812066 Test Loss: 0.37467903645833334\n",
      "Epoch: 9768 Training Loss: 0.11894455803765192 Test Loss: 0.37505908203125\n",
      "Epoch: 9769 Training Loss: 0.11896749369303386 Test Loss: 0.37547889539930557\n",
      "Epoch: 9770 Training Loss: 0.11900087144639757 Test Loss: 0.3760220269097222\n",
      "Epoch: 9771 Training Loss: 0.11903884887695312 Test Loss: 0.3767336697048611\n",
      "Epoch: 9772 Training Loss: 0.11908663855658637 Test Loss: 0.3774207085503472\n",
      "Epoch: 9773 Training Loss: 0.11914180925157335 Test Loss: 0.37814835611979164\n",
      "Epoch: 9774 Training Loss: 0.11920960405137804 Test Loss: 0.3788060709635417\n",
      "Epoch: 9775 Training Loss: 0.11929573567708333 Test Loss: 0.3795541449652778\n",
      "Epoch: 9776 Training Loss: 0.11938757917616102 Test Loss: 0.3802150065104167\n",
      "Epoch: 9777 Training Loss: 0.11948227183024089 Test Loss: 0.3807717556423611\n",
      "Epoch: 9778 Training Loss: 0.11959068552652995 Test Loss: 0.38127867296006945\n",
      "Epoch: 9779 Training Loss: 0.1197040540907118 Test Loss: 0.381673828125\n",
      "Epoch: 9780 Training Loss: 0.11982732899983724 Test Loss: 0.3818240017361111\n",
      "Epoch: 9781 Training Loss: 0.11995003594292535 Test Loss: 0.38187364366319443\n",
      "Epoch: 9782 Training Loss: 0.12006516350640191 Test Loss: 0.3817941351996528\n",
      "Epoch: 9783 Training Loss: 0.12017309146457249 Test Loss: 0.3814462890625\n",
      "Epoch: 9784 Training Loss: 0.120272338019477 Test Loss: 0.3811045735677083\n",
      "Epoch: 9785 Training Loss: 0.12035639360215929 Test Loss: 0.3810876193576389\n",
      "Epoch: 9786 Training Loss: 0.12043742455376519 Test Loss: 0.3808947211371528\n",
      "Epoch: 9787 Training Loss: 0.12050771331787109 Test Loss: 0.38088037109375\n",
      "Epoch: 9788 Training Loss: 0.12057714758978949 Test Loss: 0.38113416883680556\n",
      "Epoch: 9789 Training Loss: 0.12064661492241753 Test Loss: 0.38132663302951386\n",
      "Epoch: 9790 Training Loss: 0.12074036407470704 Test Loss: 0.3815911458333333\n",
      "Epoch: 9791 Training Loss: 0.12085578918457031 Test Loss: 0.381790283203125\n",
      "Epoch: 9792 Training Loss: 0.12101084560818143 Test Loss: 0.3818059624565972\n",
      "Epoch: 9793 Training Loss: 0.1212062725490994 Test Loss: 0.3816496310763889\n",
      "Epoch: 9794 Training Loss: 0.12144025675455729 Test Loss: 0.38145817057291664\n",
      "Epoch: 9795 Training Loss: 0.12170099470350478 Test Loss: 0.3810608181423611\n",
      "Epoch: 9796 Training Loss: 0.12199720933702257 Test Loss: 0.38057628038194447\n",
      "Epoch: 9797 Training Loss: 0.12231904771592882 Test Loss: 0.3798468967013889\n",
      "Epoch: 9798 Training Loss: 0.12264607747395834 Test Loss: 0.37906450737847225\n",
      "Epoch: 9799 Training Loss: 0.12297516292995876 Test Loss: 0.37838986545138886\n",
      "Epoch: 9800 Training Loss: 0.12325971900092231 Test Loss: 0.37835221354166665\n",
      "Epoch: 9801 Training Loss: 0.12348968336317274 Test Loss: 0.3788280707465278\n",
      "Epoch: 9802 Training Loss: 0.12367553202311198 Test Loss: 0.3798079155815972\n",
      "Epoch: 9803 Training Loss: 0.12385693868001302 Test Loss: 0.38098326280381944\n",
      "Epoch: 9804 Training Loss: 0.12401442464192708 Test Loss: 0.381970458984375\n",
      "Epoch: 9805 Training Loss: 0.12416705746120876 Test Loss: 0.38234299045138886\n",
      "Epoch: 9806 Training Loss: 0.12427856106228298 Test Loss: 0.3821721462673611\n",
      "Epoch: 9807 Training Loss: 0.12431623925103082 Test Loss: 0.38106385633680556\n",
      "Epoch: 9808 Training Loss: 0.12421233283148872 Test Loss: 0.3794590115017361\n",
      "Epoch: 9809 Training Loss: 0.12391857401529947 Test Loss: 0.3778622775607639\n",
      "Epoch: 9810 Training Loss: 0.12350449964735243 Test Loss: 0.37723486328125\n",
      "Epoch: 9811 Training Loss: 0.12301015048556857 Test Loss: 0.3776038140190972\n",
      "Epoch: 9812 Training Loss: 0.12250722249348958 Test Loss: 0.3786454535590278\n",
      "Epoch: 9813 Training Loss: 0.12208318498399523 Test Loss: 0.37965774197048613\n",
      "Epoch: 9814 Training Loss: 0.12178308444552952 Test Loss: 0.3802837185329861\n",
      "Epoch: 9815 Training Loss: 0.12156777869330512 Test Loss: 0.3803713107638889\n",
      "Epoch: 9816 Training Loss: 0.12143216451009115 Test Loss: 0.3801621636284722\n",
      "Epoch: 9817 Training Loss: 0.12137122175428602 Test Loss: 0.3797881130642361\n",
      "Epoch: 9818 Training Loss: 0.1213787595960829 Test Loss: 0.37937095811631943\n",
      "Epoch: 9819 Training Loss: 0.12141560957166883 Test Loss: 0.37893085394965276\n",
      "Epoch: 9820 Training Loss: 0.1214982672797309 Test Loss: 0.3786474066840278\n",
      "Epoch: 9821 Training Loss: 0.12161478424072265 Test Loss: 0.37882872178819443\n",
      "Epoch: 9822 Training Loss: 0.12176832156711155 Test Loss: 0.37917475043402776\n",
      "Epoch: 9823 Training Loss: 0.12194359588623047 Test Loss: 0.37987955729166667\n",
      "Epoch: 9824 Training Loss: 0.12215269724527995 Test Loss: 0.38059752061631946\n",
      "Epoch: 9825 Training Loss: 0.12236485799153646 Test Loss: 0.38112681749131944\n",
      "Epoch: 9826 Training Loss: 0.12256740993923611 Test Loss: 0.3815485026041667\n",
      "Epoch: 9827 Training Loss: 0.12276559617784288 Test Loss: 0.38195803493923614\n",
      "Epoch: 9828 Training Loss: 0.12296375528971354 Test Loss: 0.38240407986111113\n",
      "Epoch: 9829 Training Loss: 0.12313837517632378 Test Loss: 0.38267616102430557\n",
      "Epoch: 9830 Training Loss: 0.12327485487196181 Test Loss: 0.38270502387152777\n",
      "Epoch: 9831 Training Loss: 0.12339770168728299 Test Loss: 0.38281675889756944\n",
      "Epoch: 9832 Training Loss: 0.1234916254679362 Test Loss: 0.38287760416666666\n",
      "Epoch: 9833 Training Loss: 0.12353253258599176 Test Loss: 0.38283973524305553\n",
      "Epoch: 9834 Training Loss: 0.12354904174804687 Test Loss: 0.38228694661458335\n",
      "Epoch: 9835 Training Loss: 0.12353524441189236 Test Loss: 0.3820290798611111\n",
      "Epoch: 9836 Training Loss: 0.1234561038547092 Test Loss: 0.38183745659722224\n",
      "Epoch: 9837 Training Loss: 0.12329794565836588 Test Loss: 0.3811728244357639\n",
      "Epoch: 9838 Training Loss: 0.12308275858561198 Test Loss: 0.380802001953125\n",
      "Epoch: 9839 Training Loss: 0.12282749345567491 Test Loss: 0.38067765299479167\n",
      "Epoch: 9840 Training Loss: 0.1225513678656684 Test Loss: 0.3807016330295139\n",
      "Epoch: 9841 Training Loss: 0.12225711992051866 Test Loss: 0.38096999782986113\n",
      "Epoch: 9842 Training Loss: 0.12196263207329644 Test Loss: 0.3811755642361111\n",
      "Epoch: 9843 Training Loss: 0.1216993654039171 Test Loss: 0.38141878255208334\n",
      "Epoch: 9844 Training Loss: 0.12150545077853733 Test Loss: 0.3814953884548611\n",
      "Epoch: 9845 Training Loss: 0.12138818274603949 Test Loss: 0.38145279947916666\n",
      "Epoch: 9846 Training Loss: 0.12134532504611545 Test Loss: 0.3815421278211806\n",
      "Epoch: 9847 Training Loss: 0.12136863115098741 Test Loss: 0.3812416449652778\n",
      "Epoch: 9848 Training Loss: 0.12144132741292318 Test Loss: 0.38069514973958335\n",
      "Epoch: 9849 Training Loss: 0.12157884555392795 Test Loss: 0.37996006944444444\n",
      "Epoch: 9850 Training Loss: 0.12177313995361329 Test Loss: 0.3792886284722222\n",
      "Epoch: 9851 Training Loss: 0.12199677276611329 Test Loss: 0.37869715711805557\n",
      "Epoch: 9852 Training Loss: 0.12225361802842882 Test Loss: 0.37836876085069443\n",
      "Epoch: 9853 Training Loss: 0.12255217997233073 Test Loss: 0.3781332465277778\n",
      "Epoch: 9854 Training Loss: 0.12285934363471138 Test Loss: 0.37797249348958334\n",
      "Epoch: 9855 Training Loss: 0.12314988199869792 Test Loss: 0.378310546875\n",
      "Epoch: 9856 Training Loss: 0.12342811245388455 Test Loss: 0.37925010850694446\n",
      "Epoch: 9857 Training Loss: 0.12367089335123697 Test Loss: 0.3807424587673611\n",
      "Epoch: 9858 Training Loss: 0.12385089365641276 Test Loss: 0.38253865559895833\n",
      "Epoch: 9859 Training Loss: 0.12398789299858941 Test Loss: 0.38415662977430554\n",
      "Epoch: 9860 Training Loss: 0.1240758565266927 Test Loss: 0.385123046875\n",
      "Epoch: 9861 Training Loss: 0.12411456722683377 Test Loss: 0.3854941677517361\n",
      "Epoch: 9862 Training Loss: 0.12412139214409722 Test Loss: 0.385650634765625\n",
      "Epoch: 9863 Training Loss: 0.12403289540608724 Test Loss: 0.3856259765625\n",
      "Epoch: 9864 Training Loss: 0.12379962582058376 Test Loss: 0.38494004991319447\n",
      "Epoch: 9865 Training Loss: 0.12343791876898871 Test Loss: 0.38386073133680554\n",
      "Epoch: 9866 Training Loss: 0.12296888393825955 Test Loss: 0.3827179904513889\n",
      "Epoch: 9867 Training Loss: 0.12242318979899089 Test Loss: 0.3817439778645833\n",
      "Epoch: 9868 Training Loss: 0.12188143666585287 Test Loss: 0.38139973958333334\n",
      "Epoch: 9869 Training Loss: 0.12135232628716362 Test Loss: 0.38127647569444445\n",
      "Epoch: 9870 Training Loss: 0.12090055931939019 Test Loss: 0.38148616536458335\n",
      "Epoch: 9871 Training Loss: 0.12053987291124132 Test Loss: 0.38199348958333335\n",
      "Epoch: 9872 Training Loss: 0.12024108632405599 Test Loss: 0.3822495659722222\n",
      "Epoch: 9873 Training Loss: 0.11999481879340278 Test Loss: 0.3825319552951389\n",
      "Epoch: 9874 Training Loss: 0.11979996151394315 Test Loss: 0.38259990776909725\n",
      "Epoch: 9875 Training Loss: 0.11961920589870877 Test Loss: 0.38275651041666664\n",
      "Epoch: 9876 Training Loss: 0.11944847106933594 Test Loss: 0.38287570529513887\n",
      "Epoch: 9877 Training Loss: 0.11929021623399523 Test Loss: 0.3832178005642361\n",
      "Epoch: 9878 Training Loss: 0.11914961751302083 Test Loss: 0.3835744357638889\n",
      "Epoch: 9879 Training Loss: 0.11903513505723741 Test Loss: 0.3840938856336806\n",
      "Epoch: 9880 Training Loss: 0.11893106248643663 Test Loss: 0.38438652886284724\n",
      "Epoch: 9881 Training Loss: 0.11884899393717448 Test Loss: 0.38465364583333334\n",
      "Epoch: 9882 Training Loss: 0.11879119449191623 Test Loss: 0.3847472601996528\n",
      "Epoch: 9883 Training Loss: 0.11875106981065538 Test Loss: 0.3847746853298611\n",
      "Epoch: 9884 Training Loss: 0.11872813500298394 Test Loss: 0.3845205078125\n",
      "Epoch: 9885 Training Loss: 0.11872038014729817 Test Loss: 0.3840358344184028\n",
      "Epoch: 9886 Training Loss: 0.11874693044026692 Test Loss: 0.38353925238715275\n",
      "Epoch: 9887 Training Loss: 0.11879750569661458 Test Loss: 0.3832248263888889\n",
      "Epoch: 9888 Training Loss: 0.11886217753092448 Test Loss: 0.38306751844618053\n",
      "Epoch: 9889 Training Loss: 0.11892281765407986 Test Loss: 0.3829643825954861\n",
      "Epoch: 9890 Training Loss: 0.11898512268066407 Test Loss: 0.3829709743923611\n",
      "Epoch: 9891 Training Loss: 0.11904988182915581 Test Loss: 0.38318961588541667\n",
      "Epoch: 9892 Training Loss: 0.11911554378933377 Test Loss: 0.38346869574652775\n",
      "Epoch: 9893 Training Loss: 0.11918477884928386 Test Loss: 0.38364382595486113\n",
      "Epoch: 9894 Training Loss: 0.11925587293836805 Test Loss: 0.3840697428385417\n",
      "Epoch: 9895 Training Loss: 0.11933455488416883 Test Loss: 0.3843938802083333\n",
      "Epoch: 9896 Training Loss: 0.11943436686197917 Test Loss: 0.3845466579861111\n",
      "Epoch: 9897 Training Loss: 0.11954407840304905 Test Loss: 0.3846660970052083\n",
      "Epoch: 9898 Training Loss: 0.11967029995388455 Test Loss: 0.38442301432291665\n",
      "Epoch: 9899 Training Loss: 0.11982554880777994 Test Loss: 0.3838022189670139\n",
      "Epoch: 9900 Training Loss: 0.12000020514594184 Test Loss: 0.38292366536458333\n",
      "Epoch: 9901 Training Loss: 0.12018592834472656 Test Loss: 0.3816228298611111\n",
      "Epoch: 9902 Training Loss: 0.12040978495279948 Test Loss: 0.38002020941840275\n",
      "Epoch: 9903 Training Loss: 0.12065198771158854 Test Loss: 0.3790264756944444\n",
      "Epoch: 9904 Training Loss: 0.12088996717664931 Test Loss: 0.37802411566840277\n",
      "Epoch: 9905 Training Loss: 0.12113235727945963 Test Loss: 0.37753862847222225\n",
      "Epoch: 9906 Training Loss: 0.12136441548665365 Test Loss: 0.37742032877604165\n",
      "Epoch: 9907 Training Loss: 0.12159098052978516 Test Loss: 0.37781309678819447\n",
      "Epoch: 9908 Training Loss: 0.12181814575195313 Test Loss: 0.37891343858506943\n",
      "Epoch: 9909 Training Loss: 0.12201706695556641 Test Loss: 0.38029277886284724\n",
      "Epoch: 9910 Training Loss: 0.12218411085340712 Test Loss: 0.3819031575520833\n",
      "Epoch: 9911 Training Loss: 0.12231394788953993 Test Loss: 0.3834407552083333\n",
      "Epoch: 9912 Training Loss: 0.122443359375 Test Loss: 0.38466946072048613\n",
      "Epoch: 9913 Training Loss: 0.12256499650743273 Test Loss: 0.38539466688368057\n",
      "Epoch: 9914 Training Loss: 0.1226404300265842 Test Loss: 0.38590776909722224\n",
      "Epoch: 9915 Training Loss: 0.12267356957329643 Test Loss: 0.3862345920138889\n",
      "Epoch: 9916 Training Loss: 0.12266262223985461 Test Loss: 0.3861455620659722\n",
      "Epoch: 9917 Training Loss: 0.12260827043321397 Test Loss: 0.38580669487847224\n",
      "Epoch: 9918 Training Loss: 0.12250160471598308 Test Loss: 0.3852688259548611\n",
      "Epoch: 9919 Training Loss: 0.12234569888644749 Test Loss: 0.38479530164930553\n",
      "Epoch: 9920 Training Loss: 0.12221470472547744 Test Loss: 0.3846239963107639\n",
      "Epoch: 9921 Training Loss: 0.12212936316596137 Test Loss: 0.38439697265625\n",
      "Epoch: 9922 Training Loss: 0.12214388105604383 Test Loss: 0.38469580078125\n",
      "Epoch: 9923 Training Loss: 0.1222086927625868 Test Loss: 0.3849351671006944\n",
      "Epoch: 9924 Training Loss: 0.12230396525065104 Test Loss: 0.3852687717013889\n",
      "Epoch: 9925 Training Loss: 0.12241876644558376 Test Loss: 0.384355712890625\n",
      "Epoch: 9926 Training Loss: 0.12256629604763455 Test Loss: 0.38316221788194443\n",
      "Epoch: 9927 Training Loss: 0.12274126264784072 Test Loss: 0.3813389214409722\n",
      "Epoch: 9928 Training Loss: 0.12288671281602648 Test Loss: 0.3793003472222222\n",
      "Epoch: 9929 Training Loss: 0.12302063327365452 Test Loss: 0.37845743815104166\n",
      "Epoch: 9930 Training Loss: 0.12322350141737196 Test Loss: 0.3797204861111111\n",
      "Epoch: 9931 Training Loss: 0.12354703013102214 Test Loss: 0.38179747178819445\n",
      "Epoch: 9932 Training Loss: 0.12395138804117839 Test Loss: 0.3837699652777778\n",
      "Epoch: 9933 Training Loss: 0.12442663404676649 Test Loss: 0.38498768446180553\n",
      "Epoch: 9934 Training Loss: 0.12489205508761936 Test Loss: 0.38476784939236114\n",
      "Epoch: 9935 Training Loss: 0.12535987514919705 Test Loss: 0.38373567708333334\n",
      "Epoch: 9936 Training Loss: 0.12584283616807726 Test Loss: 0.3838468967013889\n",
      "Epoch: 9937 Training Loss: 0.12637884182400175 Test Loss: 0.38547743055555556\n",
      "Epoch: 9938 Training Loss: 0.12694322035047742 Test Loss: 0.3880703396267361\n",
      "Epoch: 9939 Training Loss: 0.12754452006022135 Test Loss: 0.3921612955729167\n",
      "Epoch: 9940 Training Loss: 0.12815050591362848 Test Loss: 0.39762928602430553\n",
      "Epoch: 9941 Training Loss: 0.12875914340549044 Test Loss: 0.4040110948350694\n",
      "Epoch: 9942 Training Loss: 0.1293833033243815 Test Loss: 0.4111861165364583\n",
      "Epoch: 9943 Training Loss: 0.13001510026719834 Test Loss: 0.41694970703125\n",
      "Epoch: 9944 Training Loss: 0.13063811492919922 Test Loss: 0.4194929741753472\n",
      "Epoch: 9945 Training Loss: 0.13117127227783204 Test Loss: 0.41654231770833333\n",
      "Epoch: 9946 Training Loss: 0.13150845082600912 Test Loss: 0.40878312174479164\n",
      "Epoch: 9947 Training Loss: 0.13156758965386284 Test Loss: 0.3993617892795139\n",
      "Epoch: 9948 Training Loss: 0.13128648461235895 Test Loss: 0.3907634819878472\n",
      "Epoch: 9949 Training Loss: 0.13062763383653428 Test Loss: 0.38555501302083334\n",
      "Epoch: 9950 Training Loss: 0.1295450405544705 Test Loss: 0.3832545572916667\n",
      "Epoch: 9951 Training Loss: 0.12817678493923612 Test Loss: 0.38262836371527775\n",
      "Epoch: 9952 Training Loss: 0.12673048231336806 Test Loss: 0.3816408962673611\n",
      "Epoch: 9953 Training Loss: 0.12531020948621963 Test Loss: 0.3803518880208333\n",
      "Epoch: 9954 Training Loss: 0.12402953592936197 Test Loss: 0.37883116319444443\n",
      "Epoch: 9955 Training Loss: 0.12288061099582248 Test Loss: 0.37799940321180553\n",
      "Epoch: 9956 Training Loss: 0.12188362291124132 Test Loss: 0.3777510308159722\n",
      "Epoch: 9957 Training Loss: 0.12107160525851779 Test Loss: 0.3777257758246528\n",
      "Epoch: 9958 Training Loss: 0.12039976077609592 Test Loss: 0.3776421169704861\n",
      "Epoch: 9959 Training Loss: 0.11985718197292752 Test Loss: 0.3776328938802083\n",
      "Epoch: 9960 Training Loss: 0.11942620425754123 Test Loss: 0.37764756944444444\n",
      "Epoch: 9961 Training Loss: 0.11910906304253473 Test Loss: 0.3777821723090278\n",
      "Epoch: 9962 Training Loss: 0.11887660386827258 Test Loss: 0.37799153645833333\n",
      "Epoch: 9963 Training Loss: 0.11871411302354601 Test Loss: 0.37809629991319443\n",
      "Epoch: 9964 Training Loss: 0.11862413109673393 Test Loss: 0.37849671766493054\n",
      "Epoch: 9965 Training Loss: 0.11857973649766711 Test Loss: 0.37891943359375\n",
      "Epoch: 9966 Training Loss: 0.11857598792182075 Test Loss: 0.3792273220486111\n",
      "Epoch: 9967 Training Loss: 0.11861410692003038 Test Loss: 0.37960400390625\n",
      "Epoch: 9968 Training Loss: 0.1186874491373698 Test Loss: 0.37992822265625\n",
      "Epoch: 9969 Training Loss: 0.11879938337537978 Test Loss: 0.38022287326388887\n",
      "Epoch: 9970 Training Loss: 0.11895068020290799 Test Loss: 0.38055571831597224\n",
      "Epoch: 9971 Training Loss: 0.11913275146484376 Test Loss: 0.3807465277777778\n",
      "Epoch: 9972 Training Loss: 0.11934275563557943 Test Loss: 0.3809979926215278\n",
      "Epoch: 9973 Training Loss: 0.11959401702880859 Test Loss: 0.3813756239149306\n",
      "Epoch: 9974 Training Loss: 0.11987466006808811 Test Loss: 0.381697509765625\n",
      "Epoch: 9975 Training Loss: 0.12018546888563368 Test Loss: 0.38173985460069443\n",
      "Epoch: 9976 Training Loss: 0.12054206932915582 Test Loss: 0.38216541883680555\n",
      "Epoch: 9977 Training Loss: 0.120953370836046 Test Loss: 0.3823156467013889\n",
      "Epoch: 9978 Training Loss: 0.12144415622287326 Test Loss: 0.38249009874131945\n",
      "Epoch: 9979 Training Loss: 0.12201541815863716 Test Loss: 0.3824506293402778\n",
      "Epoch: 9980 Training Loss: 0.12268528238932291 Test Loss: 0.382460693359375\n",
      "Epoch: 9981 Training Loss: 0.12346873304578992 Test Loss: 0.3823558756510417\n",
      "Epoch: 9982 Training Loss: 0.12440203009711372 Test Loss: 0.3823810492621528\n",
      "Epoch: 9983 Training Loss: 0.12546402231852213 Test Loss: 0.38223025173611114\n",
      "Epoch: 9984 Training Loss: 0.12667669762505426 Test Loss: 0.3826454535590278\n",
      "Epoch: 9985 Training Loss: 0.12796486155192058 Test Loss: 0.3835177408854167\n",
      "Epoch: 9986 Training Loss: 0.12923066711425782 Test Loss: 0.3852319064670139\n",
      "Epoch: 9987 Training Loss: 0.13039127434624567 Test Loss: 0.38866327582465277\n",
      "Epoch: 9988 Training Loss: 0.13137024773491754 Test Loss: 0.39316840277777776\n",
      "Epoch: 9989 Training Loss: 0.13214409637451172 Test Loss: 0.3963615451388889\n",
      "Epoch: 9990 Training Loss: 0.13265703921847874 Test Loss: 0.3963117404513889\n",
      "Epoch: 9991 Training Loss: 0.13276121266682941 Test Loss: 0.3932058376736111\n",
      "Epoch: 9992 Training Loss: 0.13230961354573567 Test Loss: 0.38871723090277777\n",
      "Epoch: 9993 Training Loss: 0.13116375308566625 Test Loss: 0.3836909722222222\n",
      "Epoch: 9994 Training Loss: 0.1295212131076389 Test Loss: 0.3797704535590278\n",
      "Epoch: 9995 Training Loss: 0.1277040312025282 Test Loss: 0.376242919921875\n",
      "Epoch: 9996 Training Loss: 0.1260056855943468 Test Loss: 0.3742831759982639\n",
      "Epoch: 9997 Training Loss: 0.12454778374565972 Test Loss: 0.37359144422743057\n",
      "Epoch: 9998 Training Loss: 0.12334368981255425 Test Loss: 0.3735851779513889\n",
      "Epoch: 9999 Training Loss: 0.12236441463894314 Test Loss: 0.37382969835069446\n",
      "Epoch: 10000 Training Loss: 0.12159073723687067 Test Loss: 0.3738081325954861\n",
      "Epoch: 10001 Training Loss: 0.12098153940836588 Test Loss: 0.37391959635416666\n",
      "Epoch: 10002 Training Loss: 0.12051997884114583 Test Loss: 0.3740344509548611\n",
      "Epoch: 10003 Training Loss: 0.1201641599867079 Test Loss: 0.37440776909722223\n",
      "Epoch: 10004 Training Loss: 0.11990365515814887 Test Loss: 0.3747990451388889\n",
      "Epoch: 10005 Training Loss: 0.11971335771348741 Test Loss: 0.3752897135416667\n",
      "Epoch: 10006 Training Loss: 0.11956174892849393 Test Loss: 0.3758600802951389\n",
      "Epoch: 10007 Training Loss: 0.119446779039171 Test Loss: 0.3762546657986111\n",
      "Epoch: 10008 Training Loss: 0.11936783175998264 Test Loss: 0.3766131456163194\n",
      "Epoch: 10009 Training Loss: 0.11932186381022135 Test Loss: 0.37679508463541667\n",
      "Epoch: 10010 Training Loss: 0.1193089836968316 Test Loss: 0.3770210503472222\n",
      "Epoch: 10011 Training Loss: 0.11930813259548612 Test Loss: 0.37699408637152776\n",
      "Epoch: 10012 Training Loss: 0.11929402414957682 Test Loss: 0.3769608561197917\n",
      "Epoch: 10013 Training Loss: 0.11928695763481988 Test Loss: 0.37688050672743056\n",
      "Epoch: 10014 Training Loss: 0.11928536393907335 Test Loss: 0.37681144205729167\n",
      "Epoch: 10015 Training Loss: 0.119280030992296 Test Loss: 0.3767646484375\n",
      "Epoch: 10016 Training Loss: 0.11927193450927734 Test Loss: 0.376783203125\n",
      "Epoch: 10017 Training Loss: 0.11927313571506076 Test Loss: 0.37688829210069447\n",
      "Epoch: 10018 Training Loss: 0.11927461496988932 Test Loss: 0.3772361653645833\n",
      "Epoch: 10019 Training Loss: 0.11928256734212239 Test Loss: 0.3774446072048611\n",
      "Epoch: 10020 Training Loss: 0.11929575941297743 Test Loss: 0.3778291015625\n",
      "Epoch: 10021 Training Loss: 0.11931769307454428 Test Loss: 0.37812727864583334\n",
      "Epoch: 10022 Training Loss: 0.1193457760281033 Test Loss: 0.3783139377170139\n",
      "Epoch: 10023 Training Loss: 0.1193693339029948 Test Loss: 0.37852864583333334\n",
      "Epoch: 10024 Training Loss: 0.11941128624810113 Test Loss: 0.3787626953125\n",
      "Epoch: 10025 Training Loss: 0.11945896742078993 Test Loss: 0.37884049479166665\n",
      "Epoch: 10026 Training Loss: 0.11951918453640407 Test Loss: 0.3788598361545139\n",
      "Epoch: 10027 Training Loss: 0.11961177402072483 Test Loss: 0.3786170789930556\n",
      "Epoch: 10028 Training Loss: 0.11971934763590494 Test Loss: 0.3781706271701389\n",
      "Epoch: 10029 Training Loss: 0.11985068935818143 Test Loss: 0.3776951768663194\n",
      "Epoch: 10030 Training Loss: 0.11998809136284722 Test Loss: 0.37703352864583334\n",
      "Epoch: 10031 Training Loss: 0.1201537085639106 Test Loss: 0.3760285915798611\n",
      "Epoch: 10032 Training Loss: 0.1203466551038954 Test Loss: 0.37507606336805555\n",
      "Epoch: 10033 Training Loss: 0.12058768378363716 Test Loss: 0.3738736436631944\n",
      "Epoch: 10034 Training Loss: 0.12086284806993272 Test Loss: 0.372803955078125\n",
      "Epoch: 10035 Training Loss: 0.12115648990207248 Test Loss: 0.3720254177517361\n",
      "Epoch: 10036 Training Loss: 0.1214771728515625 Test Loss: 0.37183946397569445\n",
      "Epoch: 10037 Training Loss: 0.12179546525743272 Test Loss: 0.37249894205729167\n",
      "Epoch: 10038 Training Loss: 0.12210536193847656 Test Loss: 0.3735635850694444\n",
      "Epoch: 10039 Training Loss: 0.12239585367838542 Test Loss: 0.3755099283854167\n",
      "Epoch: 10040 Training Loss: 0.12266664801703558 Test Loss: 0.3775678168402778\n",
      "Epoch: 10041 Training Loss: 0.12291402265760634 Test Loss: 0.37932085503472224\n",
      "Epoch: 10042 Training Loss: 0.12311703406439888 Test Loss: 0.3807687174479167\n",
      "Epoch: 10043 Training Loss: 0.1232330576578776 Test Loss: 0.3813379448784722\n",
      "Epoch: 10044 Training Loss: 0.12324821387396918 Test Loss: 0.3811203884548611\n",
      "Epoch: 10045 Training Loss: 0.12311719936794704 Test Loss: 0.3800476345486111\n",
      "Epoch: 10046 Training Loss: 0.12281210496690538 Test Loss: 0.37840158420138886\n",
      "Epoch: 10047 Training Loss: 0.12236868116590711 Test Loss: 0.3770165473090278\n",
      "Epoch: 10048 Training Loss: 0.1218581331041124 Test Loss: 0.3765152180989583\n",
      "Epoch: 10049 Training Loss: 0.12135642666286893 Test Loss: 0.3767335611979167\n",
      "Epoch: 10050 Training Loss: 0.12092419687906901 Test Loss: 0.37761295572916664\n",
      "Epoch: 10051 Training Loss: 0.12059386444091796 Test Loss: 0.3784010959201389\n",
      "Epoch: 10052 Training Loss: 0.12036649576822916 Test Loss: 0.379014404296875\n",
      "Epoch: 10053 Training Loss: 0.12022931162516276 Test Loss: 0.379276123046875\n",
      "Epoch: 10054 Training Loss: 0.12014779663085938 Test Loss: 0.37902935112847225\n",
      "Epoch: 10055 Training Loss: 0.12010484822591146 Test Loss: 0.37839697265625\n",
      "Epoch: 10056 Training Loss: 0.1200887951321072 Test Loss: 0.3777361111111111\n",
      "Epoch: 10057 Training Loss: 0.12008428955078125 Test Loss: 0.3774128146701389\n",
      "Epoch: 10058 Training Loss: 0.12007985348171658 Test Loss: 0.3774839952256944\n",
      "Epoch: 10059 Training Loss: 0.12005622016059028 Test Loss: 0.37806684027777776\n",
      "Epoch: 10060 Training Loss: 0.12002190992567274 Test Loss: 0.3788682454427083\n",
      "Epoch: 10061 Training Loss: 0.11997981686062283 Test Loss: 0.37965386284722225\n",
      "Epoch: 10062 Training Loss: 0.11992611440022787 Test Loss: 0.38035780164930555\n",
      "Epoch: 10063 Training Loss: 0.11988113233778212 Test Loss: 0.380749755859375\n",
      "Epoch: 10064 Training Loss: 0.11983599429660373 Test Loss: 0.38092228190104166\n",
      "Epoch: 10065 Training Loss: 0.11978770531548394 Test Loss: 0.38056749131944445\n",
      "Epoch: 10066 Training Loss: 0.11972396765814887 Test Loss: 0.37975526258680553\n",
      "Epoch: 10067 Training Loss: 0.11966598425971137 Test Loss: 0.379038818359375\n",
      "Epoch: 10068 Training Loss: 0.11961467912462023 Test Loss: 0.3780179036458333\n",
      "Epoch: 10069 Training Loss: 0.11957464938693577 Test Loss: 0.3774120822482639\n",
      "Epoch: 10070 Training Loss: 0.11953589206271702 Test Loss: 0.3769252387152778\n",
      "Epoch: 10071 Training Loss: 0.11950865342881944 Test Loss: 0.3767477756076389\n",
      "Epoch: 10072 Training Loss: 0.11950035858154297 Test Loss: 0.37690169270833335\n",
      "Epoch: 10073 Training Loss: 0.11951547580295138 Test Loss: 0.37732644314236113\n",
      "Epoch: 10074 Training Loss: 0.11952123090955946 Test Loss: 0.3779181586371528\n",
      "Epoch: 10075 Training Loss: 0.1195293197631836 Test Loss: 0.3787112358940972\n",
      "Epoch: 10076 Training Loss: 0.11955355326334635 Test Loss: 0.3796439073350694\n",
      "Epoch: 10077 Training Loss: 0.11959427897135416 Test Loss: 0.3807891710069444\n",
      "Epoch: 10078 Training Loss: 0.11965154435899522 Test Loss: 0.38168798828125\n",
      "Epoch: 10079 Training Loss: 0.11971846601698134 Test Loss: 0.38250016276041665\n",
      "Epoch: 10080 Training Loss: 0.11978231557210287 Test Loss: 0.38298177083333335\n",
      "Epoch: 10081 Training Loss: 0.11983191511366102 Test Loss: 0.38370057508680555\n",
      "Epoch: 10082 Training Loss: 0.1198784688313802 Test Loss: 0.38404728190104165\n",
      "Epoch: 10083 Training Loss: 0.11991886393229166 Test Loss: 0.3842496202256944\n",
      "Epoch: 10084 Training Loss: 0.11996948835584853 Test Loss: 0.3841761610243056\n",
      "Epoch: 10085 Training Loss: 0.12000913577609593 Test Loss: 0.3841725802951389\n",
      "Epoch: 10086 Training Loss: 0.12005933803982205 Test Loss: 0.3837620442708333\n",
      "Epoch: 10087 Training Loss: 0.1201184310913086 Test Loss: 0.3831718207465278\n",
      "Epoch: 10088 Training Loss: 0.12019670189751519 Test Loss: 0.3826103515625\n",
      "Epoch: 10089 Training Loss: 0.1202831293741862 Test Loss: 0.38196671549479166\n",
      "Epoch: 10090 Training Loss: 0.12037415144178602 Test Loss: 0.3810807834201389\n",
      "Epoch: 10091 Training Loss: 0.12051002248128255 Test Loss: 0.38016075303819447\n",
      "Epoch: 10092 Training Loss: 0.12066160922580295 Test Loss: 0.3797022026909722\n",
      "Epoch: 10093 Training Loss: 0.12082610660129123 Test Loss: 0.379733154296875\n",
      "Epoch: 10094 Training Loss: 0.1209946297539605 Test Loss: 0.38017046440972224\n",
      "Epoch: 10095 Training Loss: 0.12117943996853299 Test Loss: 0.38092258029513887\n",
      "Epoch: 10096 Training Loss: 0.12137281290690104 Test Loss: 0.3816824544270833\n",
      "Epoch: 10097 Training Loss: 0.1215633070203993 Test Loss: 0.38251611328125\n",
      "Epoch: 10098 Training Loss: 0.12172901238335504 Test Loss: 0.38330105251736113\n",
      "Epoch: 10099 Training Loss: 0.12186240810818143 Test Loss: 0.38396725802951387\n",
      "Epoch: 10100 Training Loss: 0.12197353702121311 Test Loss: 0.3844317762586806\n",
      "Epoch: 10101 Training Loss: 0.12206190490722656 Test Loss: 0.38454820421006947\n",
      "Epoch: 10102 Training Loss: 0.12211097802056206 Test Loss: 0.38397987196180555\n",
      "Epoch: 10103 Training Loss: 0.12213470713297526 Test Loss: 0.3827395290798611\n",
      "Epoch: 10104 Training Loss: 0.12213043891059028 Test Loss: 0.38074940321180556\n",
      "Epoch: 10105 Training Loss: 0.12206962331136068 Test Loss: 0.37839122178819445\n",
      "Epoch: 10106 Training Loss: 0.12193979983859592 Test Loss: 0.37621907552083333\n",
      "Epoch: 10107 Training Loss: 0.12172073533799913 Test Loss: 0.37483799913194443\n",
      "Epoch: 10108 Training Loss: 0.12146216752794053 Test Loss: 0.37453662109375\n",
      "Epoch: 10109 Training Loss: 0.12117017194959852 Test Loss: 0.37499739583333336\n",
      "Epoch: 10110 Training Loss: 0.12083918168809679 Test Loss: 0.375916015625\n",
      "Epoch: 10111 Training Loss: 0.12052997504340278 Test Loss: 0.37687481011284724\n",
      "Epoch: 10112 Training Loss: 0.12022512647840712 Test Loss: 0.3775781792534722\n",
      "Epoch: 10113 Training Loss: 0.1199324467976888 Test Loss: 0.3775873480902778\n",
      "Epoch: 10114 Training Loss: 0.11965562523735894 Test Loss: 0.3774689670138889\n",
      "Epoch: 10115 Training Loss: 0.11939037662082248 Test Loss: 0.377320068359375\n",
      "Epoch: 10116 Training Loss: 0.11914690568712022 Test Loss: 0.3770654568142361\n",
      "Epoch: 10117 Training Loss: 0.1189371566772461 Test Loss: 0.3770775824652778\n",
      "Epoch: 10118 Training Loss: 0.11875508371988933 Test Loss: 0.3770775824652778\n",
      "Epoch: 10119 Training Loss: 0.1186111551920573 Test Loss: 0.37705772569444446\n",
      "Epoch: 10120 Training Loss: 0.11851678127712674 Test Loss: 0.37687820095486113\n",
      "Epoch: 10121 Training Loss: 0.1184677980211046 Test Loss: 0.37662518988715277\n",
      "Epoch: 10122 Training Loss: 0.11846218956841363 Test Loss: 0.3765735134548611\n",
      "Epoch: 10123 Training Loss: 0.11849567498101128 Test Loss: 0.3764531792534722\n",
      "Epoch: 10124 Training Loss: 0.11856563313802083 Test Loss: 0.37630734592013887\n",
      "Epoch: 10125 Training Loss: 0.11867314910888672 Test Loss: 0.3763540581597222\n",
      "Epoch: 10126 Training Loss: 0.11882606421576605 Test Loss: 0.37663297526041667\n",
      "Epoch: 10127 Training Loss: 0.11902104441324869 Test Loss: 0.3770053982204861\n",
      "Epoch: 10128 Training Loss: 0.11926515452067057 Test Loss: 0.37758867730034723\n",
      "Epoch: 10129 Training Loss: 0.11954446750217014 Test Loss: 0.37794303385416667\n",
      "Epoch: 10130 Training Loss: 0.11985852389865452 Test Loss: 0.37858968098958334\n",
      "Epoch: 10131 Training Loss: 0.12022101593017578 Test Loss: 0.3795461697048611\n",
      "Epoch: 10132 Training Loss: 0.12063101450602214 Test Loss: 0.3804849446614583\n",
      "Epoch: 10133 Training Loss: 0.12108374786376953 Test Loss: 0.3813819173177083\n",
      "Epoch: 10134 Training Loss: 0.12156935204399956 Test Loss: 0.38238807508680556\n",
      "Epoch: 10135 Training Loss: 0.12207385592990451 Test Loss: 0.3835259060329861\n",
      "Epoch: 10136 Training Loss: 0.12262867567274306 Test Loss: 0.3844565972222222\n",
      "Epoch: 10137 Training Loss: 0.12320004357231988 Test Loss: 0.38544745551215276\n",
      "Epoch: 10138 Training Loss: 0.12378674825032553 Test Loss: 0.3861627061631944\n",
      "Epoch: 10139 Training Loss: 0.12438017781575521 Test Loss: 0.38650569661458334\n",
      "Epoch: 10140 Training Loss: 0.12496044921875 Test Loss: 0.3862538519965278\n",
      "Epoch: 10141 Training Loss: 0.1254639612833659 Test Loss: 0.38574650065104166\n",
      "Epoch: 10142 Training Loss: 0.1258418706258138 Test Loss: 0.3846956651475694\n",
      "Epoch: 10143 Training Loss: 0.12600305599636502 Test Loss: 0.38431556532118055\n",
      "Epoch: 10144 Training Loss: 0.12592994435628255 Test Loss: 0.383519287109375\n",
      "Epoch: 10145 Training Loss: 0.1256349826388889 Test Loss: 0.38329581705729165\n",
      "Epoch: 10146 Training Loss: 0.1251644041273329 Test Loss: 0.3831759168836806\n",
      "Epoch: 10147 Training Loss: 0.12458902825249565 Test Loss: 0.38291240776909724\n",
      "Epoch: 10148 Training Loss: 0.12395019192165799 Test Loss: 0.38161998155381943\n",
      "Epoch: 10149 Training Loss: 0.12328996276855468 Test Loss: 0.38015174696180554\n",
      "Epoch: 10150 Training Loss: 0.12266369459364149 Test Loss: 0.37825393337673613\n",
      "Epoch: 10151 Training Loss: 0.12205989668104383 Test Loss: 0.37623353407118054\n",
      "Epoch: 10152 Training Loss: 0.1215179926554362 Test Loss: 0.3745795355902778\n",
      "Epoch: 10153 Training Loss: 0.12100573984781901 Test Loss: 0.37351264105902776\n",
      "Epoch: 10154 Training Loss: 0.12053148905436198 Test Loss: 0.37293524848090276\n",
      "Epoch: 10155 Training Loss: 0.12011319902208116 Test Loss: 0.372544677734375\n",
      "Epoch: 10156 Training Loss: 0.11975710890028211 Test Loss: 0.37265201822916666\n",
      "Epoch: 10157 Training Loss: 0.11945800442165799 Test Loss: 0.3728813205295139\n",
      "Epoch: 10158 Training Loss: 0.11921034240722657 Test Loss: 0.37365440538194444\n",
      "Epoch: 10159 Training Loss: 0.11898428938123914 Test Loss: 0.37437581380208335\n",
      "Epoch: 10160 Training Loss: 0.1187808592054579 Test Loss: 0.37466986762152776\n",
      "Epoch: 10161 Training Loss: 0.11858790588378906 Test Loss: 0.37473828125\n",
      "Epoch: 10162 Training Loss: 0.11840193769666883 Test Loss: 0.37452655707465277\n",
      "Epoch: 10163 Training Loss: 0.11822887929280598 Test Loss: 0.37429806857638886\n",
      "Epoch: 10164 Training Loss: 0.11806148868136936 Test Loss: 0.3738840060763889\n",
      "Epoch: 10165 Training Loss: 0.1179087888929579 Test Loss: 0.37327088758680554\n",
      "Epoch: 10166 Training Loss: 0.11776947953965929 Test Loss: 0.3728276638454861\n",
      "Epoch: 10167 Training Loss: 0.1176313696967231 Test Loss: 0.3727096354166667\n",
      "Epoch: 10168 Training Loss: 0.11750022803412544 Test Loss: 0.3726142578125\n",
      "Epoch: 10169 Training Loss: 0.11738426632351345 Test Loss: 0.3724948187934028\n",
      "Epoch: 10170 Training Loss: 0.1172672831217448 Test Loss: 0.37262958441840277\n",
      "Epoch: 10171 Training Loss: 0.1171586422390408 Test Loss: 0.3727984483506944\n",
      "Epoch: 10172 Training Loss: 0.11705789947509766 Test Loss: 0.37312044270833333\n",
      "Epoch: 10173 Training Loss: 0.11696386888292101 Test Loss: 0.3734980740017361\n",
      "Epoch: 10174 Training Loss: 0.11687636481391059 Test Loss: 0.37396427408854166\n",
      "Epoch: 10175 Training Loss: 0.11680098470052083 Test Loss: 0.3745609809027778\n",
      "Epoch: 10176 Training Loss: 0.11674552408854166 Test Loss: 0.3752491319444444\n",
      "Epoch: 10177 Training Loss: 0.1167046635945638 Test Loss: 0.37562928602430556\n",
      "Epoch: 10178 Training Loss: 0.11667732153998481 Test Loss: 0.37612782118055554\n",
      "Epoch: 10179 Training Loss: 0.11667137739393446 Test Loss: 0.3762836371527778\n",
      "Epoch: 10180 Training Loss: 0.1166675304836697 Test Loss: 0.3762848307291667\n",
      "Epoch: 10181 Training Loss: 0.11668233744303386 Test Loss: 0.37615055338541664\n",
      "Epoch: 10182 Training Loss: 0.11671481662326388 Test Loss: 0.3759871419270833\n",
      "Epoch: 10183 Training Loss: 0.11676042938232421 Test Loss: 0.37580213758680553\n",
      "Epoch: 10184 Training Loss: 0.11682760789659288 Test Loss: 0.37583707682291667\n",
      "Epoch: 10185 Training Loss: 0.11691082170274522 Test Loss: 0.3759729275173611\n",
      "Epoch: 10186 Training Loss: 0.11702484130859375 Test Loss: 0.3761682400173611\n",
      "Epoch: 10187 Training Loss: 0.11715849982367621 Test Loss: 0.3765010036892361\n",
      "Epoch: 10188 Training Loss: 0.11730983395046658 Test Loss: 0.3769494900173611\n",
      "Epoch: 10189 Training Loss: 0.11746957651774088 Test Loss: 0.37763525390625\n",
      "Epoch: 10190 Training Loss: 0.11763788774278429 Test Loss: 0.37842941623263887\n",
      "Epoch: 10191 Training Loss: 0.11782770453559027 Test Loss: 0.3790218370225694\n",
      "Epoch: 10192 Training Loss: 0.11804158274332682 Test Loss: 0.37967011176215276\n",
      "Epoch: 10193 Training Loss: 0.11828654988606771 Test Loss: 0.38018663194444446\n",
      "Epoch: 10194 Training Loss: 0.11857173156738281 Test Loss: 0.38026478407118053\n",
      "Epoch: 10195 Training Loss: 0.11887363603379991 Test Loss: 0.3798225640190972\n",
      "Epoch: 10196 Training Loss: 0.11920971086290147 Test Loss: 0.37878987630208333\n",
      "Epoch: 10197 Training Loss: 0.11954852294921875 Test Loss: 0.3772333170572917\n",
      "Epoch: 10198 Training Loss: 0.11987605285644531 Test Loss: 0.3753107096354167\n",
      "Epoch: 10199 Training Loss: 0.12016312154134115 Test Loss: 0.373485107421875\n",
      "Epoch: 10200 Training Loss: 0.12041345130072699 Test Loss: 0.37204964192708334\n",
      "Epoch: 10201 Training Loss: 0.12058042822943793 Test Loss: 0.3714798177083333\n",
      "Epoch: 10202 Training Loss: 0.12063924662272135 Test Loss: 0.3714908854166667\n",
      "Epoch: 10203 Training Loss: 0.1206054695977105 Test Loss: 0.37241609700520834\n",
      "Epoch: 10204 Training Loss: 0.12050443776448568 Test Loss: 0.3738649631076389\n",
      "Epoch: 10205 Training Loss: 0.12034829796685113 Test Loss: 0.3753576388888889\n",
      "Epoch: 10206 Training Loss: 0.12015727912055121 Test Loss: 0.3764215494791667\n",
      "Epoch: 10207 Training Loss: 0.11994117652045357 Test Loss: 0.3767606879340278\n",
      "Epoch: 10208 Training Loss: 0.11971441989474826 Test Loss: 0.3766742350260417\n",
      "Epoch: 10209 Training Loss: 0.11946248457166883 Test Loss: 0.37571454535590276\n",
      "Epoch: 10210 Training Loss: 0.11918284946017794 Test Loss: 0.3748307020399306\n",
      "Epoch: 10211 Training Loss: 0.11890467325846354 Test Loss: 0.37401567925347223\n",
      "Epoch: 10212 Training Loss: 0.11863658481174046 Test Loss: 0.3737248263888889\n",
      "Epoch: 10213 Training Loss: 0.11840675269232856 Test Loss: 0.3736726888020833\n",
      "Epoch: 10214 Training Loss: 0.1182318361070421 Test Loss: 0.3742585991753472\n",
      "Epoch: 10215 Training Loss: 0.1181245600382487 Test Loss: 0.3752688259548611\n",
      "Epoch: 10216 Training Loss: 0.11807247161865235 Test Loss: 0.37637790256076387\n",
      "Epoch: 10217 Training Loss: 0.11803736114501953 Test Loss: 0.37725992838541667\n",
      "Epoch: 10218 Training Loss: 0.1180096901787652 Test Loss: 0.3775387369791667\n",
      "Epoch: 10219 Training Loss: 0.11796144612630208 Test Loss: 0.3771516384548611\n",
      "Epoch: 10220 Training Loss: 0.11790604994032118 Test Loss: 0.37684857855902776\n",
      "Epoch: 10221 Training Loss: 0.11785250769721137 Test Loss: 0.37607112630208334\n",
      "Epoch: 10222 Training Loss: 0.11779103088378906 Test Loss: 0.37520692274305556\n",
      "Epoch: 10223 Training Loss: 0.11773533799913194 Test Loss: 0.37448082139756944\n",
      "Epoch: 10224 Training Loss: 0.11769340006510416 Test Loss: 0.3736875\n",
      "Epoch: 10225 Training Loss: 0.11767507595486111 Test Loss: 0.3732061089409722\n",
      "Epoch: 10226 Training Loss: 0.11766141255696615 Test Loss: 0.37311241319444444\n",
      "Epoch: 10227 Training Loss: 0.1176642574734158 Test Loss: 0.3732577039930556\n",
      "Epoch: 10228 Training Loss: 0.11767034996880425 Test Loss: 0.37330061848958335\n",
      "Epoch: 10229 Training Loss: 0.11767756737603081 Test Loss: 0.37350227864583335\n",
      "Epoch: 10230 Training Loss: 0.11769085099962022 Test Loss: 0.37375124782986113\n",
      "Epoch: 10231 Training Loss: 0.11771341366238064 Test Loss: 0.3738425835503472\n",
      "Epoch: 10232 Training Loss: 0.11776207394070096 Test Loss: 0.3737096354166667\n",
      "Epoch: 10233 Training Loss: 0.11781803978814019 Test Loss: 0.37374541558159724\n",
      "Epoch: 10234 Training Loss: 0.11788071441650391 Test Loss: 0.37370789930555554\n",
      "Epoch: 10235 Training Loss: 0.11795173136393229 Test Loss: 0.3740534396701389\n",
      "Epoch: 10236 Training Loss: 0.11804249911838108 Test Loss: 0.3746769205729167\n",
      "Epoch: 10237 Training Loss: 0.1181651857164171 Test Loss: 0.375434326171875\n",
      "Epoch: 10238 Training Loss: 0.1183064939710829 Test Loss: 0.376234375\n",
      "Epoch: 10239 Training Loss: 0.1184531029595269 Test Loss: 0.3774660915798611\n",
      "Epoch: 10240 Training Loss: 0.11858908335367839 Test Loss: 0.3784772135416667\n",
      "Epoch: 10241 Training Loss: 0.11867463853624131 Test Loss: 0.37927582465277776\n",
      "Epoch: 10242 Training Loss: 0.11871955871582031 Test Loss: 0.37959307183159724\n",
      "Epoch: 10243 Training Loss: 0.11872939978705511 Test Loss: 0.3794357096354167\n",
      "Epoch: 10244 Training Loss: 0.11872271813286675 Test Loss: 0.3790535481770833\n",
      "Epoch: 10245 Training Loss: 0.11870319196912978 Test Loss: 0.3782655707465278\n",
      "Epoch: 10246 Training Loss: 0.11867599402533636 Test Loss: 0.37738473849826387\n",
      "Epoch: 10247 Training Loss: 0.11862510426839193 Test Loss: 0.3763914388020833\n",
      "Epoch: 10248 Training Loss: 0.11856129116482204 Test Loss: 0.3755685221354167\n",
      "Epoch: 10249 Training Loss: 0.11847318183051216 Test Loss: 0.3747724609375\n",
      "Epoch: 10250 Training Loss: 0.11837145487467447 Test Loss: 0.37427802191840276\n",
      "Epoch: 10251 Training Loss: 0.11827753533257379 Test Loss: 0.3740336642795139\n",
      "Epoch: 10252 Training Loss: 0.11817835405137804 Test Loss: 0.37410489908854166\n",
      "Epoch: 10253 Training Loss: 0.11810236104329427 Test Loss: 0.37458349609375\n",
      "Epoch: 10254 Training Loss: 0.1180348392062717 Test Loss: 0.37516438802083335\n",
      "Epoch: 10255 Training Loss: 0.11798172590467665 Test Loss: 0.3758767903645833\n",
      "Epoch: 10256 Training Loss: 0.11793067338731554 Test Loss: 0.376665283203125\n",
      "Epoch: 10257 Training Loss: 0.11788184611002604 Test Loss: 0.3773718532986111\n",
      "Epoch: 10258 Training Loss: 0.1178260023328993 Test Loss: 0.37826011827256945\n",
      "Epoch: 10259 Training Loss: 0.11779090711805555 Test Loss: 0.37902669270833333\n",
      "Epoch: 10260 Training Loss: 0.11777831607394748 Test Loss: 0.3797572428385417\n",
      "Epoch: 10261 Training Loss: 0.11779430728488498 Test Loss: 0.38035655381944444\n",
      "Epoch: 10262 Training Loss: 0.11786323632134331 Test Loss: 0.3807237413194444\n",
      "Epoch: 10263 Training Loss: 0.1179754138522678 Test Loss: 0.38080414496527776\n",
      "Epoch: 10264 Training Loss: 0.11815045420328776 Test Loss: 0.38060801866319444\n",
      "Epoch: 10265 Training Loss: 0.11838653225368924 Test Loss: 0.38027650282118053\n",
      "Epoch: 10266 Training Loss: 0.1186522216796875 Test Loss: 0.37946272786458335\n",
      "Epoch: 10267 Training Loss: 0.11894061194525825 Test Loss: 0.3786548394097222\n",
      "Epoch: 10268 Training Loss: 0.11922363874647353 Test Loss: 0.3779739040798611\n",
      "Epoch: 10269 Training Loss: 0.11951177639431423 Test Loss: 0.3773365885416667\n",
      "Epoch: 10270 Training Loss: 0.11978202819824219 Test Loss: 0.37703667534722224\n",
      "Epoch: 10271 Training Loss: 0.12004031372070313 Test Loss: 0.37681401909722223\n",
      "Epoch: 10272 Training Loss: 0.1202949939303928 Test Loss: 0.37677029079861113\n",
      "Epoch: 10273 Training Loss: 0.12053255801730686 Test Loss: 0.37710389539930556\n",
      "Epoch: 10274 Training Loss: 0.12074529435899523 Test Loss: 0.3780505642361111\n",
      "Epoch: 10275 Training Loss: 0.12095747460259332 Test Loss: 0.37930528428819443\n",
      "Epoch: 10276 Training Loss: 0.1211494886610243 Test Loss: 0.3813068576388889\n",
      "Epoch: 10277 Training Loss: 0.12141325887044271 Test Loss: 0.38222585720486113\n",
      "Epoch: 10278 Training Loss: 0.1217353973388672 Test Loss: 0.3831861979166667\n",
      "Epoch: 10279 Training Loss: 0.12188178422715928 Test Loss: 0.3840984157986111\n",
      "Epoch: 10280 Training Loss: 0.12199269273546007 Test Loss: 0.3838065592447917\n",
      "Epoch: 10281 Training Loss: 0.12206682840983073 Test Loss: 0.38318023003472224\n",
      "Epoch: 10282 Training Loss: 0.1220822279188368 Test Loss: 0.3824449869791667\n",
      "Epoch: 10283 Training Loss: 0.12207290818956164 Test Loss: 0.38190003797743055\n",
      "Epoch: 10284 Training Loss: 0.12202056715223525 Test Loss: 0.3813982747395833\n",
      "Epoch: 10285 Training Loss: 0.12193343861897786 Test Loss: 0.3807472873263889\n",
      "Epoch: 10286 Training Loss: 0.12182464006212022 Test Loss: 0.37991229926215275\n",
      "Epoch: 10287 Training Loss: 0.12172679562038846 Test Loss: 0.3790663519965278\n",
      "Epoch: 10288 Training Loss: 0.1216028052435981 Test Loss: 0.3782398546006944\n",
      "Epoch: 10289 Training Loss: 0.12147195688883464 Test Loss: 0.37785118272569446\n",
      "Epoch: 10290 Training Loss: 0.12133630879720052 Test Loss: 0.37809247504340276\n",
      "Epoch: 10291 Training Loss: 0.12120423550075955 Test Loss: 0.37920711263020834\n",
      "Epoch: 10292 Training Loss: 0.12109966956244575 Test Loss: 0.3802106662326389\n",
      "Epoch: 10293 Training Loss: 0.12101192474365234 Test Loss: 0.38130433485243054\n",
      "Epoch: 10294 Training Loss: 0.12090320756700304 Test Loss: 0.38249544270833336\n",
      "Epoch: 10295 Training Loss: 0.1208160163031684 Test Loss: 0.38290741644965276\n",
      "Epoch: 10296 Training Loss: 0.12073817782931857 Test Loss: 0.3834351671006944\n",
      "Epoch: 10297 Training Loss: 0.12067217000325521 Test Loss: 0.3838205295138889\n",
      "Epoch: 10298 Training Loss: 0.1206322038438585 Test Loss: 0.3837632378472222\n",
      "Epoch: 10299 Training Loss: 0.12063707394070096 Test Loss: 0.38346451822916666\n",
      "Epoch: 10300 Training Loss: 0.1206703847249349 Test Loss: 0.3828246527777778\n",
      "Epoch: 10301 Training Loss: 0.12074747890896267 Test Loss: 0.38196511501736113\n",
      "Epoch: 10302 Training Loss: 0.12087114376491971 Test Loss: 0.3811376139322917\n",
      "Epoch: 10303 Training Loss: 0.12106613413492838 Test Loss: 0.38048876953125\n",
      "Epoch: 10304 Training Loss: 0.12132751973470052 Test Loss: 0.38010413953993055\n",
      "Epoch: 10305 Training Loss: 0.12164715576171875 Test Loss: 0.37987760416666666\n",
      "Epoch: 10306 Training Loss: 0.12203278435601128 Test Loss: 0.3801323784722222\n",
      "Epoch: 10307 Training Loss: 0.1224532953898112 Test Loss: 0.3809166666666667\n",
      "Epoch: 10308 Training Loss: 0.12293258497450087 Test Loss: 0.3823041178385417\n",
      "Epoch: 10309 Training Loss: 0.12342422909206814 Test Loss: 0.38427113172743055\n",
      "Epoch: 10310 Training Loss: 0.12391987609863281 Test Loss: 0.38656599934895836\n",
      "Epoch: 10311 Training Loss: 0.12439435238308377 Test Loss: 0.3889577365451389\n",
      "Epoch: 10312 Training Loss: 0.12484263865152995 Test Loss: 0.3904009060329861\n",
      "Epoch: 10313 Training Loss: 0.1251966306898329 Test Loss: 0.39166118706597225\n",
      "Epoch: 10314 Training Loss: 0.1254348898993598 Test Loss: 0.39332896592881944\n",
      "Epoch: 10315 Training Loss: 0.12568231879340278 Test Loss: 0.3939807400173611\n",
      "Epoch: 10316 Training Loss: 0.12585544670952692 Test Loss: 0.39266221788194444\n",
      "Epoch: 10317 Training Loss: 0.125962525261773 Test Loss: 0.39054329427083334\n",
      "Epoch: 10318 Training Loss: 0.1259607933892144 Test Loss: 0.3864966362847222\n",
      "Epoch: 10319 Training Loss: 0.12576434750027127 Test Loss: 0.38218277994791666\n",
      "Epoch: 10320 Training Loss: 0.12525467258029513 Test Loss: 0.37890616861979165\n",
      "Epoch: 10321 Training Loss: 0.12442901187472873 Test Loss: 0.37793017578125\n",
      "Epoch: 10322 Training Loss: 0.12347277662489149 Test Loss: 0.3781474880642361\n",
      "Epoch: 10323 Training Loss: 0.12253732130262587 Test Loss: 0.37877582465277776\n",
      "Epoch: 10324 Training Loss: 0.12165167405870225 Test Loss: 0.380203125\n",
      "Epoch: 10325 Training Loss: 0.12082215203179253 Test Loss: 0.3812202962239583\n",
      "Epoch: 10326 Training Loss: 0.12006073252360026 Test Loss: 0.38149793836805557\n",
      "Epoch: 10327 Training Loss: 0.11938544379340278 Test Loss: 0.3815264214409722\n",
      "Epoch: 10328 Training Loss: 0.11883804066975912 Test Loss: 0.38123876953125\n",
      "Epoch: 10329 Training Loss: 0.11840648312038846 Test Loss: 0.3810661078559028\n",
      "Epoch: 10330 Training Loss: 0.11806703694661458 Test Loss: 0.38083710394965276\n",
      "Epoch: 10331 Training Loss: 0.11784047444661458 Test Loss: 0.3807440321180556\n",
      "Epoch: 10332 Training Loss: 0.11769928826226128 Test Loss: 0.3805559353298611\n",
      "Epoch: 10333 Training Loss: 0.117600952996148 Test Loss: 0.3803904079861111\n",
      "Epoch: 10334 Training Loss: 0.11756682840983072 Test Loss: 0.38067613389756944\n",
      "Epoch: 10335 Training Loss: 0.11758148023817275 Test Loss: 0.3807558051215278\n",
      "Epoch: 10336 Training Loss: 0.11762666236029731 Test Loss: 0.3808642306857639\n",
      "Epoch: 10337 Training Loss: 0.11768691507975261 Test Loss: 0.38106138780381943\n",
      "Epoch: 10338 Training Loss: 0.11776797739664714 Test Loss: 0.3810735134548611\n",
      "Epoch: 10339 Training Loss: 0.1178552017211914 Test Loss: 0.3812828776041667\n",
      "Epoch: 10340 Training Loss: 0.11794771406385633 Test Loss: 0.3815029296875\n",
      "Epoch: 10341 Training Loss: 0.11802973853217232 Test Loss: 0.38175786675347223\n",
      "Epoch: 10342 Training Loss: 0.11810548909505209 Test Loss: 0.3823515625\n",
      "Epoch: 10343 Training Loss: 0.11819942898220485 Test Loss: 0.3831003689236111\n",
      "Epoch: 10344 Training Loss: 0.11829457261827257 Test Loss: 0.3837296820746528\n",
      "Epoch: 10345 Training Loss: 0.1184021725124783 Test Loss: 0.3843005913628472\n",
      "Epoch: 10346 Training Loss: 0.11852395714653863 Test Loss: 0.3847199435763889\n",
      "Epoch: 10347 Training Loss: 0.1186635971069336 Test Loss: 0.38477501085069443\n",
      "Epoch: 10348 Training Loss: 0.11884662967258029 Test Loss: 0.3848489040798611\n",
      "Epoch: 10349 Training Loss: 0.11906588406032986 Test Loss: 0.38477067057291664\n",
      "Epoch: 10350 Training Loss: 0.11932992129855685 Test Loss: 0.3845743815104167\n",
      "Epoch: 10351 Training Loss: 0.11962418874104817 Test Loss: 0.38415787760416664\n",
      "Epoch: 10352 Training Loss: 0.11997914293077257 Test Loss: 0.38375547960069445\n",
      "Epoch: 10353 Training Loss: 0.12036941189236111 Test Loss: 0.3826139865451389\n",
      "Epoch: 10354 Training Loss: 0.12078541904025608 Test Loss: 0.3814901801215278\n",
      "Epoch: 10355 Training Loss: 0.12121732160780165 Test Loss: 0.3802997775607639\n",
      "Epoch: 10356 Training Loss: 0.12161216311984592 Test Loss: 0.37940972222222225\n",
      "Epoch: 10357 Training Loss: 0.1219412129720052 Test Loss: 0.3790451388888889\n",
      "Epoch: 10358 Training Loss: 0.12217169274224175 Test Loss: 0.37931366644965275\n",
      "Epoch: 10359 Training Loss: 0.12230221896701389 Test Loss: 0.38028011067708334\n",
      "Epoch: 10360 Training Loss: 0.12232165696885851 Test Loss: 0.38193454318576386\n",
      "Epoch: 10361 Training Loss: 0.12229973941379123 Test Loss: 0.38384315321180557\n",
      "Epoch: 10362 Training Loss: 0.12220171017116971 Test Loss: 0.3858073187934028\n",
      "Epoch: 10363 Training Loss: 0.12207822672526042 Test Loss: 0.38685400390625\n",
      "Epoch: 10364 Training Loss: 0.12193086242675781 Test Loss: 0.3872195095486111\n",
      "Epoch: 10365 Training Loss: 0.12179598829481336 Test Loss: 0.38655582682291667\n",
      "Epoch: 10366 Training Loss: 0.12165372297498915 Test Loss: 0.3853395724826389\n",
      "Epoch: 10367 Training Loss: 0.12151799943712023 Test Loss: 0.3840805121527778\n",
      "Epoch: 10368 Training Loss: 0.12139972941080729 Test Loss: 0.38275377061631943\n",
      "Epoch: 10369 Training Loss: 0.12127894677056207 Test Loss: 0.38176057942708336\n",
      "Epoch: 10370 Training Loss: 0.12115431722005209 Test Loss: 0.38104549153645834\n",
      "Epoch: 10371 Training Loss: 0.12102925025092232 Test Loss: 0.38053786892361113\n",
      "Epoch: 10372 Training Loss: 0.12091685401068794 Test Loss: 0.3799025065104167\n",
      "Epoch: 10373 Training Loss: 0.12082996707492405 Test Loss: 0.3794211154513889\n",
      "Epoch: 10374 Training Loss: 0.12077507697211372 Test Loss: 0.3792565646701389\n",
      "Epoch: 10375 Training Loss: 0.12076411607530382 Test Loss: 0.37911018880208336\n",
      "Epoch: 10376 Training Loss: 0.12078512149386936 Test Loss: 0.37897998046875\n",
      "Epoch: 10377 Training Loss: 0.12085216013590495 Test Loss: 0.3789191080729167\n",
      "Epoch: 10378 Training Loss: 0.1209671885172526 Test Loss: 0.3788087565104167\n",
      "Epoch: 10379 Training Loss: 0.12107917870415581 Test Loss: 0.378691162109375\n",
      "Epoch: 10380 Training Loss: 0.121199951171875 Test Loss: 0.3787453884548611\n",
      "Epoch: 10381 Training Loss: 0.12133067915174696 Test Loss: 0.3792380099826389\n",
      "Epoch: 10382 Training Loss: 0.12144549984402127 Test Loss: 0.3801216091579861\n",
      "Epoch: 10383 Training Loss: 0.12155256398518881 Test Loss: 0.3811955295138889\n",
      "Epoch: 10384 Training Loss: 0.12161292182074653 Test Loss: 0.3825732150607639\n",
      "Epoch: 10385 Training Loss: 0.1216714129977756 Test Loss: 0.3843607584635417\n",
      "Epoch: 10386 Training Loss: 0.12172048780653212 Test Loss: 0.38607183159722225\n",
      "Epoch: 10387 Training Loss: 0.12176104566786024 Test Loss: 0.38783121744791665\n",
      "Epoch: 10388 Training Loss: 0.12181177775065104 Test Loss: 0.38958029513888887\n",
      "Epoch: 10389 Training Loss: 0.12186411963568794 Test Loss: 0.3909228515625\n",
      "Epoch: 10390 Training Loss: 0.12192937893337674 Test Loss: 0.39178618706597224\n",
      "Epoch: 10391 Training Loss: 0.1219809332953559 Test Loss: 0.39175651041666665\n",
      "Epoch: 10392 Training Loss: 0.12201308865017362 Test Loss: 0.39122235785590276\n",
      "Epoch: 10393 Training Loss: 0.12200582716200087 Test Loss: 0.3899167751736111\n",
      "Epoch: 10394 Training Loss: 0.12195186360677084 Test Loss: 0.388264892578125\n",
      "Epoch: 10395 Training Loss: 0.12181433868408204 Test Loss: 0.38612744140625\n",
      "Epoch: 10396 Training Loss: 0.12158734469943576 Test Loss: 0.38360091145833336\n",
      "Epoch: 10397 Training Loss: 0.1212820061577691 Test Loss: 0.3812596842447917\n",
      "Epoch: 10398 Training Loss: 0.1209422861735026 Test Loss: 0.3794255642361111\n",
      "Epoch: 10399 Training Loss: 0.12058142937554253 Test Loss: 0.37759912109375\n",
      "Epoch: 10400 Training Loss: 0.12021484375 Test Loss: 0.3764998101128472\n",
      "Epoch: 10401 Training Loss: 0.11984191131591797 Test Loss: 0.37588568793402777\n",
      "Epoch: 10402 Training Loss: 0.1194730453491211 Test Loss: 0.37556241861979167\n",
      "Epoch: 10403 Training Loss: 0.11912825605604384 Test Loss: 0.37551019965277777\n",
      "Epoch: 10404 Training Loss: 0.11881823560926649 Test Loss: 0.375732177734375\n",
      "Epoch: 10405 Training Loss: 0.11854707166883681 Test Loss: 0.37573787434895833\n",
      "Epoch: 10406 Training Loss: 0.11831419118245443 Test Loss: 0.3760461154513889\n",
      "Epoch: 10407 Training Loss: 0.11811830393473308 Test Loss: 0.37616897243923614\n",
      "Epoch: 10408 Training Loss: 0.11793690999348959 Test Loss: 0.37616748046875\n",
      "Epoch: 10409 Training Loss: 0.11778186374240451 Test Loss: 0.3763021918402778\n",
      "Epoch: 10410 Training Loss: 0.11769312795003255 Test Loss: 0.37640264214409724\n",
      "Epoch: 10411 Training Loss: 0.1176586185031467 Test Loss: 0.37655523003472224\n",
      "Epoch: 10412 Training Loss: 0.11767281934950087 Test Loss: 0.37646714952256943\n",
      "Epoch: 10413 Training Loss: 0.1177421866522895 Test Loss: 0.37606903754340276\n",
      "Epoch: 10414 Training Loss: 0.11786304134792752 Test Loss: 0.3757616373697917\n",
      "Epoch: 10415 Training Loss: 0.11801748741997613 Test Loss: 0.3754233669704861\n",
      "Epoch: 10416 Training Loss: 0.11818045552571614 Test Loss: 0.374878662109375\n",
      "Epoch: 10417 Training Loss: 0.11834102121988932 Test Loss: 0.3745884060329861\n",
      "Epoch: 10418 Training Loss: 0.11849379052056207 Test Loss: 0.374318359375\n",
      "Epoch: 10419 Training Loss: 0.11864138624403212 Test Loss: 0.37419919162326387\n",
      "Epoch: 10420 Training Loss: 0.11876327345106337 Test Loss: 0.37445735677083336\n",
      "Epoch: 10421 Training Loss: 0.11886704762776693 Test Loss: 0.3746669379340278\n",
      "Epoch: 10422 Training Loss: 0.11893058437771267 Test Loss: 0.37521419270833334\n",
      "Epoch: 10423 Training Loss: 0.1190013682047526 Test Loss: 0.37557427300347224\n",
      "Epoch: 10424 Training Loss: 0.11906148105197482 Test Loss: 0.37602300347222223\n",
      "Epoch: 10425 Training Loss: 0.11910830349392361 Test Loss: 0.3763213161892361\n",
      "Epoch: 10426 Training Loss: 0.11915709686279297 Test Loss: 0.37675547960069444\n",
      "Epoch: 10427 Training Loss: 0.11922145673963759 Test Loss: 0.3769665256076389\n",
      "Epoch: 10428 Training Loss: 0.11929876708984374 Test Loss: 0.3773952907986111\n",
      "Epoch: 10429 Training Loss: 0.11937213728162978 Test Loss: 0.3774450412326389\n",
      "Epoch: 10430 Training Loss: 0.11947045644124349 Test Loss: 0.3775437282986111\n",
      "Epoch: 10431 Training Loss: 0.11957779439290364 Test Loss: 0.3774535590277778\n",
      "Epoch: 10432 Training Loss: 0.11971212514241536 Test Loss: 0.3773505859375\n",
      "Epoch: 10433 Training Loss: 0.11986451382107205 Test Loss: 0.37697325303819446\n",
      "Epoch: 10434 Training Loss: 0.12002993520100912 Test Loss: 0.37666856553819444\n",
      "Epoch: 10435 Training Loss: 0.12020878431532118 Test Loss: 0.37630311414930556\n",
      "Epoch: 10436 Training Loss: 0.12037412685818143 Test Loss: 0.3757595486111111\n",
      "Epoch: 10437 Training Loss: 0.12054354180230034 Test Loss: 0.3752365451388889\n",
      "Epoch: 10438 Training Loss: 0.12073187255859374 Test Loss: 0.37529893663194447\n",
      "Epoch: 10439 Training Loss: 0.1209323976304796 Test Loss: 0.3758729383680556\n",
      "Epoch: 10440 Training Loss: 0.12111497158474392 Test Loss: 0.37646023220486113\n",
      "Epoch: 10441 Training Loss: 0.1212355456882053 Test Loss: 0.37752804904513887\n",
      "Epoch: 10442 Training Loss: 0.12128908454047309 Test Loss: 0.3785493706597222\n",
      "Epoch: 10443 Training Loss: 0.12124693976508247 Test Loss: 0.3795480143229167\n",
      "Epoch: 10444 Training Loss: 0.12110716332329644 Test Loss: 0.38011385091145833\n",
      "Epoch: 10445 Training Loss: 0.1208977042304145 Test Loss: 0.37984792751736113\n",
      "Epoch: 10446 Training Loss: 0.12066446261935763 Test Loss: 0.37928200954861113\n",
      "Epoch: 10447 Training Loss: 0.12041637335883247 Test Loss: 0.3784019639756944\n",
      "Epoch: 10448 Training Loss: 0.12011187744140625 Test Loss: 0.377344970703125\n",
      "Epoch: 10449 Training Loss: 0.11975352647569444 Test Loss: 0.3763364529079861\n",
      "Epoch: 10450 Training Loss: 0.11938410356309678 Test Loss: 0.37540657552083334\n",
      "Epoch: 10451 Training Loss: 0.11898357815212673 Test Loss: 0.374798828125\n",
      "Epoch: 10452 Training Loss: 0.11855719502766927 Test Loss: 0.37446666124131944\n",
      "Epoch: 10453 Training Loss: 0.11811737145317926 Test Loss: 0.3743952907986111\n",
      "Epoch: 10454 Training Loss: 0.11768120659722223 Test Loss: 0.374689697265625\n",
      "Epoch: 10455 Training Loss: 0.11724618360731337 Test Loss: 0.37471373155381943\n",
      "Epoch: 10456 Training Loss: 0.11686538781060113 Test Loss: 0.374925537109375\n",
      "Epoch: 10457 Training Loss: 0.11652928670247396 Test Loss: 0.3748390842013889\n",
      "Epoch: 10458 Training Loss: 0.11623112996419271 Test Loss: 0.3749524739583333\n",
      "Epoch: 10459 Training Loss: 0.11599281480577257 Test Loss: 0.3749854871961806\n",
      "Epoch: 10460 Training Loss: 0.1158032709757487 Test Loss: 0.374822998046875\n",
      "Epoch: 10461 Training Loss: 0.11564505513509114 Test Loss: 0.3745703125\n",
      "Epoch: 10462 Training Loss: 0.11552931891547309 Test Loss: 0.37458100043402776\n",
      "Epoch: 10463 Training Loss: 0.11545646667480469 Test Loss: 0.3744883355034722\n",
      "Epoch: 10464 Training Loss: 0.11540376281738281 Test Loss: 0.37440828450520836\n",
      "Epoch: 10465 Training Loss: 0.11537649790445964 Test Loss: 0.37428776041666667\n",
      "Epoch: 10466 Training Loss: 0.1153537105984158 Test Loss: 0.3742739529079861\n",
      "Epoch: 10467 Training Loss: 0.11534264204237196 Test Loss: 0.37421131727430557\n",
      "Epoch: 10468 Training Loss: 0.11533753797743056 Test Loss: 0.37414906141493054\n",
      "Epoch: 10469 Training Loss: 0.11534181722005209 Test Loss: 0.374348876953125\n",
      "Epoch: 10470 Training Loss: 0.11536891682942708 Test Loss: 0.37444921875\n",
      "Epoch: 10471 Training Loss: 0.11541060638427734 Test Loss: 0.3746216091579861\n",
      "Epoch: 10472 Training Loss: 0.11547257826063367 Test Loss: 0.3749379611545139\n",
      "Epoch: 10473 Training Loss: 0.11556405893961588 Test Loss: 0.3753664279513889\n",
      "Epoch: 10474 Training Loss: 0.11568111250135633 Test Loss: 0.37572059461805557\n",
      "Epoch: 10475 Training Loss: 0.11582457224527995 Test Loss: 0.3762323676215278\n",
      "Epoch: 10476 Training Loss: 0.11598274739583334 Test Loss: 0.376745849609375\n",
      "Epoch: 10477 Training Loss: 0.11617192247178819 Test Loss: 0.37724815538194445\n",
      "Epoch: 10478 Training Loss: 0.11638380008273655 Test Loss: 0.37748082139756944\n",
      "Epoch: 10479 Training Loss: 0.11662074703640408 Test Loss: 0.3777213541666667\n",
      "Epoch: 10480 Training Loss: 0.11686024220784505 Test Loss: 0.37749224175347224\n",
      "Epoch: 10481 Training Loss: 0.1170928717719184 Test Loss: 0.3773300509982639\n",
      "Epoch: 10482 Training Loss: 0.11731980133056641 Test Loss: 0.3770297309027778\n",
      "Epoch: 10483 Training Loss: 0.11755274285210504 Test Loss: 0.3765309787326389\n",
      "Epoch: 10484 Training Loss: 0.11777226087782118 Test Loss: 0.375994140625\n",
      "Epoch: 10485 Training Loss: 0.1179827406141493 Test Loss: 0.3758135036892361\n",
      "Epoch: 10486 Training Loss: 0.11816700998942058 Test Loss: 0.37521525065104167\n",
      "Epoch: 10487 Training Loss: 0.11833481089274088 Test Loss: 0.37503065321180556\n",
      "Epoch: 10488 Training Loss: 0.11849158647325304 Test Loss: 0.3752998318142361\n",
      "Epoch: 10489 Training Loss: 0.11865807342529297 Test Loss: 0.3758658040364583\n",
      "Epoch: 10490 Training Loss: 0.11883556535508898 Test Loss: 0.3764430881076389\n",
      "Epoch: 10491 Training Loss: 0.119045285542806 Test Loss: 0.37715399848090275\n",
      "Epoch: 10492 Training Loss: 0.1192905519273546 Test Loss: 0.3778497992621528\n",
      "Epoch: 10493 Training Loss: 0.11956515842013889 Test Loss: 0.378315673828125\n",
      "Epoch: 10494 Training Loss: 0.1198391342163086 Test Loss: 0.3789313693576389\n",
      "Epoch: 10495 Training Loss: 0.12009251658121745 Test Loss: 0.37964879014756947\n",
      "Epoch: 10496 Training Loss: 0.12030824534098307 Test Loss: 0.38028428819444443\n",
      "Epoch: 10497 Training Loss: 0.12045406850179037 Test Loss: 0.3809840766059028\n",
      "Epoch: 10498 Training Loss: 0.12049746958414713 Test Loss: 0.3818644748263889\n",
      "Epoch: 10499 Training Loss: 0.12044496324327257 Test Loss: 0.3820794270833333\n",
      "Epoch: 10500 Training Loss: 0.12031206342909072 Test Loss: 0.3824130316840278\n",
      "Epoch: 10501 Training Loss: 0.12012190755208334 Test Loss: 0.3819845920138889\n",
      "Epoch: 10502 Training Loss: 0.11987872568766277 Test Loss: 0.3812217339409722\n",
      "Epoch: 10503 Training Loss: 0.11963875071207682 Test Loss: 0.3801254611545139\n",
      "Epoch: 10504 Training Loss: 0.11944714779324002 Test Loss: 0.3796638997395833\n",
      "Epoch: 10505 Training Loss: 0.11923817952473958 Test Loss: 0.3796318359375\n",
      "Epoch: 10506 Training Loss: 0.1190146704779731 Test Loss: 0.3800218370225694\n",
      "Epoch: 10507 Training Loss: 0.11881947157118056 Test Loss: 0.3809216579861111\n",
      "Epoch: 10508 Training Loss: 0.11862960476345485 Test Loss: 0.38189564344618054\n",
      "Epoch: 10509 Training Loss: 0.11841150241427952 Test Loss: 0.3833390842013889\n",
      "Epoch: 10510 Training Loss: 0.11818984137641059 Test Loss: 0.38457874891493055\n",
      "Epoch: 10511 Training Loss: 0.11796713172064888 Test Loss: 0.38594268120659725\n",
      "Epoch: 10512 Training Loss: 0.11774412112765842 Test Loss: 0.3869367133246528\n",
      "Epoch: 10513 Training Loss: 0.11751805029975043 Test Loss: 0.3873773600260417\n",
      "Epoch: 10514 Training Loss: 0.11730732981363932 Test Loss: 0.38743229166666665\n",
      "Epoch: 10515 Training Loss: 0.11711474015977648 Test Loss: 0.3867948404947917\n",
      "Epoch: 10516 Training Loss: 0.11693110656738281 Test Loss: 0.38580164930555555\n",
      "Epoch: 10517 Training Loss: 0.11675950113932292 Test Loss: 0.3843227267795139\n",
      "Epoch: 10518 Training Loss: 0.11660565524631077 Test Loss: 0.3828125\n",
      "Epoch: 10519 Training Loss: 0.11647555796305338 Test Loss: 0.3812834743923611\n",
      "Epoch: 10520 Training Loss: 0.11636671278211806 Test Loss: 0.37970133463541667\n",
      "Epoch: 10521 Training Loss: 0.11629150390625 Test Loss: 0.3781250542534722\n",
      "Epoch: 10522 Training Loss: 0.11623042551676432 Test Loss: 0.37680520290798614\n",
      "Epoch: 10523 Training Loss: 0.11618665737575955 Test Loss: 0.37574650065104165\n",
      "Epoch: 10524 Training Loss: 0.11617189788818359 Test Loss: 0.3749743381076389\n",
      "Epoch: 10525 Training Loss: 0.1162134051852756 Test Loss: 0.37474072265625\n",
      "Epoch: 10526 Training Loss: 0.11628216213650173 Test Loss: 0.37481697591145835\n",
      "Epoch: 10527 Training Loss: 0.11636334821912978 Test Loss: 0.3753783908420139\n",
      "Epoch: 10528 Training Loss: 0.11645538245307074 Test Loss: 0.37600675455729166\n",
      "Epoch: 10529 Training Loss: 0.11654945034450954 Test Loss: 0.3767778862847222\n",
      "Epoch: 10530 Training Loss: 0.11664484575059679 Test Loss: 0.37715709092881944\n",
      "Epoch: 10531 Training Loss: 0.11673729790581597 Test Loss: 0.3775443793402778\n",
      "Epoch: 10532 Training Loss: 0.11683481174045139 Test Loss: 0.37783672417534725\n",
      "Epoch: 10533 Training Loss: 0.11694845241970486 Test Loss: 0.37800520833333334\n",
      "Epoch: 10534 Training Loss: 0.11709820556640625 Test Loss: 0.3782421603732639\n",
      "Epoch: 10535 Training Loss: 0.11725647820366754 Test Loss: 0.37855257161458333\n",
      "Epoch: 10536 Training Loss: 0.1174190656873915 Test Loss: 0.3791399739583333\n",
      "Epoch: 10537 Training Loss: 0.11757300906711154 Test Loss: 0.37976036241319444\n",
      "Epoch: 10538 Training Loss: 0.11771460469563802 Test Loss: 0.3806018337673611\n",
      "Epoch: 10539 Training Loss: 0.11783124287923177 Test Loss: 0.38176866319444447\n",
      "Epoch: 10540 Training Loss: 0.1178978025648329 Test Loss: 0.3827267252604167\n",
      "Epoch: 10541 Training Loss: 0.11793237813313802 Test Loss: 0.38365125868055555\n",
      "Epoch: 10542 Training Loss: 0.11794677310519748 Test Loss: 0.3841613498263889\n",
      "Epoch: 10543 Training Loss: 0.11795877159966363 Test Loss: 0.3844598524305556\n",
      "Epoch: 10544 Training Loss: 0.1180045403374566 Test Loss: 0.3841046549479167\n",
      "Epoch: 10545 Training Loss: 0.1180873074001736 Test Loss: 0.3832822808159722\n",
      "Epoch: 10546 Training Loss: 0.11821092817518446 Test Loss: 0.38251453993055556\n",
      "Epoch: 10547 Training Loss: 0.11836364237467448 Test Loss: 0.3815486111111111\n",
      "Epoch: 10548 Training Loss: 0.11852000003390842 Test Loss: 0.3805231391059028\n",
      "Epoch: 10549 Training Loss: 0.11866611056857639 Test Loss: 0.37976481119791666\n",
      "Epoch: 10550 Training Loss: 0.11879352230495877 Test Loss: 0.37943760850694447\n",
      "Epoch: 10551 Training Loss: 0.11891621822781033 Test Loss: 0.38034982638888887\n",
      "Epoch: 10552 Training Loss: 0.11905670505099826 Test Loss: 0.3821797688802083\n",
      "Epoch: 10553 Training Loss: 0.1192068116929796 Test Loss: 0.38405013020833334\n",
      "Epoch: 10554 Training Loss: 0.11937105136447483 Test Loss: 0.38584686957465275\n",
      "Epoch: 10555 Training Loss: 0.11953826056586371 Test Loss: 0.386830322265625\n",
      "Epoch: 10556 Training Loss: 0.11969435373942057 Test Loss: 0.38719178602430554\n",
      "Epoch: 10557 Training Loss: 0.11979507700602214 Test Loss: 0.386688720703125\n",
      "Epoch: 10558 Training Loss: 0.11982739851209852 Test Loss: 0.38544227430555555\n",
      "Epoch: 10559 Training Loss: 0.11976127031114367 Test Loss: 0.384164794921875\n",
      "Epoch: 10560 Training Loss: 0.11956500244140625 Test Loss: 0.38308732096354164\n",
      "Epoch: 10561 Training Loss: 0.11925011783175998 Test Loss: 0.38257750108506944\n",
      "Epoch: 10562 Training Loss: 0.11886979251437717 Test Loss: 0.38243408203125\n",
      "Epoch: 10563 Training Loss: 0.11848332214355468 Test Loss: 0.3823906792534722\n",
      "Epoch: 10564 Training Loss: 0.1181515892876519 Test Loss: 0.38215983072916665\n",
      "Epoch: 10565 Training Loss: 0.1178762444390191 Test Loss: 0.3817499728732639\n",
      "Epoch: 10566 Training Loss: 0.11767017364501953 Test Loss: 0.3808298068576389\n",
      "Epoch: 10567 Training Loss: 0.11751812744140624 Test Loss: 0.37954144965277775\n",
      "Epoch: 10568 Training Loss: 0.11741391160753038 Test Loss: 0.37857587348090277\n",
      "Epoch: 10569 Training Loss: 0.11735651228162977 Test Loss: 0.3780139431423611\n",
      "Epoch: 10570 Training Loss: 0.11732334052191841 Test Loss: 0.37765245225694444\n",
      "Epoch: 10571 Training Loss: 0.11732245466444227 Test Loss: 0.37817873806423613\n",
      "Epoch: 10572 Training Loss: 0.11736850314670139 Test Loss: 0.3788906792534722\n",
      "Epoch: 10573 Training Loss: 0.11745244598388672 Test Loss: 0.37968885633680555\n",
      "Epoch: 10574 Training Loss: 0.11757166544596354 Test Loss: 0.38029866536458334\n",
      "Epoch: 10575 Training Loss: 0.11769435628255208 Test Loss: 0.3804786512586806\n",
      "Epoch: 10576 Training Loss: 0.11779780748155382 Test Loss: 0.37980753580729165\n",
      "Epoch: 10577 Training Loss: 0.1178584476047092 Test Loss: 0.378876953125\n",
      "Epoch: 10578 Training Loss: 0.11787603844536676 Test Loss: 0.3780857204861111\n",
      "Epoch: 10579 Training Loss: 0.11786212327745225 Test Loss: 0.37772067599826387\n",
      "Epoch: 10580 Training Loss: 0.11779063500298394 Test Loss: 0.3776123589409722\n",
      "Epoch: 10581 Training Loss: 0.11769480980767144 Test Loss: 0.37851877170138887\n",
      "Epoch: 10582 Training Loss: 0.11759985944959853 Test Loss: 0.37964637586805555\n",
      "Epoch: 10583 Training Loss: 0.11750523206922743 Test Loss: 0.3811151801215278\n",
      "Epoch: 10584 Training Loss: 0.11744204711914062 Test Loss: 0.38203092447916664\n",
      "Epoch: 10585 Training Loss: 0.11740223524305556 Test Loss: 0.3828625759548611\n",
      "Epoch: 10586 Training Loss: 0.11739161088731553 Test Loss: 0.38309016927083334\n",
      "Epoch: 10587 Training Loss: 0.11738055589463976 Test Loss: 0.3824951714409722\n",
      "Epoch: 10588 Training Loss: 0.11738011932373046 Test Loss: 0.38184627278645833\n",
      "Epoch: 10589 Training Loss: 0.11738631863064236 Test Loss: 0.38071525065104167\n",
      "Epoch: 10590 Training Loss: 0.11739373185899522 Test Loss: 0.3795654839409722\n",
      "Epoch: 10591 Training Loss: 0.117387941148546 Test Loss: 0.3784019639756944\n",
      "Epoch: 10592 Training Loss: 0.11738426801893447 Test Loss: 0.37755387369791665\n",
      "Epoch: 10593 Training Loss: 0.11740726470947266 Test Loss: 0.3769819878472222\n",
      "Epoch: 10594 Training Loss: 0.11746212938096788 Test Loss: 0.376785888671875\n",
      "Epoch: 10595 Training Loss: 0.1175487543741862 Test Loss: 0.3765781792534722\n",
      "Epoch: 10596 Training Loss: 0.11766716088189019 Test Loss: 0.3764456380208333\n",
      "Epoch: 10597 Training Loss: 0.1178401379055447 Test Loss: 0.3765967068142361\n",
      "Epoch: 10598 Training Loss: 0.11804557545979817 Test Loss: 0.3766974826388889\n",
      "Epoch: 10599 Training Loss: 0.1182873772515191 Test Loss: 0.3766885308159722\n",
      "Epoch: 10600 Training Loss: 0.11856452348497179 Test Loss: 0.37677354600694446\n",
      "Epoch: 10601 Training Loss: 0.11885878753662109 Test Loss: 0.37697553168402775\n",
      "Epoch: 10602 Training Loss: 0.11914107767740885 Test Loss: 0.3771380208333333\n",
      "Epoch: 10603 Training Loss: 0.11941059451633029 Test Loss: 0.3776475151909722\n",
      "Epoch: 10604 Training Loss: 0.11963114929199219 Test Loss: 0.3781713324652778\n",
      "Epoch: 10605 Training Loss: 0.11983468204074436 Test Loss: 0.3790594889322917\n",
      "Epoch: 10606 Training Loss: 0.11998614078097873 Test Loss: 0.38062128363715275\n",
      "Epoch: 10607 Training Loss: 0.12012176174587674 Test Loss: 0.38225173611111113\n",
      "Epoch: 10608 Training Loss: 0.1202398215399848 Test Loss: 0.38341357421875\n",
      "Epoch: 10609 Training Loss: 0.12030810970730252 Test Loss: 0.3845107421875\n",
      "Epoch: 10610 Training Loss: 0.1203276121351454 Test Loss: 0.3852516547309028\n",
      "Epoch: 10611 Training Loss: 0.12028716617160373 Test Loss: 0.38574842664930553\n",
      "Epoch: 10612 Training Loss: 0.12019440205891926 Test Loss: 0.38579383680555557\n",
      "Epoch: 10613 Training Loss: 0.12003243425157335 Test Loss: 0.3859150661892361\n",
      "Epoch: 10614 Training Loss: 0.11983758544921876 Test Loss: 0.38652275933159724\n",
      "Epoch: 10615 Training Loss: 0.11961654408772786 Test Loss: 0.3865667046440972\n",
      "Epoch: 10616 Training Loss: 0.11935741085476345 Test Loss: 0.38693375651041667\n",
      "Epoch: 10617 Training Loss: 0.11909098137749566 Test Loss: 0.3865561794704861\n",
      "Epoch: 10618 Training Loss: 0.11879658338758681 Test Loss: 0.38594536675347224\n",
      "Epoch: 10619 Training Loss: 0.11850947909884983 Test Loss: 0.3852058919270833\n",
      "Epoch: 10620 Training Loss: 0.11823474460177952 Test Loss: 0.3844178602430556\n",
      "Epoch: 10621 Training Loss: 0.1179822981092665 Test Loss: 0.38390473090277777\n",
      "Epoch: 10622 Training Loss: 0.11771998002794054 Test Loss: 0.38330067274305557\n",
      "Epoch: 10623 Training Loss: 0.11743878597683377 Test Loss: 0.3830283745659722\n",
      "Epoch: 10624 Training Loss: 0.11716480170355903 Test Loss: 0.3824430609809028\n",
      "Epoch: 10625 Training Loss: 0.11689314185248481 Test Loss: 0.38198491753472225\n",
      "Epoch: 10626 Training Loss: 0.1166318851047092 Test Loss: 0.38136414930555557\n",
      "Epoch: 10627 Training Loss: 0.11639029354519315 Test Loss: 0.3809870334201389\n",
      "Epoch: 10628 Training Loss: 0.11617971886528863 Test Loss: 0.38048868815104164\n",
      "Epoch: 10629 Training Loss: 0.11598099093967014 Test Loss: 0.3803219943576389\n",
      "Epoch: 10630 Training Loss: 0.11580416615804036 Test Loss: 0.3801346299913194\n",
      "Epoch: 10631 Training Loss: 0.11564769405788845 Test Loss: 0.3801810980902778\n",
      "Epoch: 10632 Training Loss: 0.11549893273247613 Test Loss: 0.38027628580729167\n",
      "Epoch: 10633 Training Loss: 0.11536284891764323 Test Loss: 0.3805129665798611\n",
      "Epoch: 10634 Training Loss: 0.11524708387586806 Test Loss: 0.3810253363715278\n",
      "Epoch: 10635 Training Loss: 0.11514505343967014 Test Loss: 0.3816594780815972\n",
      "Epoch: 10636 Training Loss: 0.11504069179958767 Test Loss: 0.38231032986111113\n",
      "Epoch: 10637 Training Loss: 0.11496676635742188 Test Loss: 0.38319509548611114\n",
      "Epoch: 10638 Training Loss: 0.1149024412367079 Test Loss: 0.3839608832465278\n",
      "Epoch: 10639 Training Loss: 0.11485775587293837 Test Loss: 0.3846595052083333\n",
      "Epoch: 10640 Training Loss: 0.11482316419813368 Test Loss: 0.38517588975694445\n",
      "Epoch: 10641 Training Loss: 0.11480185106065538 Test Loss: 0.38564946831597224\n",
      "Epoch: 10642 Training Loss: 0.11478102366129557 Test Loss: 0.3855677083333333\n",
      "Epoch: 10643 Training Loss: 0.11475933583577475 Test Loss: 0.3853869900173611\n",
      "Epoch: 10644 Training Loss: 0.11473905690511067 Test Loss: 0.3850530598958333\n",
      "Epoch: 10645 Training Loss: 0.11472628953721788 Test Loss: 0.3844399956597222\n",
      "Epoch: 10646 Training Loss: 0.11472613186306424 Test Loss: 0.38385313585069447\n",
      "Epoch: 10647 Training Loss: 0.11475211842854818 Test Loss: 0.38337613932291664\n",
      "Epoch: 10648 Training Loss: 0.11479589080810547 Test Loss: 0.3828248697916667\n",
      "Epoch: 10649 Training Loss: 0.11486874050564236 Test Loss: 0.3823883463541667\n",
      "Epoch: 10650 Training Loss: 0.11494815572102865 Test Loss: 0.3817451443142361\n",
      "Epoch: 10651 Training Loss: 0.11505330573187934 Test Loss: 0.38105013020833334\n",
      "Epoch: 10652 Training Loss: 0.1151779539320204 Test Loss: 0.3804092339409722\n",
      "Epoch: 10653 Training Loss: 0.11534901597764757 Test Loss: 0.37983336046006944\n",
      "Epoch: 10654 Training Loss: 0.11555208418104383 Test Loss: 0.3789514431423611\n",
      "Epoch: 10655 Training Loss: 0.1157887691921658 Test Loss: 0.37829752604166667\n",
      "Epoch: 10656 Training Loss: 0.11605562591552734 Test Loss: 0.3777346462673611\n",
      "Epoch: 10657 Training Loss: 0.11636310407850478 Test Loss: 0.3773200412326389\n",
      "Epoch: 10658 Training Loss: 0.11670102183024089 Test Loss: 0.37701771375868054\n",
      "Epoch: 10659 Training Loss: 0.11706616550021702 Test Loss: 0.3770441351996528\n",
      "Epoch: 10660 Training Loss: 0.11743434651692708 Test Loss: 0.3777600368923611\n",
      "Epoch: 10661 Training Loss: 0.11782281494140626 Test Loss: 0.37900244140625\n",
      "Epoch: 10662 Training Loss: 0.11820679982503256 Test Loss: 0.38003792317708335\n",
      "Epoch: 10663 Training Loss: 0.11858958604600695 Test Loss: 0.38122108289930556\n",
      "Epoch: 10664 Training Loss: 0.11898382229275174 Test Loss: 0.3824384494357639\n",
      "Epoch: 10665 Training Loss: 0.11937213304307726 Test Loss: 0.3834845377604167\n",
      "Epoch: 10666 Training Loss: 0.11976407708062066 Test Loss: 0.3849827473958333\n",
      "Epoch: 10667 Training Loss: 0.1201409691704644 Test Loss: 0.3868027072482639\n",
      "Epoch: 10668 Training Loss: 0.12043010881212023 Test Loss: 0.3885964626736111\n",
      "Epoch: 10669 Training Loss: 0.12058680640326606 Test Loss: 0.3898718532986111\n",
      "Epoch: 10670 Training Loss: 0.12062730577256944 Test Loss: 0.38979380967881944\n",
      "Epoch: 10671 Training Loss: 0.12057765028211806 Test Loss: 0.38863509114583333\n",
      "Epoch: 10672 Training Loss: 0.12048972490098742 Test Loss: 0.38715549045138886\n",
      "Epoch: 10673 Training Loss: 0.12035704294840495 Test Loss: 0.38633878580729164\n",
      "Epoch: 10674 Training Loss: 0.12020891740587022 Test Loss: 0.3861898871527778\n",
      "Epoch: 10675 Training Loss: 0.12005398983425564 Test Loss: 0.387136962890625\n",
      "Epoch: 10676 Training Loss: 0.11990159098307292 Test Loss: 0.3881794162326389\n",
      "Epoch: 10677 Training Loss: 0.11977813212076822 Test Loss: 0.38898784722222224\n",
      "Epoch: 10678 Training Loss: 0.11971900261773004 Test Loss: 0.38900035264756944\n",
      "Epoch: 10679 Training Loss: 0.11965117645263672 Test Loss: 0.38808279079861113\n",
      "Epoch: 10680 Training Loss: 0.11957763671875 Test Loss: 0.3866332194010417\n",
      "Epoch: 10681 Training Loss: 0.11952140384250216 Test Loss: 0.38469135199652776\n",
      "Epoch: 10682 Training Loss: 0.11948561859130859 Test Loss: 0.3831608072916667\n",
      "Epoch: 10683 Training Loss: 0.11945524173312717 Test Loss: 0.3814243435329861\n",
      "Epoch: 10684 Training Loss: 0.11943579440646701 Test Loss: 0.3802250434027778\n",
      "Epoch: 10685 Training Loss: 0.11945966084798178 Test Loss: 0.3796954210069444\n",
      "Epoch: 10686 Training Loss: 0.11950783538818359 Test Loss: 0.37932210286458334\n",
      "Epoch: 10687 Training Loss: 0.11960469818115234 Test Loss: 0.3797163628472222\n",
      "Epoch: 10688 Training Loss: 0.11981889767116971 Test Loss: 0.3799396430121528\n",
      "Epoch: 10689 Training Loss: 0.12015220133463542 Test Loss: 0.3795710177951389\n",
      "Epoch: 10690 Training Loss: 0.12058687845865886 Test Loss: 0.3783823784722222\n",
      "Epoch: 10691 Training Loss: 0.12105723487006294 Test Loss: 0.3768198784722222\n",
      "Epoch: 10692 Training Loss: 0.12147637685139974 Test Loss: 0.37557725694444444\n",
      "Epoch: 10693 Training Loss: 0.12177370791965061 Test Loss: 0.3751262478298611\n",
      "Epoch: 10694 Training Loss: 0.12190170033772786 Test Loss: 0.37583924696180554\n",
      "Epoch: 10695 Training Loss: 0.12187153032090929 Test Loss: 0.37746375868055554\n",
      "Epoch: 10696 Training Loss: 0.12176512485080296 Test Loss: 0.37945659722222225\n",
      "Epoch: 10697 Training Loss: 0.1215783199734158 Test Loss: 0.3810246853298611\n",
      "Epoch: 10698 Training Loss: 0.12127683512369791 Test Loss: 0.3821991644965278\n",
      "Epoch: 10699 Training Loss: 0.12086629825168185 Test Loss: 0.38207142469618055\n",
      "Epoch: 10700 Training Loss: 0.12036926608615452 Test Loss: 0.3812542588975694\n",
      "Epoch: 10701 Training Loss: 0.11982820044623481 Test Loss: 0.38039860026041666\n",
      "Epoch: 10702 Training Loss: 0.11924469841851129 Test Loss: 0.37958984375\n",
      "Epoch: 10703 Training Loss: 0.11863670179578993 Test Loss: 0.3792139214409722\n",
      "Epoch: 10704 Training Loss: 0.11807253011067709 Test Loss: 0.3789289822048611\n",
      "Epoch: 10705 Training Loss: 0.1175694317287869 Test Loss: 0.3786991644965278\n",
      "Epoch: 10706 Training Loss: 0.11711354149712457 Test Loss: 0.37816682942708335\n",
      "Epoch: 10707 Training Loss: 0.11669422064887153 Test Loss: 0.37793017578125\n",
      "Epoch: 10708 Training Loss: 0.11629410892062716 Test Loss: 0.37791764322916666\n",
      "Epoch: 10709 Training Loss: 0.11591703287760417 Test Loss: 0.3780037434895833\n",
      "Epoch: 10710 Training Loss: 0.11557213762071397 Test Loss: 0.3782482367621528\n",
      "Epoch: 10711 Training Loss: 0.11524381001790364 Test Loss: 0.37888392469618054\n",
      "Epoch: 10712 Training Loss: 0.11494757843017578 Test Loss: 0.379143310546875\n",
      "Epoch: 10713 Training Loss: 0.11468633948432075 Test Loss: 0.3790272623697917\n",
      "Epoch: 10714 Training Loss: 0.11444657050238716 Test Loss: 0.37860009765625\n",
      "Epoch: 10715 Training Loss: 0.11423202429877387 Test Loss: 0.37809814453125\n",
      "Epoch: 10716 Training Loss: 0.11404155561659071 Test Loss: 0.37779117838541665\n",
      "Epoch: 10717 Training Loss: 0.11386775122748481 Test Loss: 0.3775676540798611\n",
      "Epoch: 10718 Training Loss: 0.11370723724365234 Test Loss: 0.37754736328125\n",
      "Epoch: 10719 Training Loss: 0.11356882646348741 Test Loss: 0.3776985134548611\n",
      "Epoch: 10720 Training Loss: 0.11343531629774306 Test Loss: 0.37775249565972224\n",
      "Epoch: 10721 Training Loss: 0.11331385633680556 Test Loss: 0.37790079752604167\n",
      "Epoch: 10722 Training Loss: 0.11321229298909505 Test Loss: 0.3780734592013889\n",
      "Epoch: 10723 Training Loss: 0.11311602867974176 Test Loss: 0.3781018337673611\n",
      "Epoch: 10724 Training Loss: 0.1130351537068685 Test Loss: 0.3781324869791667\n",
      "Epoch: 10725 Training Loss: 0.11296843634711372 Test Loss: 0.3778253580729167\n",
      "Epoch: 10726 Training Loss: 0.1129153315226237 Test Loss: 0.37757991536458335\n",
      "Epoch: 10727 Training Loss: 0.11287806193033854 Test Loss: 0.3771481662326389\n",
      "Epoch: 10728 Training Loss: 0.11285068766276042 Test Loss: 0.3766211751302083\n",
      "Epoch: 10729 Training Loss: 0.1128389409383138 Test Loss: 0.37620418294270835\n",
      "Epoch: 10730 Training Loss: 0.11283534325493706 Test Loss: 0.3757543131510417\n",
      "Epoch: 10731 Training Loss: 0.11285853152804905 Test Loss: 0.37547273763020833\n",
      "Epoch: 10732 Training Loss: 0.11291185421413845 Test Loss: 0.3751779242621528\n",
      "Epoch: 10733 Training Loss: 0.11298163096110025 Test Loss: 0.37500651041666666\n",
      "Epoch: 10734 Training Loss: 0.11305767991807726 Test Loss: 0.37475444878472225\n",
      "Epoch: 10735 Training Loss: 0.11314959038628472 Test Loss: 0.37458067491319447\n",
      "Epoch: 10736 Training Loss: 0.11325436231825087 Test Loss: 0.37441216362847224\n",
      "Epoch: 10737 Training Loss: 0.11337754313151041 Test Loss: 0.3743538953993056\n",
      "Epoch: 10738 Training Loss: 0.113505738152398 Test Loss: 0.37437201605902776\n",
      "Epoch: 10739 Training Loss: 0.11363419426812066 Test Loss: 0.37446242947048614\n",
      "Epoch: 10740 Training Loss: 0.11376702117919922 Test Loss: 0.3747334255642361\n",
      "Epoch: 10741 Training Loss: 0.11389570702446832 Test Loss: 0.3750185004340278\n",
      "Epoch: 10742 Training Loss: 0.11402491082085503 Test Loss: 0.37541786024305557\n",
      "Epoch: 10743 Training Loss: 0.11417111206054688 Test Loss: 0.37585262044270834\n",
      "Epoch: 10744 Training Loss: 0.11432629648844402 Test Loss: 0.3764167209201389\n",
      "Epoch: 10745 Training Loss: 0.11449233161078559 Test Loss: 0.37696242947048614\n",
      "Epoch: 10746 Training Loss: 0.11466827646891276 Test Loss: 0.3775973307291667\n",
      "Epoch: 10747 Training Loss: 0.11484456973605686 Test Loss: 0.3780690646701389\n",
      "Epoch: 10748 Training Loss: 0.11502758619520399 Test Loss: 0.3786510416666667\n",
      "Epoch: 10749 Training Loss: 0.1152348386976454 Test Loss: 0.37914501953125\n",
      "Epoch: 10750 Training Loss: 0.11547024197048611 Test Loss: 0.37933677842881947\n",
      "Epoch: 10751 Training Loss: 0.11573421478271484 Test Loss: 0.3795973849826389\n",
      "Epoch: 10752 Training Loss: 0.11600016360812718 Test Loss: 0.37957880316840276\n",
      "Epoch: 10753 Training Loss: 0.11626812913682726 Test Loss: 0.37961442057291667\n",
      "Epoch: 10754 Training Loss: 0.11653192223442925 Test Loss: 0.37975846354166665\n",
      "Epoch: 10755 Training Loss: 0.1167774904039171 Test Loss: 0.38036653645833335\n",
      "Epoch: 10756 Training Loss: 0.11700716230604384 Test Loss: 0.38134130859375\n",
      "Epoch: 10757 Training Loss: 0.11722778574625652 Test Loss: 0.3824304470486111\n",
      "Epoch: 10758 Training Loss: 0.1174273435804579 Test Loss: 0.38327747938368056\n",
      "Epoch: 10759 Training Loss: 0.11760002221001518 Test Loss: 0.3841466471354167\n",
      "Epoch: 10760 Training Loss: 0.11779770999484591 Test Loss: 0.385289794921875\n",
      "Epoch: 10761 Training Loss: 0.1179830305311415 Test Loss: 0.38573307291666664\n",
      "Epoch: 10762 Training Loss: 0.1181781734890408 Test Loss: 0.3861956651475694\n",
      "Epoch: 10763 Training Loss: 0.11839435238308377 Test Loss: 0.38625067816840275\n",
      "Epoch: 10764 Training Loss: 0.11861812337239583 Test Loss: 0.38572013346354167\n",
      "Epoch: 10765 Training Loss: 0.11881964959038628 Test Loss: 0.3845405002170139\n",
      "Epoch: 10766 Training Loss: 0.11900363328721789 Test Loss: 0.38297124565972224\n",
      "Epoch: 10767 Training Loss: 0.1191943096584744 Test Loss: 0.3810685763888889\n",
      "Epoch: 10768 Training Loss: 0.11940349578857422 Test Loss: 0.37941474066840275\n",
      "Epoch: 10769 Training Loss: 0.11961600748697916 Test Loss: 0.3778081597222222\n",
      "Epoch: 10770 Training Loss: 0.11982098812527127 Test Loss: 0.3772199978298611\n",
      "Epoch: 10771 Training Loss: 0.12002526177300347 Test Loss: 0.3779787055121528\n",
      "Epoch: 10772 Training Loss: 0.12025958760579428 Test Loss: 0.3798046603732639\n",
      "Epoch: 10773 Training Loss: 0.12054409535725912 Test Loss: 0.38257557508680556\n",
      "Epoch: 10774 Training Loss: 0.12092093912760417 Test Loss: 0.3853486056857639\n",
      "Epoch: 10775 Training Loss: 0.12137515936957466 Test Loss: 0.3882887912326389\n",
      "Epoch: 10776 Training Loss: 0.12195753139919704 Test Loss: 0.3905591362847222\n",
      "Epoch: 10777 Training Loss: 0.12263424852159288 Test Loss: 0.3922325032552083\n",
      "Epoch: 10778 Training Loss: 0.1232888437906901 Test Loss: 0.3934677191840278\n",
      "Epoch: 10779 Training Loss: 0.12378884633382162 Test Loss: 0.3941084255642361\n",
      "Epoch: 10780 Training Loss: 0.12403154669867622 Test Loss: 0.3942469618055556\n",
      "Epoch: 10781 Training Loss: 0.12397025129530165 Test Loss: 0.39439485677083336\n",
      "Epoch: 10782 Training Loss: 0.1237082994249132 Test Loss: 0.3955184733072917\n",
      "Epoch: 10783 Training Loss: 0.12330634223090278 Test Loss: 0.3958276909722222\n",
      "Epoch: 10784 Training Loss: 0.12281200832790799 Test Loss: 0.3959077690972222\n",
      "Epoch: 10785 Training Loss: 0.12229932742648654 Test Loss: 0.39452110460069445\n",
      "Epoch: 10786 Training Loss: 0.12176799096001518 Test Loss: 0.39323432074652775\n",
      "Epoch: 10787 Training Loss: 0.12122662183973525 Test Loss: 0.39122276475694445\n",
      "Epoch: 10788 Training Loss: 0.1207043219672309 Test Loss: 0.3890120171440972\n",
      "Epoch: 10789 Training Loss: 0.12021109347873264 Test Loss: 0.38779736328125\n",
      "Epoch: 10790 Training Loss: 0.11977949608696832 Test Loss: 0.3862509223090278\n",
      "Epoch: 10791 Training Loss: 0.11945293850368924 Test Loss: 0.38494495985243055\n",
      "Epoch: 10792 Training Loss: 0.11926571061876085 Test Loss: 0.38400390625\n",
      "Epoch: 10793 Training Loss: 0.11922677527533637 Test Loss: 0.3834448784722222\n",
      "Epoch: 10794 Training Loss: 0.11929034593370226 Test Loss: 0.3833815104166667\n",
      "Epoch: 10795 Training Loss: 0.11944790479871961 Test Loss: 0.3840842013888889\n",
      "Epoch: 10796 Training Loss: 0.11966293504503038 Test Loss: 0.38552389865451386\n",
      "Epoch: 10797 Training Loss: 0.11991103193495009 Test Loss: 0.3868920355902778\n",
      "Epoch: 10798 Training Loss: 0.12014910125732423 Test Loss: 0.38851193576388887\n",
      "Epoch: 10799 Training Loss: 0.1203603261311849 Test Loss: 0.39010378689236114\n",
      "Epoch: 10800 Training Loss: 0.12054578145345052 Test Loss: 0.3913131781684028\n",
      "Epoch: 10801 Training Loss: 0.12069450208875868 Test Loss: 0.39237548828125\n",
      "Epoch: 10802 Training Loss: 0.12081387922498915 Test Loss: 0.39320643446180553\n",
      "Epoch: 10803 Training Loss: 0.12092623307969835 Test Loss: 0.39271641710069444\n",
      "Epoch: 10804 Training Loss: 0.12100043233235677 Test Loss: 0.39109296332465276\n",
      "Epoch: 10805 Training Loss: 0.12096658409966363 Test Loss: 0.38924815538194446\n",
      "Epoch: 10806 Training Loss: 0.12075879160563151 Test Loss: 0.3879914008246528\n",
      "Epoch: 10807 Training Loss: 0.12031970977783203 Test Loss: 0.38728946940104164\n",
      "Epoch: 10808 Training Loss: 0.11966240946451823 Test Loss: 0.38721546766493053\n",
      "Epoch: 10809 Training Loss: 0.11885415734185113 Test Loss: 0.38734817165798613\n",
      "Epoch: 10810 Training Loss: 0.11802486504448785 Test Loss: 0.38752197265625\n",
      "Epoch: 10811 Training Loss: 0.11726214684380426 Test Loss: 0.3874832356770833\n",
      "Epoch: 10812 Training Loss: 0.11661545732286241 Test Loss: 0.38718741861979167\n",
      "Epoch: 10813 Training Loss: 0.11609584214952257 Test Loss: 0.38622062174479166\n",
      "Epoch: 10814 Training Loss: 0.11570893012152778 Test Loss: 0.38526714409722224\n",
      "Epoch: 10815 Training Loss: 0.11543594190809461 Test Loss: 0.38447374131944445\n",
      "Epoch: 10816 Training Loss: 0.11525259314643012 Test Loss: 0.3840119086371528\n",
      "Epoch: 10817 Training Loss: 0.11513116115993924 Test Loss: 0.3835850151909722\n",
      "Epoch: 10818 Training Loss: 0.11503963470458985 Test Loss: 0.38343250868055556\n",
      "Epoch: 10819 Training Loss: 0.11500188954671224 Test Loss: 0.38324918619791665\n",
      "Epoch: 10820 Training Loss: 0.11498272620307075 Test Loss: 0.38287584092881943\n",
      "Epoch: 10821 Training Loss: 0.1149779535929362 Test Loss: 0.38273025173611114\n",
      "Epoch: 10822 Training Loss: 0.11498706987169054 Test Loss: 0.38263142903645836\n",
      "Epoch: 10823 Training Loss: 0.1150010257297092 Test Loss: 0.382118896484375\n",
      "Epoch: 10824 Training Loss: 0.11500634511311848 Test Loss: 0.381666015625\n",
      "Epoch: 10825 Training Loss: 0.11500795576307508 Test Loss: 0.38139420572916666\n",
      "Epoch: 10826 Training Loss: 0.11502972327338325 Test Loss: 0.3807858615451389\n",
      "Epoch: 10827 Training Loss: 0.11506521860758463 Test Loss: 0.380181884765625\n",
      "Epoch: 10828 Training Loss: 0.1151346672905816 Test Loss: 0.37964507378472223\n",
      "Epoch: 10829 Training Loss: 0.11524151780870226 Test Loss: 0.3789922688802083\n",
      "Epoch: 10830 Training Loss: 0.11538399675157335 Test Loss: 0.37828884548611114\n",
      "Epoch: 10831 Training Loss: 0.11558428446451822 Test Loss: 0.37779288736979166\n",
      "Epoch: 10832 Training Loss: 0.11581821865505643 Test Loss: 0.3773690592447917\n",
      "Epoch: 10833 Training Loss: 0.11607541402180989 Test Loss: 0.37712535264756947\n",
      "Epoch: 10834 Training Loss: 0.1163475816514757 Test Loss: 0.3771320258246528\n",
      "Epoch: 10835 Training Loss: 0.11660135735405816 Test Loss: 0.37728580729166666\n",
      "Epoch: 10836 Training Loss: 0.11686697642008463 Test Loss: 0.3775625813802083\n",
      "Epoch: 10837 Training Loss: 0.11712591298421224 Test Loss: 0.37776307508680557\n",
      "Epoch: 10838 Training Loss: 0.11739293501112197 Test Loss: 0.37831437174479166\n",
      "Epoch: 10839 Training Loss: 0.11765843115912543 Test Loss: 0.37903776041666665\n",
      "Epoch: 10840 Training Loss: 0.11797010803222656 Test Loss: 0.38013525390625\n",
      "Epoch: 10841 Training Loss: 0.11833193037245009 Test Loss: 0.38169303385416664\n",
      "Epoch: 10842 Training Loss: 0.11870068444146051 Test Loss: 0.3836210666232639\n",
      "Epoch: 10843 Training Loss: 0.11913620079888237 Test Loss: 0.38570920138888887\n",
      "Epoch: 10844 Training Loss: 0.11958706241183811 Test Loss: 0.387962890625\n",
      "Epoch: 10845 Training Loss: 0.12004503546820747 Test Loss: 0.3899166666666667\n",
      "Epoch: 10846 Training Loss: 0.12050230577256944 Test Loss: 0.3918652072482639\n",
      "Epoch: 10847 Training Loss: 0.12100000593397353 Test Loss: 0.3933844943576389\n",
      "Epoch: 10848 Training Loss: 0.12152351379394531 Test Loss: 0.3939892578125\n",
      "Epoch: 10849 Training Loss: 0.12210375128851997 Test Loss: 0.39322336154513887\n",
      "Epoch: 10850 Training Loss: 0.12267735205756293 Test Loss: 0.3915492621527778\n",
      "Epoch: 10851 Training Loss: 0.12318501366509331 Test Loss: 0.3888260091145833\n",
      "Epoch: 10852 Training Loss: 0.12358283996582031 Test Loss: 0.3862138129340278\n",
      "Epoch: 10853 Training Loss: 0.12383395724826389 Test Loss: 0.3837816297743056\n",
      "Epoch: 10854 Training Loss: 0.12395452626546224 Test Loss: 0.3824766167534722\n",
      "Epoch: 10855 Training Loss: 0.12397276899549696 Test Loss: 0.3822194552951389\n",
      "Epoch: 10856 Training Loss: 0.12392393917507595 Test Loss: 0.3826947699652778\n",
      "Epoch: 10857 Training Loss: 0.12386796145968967 Test Loss: 0.3837458224826389\n",
      "Epoch: 10858 Training Loss: 0.12375589836968316 Test Loss: 0.3849101291232639\n",
      "Epoch: 10859 Training Loss: 0.12352357906765409 Test Loss: 0.3858304036458333\n",
      "Epoch: 10860 Training Loss: 0.12318367258707683 Test Loss: 0.3860895453559028\n",
      "Epoch: 10861 Training Loss: 0.1227125481499566 Test Loss: 0.38504191080729167\n",
      "Epoch: 10862 Training Loss: 0.12210514577229818 Test Loss: 0.382721923828125\n",
      "Epoch: 10863 Training Loss: 0.12146250491672092 Test Loss: 0.379956787109375\n",
      "Epoch: 10864 Training Loss: 0.12085476430257161 Test Loss: 0.3774352756076389\n",
      "Epoch: 10865 Training Loss: 0.12028224097357856 Test Loss: 0.37555870225694443\n",
      "Epoch: 10866 Training Loss: 0.11972703976101345 Test Loss: 0.3744524197048611\n",
      "Epoch: 10867 Training Loss: 0.11915893809000651 Test Loss: 0.37387879774305555\n",
      "Epoch: 10868 Training Loss: 0.11860319264729818 Test Loss: 0.373775146484375\n",
      "Epoch: 10869 Training Loss: 0.11802013058132596 Test Loss: 0.37441086154513886\n",
      "Epoch: 10870 Training Loss: 0.11744237942165799 Test Loss: 0.3752319878472222\n",
      "Epoch: 10871 Training Loss: 0.11689579688178169 Test Loss: 0.3760558810763889\n",
      "Epoch: 10872 Training Loss: 0.11637774658203125 Test Loss: 0.3766934136284722\n",
      "Epoch: 10873 Training Loss: 0.11590447998046875 Test Loss: 0.37688343641493055\n",
      "Epoch: 10874 Training Loss: 0.11549244435628256 Test Loss: 0.37683452690972224\n",
      "Epoch: 10875 Training Loss: 0.11513424343532987 Test Loss: 0.3765425889756944\n",
      "Epoch: 10876 Training Loss: 0.1148229988945855 Test Loss: 0.3762896050347222\n",
      "Epoch: 10877 Training Loss: 0.11456169213189019 Test Loss: 0.37574891493055557\n",
      "Epoch: 10878 Training Loss: 0.11434071858723958 Test Loss: 0.37535137261284723\n",
      "Epoch: 10879 Training Loss: 0.11415784454345704 Test Loss: 0.374918212890625\n",
      "Epoch: 10880 Training Loss: 0.11401298183865018 Test Loss: 0.37459396701388886\n",
      "Epoch: 10881 Training Loss: 0.11390758090549045 Test Loss: 0.3744431694878472\n",
      "Epoch: 10882 Training Loss: 0.11382042270236545 Test Loss: 0.37429383680555556\n",
      "Epoch: 10883 Training Loss: 0.11376146613226996 Test Loss: 0.3744324815538194\n",
      "Epoch: 10884 Training Loss: 0.1137272720336914 Test Loss: 0.37458365885416667\n",
      "Epoch: 10885 Training Loss: 0.11372801717122395 Test Loss: 0.3749245334201389\n",
      "Epoch: 10886 Training Loss: 0.11375364854600695 Test Loss: 0.3752726779513889\n",
      "Epoch: 10887 Training Loss: 0.11379798973931207 Test Loss: 0.3758355034722222\n",
      "Epoch: 10888 Training Loss: 0.11386143069797092 Test Loss: 0.3764254014756944\n",
      "Epoch: 10889 Training Loss: 0.11395377265082465 Test Loss: 0.3768915473090278\n",
      "Epoch: 10890 Training Loss: 0.11405067274305555 Test Loss: 0.37763536241319445\n",
      "Epoch: 10891 Training Loss: 0.11415059407552083 Test Loss: 0.3784287923177083\n",
      "Epoch: 10892 Training Loss: 0.11425441063774956 Test Loss: 0.3793114691840278\n",
      "Epoch: 10893 Training Loss: 0.11435501013861762 Test Loss: 0.3801123046875\n",
      "Epoch: 10894 Training Loss: 0.11445029788547093 Test Loss: 0.3808143988715278\n",
      "Epoch: 10895 Training Loss: 0.11454757181803385 Test Loss: 0.38134328884548613\n",
      "Epoch: 10896 Training Loss: 0.11463534715440538 Test Loss: 0.381501953125\n",
      "Epoch: 10897 Training Loss: 0.11472360314263237 Test Loss: 0.38167523871527775\n",
      "Epoch: 10898 Training Loss: 0.11482441033257379 Test Loss: 0.3818694390190972\n",
      "Epoch: 10899 Training Loss: 0.1149324451022678 Test Loss: 0.38220551215277776\n",
      "Epoch: 10900 Training Loss: 0.11503706783718533 Test Loss: 0.3825032009548611\n",
      "Epoch: 10901 Training Loss: 0.11514307488335504 Test Loss: 0.38307779947916665\n",
      "Epoch: 10902 Training Loss: 0.11526689063178168 Test Loss: 0.3836978624131944\n",
      "Epoch: 10903 Training Loss: 0.11539648013644749 Test Loss: 0.3841966145833333\n",
      "Epoch: 10904 Training Loss: 0.11551737806532118 Test Loss: 0.38451513671875\n",
      "Epoch: 10905 Training Loss: 0.11562796190049913 Test Loss: 0.38475352647569444\n",
      "Epoch: 10906 Training Loss: 0.11572763315836589 Test Loss: 0.38503526475694444\n",
      "Epoch: 10907 Training Loss: 0.11580933973524306 Test Loss: 0.38510723198784724\n",
      "Epoch: 10908 Training Loss: 0.11589542219373915 Test Loss: 0.38523478190104166\n",
      "Epoch: 10909 Training Loss: 0.116001099480523 Test Loss: 0.38511539713541665\n",
      "Epoch: 10910 Training Loss: 0.11610784403483072 Test Loss: 0.3848410101996528\n",
      "Epoch: 10911 Training Loss: 0.11619435882568359 Test Loss: 0.3844026150173611\n",
      "Epoch: 10912 Training Loss: 0.11628937615288629 Test Loss: 0.38413335503472223\n",
      "Epoch: 10913 Training Loss: 0.1163883327907986 Test Loss: 0.3836880425347222\n",
      "Epoch: 10914 Training Loss: 0.11648672570122613 Test Loss: 0.383109375\n",
      "Epoch: 10915 Training Loss: 0.11657750786675347 Test Loss: 0.38263245985243055\n",
      "Epoch: 10916 Training Loss: 0.1166620127360026 Test Loss: 0.3819927571614583\n",
      "Epoch: 10917 Training Loss: 0.11673433600531684 Test Loss: 0.3817258029513889\n",
      "Epoch: 10918 Training Loss: 0.11679490576850043 Test Loss: 0.38184507921006944\n",
      "Epoch: 10919 Training Loss: 0.11685432773166232 Test Loss: 0.3817032063802083\n",
      "Epoch: 10920 Training Loss: 0.1169313006930881 Test Loss: 0.3819731716579861\n",
      "Epoch: 10921 Training Loss: 0.11702152930365668 Test Loss: 0.38199408637152776\n",
      "Epoch: 10922 Training Loss: 0.11713344404432509 Test Loss: 0.38194124348958336\n",
      "Epoch: 10923 Training Loss: 0.11725916798909505 Test Loss: 0.3816037055121528\n",
      "Epoch: 10924 Training Loss: 0.11738728162977431 Test Loss: 0.38075249565972225\n",
      "Epoch: 10925 Training Loss: 0.1174873530069987 Test Loss: 0.3796594509548611\n",
      "Epoch: 10926 Training Loss: 0.11757646094428169 Test Loss: 0.37859461805555555\n",
      "Epoch: 10927 Training Loss: 0.11762713877360026 Test Loss: 0.377667236328125\n",
      "Epoch: 10928 Training Loss: 0.11760674709743923 Test Loss: 0.3774166666666667\n",
      "Epoch: 10929 Training Loss: 0.1175391108194987 Test Loss: 0.3774678276909722\n",
      "Epoch: 10930 Training Loss: 0.11740533192952474 Test Loss: 0.37802918836805555\n",
      "Epoch: 10931 Training Loss: 0.11720867411295573 Test Loss: 0.37905224609375\n",
      "Epoch: 10932 Training Loss: 0.11697899458143446 Test Loss: 0.38045811631944443\n",
      "Epoch: 10933 Training Loss: 0.1167251476711697 Test Loss: 0.38144547526041667\n",
      "Epoch: 10934 Training Loss: 0.11645813751220703 Test Loss: 0.3822073296440972\n",
      "Epoch: 10935 Training Loss: 0.1161844253540039 Test Loss: 0.38294911024305556\n",
      "Epoch: 10936 Training Loss: 0.11591256629096137 Test Loss: 0.3832030978732639\n",
      "Epoch: 10937 Training Loss: 0.1156523429022895 Test Loss: 0.38352115885416665\n",
      "Epoch: 10938 Training Loss: 0.11541563500298394 Test Loss: 0.38335601128472224\n",
      "Epoch: 10939 Training Loss: 0.11519418589274089 Test Loss: 0.3834696723090278\n",
      "Epoch: 10940 Training Loss: 0.11497806803385417 Test Loss: 0.3834184027777778\n",
      "Epoch: 10941 Training Loss: 0.11479336547851562 Test Loss: 0.38326744249131944\n",
      "Epoch: 10942 Training Loss: 0.11461559634738498 Test Loss: 0.38332912868923613\n",
      "Epoch: 10943 Training Loss: 0.11446080780029297 Test Loss: 0.38321733940972225\n",
      "Epoch: 10944 Training Loss: 0.11433740658230251 Test Loss: 0.3832589518229167\n",
      "Epoch: 10945 Training Loss: 0.11422561560736762 Test Loss: 0.3831277398003472\n",
      "Epoch: 10946 Training Loss: 0.11412155151367187 Test Loss: 0.38309337022569445\n",
      "Epoch: 10947 Training Loss: 0.11402913920084636 Test Loss: 0.38285584852430554\n",
      "Epoch: 10948 Training Loss: 0.11396078660753038 Test Loss: 0.3825543891059028\n",
      "Epoch: 10949 Training Loss: 0.11392569478352865 Test Loss: 0.382550537109375\n",
      "Epoch: 10950 Training Loss: 0.11392817942301432 Test Loss: 0.3823400607638889\n",
      "Epoch: 10951 Training Loss: 0.11395999315049914 Test Loss: 0.38246351453993055\n",
      "Epoch: 10952 Training Loss: 0.11401863013373481 Test Loss: 0.38235731336805556\n",
      "Epoch: 10953 Training Loss: 0.11410148705376519 Test Loss: 0.38257405598958333\n",
      "Epoch: 10954 Training Loss: 0.11422997368706597 Test Loss: 0.3828186848958333\n",
      "Epoch: 10955 Training Loss: 0.11439981757269965 Test Loss: 0.3833741861979167\n",
      "Epoch: 10956 Training Loss: 0.1146074964735243 Test Loss: 0.3840099826388889\n",
      "Epoch: 10957 Training Loss: 0.11484905412462022 Test Loss: 0.3847744411892361\n",
      "Epoch: 10958 Training Loss: 0.11513682725694445 Test Loss: 0.38546994357638886\n",
      "Epoch: 10959 Training Loss: 0.11545038435194227 Test Loss: 0.3859456380208333\n",
      "Epoch: 10960 Training Loss: 0.11575941806369358 Test Loss: 0.3861559787326389\n",
      "Epoch: 10961 Training Loss: 0.1160648193359375 Test Loss: 0.3865883517795139\n",
      "Epoch: 10962 Training Loss: 0.11635769653320313 Test Loss: 0.3866235080295139\n",
      "Epoch: 10963 Training Loss: 0.11662056816948785 Test Loss: 0.3866887478298611\n",
      "Epoch: 10964 Training Loss: 0.11688073815239801 Test Loss: 0.3865354546440972\n",
      "Epoch: 10965 Training Loss: 0.11711731041802301 Test Loss: 0.38675214301215277\n",
      "Epoch: 10966 Training Loss: 0.11735000949435764 Test Loss: 0.3868763292100694\n",
      "Epoch: 10967 Training Loss: 0.11757160101996528 Test Loss: 0.38713416883680557\n",
      "Epoch: 10968 Training Loss: 0.1177847171359592 Test Loss: 0.3877860785590278\n",
      "Epoch: 10969 Training Loss: 0.11798848385281033 Test Loss: 0.38843573676215276\n",
      "Epoch: 10970 Training Loss: 0.1181786609225803 Test Loss: 0.3891290418836806\n",
      "Epoch: 10971 Training Loss: 0.1183252453274197 Test Loss: 0.3894621853298611\n",
      "Epoch: 10972 Training Loss: 0.11841897667778863 Test Loss: 0.39038487413194445\n",
      "Epoch: 10973 Training Loss: 0.1184843995836046 Test Loss: 0.3908614637586806\n",
      "Epoch: 10974 Training Loss: 0.11851351081000434 Test Loss: 0.3915092230902778\n",
      "Epoch: 10975 Training Loss: 0.11853190273708768 Test Loss: 0.39210004340277776\n",
      "Epoch: 10976 Training Loss: 0.11855410936143664 Test Loss: 0.39227446831597224\n",
      "Epoch: 10977 Training Loss: 0.11858135562472873 Test Loss: 0.39196061197916665\n",
      "Epoch: 10978 Training Loss: 0.11861156039767795 Test Loss: 0.39108534071180556\n",
      "Epoch: 10979 Training Loss: 0.11861968909369575 Test Loss: 0.3894846733940972\n",
      "Epoch: 10980 Training Loss: 0.11857529703776042 Test Loss: 0.3879289279513889\n",
      "Epoch: 10981 Training Loss: 0.1185063222249349 Test Loss: 0.38623914930555553\n",
      "Epoch: 10982 Training Loss: 0.1184821268717448 Test Loss: 0.3845218912760417\n",
      "Epoch: 10983 Training Loss: 0.11847127278645833 Test Loss: 0.3836465386284722\n",
      "Epoch: 10984 Training Loss: 0.11846998087565104 Test Loss: 0.38325328233506945\n",
      "Epoch: 10985 Training Loss: 0.11851698642306857 Test Loss: 0.3834658203125\n",
      "Epoch: 10986 Training Loss: 0.11861957804361979 Test Loss: 0.3836233181423611\n",
      "Epoch: 10987 Training Loss: 0.11876907263861762 Test Loss: 0.38331494140625\n",
      "Epoch: 10988 Training Loss: 0.11896040683322483 Test Loss: 0.3824492730034722\n",
      "Epoch: 10989 Training Loss: 0.11919162156846788 Test Loss: 0.3814365234375\n",
      "Epoch: 10990 Training Loss: 0.11941337670220269 Test Loss: 0.3802057291666667\n",
      "Epoch: 10991 Training Loss: 0.1196038326687283 Test Loss: 0.3800819769965278\n",
      "Epoch: 10992 Training Loss: 0.1197676010131836 Test Loss: 0.38046077473958334\n",
      "Epoch: 10993 Training Loss: 0.1199095442030165 Test Loss: 0.38150840928819446\n",
      "Epoch: 10994 Training Loss: 0.12002544148763021 Test Loss: 0.3824492730034722\n",
      "Epoch: 10995 Training Loss: 0.12009725613064236 Test Loss: 0.38290885416666665\n",
      "Epoch: 10996 Training Loss: 0.12016429816351996 Test Loss: 0.3829613986545139\n",
      "Epoch: 10997 Training Loss: 0.12020716942681206 Test Loss: 0.3826708713107639\n",
      "Epoch: 10998 Training Loss: 0.12020740169949001 Test Loss: 0.3830103081597222\n",
      "Epoch: 10999 Training Loss: 0.12015791490342882 Test Loss: 0.384056396484375\n",
      "Epoch: 11000 Training Loss: 0.12005896759033204 Test Loss: 0.384330322265625\n",
      "Epoch: 11001 Training Loss: 0.11988907538519965 Test Loss: 0.38393419053819444\n",
      "Epoch: 11002 Training Loss: 0.1196864997016059 Test Loss: 0.3827642415364583\n",
      "Epoch: 11003 Training Loss: 0.11947139570448134 Test Loss: 0.3810958387586806\n",
      "Epoch: 11004 Training Loss: 0.11924144236246745 Test Loss: 0.37959971788194447\n",
      "Epoch: 11005 Training Loss: 0.11901276397705078 Test Loss: 0.3786589084201389\n",
      "Epoch: 11006 Training Loss: 0.1187604031032986 Test Loss: 0.378410400390625\n",
      "Epoch: 11007 Training Loss: 0.11849768405490452 Test Loss: 0.37909326171875\n",
      "Epoch: 11008 Training Loss: 0.1182543707953559 Test Loss: 0.380095458984375\n",
      "Epoch: 11009 Training Loss: 0.1180242648654514 Test Loss: 0.3812084689670139\n",
      "Epoch: 11010 Training Loss: 0.11781730397542317 Test Loss: 0.38214979383680553\n",
      "Epoch: 11011 Training Loss: 0.11763506910536024 Test Loss: 0.3829411349826389\n",
      "Epoch: 11012 Training Loss: 0.11743797132703993 Test Loss: 0.3832985297309028\n",
      "Epoch: 11013 Training Loss: 0.11721361372205946 Test Loss: 0.38354584418402776\n",
      "Epoch: 11014 Training Loss: 0.11696496497260199 Test Loss: 0.3836086697048611\n",
      "Epoch: 11015 Training Loss: 0.11673522016737196 Test Loss: 0.38315565321180556\n",
      "Epoch: 11016 Training Loss: 0.11651512654622395 Test Loss: 0.3828402777777778\n",
      "Epoch: 11017 Training Loss: 0.11632837337917752 Test Loss: 0.38266731770833334\n",
      "Epoch: 11018 Training Loss: 0.11619053226047092 Test Loss: 0.3825476345486111\n",
      "Epoch: 11019 Training Loss: 0.11610871124267579 Test Loss: 0.3825521918402778\n",
      "Epoch: 11020 Training Loss: 0.11608155059814453 Test Loss: 0.3829906141493056\n",
      "Epoch: 11021 Training Loss: 0.11610289171006945 Test Loss: 0.383468017578125\n",
      "Epoch: 11022 Training Loss: 0.11617414771185981 Test Loss: 0.3842183973524306\n",
      "Epoch: 11023 Training Loss: 0.11627015601264105 Test Loss: 0.38505631510416666\n",
      "Epoch: 11024 Training Loss: 0.1163702612982856 Test Loss: 0.3858800998263889\n",
      "Epoch: 11025 Training Loss: 0.11647141435411242 Test Loss: 0.38617664930555556\n",
      "Epoch: 11026 Training Loss: 0.11657607523600261 Test Loss: 0.3858992241753472\n",
      "Epoch: 11027 Training Loss: 0.11665526750352648 Test Loss: 0.38479747178819446\n",
      "Epoch: 11028 Training Loss: 0.11670397525363499 Test Loss: 0.3833556315104167\n",
      "Epoch: 11029 Training Loss: 0.1167168206108941 Test Loss: 0.38181532118055556\n",
      "Epoch: 11030 Training Loss: 0.11670259772406684 Test Loss: 0.3800745442708333\n",
      "Epoch: 11031 Training Loss: 0.11664431508382161 Test Loss: 0.37841248914930553\n",
      "Epoch: 11032 Training Loss: 0.11657672542995877 Test Loss: 0.377037353515625\n",
      "Epoch: 11033 Training Loss: 0.11650454881456163 Test Loss: 0.37585237630208335\n",
      "Epoch: 11034 Training Loss: 0.11644532860649956 Test Loss: 0.3749015842013889\n",
      "Epoch: 11035 Training Loss: 0.1163794436984592 Test Loss: 0.37393994140625\n",
      "Epoch: 11036 Training Loss: 0.11631700812445747 Test Loss: 0.37347319878472224\n",
      "Epoch: 11037 Training Loss: 0.1162736078898112 Test Loss: 0.37332834201388887\n",
      "Epoch: 11038 Training Loss: 0.11624921332465278 Test Loss: 0.37360487196180553\n",
      "Epoch: 11039 Training Loss: 0.11622283426920572 Test Loss: 0.3744347330729167\n",
      "Epoch: 11040 Training Loss: 0.11618754492865668 Test Loss: 0.3753146430121528\n",
      "Epoch: 11041 Training Loss: 0.11616309611002604 Test Loss: 0.3764453396267361\n",
      "Epoch: 11042 Training Loss: 0.11613916948106554 Test Loss: 0.37734315321180556\n",
      "Epoch: 11043 Training Loss: 0.11611519029405382 Test Loss: 0.37837586805555556\n",
      "Epoch: 11044 Training Loss: 0.11610629018147786 Test Loss: 0.37922417534722225\n",
      "Epoch: 11045 Training Loss: 0.11610010189480252 Test Loss: 0.38002162000868056\n",
      "Epoch: 11046 Training Loss: 0.11609577772352431 Test Loss: 0.38082996961805554\n",
      "Epoch: 11047 Training Loss: 0.11612002902560764 Test Loss: 0.38140407986111113\n",
      "Epoch: 11048 Training Loss: 0.11615486568874783 Test Loss: 0.38208355034722224\n",
      "Epoch: 11049 Training Loss: 0.11621805572509766 Test Loss: 0.3828134765625\n",
      "Epoch: 11050 Training Loss: 0.11630000559488932 Test Loss: 0.38330010308159723\n",
      "Epoch: 11051 Training Loss: 0.11639135318332249 Test Loss: 0.3834167751736111\n",
      "Epoch: 11052 Training Loss: 0.11646169874403212 Test Loss: 0.38289021809895835\n",
      "Epoch: 11053 Training Loss: 0.11650271691216363 Test Loss: 0.38249028862847223\n",
      "Epoch: 11054 Training Loss: 0.11649934895833333 Test Loss: 0.3814648980034722\n",
      "Epoch: 11055 Training Loss: 0.11642725033230251 Test Loss: 0.38055976019965276\n",
      "Epoch: 11056 Training Loss: 0.11629415639241536 Test Loss: 0.3800334743923611\n",
      "Epoch: 11057 Training Loss: 0.11612130991617839 Test Loss: 0.3794631076388889\n",
      "Epoch: 11058 Training Loss: 0.11591901397705077 Test Loss: 0.37918495008680553\n",
      "Epoch: 11059 Training Loss: 0.11568927086724175 Test Loss: 0.37896495225694443\n",
      "Epoch: 11060 Training Loss: 0.11545249515109592 Test Loss: 0.37867613389756943\n",
      "Epoch: 11061 Training Loss: 0.11524295722113716 Test Loss: 0.37854695638020835\n",
      "Epoch: 11062 Training Loss: 0.11507173919677735 Test Loss: 0.3781155327690972\n",
      "Epoch: 11063 Training Loss: 0.11492034403483073 Test Loss: 0.3776194118923611\n",
      "Epoch: 11064 Training Loss: 0.11477794053819444 Test Loss: 0.37701795789930553\n",
      "Epoch: 11065 Training Loss: 0.11464545694986979 Test Loss: 0.3763505045572917\n",
      "Epoch: 11066 Training Loss: 0.1145189683702257 Test Loss: 0.37549004448784723\n",
      "Epoch: 11067 Training Loss: 0.11439149559868707 Test Loss: 0.3745518120659722\n",
      "Epoch: 11068 Training Loss: 0.11425033908420139 Test Loss: 0.37390245225694446\n",
      "Epoch: 11069 Training Loss: 0.11409007263183593 Test Loss: 0.3737360568576389\n",
      "Epoch: 11070 Training Loss: 0.11391683875189887 Test Loss: 0.37405064561631945\n",
      "Epoch: 11071 Training Loss: 0.11373951297336155 Test Loss: 0.37452647569444447\n",
      "Epoch: 11072 Training Loss: 0.11355548350016276 Test Loss: 0.3749836968315972\n",
      "Epoch: 11073 Training Loss: 0.11337946234809028 Test Loss: 0.37548426649305555\n",
      "Epoch: 11074 Training Loss: 0.1132198503282335 Test Loss: 0.37583810763888886\n",
      "Epoch: 11075 Training Loss: 0.11307396867540148 Test Loss: 0.37590877278645835\n",
      "Epoch: 11076 Training Loss: 0.11293897755940756 Test Loss: 0.37589889865451387\n",
      "Epoch: 11077 Training Loss: 0.11282508850097656 Test Loss: 0.37587331814236113\n",
      "Epoch: 11078 Training Loss: 0.11272740088568793 Test Loss: 0.3757399088541667\n",
      "Epoch: 11079 Training Loss: 0.11264640299479167 Test Loss: 0.3758544379340278\n",
      "Epoch: 11080 Training Loss: 0.11258591715494792 Test Loss: 0.37626605902777777\n",
      "Epoch: 11081 Training Loss: 0.11253785451253255 Test Loss: 0.37661046006944443\n",
      "Epoch: 11082 Training Loss: 0.11251028781467014 Test Loss: 0.37706431749131947\n",
      "Epoch: 11083 Training Loss: 0.11250343661838108 Test Loss: 0.37736946614583333\n",
      "Epoch: 11084 Training Loss: 0.11250011444091797 Test Loss: 0.3776992458767361\n",
      "Epoch: 11085 Training Loss: 0.11250218539767795 Test Loss: 0.3780383572048611\n",
      "Epoch: 11086 Training Loss: 0.11251719156901041 Test Loss: 0.37808260091145834\n",
      "Epoch: 11087 Training Loss: 0.11254238043891059 Test Loss: 0.37819365776909725\n",
      "Epoch: 11088 Training Loss: 0.11257992214626736 Test Loss: 0.3784043511284722\n",
      "Epoch: 11089 Training Loss: 0.11262713114420574 Test Loss: 0.3787478298611111\n",
      "Epoch: 11090 Training Loss: 0.11269255828857422 Test Loss: 0.3789715711805556\n",
      "Epoch: 11091 Training Loss: 0.11276627773708767 Test Loss: 0.3792822536892361\n",
      "Epoch: 11092 Training Loss: 0.11285171424018012 Test Loss: 0.37954071723090277\n",
      "Epoch: 11093 Training Loss: 0.11292719184027777 Test Loss: 0.37940104166666666\n",
      "Epoch: 11094 Training Loss: 0.1129969728257921 Test Loss: 0.37945421006944446\n",
      "Epoch: 11095 Training Loss: 0.11306792365180121 Test Loss: 0.3791984049479167\n",
      "Epoch: 11096 Training Loss: 0.11314285617404514 Test Loss: 0.37874354383680553\n",
      "Epoch: 11097 Training Loss: 0.1132064938015408 Test Loss: 0.37801692708333334\n",
      "Epoch: 11098 Training Loss: 0.11325237274169922 Test Loss: 0.37748551432291666\n",
      "Epoch: 11099 Training Loss: 0.1132843526204427 Test Loss: 0.3770014377170139\n",
      "Epoch: 11100 Training Loss: 0.11331535339355468 Test Loss: 0.3766444498697917\n",
      "Epoch: 11101 Training Loss: 0.11333694203694661 Test Loss: 0.37639925130208335\n",
      "Epoch: 11102 Training Loss: 0.11336310831705729 Test Loss: 0.37641121419270834\n",
      "Epoch: 11103 Training Loss: 0.11339228990342883 Test Loss: 0.37642287868923613\n",
      "Epoch: 11104 Training Loss: 0.11345748647054037 Test Loss: 0.37630579969618055\n",
      "Epoch: 11105 Training Loss: 0.11355049557156033 Test Loss: 0.37620537651909725\n",
      "Epoch: 11106 Training Loss: 0.11364600118001301 Test Loss: 0.37609502495659725\n",
      "Epoch: 11107 Training Loss: 0.11373202684190538 Test Loss: 0.3755090060763889\n",
      "Epoch: 11108 Training Loss: 0.11382147555881077 Test Loss: 0.3750166558159722\n",
      "Epoch: 11109 Training Loss: 0.11391365475124783 Test Loss: 0.3742953016493056\n",
      "Epoch: 11110 Training Loss: 0.11402362908257378 Test Loss: 0.3738280164930556\n",
      "Epoch: 11111 Training Loss: 0.11415557522243924 Test Loss: 0.3739000651041667\n",
      "Epoch: 11112 Training Loss: 0.11429899597167968 Test Loss: 0.37421649848090277\n",
      "Epoch: 11113 Training Loss: 0.1144545169406467 Test Loss: 0.3752575954861111\n",
      "Epoch: 11114 Training Loss: 0.11463692898220486 Test Loss: 0.3769853515625\n",
      "Epoch: 11115 Training Loss: 0.11485326300726996 Test Loss: 0.3791020236545139\n",
      "Epoch: 11116 Training Loss: 0.11509833611382378 Test Loss: 0.3812988552517361\n",
      "Epoch: 11117 Training Loss: 0.11535601891411676 Test Loss: 0.383286376953125\n",
      "Epoch: 11118 Training Loss: 0.11559224107530382 Test Loss: 0.3843976779513889\n",
      "Epoch: 11119 Training Loss: 0.11580840216742622 Test Loss: 0.3855023328993056\n",
      "Epoch: 11120 Training Loss: 0.1160220692952474 Test Loss: 0.3863834635416667\n",
      "Epoch: 11121 Training Loss: 0.11617513359917535 Test Loss: 0.3870439453125\n",
      "Epoch: 11122 Training Loss: 0.11626600392659506 Test Loss: 0.3868312445746528\n",
      "Epoch: 11123 Training Loss: 0.11628997972276475 Test Loss: 0.38631431749131945\n",
      "Epoch: 11124 Training Loss: 0.11624958801269532 Test Loss: 0.38518351236979165\n",
      "Epoch: 11125 Training Loss: 0.1161402087741428 Test Loss: 0.38394495985243055\n",
      "Epoch: 11126 Training Loss: 0.11599393802218967 Test Loss: 0.3830894097222222\n",
      "Epoch: 11127 Training Loss: 0.11583999718560113 Test Loss: 0.38207486979166666\n",
      "Epoch: 11128 Training Loss: 0.11570065985785591 Test Loss: 0.38124853515625\n",
      "Epoch: 11129 Training Loss: 0.11558423021104601 Test Loss: 0.3807484266493056\n",
      "Epoch: 11130 Training Loss: 0.11553399319118923 Test Loss: 0.3804733072916667\n",
      "Epoch: 11131 Training Loss: 0.11553275977240668 Test Loss: 0.37986328125\n",
      "Epoch: 11132 Training Loss: 0.11560096910264757 Test Loss: 0.37950699869791665\n",
      "Epoch: 11133 Training Loss: 0.1157429207695855 Test Loss: 0.3791910807291667\n",
      "Epoch: 11134 Training Loss: 0.11596410793728298 Test Loss: 0.3787981228298611\n",
      "Epoch: 11135 Training Loss: 0.11626844278971354 Test Loss: 0.37886279296875\n",
      "Epoch: 11136 Training Loss: 0.11665622541639539 Test Loss: 0.3789363335503472\n",
      "Epoch: 11137 Training Loss: 0.11710413869222006 Test Loss: 0.37883094618055557\n",
      "Epoch: 11138 Training Loss: 0.11760635291205512 Test Loss: 0.3788157009548611\n",
      "Epoch: 11139 Training Loss: 0.11813525984022352 Test Loss: 0.3788808322482639\n",
      "Epoch: 11140 Training Loss: 0.11869177669949002 Test Loss: 0.3793527018229167\n",
      "Epoch: 11141 Training Loss: 0.11925254821777344 Test Loss: 0.3802768012152778\n",
      "Epoch: 11142 Training Loss: 0.11983146667480468 Test Loss: 0.3816498752170139\n",
      "Epoch: 11143 Training Loss: 0.12040883043077256 Test Loss: 0.3827839084201389\n",
      "Epoch: 11144 Training Loss: 0.12099090491400824 Test Loss: 0.3839240993923611\n",
      "Epoch: 11145 Training Loss: 0.12159253268771701 Test Loss: 0.38550800238715277\n",
      "Epoch: 11146 Training Loss: 0.12221540154351128 Test Loss: 0.3876232096354167\n",
      "Epoch: 11147 Training Loss: 0.12281700388590495 Test Loss: 0.38962245008680557\n",
      "Epoch: 11148 Training Loss: 0.12338457912868923 Test Loss: 0.3909392632378472\n",
      "Epoch: 11149 Training Loss: 0.12381121741400825 Test Loss: 0.39038655598958333\n",
      "Epoch: 11150 Training Loss: 0.12401448737250434 Test Loss: 0.38826953125\n",
      "Epoch: 11151 Training Loss: 0.12389945305718315 Test Loss: 0.3857139214409722\n",
      "Epoch: 11152 Training Loss: 0.1234314227634006 Test Loss: 0.38365030924479165\n",
      "Epoch: 11153 Training Loss: 0.12264860619439019 Test Loss: 0.38240516493055554\n",
      "Epoch: 11154 Training Loss: 0.1215897216796875 Test Loss: 0.38218760850694444\n",
      "Epoch: 11155 Training Loss: 0.12043429226345487 Test Loss: 0.3826819390190972\n",
      "Epoch: 11156 Training Loss: 0.11937915886773004 Test Loss: 0.38401806640625\n",
      "Epoch: 11157 Training Loss: 0.11848219892713759 Test Loss: 0.38541883680555555\n",
      "Epoch: 11158 Training Loss: 0.11776225026448568 Test Loss: 0.38603287760416666\n",
      "Epoch: 11159 Training Loss: 0.1171688741048177 Test Loss: 0.3854377983940972\n",
      "Epoch: 11160 Training Loss: 0.11666527048746744 Test Loss: 0.3845878092447917\n",
      "Epoch: 11161 Training Loss: 0.11625055355495877 Test Loss: 0.38379730902777776\n",
      "Epoch: 11162 Training Loss: 0.11592430792914496 Test Loss: 0.38351871744791666\n",
      "Epoch: 11163 Training Loss: 0.11568840620252821 Test Loss: 0.38418671332465276\n",
      "Epoch: 11164 Training Loss: 0.1155401857164171 Test Loss: 0.3858794216579861\n",
      "Epoch: 11165 Training Loss: 0.11545965152316623 Test Loss: 0.38777267795138887\n",
      "Epoch: 11166 Training Loss: 0.11543348185221354 Test Loss: 0.3898240559895833\n",
      "Epoch: 11167 Training Loss: 0.11543058946397569 Test Loss: 0.39177842881944447\n",
      "Epoch: 11168 Training Loss: 0.11545547315809462 Test Loss: 0.3929925130208333\n",
      "Epoch: 11169 Training Loss: 0.1155068105061849 Test Loss: 0.39370361328125\n",
      "Epoch: 11170 Training Loss: 0.11553537495930989 Test Loss: 0.3940239800347222\n",
      "Epoch: 11171 Training Loss: 0.1155199703640408 Test Loss: 0.39344514973958333\n",
      "Epoch: 11172 Training Loss: 0.11543782467312283 Test Loss: 0.3924189181857639\n",
      "Epoch: 11173 Training Loss: 0.11527643754747179 Test Loss: 0.39104066297743056\n",
      "Epoch: 11174 Training Loss: 0.1150621083577474 Test Loss: 0.38922379557291664\n",
      "Epoch: 11175 Training Loss: 0.11480447726779514 Test Loss: 0.3874045138888889\n",
      "Epoch: 11176 Training Loss: 0.11455023786756728 Test Loss: 0.3855148383246528\n",
      "Epoch: 11177 Training Loss: 0.11430328877766927 Test Loss: 0.3840897894965278\n",
      "Epoch: 11178 Training Loss: 0.11407870992024739 Test Loss: 0.382808837890625\n",
      "Epoch: 11179 Training Loss: 0.11389299519856771 Test Loss: 0.381877197265625\n",
      "Epoch: 11180 Training Loss: 0.11373607720269097 Test Loss: 0.3814120279947917\n",
      "Epoch: 11181 Training Loss: 0.11362079026963975 Test Loss: 0.381069091796875\n",
      "Epoch: 11182 Training Loss: 0.11352996487087674 Test Loss: 0.3809924587673611\n",
      "Epoch: 11183 Training Loss: 0.11347608608669704 Test Loss: 0.3811517740885417\n",
      "Epoch: 11184 Training Loss: 0.11345228915744357 Test Loss: 0.3813528103298611\n",
      "Epoch: 11185 Training Loss: 0.11343879614935981 Test Loss: 0.38171622721354165\n",
      "Epoch: 11186 Training Loss: 0.11344657474093967 Test Loss: 0.3821768663194444\n",
      "Epoch: 11187 Training Loss: 0.1134562276204427 Test Loss: 0.3829982367621528\n",
      "Epoch: 11188 Training Loss: 0.11349440341525607 Test Loss: 0.3838536783854167\n",
      "Epoch: 11189 Training Loss: 0.11355371687147353 Test Loss: 0.3846191677517361\n",
      "Epoch: 11190 Training Loss: 0.11362331220838759 Test Loss: 0.38528195529513887\n",
      "Epoch: 11191 Training Loss: 0.11370338270399305 Test Loss: 0.385724609375\n",
      "Epoch: 11192 Training Loss: 0.11379060448540582 Test Loss: 0.38604966905381943\n",
      "Epoch: 11193 Training Loss: 0.11388267856174045 Test Loss: 0.38624191623263887\n",
      "Epoch: 11194 Training Loss: 0.11396377393934462 Test Loss: 0.3863417697482639\n",
      "Epoch: 11195 Training Loss: 0.11405464850531684 Test Loss: 0.38653087022569443\n",
      "Epoch: 11196 Training Loss: 0.11417008633083767 Test Loss: 0.3868632541232639\n",
      "Epoch: 11197 Training Loss: 0.1143084699842665 Test Loss: 0.3870525444878472\n",
      "Epoch: 11198 Training Loss: 0.11444009823269315 Test Loss: 0.3872477213541667\n",
      "Epoch: 11199 Training Loss: 0.11453400336371528 Test Loss: 0.38721104600694445\n",
      "Epoch: 11200 Training Loss: 0.1146315451727973 Test Loss: 0.3869940592447917\n",
      "Epoch: 11201 Training Loss: 0.11472694057888455 Test Loss: 0.38652652994791664\n",
      "Epoch: 11202 Training Loss: 0.1148180414835612 Test Loss: 0.38580902777777776\n",
      "Epoch: 11203 Training Loss: 0.11492733425564236 Test Loss: 0.38502425130208334\n",
      "Epoch: 11204 Training Loss: 0.11508104451497396 Test Loss: 0.38434054904513887\n",
      "Epoch: 11205 Training Loss: 0.11526895565456814 Test Loss: 0.38378062608506947\n",
      "Epoch: 11206 Training Loss: 0.11549627855088976 Test Loss: 0.3835196397569444\n",
      "Epoch: 11207 Training Loss: 0.11576881323920356 Test Loss: 0.38333900282118055\n",
      "Epoch: 11208 Training Loss: 0.11609459601508247 Test Loss: 0.3831982964409722\n",
      "Epoch: 11209 Training Loss: 0.11648331027560764 Test Loss: 0.38269997829861113\n",
      "Epoch: 11210 Training Loss: 0.11691516452365451 Test Loss: 0.38215635850694446\n",
      "Epoch: 11211 Training Loss: 0.11738610246446397 Test Loss: 0.38121337890625\n",
      "Epoch: 11212 Training Loss: 0.11784929911295573 Test Loss: 0.37997309027777776\n",
      "Epoch: 11213 Training Loss: 0.1182918472290039 Test Loss: 0.37965684678819445\n",
      "Epoch: 11214 Training Loss: 0.11869069163004557 Test Loss: 0.37988319227430556\n",
      "Epoch: 11215 Training Loss: 0.11903162044949002 Test Loss: 0.3806344401041667\n",
      "Epoch: 11216 Training Loss: 0.11930017768012152 Test Loss: 0.3821099989149306\n",
      "Epoch: 11217 Training Loss: 0.11947242397732205 Test Loss: 0.38383043077256945\n",
      "Epoch: 11218 Training Loss: 0.11958298322889539 Test Loss: 0.3851494140625\n",
      "Epoch: 11219 Training Loss: 0.11964698537190756 Test Loss: 0.3856824544270833\n",
      "Epoch: 11220 Training Loss: 0.11968251207139757 Test Loss: 0.3851113823784722\n",
      "Epoch: 11221 Training Loss: 0.11967780134412978 Test Loss: 0.3843240017361111\n",
      "Epoch: 11222 Training Loss: 0.11958886125352648 Test Loss: 0.38334309895833335\n",
      "Epoch: 11223 Training Loss: 0.11940357971191407 Test Loss: 0.3827649197048611\n",
      "Epoch: 11224 Training Loss: 0.1191160651312934 Test Loss: 0.3829589029947917\n",
      "Epoch: 11225 Training Loss: 0.1187486343383789 Test Loss: 0.383708251953125\n",
      "Epoch: 11226 Training Loss: 0.11839838748508029 Test Loss: 0.38431632486979167\n",
      "Epoch: 11227 Training Loss: 0.11808814239501954 Test Loss: 0.3845757921006944\n",
      "Epoch: 11228 Training Loss: 0.1178416247897678 Test Loss: 0.3842909071180556\n",
      "Epoch: 11229 Training Loss: 0.11765543365478516 Test Loss: 0.38352408854166664\n",
      "Epoch: 11230 Training Loss: 0.11749155510796441 Test Loss: 0.38295429144965276\n",
      "Epoch: 11231 Training Loss: 0.11733100382486979 Test Loss: 0.38272930230034724\n",
      "Epoch: 11232 Training Loss: 0.11720617421468099 Test Loss: 0.3828653971354167\n",
      "Epoch: 11233 Training Loss: 0.11712138281928168 Test Loss: 0.38311474609375\n",
      "Epoch: 11234 Training Loss: 0.11709477996826172 Test Loss: 0.3835080837673611\n",
      "Epoch: 11235 Training Loss: 0.11715370262993706 Test Loss: 0.38338123914930555\n",
      "Epoch: 11236 Training Loss: 0.11729625786675348 Test Loss: 0.38353412543402776\n",
      "Epoch: 11237 Training Loss: 0.11753847757975261 Test Loss: 0.38476014539930553\n",
      "Epoch: 11238 Training Loss: 0.11786322869194879 Test Loss: 0.38659928385416664\n",
      "Epoch: 11239 Training Loss: 0.11828377193874783 Test Loss: 0.38897865125868053\n",
      "Epoch: 11240 Training Loss: 0.11875703345404731 Test Loss: 0.39068212890625\n",
      "Epoch: 11241 Training Loss: 0.11924107615152994 Test Loss: 0.3915793728298611\n",
      "Epoch: 11242 Training Loss: 0.11976485527886285 Test Loss: 0.39195399305555556\n",
      "Epoch: 11243 Training Loss: 0.1202903060913086 Test Loss: 0.39191498480902776\n",
      "Epoch: 11244 Training Loss: 0.12077814737955729 Test Loss: 0.391914794921875\n",
      "Epoch: 11245 Training Loss: 0.12122565290662977 Test Loss: 0.3918019748263889\n",
      "Epoch: 11246 Training Loss: 0.1215623779296875 Test Loss: 0.39216615125868054\n",
      "Epoch: 11247 Training Loss: 0.12176720258924696 Test Loss: 0.3929701605902778\n",
      "Epoch: 11248 Training Loss: 0.12184674326578776 Test Loss: 0.39401700846354165\n",
      "Epoch: 11249 Training Loss: 0.12176990593804253 Test Loss: 0.3940799153645833\n",
      "Epoch: 11250 Training Loss: 0.12159800381130642 Test Loss: 0.39334895833333333\n",
      "Epoch: 11251 Training Loss: 0.12136894734700521 Test Loss: 0.39210777452256945\n",
      "Epoch: 11252 Training Loss: 0.12108598836263021 Test Loss: 0.3909953342013889\n",
      "Epoch: 11253 Training Loss: 0.12075541263156467 Test Loss: 0.3898413628472222\n",
      "Epoch: 11254 Training Loss: 0.12035983445909289 Test Loss: 0.3884309353298611\n",
      "Epoch: 11255 Training Loss: 0.11995487043592665 Test Loss: 0.38728059895833333\n",
      "Epoch: 11256 Training Loss: 0.11956784990098741 Test Loss: 0.38574959309895834\n",
      "Epoch: 11257 Training Loss: 0.11921551683213975 Test Loss: 0.38392732747395836\n",
      "Epoch: 11258 Training Loss: 0.11890535905626085 Test Loss: 0.3820693359375\n",
      "Epoch: 11259 Training Loss: 0.11865380096435547 Test Loss: 0.38033729383680553\n",
      "Epoch: 11260 Training Loss: 0.11845879279242622 Test Loss: 0.3790195855034722\n",
      "Epoch: 11261 Training Loss: 0.11830708736843533 Test Loss: 0.3777175564236111\n",
      "Epoch: 11262 Training Loss: 0.11821858639187283 Test Loss: 0.3769298773871528\n",
      "Epoch: 11263 Training Loss: 0.11820882161458333 Test Loss: 0.3764704861111111\n",
      "Epoch: 11264 Training Loss: 0.1182878146701389 Test Loss: 0.3765879177517361\n",
      "Epoch: 11265 Training Loss: 0.11845271809895834 Test Loss: 0.37785584852430554\n",
      "Epoch: 11266 Training Loss: 0.11868661753336589 Test Loss: 0.37897493489583334\n",
      "Epoch: 11267 Training Loss: 0.11894859313964844 Test Loss: 0.3801772189670139\n",
      "Epoch: 11268 Training Loss: 0.1192083740234375 Test Loss: 0.38114778645833336\n",
      "Epoch: 11269 Training Loss: 0.1194670401679145 Test Loss: 0.38208213975694444\n",
      "Epoch: 11270 Training Loss: 0.11969717831081814 Test Loss: 0.38263400607638887\n",
      "Epoch: 11271 Training Loss: 0.11987838575575087 Test Loss: 0.38348600260416665\n",
      "Epoch: 11272 Training Loss: 0.11997237226698133 Test Loss: 0.3841305881076389\n",
      "Epoch: 11273 Training Loss: 0.11996226331922744 Test Loss: 0.3844343532986111\n",
      "Epoch: 11274 Training Loss: 0.11988692643907335 Test Loss: 0.38509516059027776\n",
      "Epoch: 11275 Training Loss: 0.11971959092881944 Test Loss: 0.38587944878472225\n",
      "Epoch: 11276 Training Loss: 0.11948227267795139 Test Loss: 0.386435791015625\n",
      "Epoch: 11277 Training Loss: 0.11919508446587457 Test Loss: 0.38680517578125\n",
      "Epoch: 11278 Training Loss: 0.11887652503119575 Test Loss: 0.3873451877170139\n",
      "Epoch: 11279 Training Loss: 0.1185683585272895 Test Loss: 0.38797607421875\n",
      "Epoch: 11280 Training Loss: 0.118257445441352 Test Loss: 0.38864108615451387\n",
      "Epoch: 11281 Training Loss: 0.11795679134792751 Test Loss: 0.38913080512152776\n",
      "Epoch: 11282 Training Loss: 0.11768660905626085 Test Loss: 0.3894074435763889\n",
      "Epoch: 11283 Training Loss: 0.11742528194851345 Test Loss: 0.3894701877170139\n",
      "Epoch: 11284 Training Loss: 0.11716942596435546 Test Loss: 0.3892907986111111\n",
      "Epoch: 11285 Training Loss: 0.11692763349745008 Test Loss: 0.38890985785590276\n",
      "Epoch: 11286 Training Loss: 0.11672035471598308 Test Loss: 0.3883314887152778\n",
      "Epoch: 11287 Training Loss: 0.11652708943684896 Test Loss: 0.38810416666666664\n",
      "Epoch: 11288 Training Loss: 0.11635431925455729 Test Loss: 0.38747265625\n",
      "Epoch: 11289 Training Loss: 0.11619790395100911 Test Loss: 0.386763427734375\n",
      "Epoch: 11290 Training Loss: 0.11605788252088758 Test Loss: 0.3861602105034722\n",
      "Epoch: 11291 Training Loss: 0.11593966674804687 Test Loss: 0.38592317708333335\n",
      "Epoch: 11292 Training Loss: 0.11585789320203993 Test Loss: 0.38546058485243057\n",
      "Epoch: 11293 Training Loss: 0.11578859795464409 Test Loss: 0.3852298177083333\n",
      "Epoch: 11294 Training Loss: 0.11571490224202474 Test Loss: 0.3851940104166667\n",
      "Epoch: 11295 Training Loss: 0.11564603084988065 Test Loss: 0.38520027669270834\n",
      "Epoch: 11296 Training Loss: 0.11560871293809678 Test Loss: 0.38499517144097223\n",
      "Epoch: 11297 Training Loss: 0.11557611592610677 Test Loss: 0.3850466579861111\n",
      "Epoch: 11298 Training Loss: 0.11554365624321832 Test Loss: 0.3849070095486111\n",
      "Epoch: 11299 Training Loss: 0.11551749081081815 Test Loss: 0.3847102322048611\n",
      "Epoch: 11300 Training Loss: 0.11550794050428602 Test Loss: 0.384619140625\n",
      "Epoch: 11301 Training Loss: 0.1155095689561632 Test Loss: 0.3846317816840278\n",
      "Epoch: 11302 Training Loss: 0.11551556905110677 Test Loss: 0.38485302734375\n",
      "Epoch: 11303 Training Loss: 0.11552635447184245 Test Loss: 0.3852318793402778\n",
      "Epoch: 11304 Training Loss: 0.1155405748155382 Test Loss: 0.38561669921875\n",
      "Epoch: 11305 Training Loss: 0.11556554158528645 Test Loss: 0.3858235677083333\n",
      "Epoch: 11306 Training Loss: 0.11558374192979601 Test Loss: 0.3859287109375\n",
      "Epoch: 11307 Training Loss: 0.11561569129096137 Test Loss: 0.3855293240017361\n",
      "Epoch: 11308 Training Loss: 0.11565050082736546 Test Loss: 0.3847746853298611\n",
      "Epoch: 11309 Training Loss: 0.11568865627712674 Test Loss: 0.38380406358506947\n",
      "Epoch: 11310 Training Loss: 0.11574709998236762 Test Loss: 0.3827498101128472\n",
      "Epoch: 11311 Training Loss: 0.11581512790256077 Test Loss: 0.38146104600694447\n",
      "Epoch: 11312 Training Loss: 0.1158875732421875 Test Loss: 0.38030460611979167\n",
      "Epoch: 11313 Training Loss: 0.11598746914333767 Test Loss: 0.37932948133680555\n",
      "Epoch: 11314 Training Loss: 0.11610433790418836 Test Loss: 0.3785467664930556\n",
      "Epoch: 11315 Training Loss: 0.1162497795952691 Test Loss: 0.37816861979166666\n",
      "Epoch: 11316 Training Loss: 0.11644071027967665 Test Loss: 0.3782179904513889\n",
      "Epoch: 11317 Training Loss: 0.11669148763020833 Test Loss: 0.37868994140625\n",
      "Epoch: 11318 Training Loss: 0.11700227949354383 Test Loss: 0.3796003689236111\n",
      "Epoch: 11319 Training Loss: 0.11736996036105686 Test Loss: 0.3801016167534722\n",
      "Epoch: 11320 Training Loss: 0.11773501332600911 Test Loss: 0.38032481553819447\n",
      "Epoch: 11321 Training Loss: 0.11809021759033203 Test Loss: 0.3796882866753472\n",
      "Epoch: 11322 Training Loss: 0.11844177754720052 Test Loss: 0.37913880750868056\n",
      "Epoch: 11323 Training Loss: 0.1187477052476671 Test Loss: 0.3782801920572917\n",
      "Epoch: 11324 Training Loss: 0.11899189673529731 Test Loss: 0.37706629774305556\n",
      "Epoch: 11325 Training Loss: 0.1192113766140408 Test Loss: 0.376420166015625\n",
      "Epoch: 11326 Training Loss: 0.11939106157090928 Test Loss: 0.3757029622395833\n",
      "Epoch: 11327 Training Loss: 0.11952538469102647 Test Loss: 0.37545475260416666\n",
      "Epoch: 11328 Training Loss: 0.11957841491699218 Test Loss: 0.3751622178819444\n",
      "Epoch: 11329 Training Loss: 0.11954971398247613 Test Loss: 0.37472086588541664\n",
      "Epoch: 11330 Training Loss: 0.1194219012790256 Test Loss: 0.3740239800347222\n",
      "Epoch: 11331 Training Loss: 0.11925044843885634 Test Loss: 0.3733593207465278\n",
      "Epoch: 11332 Training Loss: 0.11903984663221572 Test Loss: 0.3725710991753472\n",
      "Epoch: 11333 Training Loss: 0.11880227491590711 Test Loss: 0.37189293077256946\n",
      "Epoch: 11334 Training Loss: 0.11855629052056206 Test Loss: 0.37166826714409723\n",
      "Epoch: 11335 Training Loss: 0.1182978023952908 Test Loss: 0.37222672526041667\n",
      "Epoch: 11336 Training Loss: 0.1180326656765408 Test Loss: 0.3732281358506944\n",
      "Epoch: 11337 Training Loss: 0.11774182637532551 Test Loss: 0.37489534505208333\n",
      "Epoch: 11338 Training Loss: 0.11744859059651692 Test Loss: 0.3764600151909722\n",
      "Epoch: 11339 Training Loss: 0.11716141001383464 Test Loss: 0.37834285481770835\n",
      "Epoch: 11340 Training Loss: 0.116918091668023 Test Loss: 0.38022762044270836\n",
      "Epoch: 11341 Training Loss: 0.11674753146701389 Test Loss: 0.38234364149305555\n",
      "Epoch: 11342 Training Loss: 0.11663792673746745 Test Loss: 0.38408973524305556\n",
      "Epoch: 11343 Training Loss: 0.11655072869194878 Test Loss: 0.38541758897569445\n",
      "Epoch: 11344 Training Loss: 0.11646914164225261 Test Loss: 0.3861318901909722\n",
      "Epoch: 11345 Training Loss: 0.11636722056070964 Test Loss: 0.3863342013888889\n",
      "Epoch: 11346 Training Loss: 0.11624598269992405 Test Loss: 0.38628181966145836\n",
      "Epoch: 11347 Training Loss: 0.11611675092909071 Test Loss: 0.38562586805555554\n",
      "Epoch: 11348 Training Loss: 0.11597108205159505 Test Loss: 0.3849760199652778\n",
      "Epoch: 11349 Training Loss: 0.11581045871310763 Test Loss: 0.38412847222222224\n",
      "Epoch: 11350 Training Loss: 0.115639035542806 Test Loss: 0.3835608723958333\n",
      "Epoch: 11351 Training Loss: 0.11545937432183159 Test Loss: 0.38320377604166667\n",
      "Epoch: 11352 Training Loss: 0.11528024122450087 Test Loss: 0.38264827473958335\n",
      "Epoch: 11353 Training Loss: 0.11510616132948134 Test Loss: 0.382078125\n",
      "Epoch: 11354 Training Loss: 0.11493343861897787 Test Loss: 0.381322021484375\n",
      "Epoch: 11355 Training Loss: 0.11475528632269966 Test Loss: 0.3806369900173611\n",
      "Epoch: 11356 Training Loss: 0.11458446841769748 Test Loss: 0.3798897026909722\n",
      "Epoch: 11357 Training Loss: 0.11443700069851345 Test Loss: 0.37933984375\n",
      "Epoch: 11358 Training Loss: 0.11433161332872178 Test Loss: 0.37870982530381947\n",
      "Epoch: 11359 Training Loss: 0.11425627729627821 Test Loss: 0.37834494357638887\n",
      "Epoch: 11360 Training Loss: 0.11420638868543836 Test Loss: 0.37810799153645835\n",
      "Epoch: 11361 Training Loss: 0.11417204708523221 Test Loss: 0.37822894965277776\n",
      "Epoch: 11362 Training Loss: 0.11416493903266059 Test Loss: 0.3786835394965278\n",
      "Epoch: 11363 Training Loss: 0.11417997826470269 Test Loss: 0.37921245659722225\n",
      "Epoch: 11364 Training Loss: 0.11420856136745877 Test Loss: 0.38010704210069446\n",
      "Epoch: 11365 Training Loss: 0.11426702965630425 Test Loss: 0.38070884874131944\n",
      "Epoch: 11366 Training Loss: 0.11433230251736111 Test Loss: 0.3814966634114583\n",
      "Epoch: 11367 Training Loss: 0.11438620249430338 Test Loss: 0.3819079861111111\n",
      "Epoch: 11368 Training Loss: 0.1144612316555447 Test Loss: 0.38259440104166664\n",
      "Epoch: 11369 Training Loss: 0.1145636478000217 Test Loss: 0.3826145833333333\n",
      "Epoch: 11370 Training Loss: 0.11467619408501518 Test Loss: 0.38234214952256945\n",
      "Epoch: 11371 Training Loss: 0.11481283145480686 Test Loss: 0.38157568359375\n",
      "Epoch: 11372 Training Loss: 0.11498597802056207 Test Loss: 0.3806278483072917\n",
      "Epoch: 11373 Training Loss: 0.11519933827718098 Test Loss: 0.37956743706597224\n",
      "Epoch: 11374 Training Loss: 0.11545339796278212 Test Loss: 0.37827327473958333\n",
      "Epoch: 11375 Training Loss: 0.11575674947102865 Test Loss: 0.37761162651909724\n",
      "Epoch: 11376 Training Loss: 0.11611146121554905 Test Loss: 0.3768987087673611\n",
      "Epoch: 11377 Training Loss: 0.11650287119547525 Test Loss: 0.3768713107638889\n",
      "Epoch: 11378 Training Loss: 0.1169649904039171 Test Loss: 0.3777577039930556\n",
      "Epoch: 11379 Training Loss: 0.11743599107530382 Test Loss: 0.37891948784722224\n",
      "Epoch: 11380 Training Loss: 0.11789964803059896 Test Loss: 0.3807204861111111\n",
      "Epoch: 11381 Training Loss: 0.11830722893608941 Test Loss: 0.3827255859375\n",
      "Epoch: 11382 Training Loss: 0.11865305497911241 Test Loss: 0.38405159505208336\n",
      "Epoch: 11383 Training Loss: 0.11889362589518229 Test Loss: 0.384578369140625\n",
      "Epoch: 11384 Training Loss: 0.11898840586344402 Test Loss: 0.3835257703993056\n",
      "Epoch: 11385 Training Loss: 0.11889638349745009 Test Loss: 0.38128106011284724\n",
      "Epoch: 11386 Training Loss: 0.11861976962619358 Test Loss: 0.37877018229166665\n",
      "Epoch: 11387 Training Loss: 0.11818884785970052 Test Loss: 0.37681401909722223\n",
      "Epoch: 11388 Training Loss: 0.11770240190294054 Test Loss: 0.3760941840277778\n",
      "Epoch: 11389 Training Loss: 0.1172291497124566 Test Loss: 0.37645692274305553\n",
      "Epoch: 11390 Training Loss: 0.11682210625542534 Test Loss: 0.3777283799913194\n",
      "Epoch: 11391 Training Loss: 0.11654469129774306 Test Loss: 0.3795148654513889\n",
      "Epoch: 11392 Training Loss: 0.11635230170355902 Test Loss: 0.3811994086371528\n",
      "Epoch: 11393 Training Loss: 0.11622330474853515 Test Loss: 0.38208921983506944\n",
      "Epoch: 11394 Training Loss: 0.11613587273491753 Test Loss: 0.38244173177083335\n",
      "Epoch: 11395 Training Loss: 0.11608257802327474 Test Loss: 0.3823839518229167\n",
      "Epoch: 11396 Training Loss: 0.11603609890407986 Test Loss: 0.38190028211805555\n",
      "Epoch: 11397 Training Loss: 0.11599959988064236 Test Loss: 0.38135655381944444\n",
      "Epoch: 11398 Training Loss: 0.1160082270304362 Test Loss: 0.3810295681423611\n",
      "Epoch: 11399 Training Loss: 0.11609100087483724 Test Loss: 0.3811538357204861\n",
      "Epoch: 11400 Training Loss: 0.1162349107530382 Test Loss: 0.3818329535590278\n",
      "Epoch: 11401 Training Loss: 0.11643643697102865 Test Loss: 0.3829236382378472\n",
      "Epoch: 11402 Training Loss: 0.11669623565673828 Test Loss: 0.38450336371527777\n",
      "Epoch: 11403 Training Loss: 0.11700207943386502 Test Loss: 0.38633146158854165\n",
      "Epoch: 11404 Training Loss: 0.11734258355034723 Test Loss: 0.3881856011284722\n",
      "Epoch: 11405 Training Loss: 0.11766439904106987 Test Loss: 0.390041015625\n",
      "Epoch: 11406 Training Loss: 0.11798372141520182 Test Loss: 0.3918790690104167\n",
      "Epoch: 11407 Training Loss: 0.11826702796088324 Test Loss: 0.39296381293402777\n",
      "Epoch: 11408 Training Loss: 0.11854198201497396 Test Loss: 0.3932854275173611\n",
      "Epoch: 11409 Training Loss: 0.11881932661268446 Test Loss: 0.39325352647569445\n",
      "Epoch: 11410 Training Loss: 0.11908259667290581 Test Loss: 0.39242664930555554\n",
      "Epoch: 11411 Training Loss: 0.11931661563449436 Test Loss: 0.3914779459635417\n",
      "Epoch: 11412 Training Loss: 0.11950778452555338 Test Loss: 0.39031201171875\n",
      "Epoch: 11413 Training Loss: 0.11971396297878689 Test Loss: 0.38904399956597224\n",
      "Epoch: 11414 Training Loss: 0.1199621310763889 Test Loss: 0.3879997016059028\n",
      "Epoch: 11415 Training Loss: 0.12027206251356337 Test Loss: 0.3872637803819444\n",
      "Epoch: 11416 Training Loss: 0.12064361148410373 Test Loss: 0.3865775824652778\n",
      "Epoch: 11417 Training Loss: 0.12112232293023004 Test Loss: 0.3864797905815972\n",
      "Epoch: 11418 Training Loss: 0.12168944973415799 Test Loss: 0.38658897569444445\n",
      "Epoch: 11419 Training Loss: 0.12234801652696398 Test Loss: 0.3869222005208333\n",
      "Epoch: 11420 Training Loss: 0.12305688052707248 Test Loss: 0.3875576443142361\n",
      "Epoch: 11421 Training Loss: 0.12376160939534506 Test Loss: 0.38908308919270834\n",
      "Epoch: 11422 Training Loss: 0.12440706210666233 Test Loss: 0.39167936197916664\n",
      "Epoch: 11423 Training Loss: 0.12494851599799262 Test Loss: 0.39496568467881943\n",
      "Epoch: 11424 Training Loss: 0.1253681174384223 Test Loss: 0.39965633138020834\n",
      "Epoch: 11425 Training Loss: 0.1256469972398546 Test Loss: 0.40503016493055555\n",
      "Epoch: 11426 Training Loss: 0.12576549530029296 Test Loss: 0.40859404839409724\n",
      "Epoch: 11427 Training Loss: 0.12567077212863498 Test Loss: 0.4096385091145833\n",
      "Epoch: 11428 Training Loss: 0.12526625654432508 Test Loss: 0.4073484429253472\n",
      "Epoch: 11429 Training Loss: 0.12454080030653211 Test Loss: 0.40301432291666667\n",
      "Epoch: 11430 Training Loss: 0.12358612399631076 Test Loss: 0.39913129340277775\n",
      "Epoch: 11431 Training Loss: 0.12260415479871961 Test Loss: 0.39522998046875\n",
      "Epoch: 11432 Training Loss: 0.12168338860405815 Test Loss: 0.39151448567708336\n",
      "Epoch: 11433 Training Loss: 0.12091839514838325 Test Loss: 0.3888935546875\n",
      "Epoch: 11434 Training Loss: 0.12034834967719184 Test Loss: 0.3873576117621528\n",
      "Epoch: 11435 Training Loss: 0.11990545060899523 Test Loss: 0.38641373697916664\n",
      "Epoch: 11436 Training Loss: 0.11957549540201823 Test Loss: 0.3859411349826389\n",
      "Epoch: 11437 Training Loss: 0.1193144276936849 Test Loss: 0.3853976779513889\n",
      "Epoch: 11438 Training Loss: 0.11909231313069661 Test Loss: 0.38491764322916666\n",
      "Epoch: 11439 Training Loss: 0.11889349704318576 Test Loss: 0.3849236382378472\n",
      "Epoch: 11440 Training Loss: 0.11871798282199436 Test Loss: 0.3856523166232639\n",
      "Epoch: 11441 Training Loss: 0.11854679616292317 Test Loss: 0.3871064995659722\n",
      "Epoch: 11442 Training Loss: 0.11842469787597656 Test Loss: 0.38854288736979165\n",
      "Epoch: 11443 Training Loss: 0.11832754855685763 Test Loss: 0.38950379774305555\n",
      "Epoch: 11444 Training Loss: 0.11824534606933594 Test Loss: 0.39013758680555555\n",
      "Epoch: 11445 Training Loss: 0.11814432271321615 Test Loss: 0.3904867892795139\n",
      "Epoch: 11446 Training Loss: 0.11805853271484375 Test Loss: 0.39064960394965276\n",
      "Epoch: 11447 Training Loss: 0.11799091762966579 Test Loss: 0.3907842068142361\n",
      "Epoch: 11448 Training Loss: 0.11799797397189671 Test Loss: 0.39104356553819447\n",
      "Epoch: 11449 Training Loss: 0.11806288146972656 Test Loss: 0.3912785915798611\n",
      "Epoch: 11450 Training Loss: 0.11821598222520616 Test Loss: 0.3914362521701389\n",
      "Epoch: 11451 Training Loss: 0.11845022837320963 Test Loss: 0.39145548502604166\n",
      "Epoch: 11452 Training Loss: 0.11877497270372178 Test Loss: 0.39114491102430554\n",
      "Epoch: 11453 Training Loss: 0.119172120836046 Test Loss: 0.39102039930555554\n",
      "Epoch: 11454 Training Loss: 0.1196275134616428 Test Loss: 0.3908045247395833\n",
      "Epoch: 11455 Training Loss: 0.12011766560872396 Test Loss: 0.39085975477430557\n",
      "Epoch: 11456 Training Loss: 0.1206118172539605 Test Loss: 0.39146044921875\n",
      "Epoch: 11457 Training Loss: 0.12103390079074436 Test Loss: 0.39300032552083336\n",
      "Epoch: 11458 Training Loss: 0.12137401072184245 Test Loss: 0.3957670627170139\n",
      "Epoch: 11459 Training Loss: 0.12167120700412326 Test Loss: 0.398821044921875\n",
      "Epoch: 11460 Training Loss: 0.12190049658881294 Test Loss: 0.4013899197048611\n",
      "Epoch: 11461 Training Loss: 0.12207676951090494 Test Loss: 0.40319463433159725\n",
      "Epoch: 11462 Training Loss: 0.1221922370062934 Test Loss: 0.40347287326388886\n",
      "Epoch: 11463 Training Loss: 0.12227110120985243 Test Loss: 0.4029592827690972\n",
      "Epoch: 11464 Training Loss: 0.12231312645806207 Test Loss: 0.40130962456597224\n",
      "Epoch: 11465 Training Loss: 0.1224192894829644 Test Loss: 0.4000677083333333\n",
      "Epoch: 11466 Training Loss: 0.12259039900037977 Test Loss: 0.39993853081597225\n",
      "Epoch: 11467 Training Loss: 0.1228693584865994 Test Loss: 0.4004259711371528\n",
      "Epoch: 11468 Training Loss: 0.12323808966742622 Test Loss: 0.40033181423611114\n",
      "Epoch: 11469 Training Loss: 0.12369834391276041 Test Loss: 0.39803895399305556\n",
      "Epoch: 11470 Training Loss: 0.12422486707899305 Test Loss: 0.3945566677517361\n",
      "Epoch: 11471 Training Loss: 0.12475826432969835 Test Loss: 0.3918486056857639\n",
      "Epoch: 11472 Training Loss: 0.12522875298394098 Test Loss: 0.3903896213107639\n",
      "Epoch: 11473 Training Loss: 0.12554954783121744 Test Loss: 0.38994080946180554\n",
      "Epoch: 11474 Training Loss: 0.1256347147623698 Test Loss: 0.3923472493489583\n",
      "Epoch: 11475 Training Loss: 0.1255168685913086 Test Loss: 0.39607386610243056\n",
      "Epoch: 11476 Training Loss: 0.12527681816948785 Test Loss: 0.4006180555555556\n",
      "Epoch: 11477 Training Loss: 0.12487831285264757 Test Loss: 0.4050676540798611\n",
      "Epoch: 11478 Training Loss: 0.12433907741970486 Test Loss: 0.40837923177083335\n",
      "Epoch: 11479 Training Loss: 0.12369266679551866 Test Loss: 0.408576904296875\n",
      "Epoch: 11480 Training Loss: 0.12296268717447917 Test Loss: 0.40606285264756947\n",
      "Epoch: 11481 Training Loss: 0.12220085906982422 Test Loss: 0.4013172743055556\n",
      "Epoch: 11482 Training Loss: 0.12147184160020616 Test Loss: 0.3966537814670139\n",
      "Epoch: 11483 Training Loss: 0.12079712337917752 Test Loss: 0.3930085720486111\n",
      "Epoch: 11484 Training Loss: 0.12020834350585938 Test Loss: 0.3905245225694444\n",
      "Epoch: 11485 Training Loss: 0.11971656968858507 Test Loss: 0.3882297634548611\n",
      "Epoch: 11486 Training Loss: 0.11931683943006727 Test Loss: 0.38622447374131946\n",
      "Epoch: 11487 Training Loss: 0.11901769595675998 Test Loss: 0.3841311306423611\n",
      "Epoch: 11488 Training Loss: 0.11878433227539062 Test Loss: 0.3818458658854167\n",
      "Epoch: 11489 Training Loss: 0.11860472022162544 Test Loss: 0.37980010308159723\n",
      "Epoch: 11490 Training Loss: 0.11847567325168186 Test Loss: 0.37776155598958333\n",
      "Epoch: 11491 Training Loss: 0.11836637878417969 Test Loss: 0.37644908311631947\n",
      "Epoch: 11492 Training Loss: 0.11824289194742839 Test Loss: 0.37565733506944443\n",
      "Epoch: 11493 Training Loss: 0.11807835218641494 Test Loss: 0.3758303493923611\n",
      "Epoch: 11494 Training Loss: 0.11787080807156033 Test Loss: 0.3767225477430556\n",
      "Epoch: 11495 Training Loss: 0.1175906236436632 Test Loss: 0.37824107530381945\n",
      "Epoch: 11496 Training Loss: 0.11723135545518663 Test Loss: 0.38006309678819444\n",
      "Epoch: 11497 Training Loss: 0.11682392883300781 Test Loss: 0.3816149359809028\n",
      "Epoch: 11498 Training Loss: 0.11637153625488281 Test Loss: 0.3825559895833333\n",
      "Epoch: 11499 Training Loss: 0.1158733639187283 Test Loss: 0.38249568684895835\n",
      "Epoch: 11500 Training Loss: 0.1153730697631836 Test Loss: 0.38180213758680553\n",
      "Epoch: 11501 Training Loss: 0.11488669925265842 Test Loss: 0.3802725151909722\n",
      "Epoch: 11502 Training Loss: 0.11442664591471355 Test Loss: 0.3782541775173611\n",
      "Epoch: 11503 Training Loss: 0.11401445770263671 Test Loss: 0.3759509006076389\n",
      "Epoch: 11504 Training Loss: 0.11365530734592014 Test Loss: 0.3736053602430556\n",
      "Epoch: 11505 Training Loss: 0.11333838060167101 Test Loss: 0.3714769694010417\n",
      "Epoch: 11506 Training Loss: 0.11304158613416884 Test Loss: 0.36965700954861114\n",
      "Epoch: 11507 Training Loss: 0.11279224565294053 Test Loss: 0.368516357421875\n",
      "Epoch: 11508 Training Loss: 0.11259439002143012 Test Loss: 0.3679450412326389\n",
      "Epoch: 11509 Training Loss: 0.11244017198350695 Test Loss: 0.3677488878038194\n",
      "Epoch: 11510 Training Loss: 0.11230880059136285 Test Loss: 0.36810923936631945\n",
      "Epoch: 11511 Training Loss: 0.11219415028889974 Test Loss: 0.36864388020833333\n",
      "Epoch: 11512 Training Loss: 0.11210137854682074 Test Loss: 0.3693305121527778\n",
      "Epoch: 11513 Training Loss: 0.11201426018608941 Test Loss: 0.36994476996527775\n",
      "Epoch: 11514 Training Loss: 0.11194395701090495 Test Loss: 0.3704919704861111\n",
      "Epoch: 11515 Training Loss: 0.11189346652560764 Test Loss: 0.3706820746527778\n",
      "Epoch: 11516 Training Loss: 0.11185864088270399 Test Loss: 0.37084852430555554\n",
      "Epoch: 11517 Training Loss: 0.11182809617784288 Test Loss: 0.3706789822048611\n",
      "Epoch: 11518 Training Loss: 0.11179409959581163 Test Loss: 0.3702663302951389\n",
      "Epoch: 11519 Training Loss: 0.1117587636311849 Test Loss: 0.36968853081597225\n",
      "Epoch: 11520 Training Loss: 0.11173035769992405 Test Loss: 0.36926331922743055\n",
      "Epoch: 11521 Training Loss: 0.11170268927680121 Test Loss: 0.36890966796875\n",
      "Epoch: 11522 Training Loss: 0.1116633767022027 Test Loss: 0.36889567057291667\n",
      "Epoch: 11523 Training Loss: 0.11163374667697483 Test Loss: 0.36899568684895834\n",
      "Epoch: 11524 Training Loss: 0.11160091145833333 Test Loss: 0.36937413194444446\n",
      "Epoch: 11525 Training Loss: 0.1115706303914388 Test Loss: 0.36990079752604166\n",
      "Epoch: 11526 Training Loss: 0.11154740736219618 Test Loss: 0.3704628092447917\n",
      "Epoch: 11527 Training Loss: 0.11152162594265408 Test Loss: 0.3709490017361111\n",
      "Epoch: 11528 Training Loss: 0.11150959099663628 Test Loss: 0.37157766384548613\n",
      "Epoch: 11529 Training Loss: 0.11151597425672743 Test Loss: 0.37171742078993053\n",
      "Epoch: 11530 Training Loss: 0.11153186628553602 Test Loss: 0.3716467013888889\n",
      "Epoch: 11531 Training Loss: 0.11155167134602864 Test Loss: 0.3708311631944444\n",
      "Epoch: 11532 Training Loss: 0.11157188245985243 Test Loss: 0.3700425889756944\n",
      "Epoch: 11533 Training Loss: 0.111592896355523 Test Loss: 0.36880425347222223\n",
      "Epoch: 11534 Training Loss: 0.11161596086290147 Test Loss: 0.3672726779513889\n",
      "Epoch: 11535 Training Loss: 0.11164790429009332 Test Loss: 0.3658675672743056\n",
      "Epoch: 11536 Training Loss: 0.11169436136881511 Test Loss: 0.36472767469618056\n",
      "Epoch: 11537 Training Loss: 0.11175136311848959 Test Loss: 0.3642160101996528\n",
      "Epoch: 11538 Training Loss: 0.111830076429579 Test Loss: 0.36419609917534723\n",
      "Epoch: 11539 Training Loss: 0.11194078572591146 Test Loss: 0.36509871419270834\n",
      "Epoch: 11540 Training Loss: 0.11207171207004123 Test Loss: 0.3663881564670139\n",
      "Epoch: 11541 Training Loss: 0.11221392737494575 Test Loss: 0.3679159342447917\n",
      "Epoch: 11542 Training Loss: 0.11236619483100044 Test Loss: 0.3694533420138889\n",
      "Epoch: 11543 Training Loss: 0.1125311041937934 Test Loss: 0.37050461154513886\n",
      "Epoch: 11544 Training Loss: 0.11270059288872612 Test Loss: 0.37090980360243053\n",
      "Epoch: 11545 Training Loss: 0.11286624315049913 Test Loss: 0.3706529947916667\n",
      "Epoch: 11546 Training Loss: 0.11301246558295355 Test Loss: 0.36957931857638887\n",
      "Epoch: 11547 Training Loss: 0.1131226069132487 Test Loss: 0.36793305121527775\n",
      "Epoch: 11548 Training Loss: 0.11320702785915798 Test Loss: 0.36630126953125\n",
      "Epoch: 11549 Training Loss: 0.11325633663601345 Test Loss: 0.36520608181423614\n",
      "Epoch: 11550 Training Loss: 0.11325698852539062 Test Loss: 0.3649875217013889\n",
      "Epoch: 11551 Training Loss: 0.11321106296115452 Test Loss: 0.3658610297309028\n",
      "Epoch: 11552 Training Loss: 0.11308970726860894 Test Loss: 0.3674982638888889\n",
      "Epoch: 11553 Training Loss: 0.1129343973795573 Test Loss: 0.3690802951388889\n",
      "Epoch: 11554 Training Loss: 0.11275692833794487 Test Loss: 0.3698974880642361\n",
      "Epoch: 11555 Training Loss: 0.1125533684624566 Test Loss: 0.36989637586805557\n",
      "Epoch: 11556 Training Loss: 0.11231974029541016 Test Loss: 0.36887396918402776\n",
      "Epoch: 11557 Training Loss: 0.11207982296413846 Test Loss: 0.36724012586805554\n",
      "Epoch: 11558 Training Loss: 0.11183957248263889 Test Loss: 0.3652253146701389\n",
      "Epoch: 11559 Training Loss: 0.11161626519097222 Test Loss: 0.363463623046875\n",
      "Epoch: 11560 Training Loss: 0.11141629028320313 Test Loss: 0.36228206380208333\n",
      "Epoch: 11561 Training Loss: 0.11123285844590929 Test Loss: 0.36186534288194444\n",
      "Epoch: 11562 Training Loss: 0.11110332912868924 Test Loss: 0.36214518229166665\n",
      "Epoch: 11563 Training Loss: 0.11103028954399957 Test Loss: 0.3630175238715278\n",
      "Epoch: 11564 Training Loss: 0.11101062181260851 Test Loss: 0.3641108669704861\n",
      "Epoch: 11565 Training Loss: 0.11105705854627822 Test Loss: 0.36520518663194446\n",
      "Epoch: 11566 Training Loss: 0.1111643558078342 Test Loss: 0.3660667317708333\n",
      "Epoch: 11567 Training Loss: 0.11131041039360894 Test Loss: 0.366849853515625\n",
      "Epoch: 11568 Training Loss: 0.11147733815511068 Test Loss: 0.36746240234375\n",
      "Epoch: 11569 Training Loss: 0.11164800177680122 Test Loss: 0.36820496961805554\n",
      "Epoch: 11570 Training Loss: 0.11180891757541232 Test Loss: 0.36912782118055554\n",
      "Epoch: 11571 Training Loss: 0.11194063822428385 Test Loss: 0.3697568088107639\n",
      "Epoch: 11572 Training Loss: 0.11203493075900607 Test Loss: 0.37050406901041666\n",
      "Epoch: 11573 Training Loss: 0.11209077707926432 Test Loss: 0.37141259765625\n",
      "Epoch: 11574 Training Loss: 0.112097413804796 Test Loss: 0.3724623480902778\n",
      "Epoch: 11575 Training Loss: 0.11206774987114801 Test Loss: 0.37332411024305556\n",
      "Epoch: 11576 Training Loss: 0.11201134830050999 Test Loss: 0.37360411241319447\n",
      "Epoch: 11577 Training Loss: 0.11192282189263238 Test Loss: 0.37321278211805553\n",
      "Epoch: 11578 Training Loss: 0.11183427259657118 Test Loss: 0.37216856553819444\n",
      "Epoch: 11579 Training Loss: 0.11175165218777126 Test Loss: 0.37083531358506944\n",
      "Epoch: 11580 Training Loss: 0.11167844475640192 Test Loss: 0.3693458658854167\n",
      "Epoch: 11581 Training Loss: 0.11159403822157118 Test Loss: 0.367933349609375\n",
      "Epoch: 11582 Training Loss: 0.11152964782714844 Test Loss: 0.366914306640625\n",
      "Epoch: 11583 Training Loss: 0.11152220831976997 Test Loss: 0.36679503038194444\n",
      "Epoch: 11584 Training Loss: 0.11158217366536459 Test Loss: 0.36718039279513887\n",
      "Epoch: 11585 Training Loss: 0.11171833123101128 Test Loss: 0.36806795247395835\n",
      "Epoch: 11586 Training Loss: 0.11191802639431424 Test Loss: 0.3692919108072917\n",
      "Epoch: 11587 Training Loss: 0.11216475084092882 Test Loss: 0.3703694118923611\n",
      "Epoch: 11588 Training Loss: 0.11244563547770182 Test Loss: 0.37120713975694447\n",
      "Epoch: 11589 Training Loss: 0.11276234520806207 Test Loss: 0.3719134928385417\n",
      "Epoch: 11590 Training Loss: 0.11308736589219835 Test Loss: 0.37244666883680555\n",
      "Epoch: 11591 Training Loss: 0.11341109975179037 Test Loss: 0.3731767578125\n",
      "Epoch: 11592 Training Loss: 0.11369492509629991 Test Loss: 0.3738773871527778\n",
      "Epoch: 11593 Training Loss: 0.11392829301622179 Test Loss: 0.374826171875\n",
      "Epoch: 11594 Training Loss: 0.11412854427761501 Test Loss: 0.37549473741319445\n",
      "Epoch: 11595 Training Loss: 0.11433464050292969 Test Loss: 0.37584852430555554\n",
      "Epoch: 11596 Training Loss: 0.1145764906141493 Test Loss: 0.37540828450520836\n",
      "Epoch: 11597 Training Loss: 0.11488343556722005 Test Loss: 0.37397309027777775\n",
      "Epoch: 11598 Training Loss: 0.11520625644259982 Test Loss: 0.3718170030381944\n",
      "Epoch: 11599 Training Loss: 0.11546702321370443 Test Loss: 0.3695886773003472\n",
      "Epoch: 11600 Training Loss: 0.11561795806884766 Test Loss: 0.3684387749565972\n",
      "Epoch: 11601 Training Loss: 0.11564626312255859 Test Loss: 0.36852840169270834\n",
      "Epoch: 11602 Training Loss: 0.11560162014431423 Test Loss: 0.36947325303819445\n",
      "Epoch: 11603 Training Loss: 0.11557935248480902 Test Loss: 0.3704563259548611\n",
      "Epoch: 11604 Training Loss: 0.1155908457438151 Test Loss: 0.3706108669704861\n",
      "Epoch: 11605 Training Loss: 0.1155747536553277 Test Loss: 0.37041167534722225\n",
      "Epoch: 11606 Training Loss: 0.11547100745307075 Test Loss: 0.3701387803819444\n",
      "Epoch: 11607 Training Loss: 0.11524383290608724 Test Loss: 0.37125439453125\n",
      "Epoch: 11608 Training Loss: 0.11498315938313802 Test Loss: 0.3729044596354167\n",
      "Epoch: 11609 Training Loss: 0.11471530490451388 Test Loss: 0.3745437825520833\n",
      "Epoch: 11610 Training Loss: 0.11446831512451172 Test Loss: 0.3755388454861111\n",
      "Epoch: 11611 Training Loss: 0.11424195692274305 Test Loss: 0.37570442708333335\n",
      "Epoch: 11612 Training Loss: 0.11398683251274956 Test Loss: 0.3749131401909722\n",
      "Epoch: 11613 Training Loss: 0.11367043897840712 Test Loss: 0.3732365451388889\n",
      "Epoch: 11614 Training Loss: 0.11330362277560764 Test Loss: 0.37142442491319444\n",
      "Epoch: 11615 Training Loss: 0.11293183729383681 Test Loss: 0.36976231553819444\n",
      "Epoch: 11616 Training Loss: 0.11259633890787761 Test Loss: 0.36835728624131947\n",
      "Epoch: 11617 Training Loss: 0.11232004631890191 Test Loss: 0.36781114366319445\n",
      "Epoch: 11618 Training Loss: 0.11212618340386285 Test Loss: 0.36731993272569446\n",
      "Epoch: 11619 Training Loss: 0.11198611280653212 Test Loss: 0.3672121853298611\n",
      "Epoch: 11620 Training Loss: 0.11189789072672526 Test Loss: 0.3675517849392361\n",
      "Epoch: 11621 Training Loss: 0.1118630616929796 Test Loss: 0.3680346950954861\n",
      "Epoch: 11622 Training Loss: 0.1118852793375651 Test Loss: 0.36877058919270833\n",
      "Epoch: 11623 Training Loss: 0.11194054667154948 Test Loss: 0.36979866536458333\n",
      "Epoch: 11624 Training Loss: 0.11202605946858724 Test Loss: 0.3708466525607639\n",
      "Epoch: 11625 Training Loss: 0.11214793480767143 Test Loss: 0.37181846788194445\n",
      "Epoch: 11626 Training Loss: 0.11230708736843532 Test Loss: 0.37248779296875\n",
      "Epoch: 11627 Training Loss: 0.11247930908203126 Test Loss: 0.37305772569444445\n",
      "Epoch: 11628 Training Loss: 0.11264399041069878 Test Loss: 0.37330978732638886\n",
      "Epoch: 11629 Training Loss: 0.11281473626030816 Test Loss: 0.37287904188368054\n",
      "Epoch: 11630 Training Loss: 0.1130043699476454 Test Loss: 0.3725615234375\n",
      "Epoch: 11631 Training Loss: 0.11322855546739366 Test Loss: 0.37243885633680557\n",
      "Epoch: 11632 Training Loss: 0.11345775519476997 Test Loss: 0.37277734375\n",
      "Epoch: 11633 Training Loss: 0.11369702572292752 Test Loss: 0.373275390625\n",
      "Epoch: 11634 Training Loss: 0.11394104597303602 Test Loss: 0.37402208116319446\n",
      "Epoch: 11635 Training Loss: 0.11419139777289497 Test Loss: 0.37475124782986113\n",
      "Epoch: 11636 Training Loss: 0.11443902418348524 Test Loss: 0.3752868109809028\n",
      "Epoch: 11637 Training Loss: 0.11468978966606988 Test Loss: 0.37557958984375\n",
      "Epoch: 11638 Training Loss: 0.1149398659600152 Test Loss: 0.3758076714409722\n",
      "Epoch: 11639 Training Loss: 0.11521062808566623 Test Loss: 0.37558251953125\n",
      "Epoch: 11640 Training Loss: 0.11547577836778429 Test Loss: 0.37580284288194443\n",
      "Epoch: 11641 Training Loss: 0.1157010964287652 Test Loss: 0.3760716145833333\n",
      "Epoch: 11642 Training Loss: 0.11583634524875216 Test Loss: 0.37662367078993053\n",
      "Epoch: 11643 Training Loss: 0.11585953945583767 Test Loss: 0.37767738172743054\n",
      "Epoch: 11644 Training Loss: 0.11574559614393447 Test Loss: 0.3789197591145833\n",
      "Epoch: 11645 Training Loss: 0.11553754255506728 Test Loss: 0.379353515625\n",
      "Epoch: 11646 Training Loss: 0.11525264655219183 Test Loss: 0.3789480794270833\n",
      "Epoch: 11647 Training Loss: 0.11493410237630208 Test Loss: 0.37779622395833334\n",
      "Epoch: 11648 Training Loss: 0.11462210676405164 Test Loss: 0.3761228298611111\n",
      "Epoch: 11649 Training Loss: 0.1143404269748264 Test Loss: 0.37465481228298614\n",
      "Epoch: 11650 Training Loss: 0.11409346516927084 Test Loss: 0.3730359157986111\n",
      "Epoch: 11651 Training Loss: 0.11386978573269314 Test Loss: 0.37158205837673614\n",
      "Epoch: 11652 Training Loss: 0.11369250233968099 Test Loss: 0.3708606228298611\n",
      "Epoch: 11653 Training Loss: 0.11357763841417101 Test Loss: 0.3709218478732639\n",
      "Epoch: 11654 Training Loss: 0.11354965125189888 Test Loss: 0.37135028754340277\n",
      "Epoch: 11655 Training Loss: 0.11359932200113933 Test Loss: 0.37204619683159723\n",
      "Epoch: 11656 Training Loss: 0.1137300525241428 Test Loss: 0.3729746365017361\n",
      "Epoch: 11657 Training Loss: 0.11392791832817925 Test Loss: 0.3741576877170139\n",
      "Epoch: 11658 Training Loss: 0.1141743884616428 Test Loss: 0.3752965494791667\n",
      "Epoch: 11659 Training Loss: 0.11447784678141276 Test Loss: 0.37646950954861114\n",
      "Epoch: 11660 Training Loss: 0.11482650332980686 Test Loss: 0.37709537760416667\n",
      "Epoch: 11661 Training Loss: 0.11523486836751302 Test Loss: 0.37758753797743055\n",
      "Epoch: 11662 Training Loss: 0.11564122348361545 Test Loss: 0.37758680555555557\n",
      "Epoch: 11663 Training Loss: 0.1160388895670573 Test Loss: 0.37708360460069446\n",
      "Epoch: 11664 Training Loss: 0.11638641187879774 Test Loss: 0.37604779730902776\n",
      "Epoch: 11665 Training Loss: 0.11662882826063369 Test Loss: 0.3736982421875\n",
      "Epoch: 11666 Training Loss: 0.1167650409274631 Test Loss: 0.37149823676215277\n",
      "Epoch: 11667 Training Loss: 0.11684311676025391 Test Loss: 0.36916853841145836\n",
      "Epoch: 11668 Training Loss: 0.11686445787217882 Test Loss: 0.36740771484375\n",
      "Epoch: 11669 Training Loss: 0.11682098727756077 Test Loss: 0.3660125325520833\n",
      "Epoch: 11670 Training Loss: 0.11674744500054253 Test Loss: 0.3648486599392361\n",
      "Epoch: 11671 Training Loss: 0.11660803731282553 Test Loss: 0.3644149848090278\n",
      "Epoch: 11672 Training Loss: 0.11641155921088325 Test Loss: 0.36467865668402777\n",
      "Epoch: 11673 Training Loss: 0.11618606652153862 Test Loss: 0.3653232150607639\n",
      "Epoch: 11674 Training Loss: 0.11592518107096354 Test Loss: 0.3661111111111111\n",
      "Epoch: 11675 Training Loss: 0.11562933180067274 Test Loss: 0.3666612684461806\n",
      "Epoch: 11676 Training Loss: 0.1153387968275282 Test Loss: 0.36636368815104164\n",
      "Epoch: 11677 Training Loss: 0.11499990759955513 Test Loss: 0.36550179036458336\n",
      "Epoch: 11678 Training Loss: 0.11460886806911892 Test Loss: 0.36492909071180557\n",
      "Epoch: 11679 Training Loss: 0.11418372005886501 Test Loss: 0.36458968098958333\n",
      "Epoch: 11680 Training Loss: 0.11373093922932943 Test Loss: 0.3645524631076389\n",
      "Epoch: 11681 Training Loss: 0.11326698557535807 Test Loss: 0.3645871310763889\n",
      "Epoch: 11682 Training Loss: 0.11285880618625217 Test Loss: 0.36475181749131946\n",
      "Epoch: 11683 Training Loss: 0.11253082021077473 Test Loss: 0.36493934461805555\n",
      "Epoch: 11684 Training Loss: 0.11226004621717665 Test Loss: 0.3649031304253472\n",
      "Epoch: 11685 Training Loss: 0.11204428948296441 Test Loss: 0.3648320041232639\n",
      "Epoch: 11686 Training Loss: 0.11185506269666884 Test Loss: 0.36467800564236114\n",
      "Epoch: 11687 Training Loss: 0.11168529680040147 Test Loss: 0.36464029947916665\n",
      "Epoch: 11688 Training Loss: 0.11154386054144966 Test Loss: 0.36464643012152775\n",
      "Epoch: 11689 Training Loss: 0.11143910895453558 Test Loss: 0.3649179144965278\n",
      "Epoch: 11690 Training Loss: 0.11137049865722656 Test Loss: 0.36549479166666665\n",
      "Epoch: 11691 Training Loss: 0.11134277513292101 Test Loss: 0.3663253309461806\n",
      "Epoch: 11692 Training Loss: 0.11136908467610677 Test Loss: 0.36729288736979165\n",
      "Epoch: 11693 Training Loss: 0.11145414310031466 Test Loss: 0.36822650824652775\n",
      "Epoch: 11694 Training Loss: 0.11156876881917317 Test Loss: 0.36895893012152775\n",
      "Epoch: 11695 Training Loss: 0.1117070066663954 Test Loss: 0.3696926540798611\n",
      "Epoch: 11696 Training Loss: 0.1118492914835612 Test Loss: 0.3701258138020833\n",
      "Epoch: 11697 Training Loss: 0.11198844401041666 Test Loss: 0.37017342122395835\n",
      "Epoch: 11698 Training Loss: 0.11213487158881294 Test Loss: 0.3701365017361111\n",
      "Epoch: 11699 Training Loss: 0.11229717424180773 Test Loss: 0.37002650282118055\n",
      "Epoch: 11700 Training Loss: 0.1124842766655816 Test Loss: 0.3698454318576389\n",
      "Epoch: 11701 Training Loss: 0.11270875634087457 Test Loss: 0.36996837022569445\n",
      "Epoch: 11702 Training Loss: 0.1129609883626302 Test Loss: 0.3700802408854167\n",
      "Epoch: 11703 Training Loss: 0.1132625478108724 Test Loss: 0.37029524739583336\n",
      "Epoch: 11704 Training Loss: 0.11361583455403645 Test Loss: 0.3704052463107639\n",
      "Epoch: 11705 Training Loss: 0.1140233171251085 Test Loss: 0.37050911458333335\n",
      "Epoch: 11706 Training Loss: 0.11447811550564237 Test Loss: 0.3699004177517361\n",
      "Epoch: 11707 Training Loss: 0.11494364251030816 Test Loss: 0.36891270616319444\n",
      "Epoch: 11708 Training Loss: 0.11538859388563368 Test Loss: 0.3677010091145833\n",
      "Epoch: 11709 Training Loss: 0.11577702162000868 Test Loss: 0.36648681640625\n",
      "Epoch: 11710 Training Loss: 0.11607396613226996 Test Loss: 0.36555099826388887\n",
      "Epoch: 11711 Training Loss: 0.11630403645833333 Test Loss: 0.3649389377170139\n",
      "Epoch: 11712 Training Loss: 0.11652986484103732 Test Loss: 0.36457850477430553\n",
      "Epoch: 11713 Training Loss: 0.11672025892469617 Test Loss: 0.36430973307291664\n",
      "Epoch: 11714 Training Loss: 0.11680436113145616 Test Loss: 0.3637725151909722\n",
      "Epoch: 11715 Training Loss: 0.1168021723429362 Test Loss: 0.3630510796440972\n",
      "Epoch: 11716 Training Loss: 0.11674612002902561 Test Loss: 0.3622827419704861\n",
      "Epoch: 11717 Training Loss: 0.11660177951388889 Test Loss: 0.3619299045138889\n",
      "Epoch: 11718 Training Loss: 0.11634524875217014 Test Loss: 0.3622911783854167\n",
      "Epoch: 11719 Training Loss: 0.11601450686984592 Test Loss: 0.3632275390625\n",
      "Epoch: 11720 Training Loss: 0.11567278628879123 Test Loss: 0.3649746365017361\n",
      "Epoch: 11721 Training Loss: 0.11537511444091797 Test Loss: 0.3667929144965278\n",
      "Epoch: 11722 Training Loss: 0.1151162584092882 Test Loss: 0.3684869791666667\n",
      "Epoch: 11723 Training Loss: 0.11488751644558377 Test Loss: 0.36997791883680553\n",
      "Epoch: 11724 Training Loss: 0.11464935557047526 Test Loss: 0.37093511284722225\n",
      "Epoch: 11725 Training Loss: 0.1143844248453776 Test Loss: 0.37163031684027775\n",
      "Epoch: 11726 Training Loss: 0.11410030364990234 Test Loss: 0.37220567491319445\n",
      "Epoch: 11727 Training Loss: 0.11384768591986762 Test Loss: 0.37284488932291665\n",
      "Epoch: 11728 Training Loss: 0.11361810133192274 Test Loss: 0.3732375217013889\n",
      "Epoch: 11729 Training Loss: 0.11342420620388455 Test Loss: 0.3735544704861111\n",
      "Epoch: 11730 Training Loss: 0.11327586788601346 Test Loss: 0.37365630425347224\n",
      "Epoch: 11731 Training Loss: 0.11313784450954861 Test Loss: 0.37339716254340277\n",
      "Epoch: 11732 Training Loss: 0.11298712073432075 Test Loss: 0.37325645616319447\n",
      "Epoch: 11733 Training Loss: 0.11282845815022786 Test Loss: 0.3728313530815972\n",
      "Epoch: 11734 Training Loss: 0.11265030924479166 Test Loss: 0.37246028645833335\n",
      "Epoch: 11735 Training Loss: 0.11244781578911675 Test Loss: 0.3719238009982639\n",
      "Epoch: 11736 Training Loss: 0.11221857706705729 Test Loss: 0.37099251302083336\n",
      "Epoch: 11737 Training Loss: 0.1119656490749783 Test Loss: 0.37004207356770835\n",
      "Epoch: 11738 Training Loss: 0.11170992448594835 Test Loss: 0.3690643717447917\n",
      "Epoch: 11739 Training Loss: 0.11147267744276258 Test Loss: 0.3680012478298611\n",
      "Epoch: 11740 Training Loss: 0.11127320861816406 Test Loss: 0.3671812065972222\n",
      "Epoch: 11741 Training Loss: 0.11108683607313367 Test Loss: 0.36663237847222224\n",
      "Epoch: 11742 Training Loss: 0.11092012447781033 Test Loss: 0.36616064453125\n",
      "Epoch: 11743 Training Loss: 0.11076821899414062 Test Loss: 0.3658763834635417\n",
      "Epoch: 11744 Training Loss: 0.11063582441541883 Test Loss: 0.36567960611979167\n",
      "Epoch: 11745 Training Loss: 0.11052411058213976 Test Loss: 0.3655285373263889\n",
      "Epoch: 11746 Training Loss: 0.11043074544270834 Test Loss: 0.36533458116319445\n",
      "Epoch: 11747 Training Loss: 0.11033765241834853 Test Loss: 0.3653350151909722\n",
      "Epoch: 11748 Training Loss: 0.11024043613009983 Test Loss: 0.36547900390625\n",
      "Epoch: 11749 Training Loss: 0.11015528700086806 Test Loss: 0.3658411458333333\n",
      "Epoch: 11750 Training Loss: 0.11007802920871311 Test Loss: 0.36624506293402775\n",
      "Epoch: 11751 Training Loss: 0.10999925486246745 Test Loss: 0.36681168619791665\n",
      "Epoch: 11752 Training Loss: 0.1099347161187066 Test Loss: 0.36763981119791667\n",
      "Epoch: 11753 Training Loss: 0.10988494957817925 Test Loss: 0.36829833984375\n",
      "Epoch: 11754 Training Loss: 0.10984268951416015 Test Loss: 0.36895296223958335\n",
      "Epoch: 11755 Training Loss: 0.10981308152940539 Test Loss: 0.36952007378472224\n",
      "Epoch: 11756 Training Loss: 0.10980218590630425 Test Loss: 0.37007996961805556\n",
      "Epoch: 11757 Training Loss: 0.1098042958577474 Test Loss: 0.3703693033854167\n",
      "Epoch: 11758 Training Loss: 0.10981358591715495 Test Loss: 0.37050887044270836\n",
      "Epoch: 11759 Training Loss: 0.10982044135199653 Test Loss: 0.37056336805555556\n",
      "Epoch: 11760 Training Loss: 0.10983285607231988 Test Loss: 0.37044292534722223\n",
      "Epoch: 11761 Training Loss: 0.10982478925916883 Test Loss: 0.37008165147569444\n",
      "Epoch: 11762 Training Loss: 0.10979901885986328 Test Loss: 0.3694821506076389\n",
      "Epoch: 11763 Training Loss: 0.10976432037353516 Test Loss: 0.3688708224826389\n",
      "Epoch: 11764 Training Loss: 0.109729855855306 Test Loss: 0.36801472981770833\n",
      "Epoch: 11765 Training Loss: 0.10970546552870009 Test Loss: 0.3669167209201389\n",
      "Epoch: 11766 Training Loss: 0.10970472378200954 Test Loss: 0.36603518337673613\n",
      "Epoch: 11767 Training Loss: 0.10973544565836589 Test Loss: 0.3650236002604167\n",
      "Epoch: 11768 Training Loss: 0.10978874037000869 Test Loss: 0.3639443359375\n",
      "Epoch: 11769 Training Loss: 0.10985943688286676 Test Loss: 0.3631531575520833\n",
      "Epoch: 11770 Training Loss: 0.10995958370632596 Test Loss: 0.3623378634982639\n",
      "Epoch: 11771 Training Loss: 0.11008343760172526 Test Loss: 0.36187122938368055\n",
      "Epoch: 11772 Training Loss: 0.11024158053927952 Test Loss: 0.3615117730034722\n",
      "Epoch: 11773 Training Loss: 0.11040839046902126 Test Loss: 0.36143731011284724\n",
      "Epoch: 11774 Training Loss: 0.11061829800075955 Test Loss: 0.3619705403645833\n",
      "Epoch: 11775 Training Loss: 0.11087195756700303 Test Loss: 0.36271904839409724\n",
      "Epoch: 11776 Training Loss: 0.11117582872178819 Test Loss: 0.3635692816840278\n",
      "Epoch: 11777 Training Loss: 0.11151389651828342 Test Loss: 0.36468180338541667\n",
      "Epoch: 11778 Training Loss: 0.11188427734375 Test Loss: 0.3655353732638889\n",
      "Epoch: 11779 Training Loss: 0.11229574669731988 Test Loss: 0.36636463758680554\n",
      "Epoch: 11780 Training Loss: 0.11278340233696832 Test Loss: 0.36691978624131943\n",
      "Epoch: 11781 Training Loss: 0.11334049055311415 Test Loss: 0.3670602756076389\n",
      "Epoch: 11782 Training Loss: 0.11392559221055773 Test Loss: 0.36674609375\n",
      "Epoch: 11783 Training Loss: 0.11454813808865018 Test Loss: 0.3659708930121528\n",
      "Epoch: 11784 Training Loss: 0.11514666832817926 Test Loss: 0.365492919921875\n",
      "Epoch: 11785 Training Loss: 0.11571123843722873 Test Loss: 0.36571934678819445\n",
      "Epoch: 11786 Training Loss: 0.11621894327799479 Test Loss: 0.3665281846788194\n",
      "Epoch: 11787 Training Loss: 0.11664737870958117 Test Loss: 0.3680728081597222\n",
      "Epoch: 11788 Training Loss: 0.11698494042290582 Test Loss: 0.3695549587673611\n",
      "Epoch: 11789 Training Loss: 0.11719430033365885 Test Loss: 0.3699829644097222\n",
      "Epoch: 11790 Training Loss: 0.11722789340549045 Test Loss: 0.36921175130208334\n",
      "Epoch: 11791 Training Loss: 0.1170264909532335 Test Loss: 0.3679218478732639\n",
      "Epoch: 11792 Training Loss: 0.11659672122531467 Test Loss: 0.3671258680555556\n",
      "Epoch: 11793 Training Loss: 0.11602598487006294 Test Loss: 0.36729812282986113\n",
      "Epoch: 11794 Training Loss: 0.1154501707288954 Test Loss: 0.3682755533854167\n",
      "Epoch: 11795 Training Loss: 0.1149583494398329 Test Loss: 0.3692798394097222\n",
      "Epoch: 11796 Training Loss: 0.11457180870903863 Test Loss: 0.37008062065972225\n",
      "Epoch: 11797 Training Loss: 0.11428724839952258 Test Loss: 0.3703719889322917\n",
      "Epoch: 11798 Training Loss: 0.11404255082872179 Test Loss: 0.370369384765625\n",
      "Epoch: 11799 Training Loss: 0.11377030775282118 Test Loss: 0.3704384223090278\n",
      "Epoch: 11800 Training Loss: 0.11347552320692274 Test Loss: 0.3703702528211806\n",
      "Epoch: 11801 Training Loss: 0.11316423204210069 Test Loss: 0.37006081814236114\n",
      "Epoch: 11802 Training Loss: 0.11287553914388021 Test Loss: 0.3694728732638889\n",
      "Epoch: 11803 Training Loss: 0.11262836371527778 Test Loss: 0.36861406792534723\n",
      "Epoch: 11804 Training Loss: 0.11240210808648003 Test Loss: 0.3676292588975694\n",
      "Epoch: 11805 Training Loss: 0.11217337205674913 Test Loss: 0.36648426649305554\n",
      "Epoch: 11806 Training Loss: 0.11192727237277561 Test Loss: 0.36530311414930555\n",
      "Epoch: 11807 Training Loss: 0.11166771358913845 Test Loss: 0.3645484212239583\n",
      "Epoch: 11808 Training Loss: 0.11141373782687718 Test Loss: 0.3639178331163194\n",
      "Epoch: 11809 Training Loss: 0.11118805101182726 Test Loss: 0.3637165798611111\n",
      "Epoch: 11810 Training Loss: 0.11097988552517361 Test Loss: 0.36372970920138886\n",
      "Epoch: 11811 Training Loss: 0.1107837159898546 Test Loss: 0.3639443088107639\n",
      "Epoch: 11812 Training Loss: 0.11062249247233073 Test Loss: 0.36431353081597223\n",
      "Epoch: 11813 Training Loss: 0.11051366763644749 Test Loss: 0.3646691623263889\n",
      "Epoch: 11814 Training Loss: 0.11044409688313803 Test Loss: 0.36525604926215277\n",
      "Epoch: 11815 Training Loss: 0.11042097388373481 Test Loss: 0.3656699490017361\n",
      "Epoch: 11816 Training Loss: 0.11041896141899957 Test Loss: 0.3661074761284722\n",
      "Epoch: 11817 Training Loss: 0.11042330254448784 Test Loss: 0.3664879828559028\n",
      "Epoch: 11818 Training Loss: 0.11042623392740886 Test Loss: 0.3666141493055556\n",
      "Epoch: 11819 Training Loss: 0.11042070007324219 Test Loss: 0.36634733072916664\n",
      "Epoch: 11820 Training Loss: 0.11040856764051649 Test Loss: 0.365992431640625\n",
      "Epoch: 11821 Training Loss: 0.11037451341417101 Test Loss: 0.36530455186631944\n",
      "Epoch: 11822 Training Loss: 0.11032843526204428 Test Loss: 0.3645823025173611\n",
      "Epoch: 11823 Training Loss: 0.11028512064615885 Test Loss: 0.3637048611111111\n",
      "Epoch: 11824 Training Loss: 0.1102408192952474 Test Loss: 0.36285999891493054\n",
      "Epoch: 11825 Training Loss: 0.11020120663113064 Test Loss: 0.3620008138020833\n",
      "Epoch: 11826 Training Loss: 0.11016602579752605 Test Loss: 0.3614255099826389\n",
      "Epoch: 11827 Training Loss: 0.11013895331488716 Test Loss: 0.3612598198784722\n",
      "Epoch: 11828 Training Loss: 0.1101388439602322 Test Loss: 0.361170166015625\n",
      "Epoch: 11829 Training Loss: 0.11016593254937065 Test Loss: 0.36118587239583333\n",
      "Epoch: 11830 Training Loss: 0.11021556939019098 Test Loss: 0.3617023111979167\n",
      "Epoch: 11831 Training Loss: 0.11027393849690756 Test Loss: 0.3623600802951389\n",
      "Epoch: 11832 Training Loss: 0.11035903082953559 Test Loss: 0.3631021321614583\n",
      "Epoch: 11833 Training Loss: 0.11046072981092665 Test Loss: 0.3640355631510417\n",
      "Epoch: 11834 Training Loss: 0.1105799306233724 Test Loss: 0.364640380859375\n",
      "Epoch: 11835 Training Loss: 0.11070806715223525 Test Loss: 0.36508414713541665\n",
      "Epoch: 11836 Training Loss: 0.11084801398383247 Test Loss: 0.36528656684027777\n",
      "Epoch: 11837 Training Loss: 0.11096389092339409 Test Loss: 0.36501209852430555\n",
      "Epoch: 11838 Training Loss: 0.1110591311984592 Test Loss: 0.364766357421875\n",
      "Epoch: 11839 Training Loss: 0.11116067250569661 Test Loss: 0.36451207139756947\n",
      "Epoch: 11840 Training Loss: 0.11125675116644965 Test Loss: 0.36440418836805555\n",
      "Epoch: 11841 Training Loss: 0.11134594133165147 Test Loss: 0.36414154730902776\n",
      "Epoch: 11842 Training Loss: 0.11141168636745877 Test Loss: 0.3638670247395833\n",
      "Epoch: 11843 Training Loss: 0.11145804426405165 Test Loss: 0.3636999782986111\n",
      "Epoch: 11844 Training Loss: 0.11148914676242405 Test Loss: 0.3634055718315972\n",
      "Epoch: 11845 Training Loss: 0.11152431996663412 Test Loss: 0.36304486762152777\n",
      "Epoch: 11846 Training Loss: 0.11156979031032986 Test Loss: 0.3622626410590278\n",
      "Epoch: 11847 Training Loss: 0.11161847008599175 Test Loss: 0.36128472222222224\n",
      "Epoch: 11848 Training Loss: 0.11167832692464193 Test Loss: 0.3602206759982639\n",
      "Epoch: 11849 Training Loss: 0.11172128211127387 Test Loss: 0.35894449869791667\n",
      "Epoch: 11850 Training Loss: 0.1117348904079861 Test Loss: 0.35819205729166664\n",
      "Epoch: 11851 Training Loss: 0.11173897891574436 Test Loss: 0.357844970703125\n",
      "Epoch: 11852 Training Loss: 0.1117132076687283 Test Loss: 0.35829733615451387\n",
      "Epoch: 11853 Training Loss: 0.11167200639512803 Test Loss: 0.35944772677951387\n",
      "Epoch: 11854 Training Loss: 0.11161744689941407 Test Loss: 0.36124915907118055\n",
      "Epoch: 11855 Training Loss: 0.11156555514865452 Test Loss: 0.3630541720920139\n",
      "Epoch: 11856 Training Loss: 0.11151421440972223 Test Loss: 0.3646823459201389\n",
      "Epoch: 11857 Training Loss: 0.11147859785291883 Test Loss: 0.3658349609375\n",
      "Epoch: 11858 Training Loss: 0.11144631788465711 Test Loss: 0.36621584743923613\n",
      "Epoch: 11859 Training Loss: 0.11138841840955946 Test Loss: 0.36613047960069445\n",
      "Epoch: 11860 Training Loss: 0.11128059726291233 Test Loss: 0.36551285807291667\n",
      "Epoch: 11861 Training Loss: 0.11115404340955946 Test Loss: 0.364945068359375\n",
      "Epoch: 11862 Training Loss: 0.11099458397759332 Test Loss: 0.36439173719618057\n",
      "Epoch: 11863 Training Loss: 0.11083890363905165 Test Loss: 0.3642146267361111\n",
      "Epoch: 11864 Training Loss: 0.11070362854003907 Test Loss: 0.36444189453125\n",
      "Epoch: 11865 Training Loss: 0.11059229109022352 Test Loss: 0.3646276041666667\n",
      "Epoch: 11866 Training Loss: 0.11049142371283636 Test Loss: 0.36468663194444445\n",
      "Epoch: 11867 Training Loss: 0.11040374416775174 Test Loss: 0.36469222005208335\n",
      "Epoch: 11868 Training Loss: 0.11035801866319445 Test Loss: 0.3643361002604167\n",
      "Epoch: 11869 Training Loss: 0.1103361070421007 Test Loss: 0.3640406087239583\n",
      "Epoch: 11870 Training Loss: 0.11031615702311198 Test Loss: 0.36323828125\n",
      "Epoch: 11871 Training Loss: 0.11027275509304471 Test Loss: 0.3623767361111111\n",
      "Epoch: 11872 Training Loss: 0.11023160044352213 Test Loss: 0.361742431640625\n",
      "Epoch: 11873 Training Loss: 0.11019722663031684 Test Loss: 0.36094373914930555\n",
      "Epoch: 11874 Training Loss: 0.11016772121853298 Test Loss: 0.36051274956597223\n",
      "Epoch: 11875 Training Loss: 0.1101408462524414 Test Loss: 0.3603595920138889\n",
      "Epoch: 11876 Training Loss: 0.11013852776421441 Test Loss: 0.3605164659288194\n",
      "Epoch: 11877 Training Loss: 0.11015721808539497 Test Loss: 0.36095225694444444\n",
      "Epoch: 11878 Training Loss: 0.11020770772298177 Test Loss: 0.36145789930555555\n",
      "Epoch: 11879 Training Loss: 0.11027540757921007 Test Loss: 0.36196202256944443\n",
      "Epoch: 11880 Training Loss: 0.1103718516031901 Test Loss: 0.36286279296875\n",
      "Epoch: 11881 Training Loss: 0.11050990973578559 Test Loss: 0.36355414496527777\n",
      "Epoch: 11882 Training Loss: 0.11068183644612631 Test Loss: 0.36406187608506946\n",
      "Epoch: 11883 Training Loss: 0.11085540262858073 Test Loss: 0.36432118055555557\n",
      "Epoch: 11884 Training Loss: 0.11101793501112196 Test Loss: 0.36449818250868055\n",
      "Epoch: 11885 Training Loss: 0.11120774756537544 Test Loss: 0.36441441514756945\n",
      "Epoch: 11886 Training Loss: 0.11144830491807725 Test Loss: 0.3645093858506944\n",
      "Epoch: 11887 Training Loss: 0.11171329243977865 Test Loss: 0.36501220703125\n",
      "Epoch: 11888 Training Loss: 0.11199328189425999 Test Loss: 0.36609358723958335\n",
      "Epoch: 11889 Training Loss: 0.11232729000515408 Test Loss: 0.36741422526041667\n",
      "Epoch: 11890 Training Loss: 0.11270927090115017 Test Loss: 0.3685061848958333\n",
      "Epoch: 11891 Training Loss: 0.1131369111802843 Test Loss: 0.36955069986979167\n",
      "Epoch: 11892 Training Loss: 0.11364153544108073 Test Loss: 0.37018519422743057\n",
      "Epoch: 11893 Training Loss: 0.11423747338189019 Test Loss: 0.37041796875\n",
      "Epoch: 11894 Training Loss: 0.11489381239149306 Test Loss: 0.3705540635850694\n",
      "Epoch: 11895 Training Loss: 0.11555922359890408 Test Loss: 0.3710560980902778\n",
      "Epoch: 11896 Training Loss: 0.11624024369981553 Test Loss: 0.3716653103298611\n",
      "Epoch: 11897 Training Loss: 0.11690984768337674 Test Loss: 0.37277926974826386\n",
      "Epoch: 11898 Training Loss: 0.11754358164469401 Test Loss: 0.3738505045572917\n",
      "Epoch: 11899 Training Loss: 0.11815787251790365 Test Loss: 0.37394873046875\n",
      "Epoch: 11900 Training Loss: 0.11868098704020183 Test Loss: 0.37314059787326387\n",
      "Epoch: 11901 Training Loss: 0.11894144100613065 Test Loss: 0.37206570095486113\n",
      "Epoch: 11902 Training Loss: 0.11880887264675564 Test Loss: 0.3712443576388889\n",
      "Epoch: 11903 Training Loss: 0.11828757815890842 Test Loss: 0.37082793511284723\n",
      "Epoch: 11904 Training Loss: 0.11746097395155165 Test Loss: 0.3703051215277778\n",
      "Epoch: 11905 Training Loss: 0.11645346069335938 Test Loss: 0.3690193142361111\n",
      "Epoch: 11906 Training Loss: 0.11539772457546658 Test Loss: 0.36737415907118054\n",
      "Epoch: 11907 Training Loss: 0.11432132805718316 Test Loss: 0.3655950520833333\n",
      "Epoch: 11908 Training Loss: 0.11328629472520617 Test Loss: 0.3639494357638889\n",
      "Epoch: 11909 Training Loss: 0.11234141370985243 Test Loss: 0.36263330078125\n",
      "Epoch: 11910 Training Loss: 0.11147689734564888 Test Loss: 0.36168614366319446\n",
      "Epoch: 11911 Training Loss: 0.1107190178765191 Test Loss: 0.36066569010416666\n",
      "Epoch: 11912 Training Loss: 0.11008661905924479 Test Loss: 0.35968798828125\n",
      "Epoch: 11913 Training Loss: 0.10955079735649957 Test Loss: 0.358927734375\n",
      "Epoch: 11914 Training Loss: 0.10909319729275174 Test Loss: 0.3581669921875\n",
      "Epoch: 11915 Training Loss: 0.10872749837239583 Test Loss: 0.35759638129340277\n",
      "Epoch: 11916 Training Loss: 0.10843218315972222 Test Loss: 0.357310546875\n",
      "Epoch: 11917 Training Loss: 0.10819504631890191 Test Loss: 0.3574654134114583\n",
      "Epoch: 11918 Training Loss: 0.10799967532687717 Test Loss: 0.35802671983506945\n",
      "Epoch: 11919 Training Loss: 0.10784964413113064 Test Loss: 0.35891731770833335\n",
      "Epoch: 11920 Training Loss: 0.10774910227457682 Test Loss: 0.3599255642361111\n",
      "Epoch: 11921 Training Loss: 0.10768577406141493 Test Loss: 0.36091737196180557\n",
      "Epoch: 11922 Training Loss: 0.1076594967312283 Test Loss: 0.3620745171440972\n",
      "Epoch: 11923 Training Loss: 0.10766664716932509 Test Loss: 0.3630222439236111\n",
      "Epoch: 11924 Training Loss: 0.10767853461371528 Test Loss: 0.3636335177951389\n",
      "Epoch: 11925 Training Loss: 0.1077046390109592 Test Loss: 0.36414922417534723\n",
      "Epoch: 11926 Training Loss: 0.10773209126790365 Test Loss: 0.36428371853298613\n",
      "Epoch: 11927 Training Loss: 0.1077676255967882 Test Loss: 0.36439472113715277\n",
      "Epoch: 11928 Training Loss: 0.10779880523681641 Test Loss: 0.36390804036458335\n",
      "Epoch: 11929 Training Loss: 0.10782247755262586 Test Loss: 0.36324213324652777\n",
      "Epoch: 11930 Training Loss: 0.10785233900282118 Test Loss: 0.3624402669270833\n",
      "Epoch: 11931 Training Loss: 0.10788320414225261 Test Loss: 0.36165462239583335\n",
      "Epoch: 11932 Training Loss: 0.10792552524142796 Test Loss: 0.3608669162326389\n",
      "Epoch: 11933 Training Loss: 0.10797301567925348 Test Loss: 0.36009651692708333\n",
      "Epoch: 11934 Training Loss: 0.10802503204345704 Test Loss: 0.3595905219184028\n",
      "Epoch: 11935 Training Loss: 0.10809239705403646 Test Loss: 0.3594423828125\n",
      "Epoch: 11936 Training Loss: 0.10818511454264323 Test Loss: 0.3596400824652778\n",
      "Epoch: 11937 Training Loss: 0.10829816097683377 Test Loss: 0.3603371853298611\n",
      "Epoch: 11938 Training Loss: 0.10845157962375217 Test Loss: 0.3612831488715278\n",
      "Epoch: 11939 Training Loss: 0.10862538316514757 Test Loss: 0.36243522135416667\n",
      "Epoch: 11940 Training Loss: 0.10881441582573785 Test Loss: 0.36358753797743054\n",
      "Epoch: 11941 Training Loss: 0.10900254737006293 Test Loss: 0.364563232421875\n",
      "Epoch: 11942 Training Loss: 0.10919436730278863 Test Loss: 0.36506993272569443\n",
      "Epoch: 11943 Training Loss: 0.10937405395507813 Test Loss: 0.3652182074652778\n",
      "Epoch: 11944 Training Loss: 0.10953457980685764 Test Loss: 0.3650283203125\n",
      "Epoch: 11945 Training Loss: 0.10967621782090929 Test Loss: 0.36444227430555554\n",
      "Epoch: 11946 Training Loss: 0.10979998185899523 Test Loss: 0.36395060221354164\n",
      "Epoch: 11947 Training Loss: 0.10992125786675347 Test Loss: 0.36353819444444446\n",
      "Epoch: 11948 Training Loss: 0.11000995635986328 Test Loss: 0.3629558376736111\n",
      "Epoch: 11949 Training Loss: 0.1100577884250217 Test Loss: 0.36247319878472223\n",
      "Epoch: 11950 Training Loss: 0.1100942645602756 Test Loss: 0.36229538302951386\n",
      "Epoch: 11951 Training Loss: 0.11014935896131728 Test Loss: 0.3620478786892361\n",
      "Epoch: 11952 Training Loss: 0.11023216586642795 Test Loss: 0.3619581705729167\n",
      "Epoch: 11953 Training Loss: 0.1103413569132487 Test Loss: 0.36198247612847223\n",
      "Epoch: 11954 Training Loss: 0.11047859785291883 Test Loss: 0.3619316134982639\n",
      "Epoch: 11955 Training Loss: 0.11061784786648221 Test Loss: 0.36173182508680557\n",
      "Epoch: 11956 Training Loss: 0.11076190609402127 Test Loss: 0.36152945963541666\n",
      "Epoch: 11957 Training Loss: 0.11091779242621527 Test Loss: 0.3611350911458333\n",
      "Epoch: 11958 Training Loss: 0.11108581288655599 Test Loss: 0.36060362413194447\n",
      "Epoch: 11959 Training Loss: 0.11124692450629341 Test Loss: 0.3600842013888889\n",
      "Epoch: 11960 Training Loss: 0.11139459652370877 Test Loss: 0.3599316948784722\n",
      "Epoch: 11961 Training Loss: 0.1115006103515625 Test Loss: 0.36003748914930556\n",
      "Epoch: 11962 Training Loss: 0.11158297220865886 Test Loss: 0.3603375922309028\n",
      "Epoch: 11963 Training Loss: 0.11166092681884765 Test Loss: 0.36101703559027776\n",
      "Epoch: 11964 Training Loss: 0.1117324973212348 Test Loss: 0.3616927897135417\n",
      "Epoch: 11965 Training Loss: 0.11179880269368489 Test Loss: 0.36168722873263887\n",
      "Epoch: 11966 Training Loss: 0.11185166676839192 Test Loss: 0.3611130099826389\n",
      "Epoch: 11967 Training Loss: 0.11187050882975261 Test Loss: 0.3597744411892361\n",
      "Epoch: 11968 Training Loss: 0.11185370635986328 Test Loss: 0.35854177517361113\n",
      "Epoch: 11969 Training Loss: 0.1118091074625651 Test Loss: 0.3575192599826389\n",
      "Epoch: 11970 Training Loss: 0.11170500776502822 Test Loss: 0.35682535807291665\n",
      "Epoch: 11971 Training Loss: 0.11154047393798829 Test Loss: 0.35671202256944445\n",
      "Epoch: 11972 Training Loss: 0.1113781975640191 Test Loss: 0.357014404296875\n",
      "Epoch: 11973 Training Loss: 0.11122915818956164 Test Loss: 0.3574125434027778\n",
      "Epoch: 11974 Training Loss: 0.11111634826660156 Test Loss: 0.35778304036458336\n",
      "Epoch: 11975 Training Loss: 0.11103742218017579 Test Loss: 0.35802001953125\n",
      "Epoch: 11976 Training Loss: 0.11097775099012587 Test Loss: 0.35865831163194445\n",
      "Epoch: 11977 Training Loss: 0.11092595672607422 Test Loss: 0.3594872775607639\n",
      "Epoch: 11978 Training Loss: 0.11088645935058594 Test Loss: 0.3606195746527778\n",
      "Epoch: 11979 Training Loss: 0.11087136586507161 Test Loss: 0.3624765353732639\n",
      "Epoch: 11980 Training Loss: 0.11087888590494792 Test Loss: 0.3643291015625\n",
      "Epoch: 11981 Training Loss: 0.11092016771104601 Test Loss: 0.3662421875\n",
      "Epoch: 11982 Training Loss: 0.11102591196695964 Test Loss: 0.3676631673177083\n",
      "Epoch: 11983 Training Loss: 0.11115752410888671 Test Loss: 0.3684303385416667\n",
      "Epoch: 11984 Training Loss: 0.11127340698242187 Test Loss: 0.3689057074652778\n",
      "Epoch: 11985 Training Loss: 0.11139643944634331 Test Loss: 0.36858284505208333\n",
      "Epoch: 11986 Training Loss: 0.11154320017496745 Test Loss: 0.3679556206597222\n",
      "Epoch: 11987 Training Loss: 0.11168990325927734 Test Loss: 0.366798828125\n",
      "Epoch: 11988 Training Loss: 0.1118021486070421 Test Loss: 0.36547987196180554\n",
      "Epoch: 11989 Training Loss: 0.11187592909071181 Test Loss: 0.36436919487847225\n",
      "Epoch: 11990 Training Loss: 0.11191605377197265 Test Loss: 0.3640407986111111\n",
      "Epoch: 11991 Training Loss: 0.11195462290445964 Test Loss: 0.3645254448784722\n",
      "Epoch: 11992 Training Loss: 0.11196771579318576 Test Loss: 0.3655154622395833\n",
      "Epoch: 11993 Training Loss: 0.11197452969021267 Test Loss: 0.3668993326822917\n",
      "Epoch: 11994 Training Loss: 0.11198026360405816 Test Loss: 0.36836759440104166\n",
      "Epoch: 11995 Training Loss: 0.11197014787462023 Test Loss: 0.36954703776041664\n",
      "Epoch: 11996 Training Loss: 0.11196584828694661 Test Loss: 0.3701965060763889\n",
      "Epoch: 11997 Training Loss: 0.1119729012383355 Test Loss: 0.3703607313368056\n",
      "Epoch: 11998 Training Loss: 0.1119826185438368 Test Loss: 0.36999473741319444\n",
      "Epoch: 11999 Training Loss: 0.1119872529771593 Test Loss: 0.36906635199652776\n",
      "Epoch: 12000 Training Loss: 0.11195812140570746 Test Loss: 0.3680199652777778\n",
      "Epoch: 12001 Training Loss: 0.11187377251519097 Test Loss: 0.3662929144965278\n",
      "Epoch: 12002 Training Loss: 0.11174329206678603 Test Loss: 0.36506439887152775\n",
      "Epoch: 12003 Training Loss: 0.11154661051432292 Test Loss: 0.3642221137152778\n",
      "Epoch: 12004 Training Loss: 0.11132053036159939 Test Loss: 0.36397021484375\n",
      "Epoch: 12005 Training Loss: 0.1111119867960612 Test Loss: 0.36394080946180557\n",
      "Epoch: 12006 Training Loss: 0.11094312710232204 Test Loss: 0.3640319552951389\n",
      "Epoch: 12007 Training Loss: 0.11080466037326389 Test Loss: 0.3639465603298611\n",
      "Epoch: 12008 Training Loss: 0.11068196275499131 Test Loss: 0.36373304578993054\n",
      "Epoch: 12009 Training Loss: 0.11059084659152561 Test Loss: 0.3631969943576389\n",
      "Epoch: 12010 Training Loss: 0.11051646847195096 Test Loss: 0.36274742296006945\n",
      "Epoch: 12011 Training Loss: 0.11045031144883898 Test Loss: 0.3623911675347222\n",
      "Epoch: 12012 Training Loss: 0.11043111928304036 Test Loss: 0.3623781467013889\n",
      "Epoch: 12013 Training Loss: 0.1104136496649848 Test Loss: 0.3625106879340278\n",
      "Epoch: 12014 Training Loss: 0.11040414343939887 Test Loss: 0.3635161675347222\n",
      "Epoch: 12015 Training Loss: 0.11039132181803385 Test Loss: 0.36498084852430557\n",
      "Epoch: 12016 Training Loss: 0.11037733629014757 Test Loss: 0.36669181315104166\n",
      "Epoch: 12017 Training Loss: 0.11037795172797309 Test Loss: 0.36829603407118056\n",
      "Epoch: 12018 Training Loss: 0.110380980597602 Test Loss: 0.3695216471354167\n",
      "Epoch: 12019 Training Loss: 0.11039745669894749 Test Loss: 0.370101318359375\n",
      "Epoch: 12020 Training Loss: 0.11039346143934461 Test Loss: 0.37031787109375\n",
      "Epoch: 12021 Training Loss: 0.11035442945692274 Test Loss: 0.36977598741319445\n",
      "Epoch: 12022 Training Loss: 0.11027654690212674 Test Loss: 0.3688624674479167\n",
      "Epoch: 12023 Training Loss: 0.11016256459554037 Test Loss: 0.36765562608506946\n",
      "Epoch: 12024 Training Loss: 0.11003447384304471 Test Loss: 0.36630322265625\n",
      "Epoch: 12025 Training Loss: 0.10989181603325737 Test Loss: 0.364727783203125\n",
      "Epoch: 12026 Training Loss: 0.10973875851101346 Test Loss: 0.3632568088107639\n",
      "Epoch: 12027 Training Loss: 0.10956639353434244 Test Loss: 0.3623300238715278\n",
      "Epoch: 12028 Training Loss: 0.10938737318250868 Test Loss: 0.3614453125\n",
      "Epoch: 12029 Training Loss: 0.10920965067545572 Test Loss: 0.3608107638888889\n",
      "Epoch: 12030 Training Loss: 0.10905955759684245 Test Loss: 0.3604522026909722\n",
      "Epoch: 12031 Training Loss: 0.10896415117051866 Test Loss: 0.36013690863715275\n",
      "Epoch: 12032 Training Loss: 0.1089121822781033 Test Loss: 0.3598199869791667\n",
      "Epoch: 12033 Training Loss: 0.10890725792778863 Test Loss: 0.3595253363715278\n",
      "Epoch: 12034 Training Loss: 0.10892799631754557 Test Loss: 0.3594429253472222\n",
      "Epoch: 12035 Training Loss: 0.10897956763373481 Test Loss: 0.3596253255208333\n",
      "Epoch: 12036 Training Loss: 0.10906985134548611 Test Loss: 0.36000851779513887\n",
      "Epoch: 12037 Training Loss: 0.10919257185194227 Test Loss: 0.3605750054253472\n",
      "Epoch: 12038 Training Loss: 0.10937145657009549 Test Loss: 0.3614193793402778\n",
      "Epoch: 12039 Training Loss: 0.1095808588663737 Test Loss: 0.3624508463541667\n",
      "Epoch: 12040 Training Loss: 0.10981212022569445 Test Loss: 0.36339461263020834\n",
      "Epoch: 12041 Training Loss: 0.11007440863715277 Test Loss: 0.36464501953125\n",
      "Epoch: 12042 Training Loss: 0.11039002566867405 Test Loss: 0.365997802734375\n",
      "Epoch: 12043 Training Loss: 0.11075768449571398 Test Loss: 0.3670443793402778\n",
      "Epoch: 12044 Training Loss: 0.11118225436740452 Test Loss: 0.3675714518229167\n",
      "Epoch: 12045 Training Loss: 0.1116427485148112 Test Loss: 0.3677964138454861\n",
      "Epoch: 12046 Training Loss: 0.11210703362358941 Test Loss: 0.3675044487847222\n",
      "Epoch: 12047 Training Loss: 0.11253275468614367 Test Loss: 0.3668323567708333\n",
      "Epoch: 12048 Training Loss: 0.11284981028238933 Test Loss: 0.36612744140625\n",
      "Epoch: 12049 Training Loss: 0.11301476542154948 Test Loss: 0.36574845377604165\n",
      "Epoch: 12050 Training Loss: 0.11306760830349392 Test Loss: 0.36613096788194444\n",
      "Epoch: 12051 Training Loss: 0.11301823510064019 Test Loss: 0.3670697157118056\n",
      "Epoch: 12052 Training Loss: 0.11287598080105252 Test Loss: 0.36860139973958334\n",
      "Epoch: 12053 Training Loss: 0.11267485131157769 Test Loss: 0.36964686414930553\n",
      "Epoch: 12054 Training Loss: 0.11244402737087673 Test Loss: 0.36980419921875\n",
      "Epoch: 12055 Training Loss: 0.11218610127766927 Test Loss: 0.3686274685329861\n",
      "Epoch: 12056 Training Loss: 0.11193538665771484 Test Loss: 0.36677020941840277\n",
      "Epoch: 12057 Training Loss: 0.11168994734022353 Test Loss: 0.36473863389756944\n",
      "Epoch: 12058 Training Loss: 0.1114453854031033 Test Loss: 0.36292323133680554\n",
      "Epoch: 12059 Training Loss: 0.11119429270426433 Test Loss: 0.36160546875\n",
      "Epoch: 12060 Training Loss: 0.11092396714952257 Test Loss: 0.3606397298177083\n",
      "Epoch: 12061 Training Loss: 0.11063566165500217 Test Loss: 0.35995789930555555\n",
      "Epoch: 12062 Training Loss: 0.11040380689832899 Test Loss: 0.35965462239583335\n",
      "Epoch: 12063 Training Loss: 0.11022874874538846 Test Loss: 0.3596135525173611\n",
      "Epoch: 12064 Training Loss: 0.11012400309244792 Test Loss: 0.35968877495659723\n",
      "Epoch: 12065 Training Loss: 0.11008219231499566 Test Loss: 0.35951405164930555\n",
      "Epoch: 12066 Training Loss: 0.11007204352484809 Test Loss: 0.35937744140625\n",
      "Epoch: 12067 Training Loss: 0.11007548268636068 Test Loss: 0.35950648328993057\n",
      "Epoch: 12068 Training Loss: 0.11009773339165582 Test Loss: 0.35981258138020833\n",
      "Epoch: 12069 Training Loss: 0.11015426381429036 Test Loss: 0.36051383463541664\n",
      "Epoch: 12070 Training Loss: 0.11017905086941189 Test Loss: 0.361909423828125\n",
      "Epoch: 12071 Training Loss: 0.11017920769585504 Test Loss: 0.3631974283854167\n",
      "Epoch: 12072 Training Loss: 0.11014439137776692 Test Loss: 0.3641923828125\n",
      "Epoch: 12073 Training Loss: 0.11007346513536241 Test Loss: 0.36458658854166665\n",
      "Epoch: 12074 Training Loss: 0.10998526340060764 Test Loss: 0.3646865234375\n",
      "Epoch: 12075 Training Loss: 0.10990372043185764 Test Loss: 0.36453493923611113\n",
      "Epoch: 12076 Training Loss: 0.10979741753472222 Test Loss: 0.36443223741319447\n",
      "Epoch: 12077 Training Loss: 0.10969078403049046 Test Loss: 0.3643928493923611\n",
      "Epoch: 12078 Training Loss: 0.10959119669596354 Test Loss: 0.36471988932291666\n",
      "Epoch: 12079 Training Loss: 0.10948951043023003 Test Loss: 0.3648913845486111\n",
      "Epoch: 12080 Training Loss: 0.1093763427734375 Test Loss: 0.36507633463541667\n",
      "Epoch: 12081 Training Loss: 0.10924801550971137 Test Loss: 0.3653302680121528\n",
      "Epoch: 12082 Training Loss: 0.1091097895304362 Test Loss: 0.36579318576388886\n",
      "Epoch: 12083 Training Loss: 0.10897087860107423 Test Loss: 0.3659995659722222\n",
      "Epoch: 12084 Training Loss: 0.10883270772298177 Test Loss: 0.36627454969618056\n",
      "Epoch: 12085 Training Loss: 0.10868868170844184 Test Loss: 0.3662756076388889\n",
      "Epoch: 12086 Training Loss: 0.10854076639811198 Test Loss: 0.3662900390625\n",
      "Epoch: 12087 Training Loss: 0.10838402557373047 Test Loss: 0.36610782877604164\n",
      "Epoch: 12088 Training Loss: 0.10823360188802084 Test Loss: 0.36594797092013887\n",
      "Epoch: 12089 Training Loss: 0.10810184648301867 Test Loss: 0.3657263997395833\n",
      "Epoch: 12090 Training Loss: 0.10797430843777127 Test Loss: 0.365533447265625\n",
      "Epoch: 12091 Training Loss: 0.10785552639431424 Test Loss: 0.36518907335069445\n",
      "Epoch: 12092 Training Loss: 0.10774195353190104 Test Loss: 0.3648591579861111\n",
      "Epoch: 12093 Training Loss: 0.10762720659044053 Test Loss: 0.36423133680555553\n",
      "Epoch: 12094 Training Loss: 0.10754441494411893 Test Loss: 0.36383946397569444\n",
      "Epoch: 12095 Training Loss: 0.10748780144585504 Test Loss: 0.36320882161458334\n",
      "Epoch: 12096 Training Loss: 0.1074469706217448 Test Loss: 0.3624789767795139\n",
      "Epoch: 12097 Training Loss: 0.10741280110677083 Test Loss: 0.3618458658854167\n",
      "Epoch: 12098 Training Loss: 0.10739818233913845 Test Loss: 0.36114963107638887\n",
      "Epoch: 12099 Training Loss: 0.1073977288140191 Test Loss: 0.36066194661458334\n",
      "Epoch: 12100 Training Loss: 0.10742134433322482 Test Loss: 0.36060405815972224\n",
      "Epoch: 12101 Training Loss: 0.10745821380615235 Test Loss: 0.3607416449652778\n",
      "Epoch: 12102 Training Loss: 0.1074940940009223 Test Loss: 0.3611559787326389\n",
      "Epoch: 12103 Training Loss: 0.10753462558322482 Test Loss: 0.36134974500868056\n",
      "Epoch: 12104 Training Loss: 0.1075891342163086 Test Loss: 0.3616242947048611\n",
      "Epoch: 12105 Training Loss: 0.1076485120985243 Test Loss: 0.3617319878472222\n",
      "Epoch: 12106 Training Loss: 0.10772599368625217 Test Loss: 0.3619490559895833\n",
      "Epoch: 12107 Training Loss: 0.10781236775716146 Test Loss: 0.3621619737413194\n",
      "Epoch: 12108 Training Loss: 0.10791492377387153 Test Loss: 0.36216802300347223\n",
      "Epoch: 12109 Training Loss: 0.10801867336697049 Test Loss: 0.36213796657986114\n",
      "Epoch: 12110 Training Loss: 0.10813367970784506 Test Loss: 0.36199156358506945\n",
      "Epoch: 12111 Training Loss: 0.10827022382948134 Test Loss: 0.36163563368055557\n",
      "Epoch: 12112 Training Loss: 0.10842436981201171 Test Loss: 0.361186279296875\n",
      "Epoch: 12113 Training Loss: 0.10860263315836588 Test Loss: 0.36091387261284724\n",
      "Epoch: 12114 Training Loss: 0.10877815500895183 Test Loss: 0.36081917317708334\n",
      "Epoch: 12115 Training Loss: 0.1089533445570204 Test Loss: 0.36084141710069445\n",
      "Epoch: 12116 Training Loss: 0.10914607323540582 Test Loss: 0.3612087673611111\n",
      "Epoch: 12117 Training Loss: 0.10936836920844184 Test Loss: 0.3618935546875\n",
      "Epoch: 12118 Training Loss: 0.10962472364637586 Test Loss: 0.36268169487847224\n",
      "Epoch: 12119 Training Loss: 0.1099183120727539 Test Loss: 0.36369316948784725\n",
      "Epoch: 12120 Training Loss: 0.11022138892279731 Test Loss: 0.36433726671006944\n",
      "Epoch: 12121 Training Loss: 0.11052536180284288 Test Loss: 0.36469091796875\n",
      "Epoch: 12122 Training Loss: 0.11084667205810547 Test Loss: 0.3647744140625\n",
      "Epoch: 12123 Training Loss: 0.11119920603434245 Test Loss: 0.3645347222222222\n",
      "Epoch: 12124 Training Loss: 0.11159737481011285 Test Loss: 0.36463107638888886\n",
      "Epoch: 12125 Training Loss: 0.11207653130425348 Test Loss: 0.3645994466145833\n",
      "Epoch: 12126 Training Loss: 0.1126082017686632 Test Loss: 0.3649994574652778\n",
      "Epoch: 12127 Training Loss: 0.11319481150309245 Test Loss: 0.36604749891493055\n",
      "Epoch: 12128 Training Loss: 0.1138010999891493 Test Loss: 0.36714054361979165\n",
      "Epoch: 12129 Training Loss: 0.1143891092936198 Test Loss: 0.3680241970486111\n",
      "Epoch: 12130 Training Loss: 0.1150016827053494 Test Loss: 0.3676789279513889\n",
      "Epoch: 12131 Training Loss: 0.11556619008382162 Test Loss: 0.36552674696180554\n",
      "Epoch: 12132 Training Loss: 0.11589754740397136 Test Loss: 0.36237548828125\n",
      "Epoch: 12133 Training Loss: 0.11582386186387804 Test Loss: 0.36021934678819445\n",
      "Epoch: 12134 Training Loss: 0.11532959153917101 Test Loss: 0.3600073513454861\n",
      "Epoch: 12135 Training Loss: 0.11449905649820964 Test Loss: 0.3617782118055556\n",
      "Epoch: 12136 Training Loss: 0.11346879492865668 Test Loss: 0.3640253363715278\n",
      "Epoch: 12137 Training Loss: 0.11237927924262153 Test Loss: 0.36442418077256944\n",
      "Epoch: 12138 Training Loss: 0.11133414628770616 Test Loss: 0.3633493109809028\n",
      "Epoch: 12139 Training Loss: 0.11038516828748915 Test Loss: 0.3611769748263889\n",
      "Epoch: 12140 Training Loss: 0.10954012298583984 Test Loss: 0.35906971571180557\n",
      "Epoch: 12141 Training Loss: 0.10881158362494575 Test Loss: 0.35733987087673613\n",
      "Epoch: 12142 Training Loss: 0.10823214467366536 Test Loss: 0.35636949327256945\n",
      "Epoch: 12143 Training Loss: 0.10777826182047526 Test Loss: 0.3558729926215278\n",
      "Epoch: 12144 Training Loss: 0.10745638699001736 Test Loss: 0.355501953125\n",
      "Epoch: 12145 Training Loss: 0.10723548380533854 Test Loss: 0.35554383680555557\n",
      "Epoch: 12146 Training Loss: 0.10708180321587457 Test Loss: 0.3554932454427083\n",
      "Epoch: 12147 Training Loss: 0.10698332299126519 Test Loss: 0.3556865776909722\n",
      "Epoch: 12148 Training Loss: 0.1069134521484375 Test Loss: 0.3557525770399306\n",
      "Epoch: 12149 Training Loss: 0.10685455152723525 Test Loss: 0.3560588107638889\n",
      "Epoch: 12150 Training Loss: 0.10680232577853732 Test Loss: 0.356589111328125\n",
      "Epoch: 12151 Training Loss: 0.10676004113091363 Test Loss: 0.3572203233506944\n",
      "Epoch: 12152 Training Loss: 0.10672622341579861 Test Loss: 0.3581484646267361\n",
      "Epoch: 12153 Training Loss: 0.106703980339898 Test Loss: 0.3590852322048611\n",
      "Epoch: 12154 Training Loss: 0.10669842868381077 Test Loss: 0.36002023654513887\n",
      "Epoch: 12155 Training Loss: 0.10670908440483941 Test Loss: 0.36069300672743054\n",
      "Epoch: 12156 Training Loss: 0.10672843509250217 Test Loss: 0.3612840983072917\n",
      "Epoch: 12157 Training Loss: 0.10676788414849175 Test Loss: 0.3615439453125\n",
      "Epoch: 12158 Training Loss: 0.1068206566704644 Test Loss: 0.36152715386284723\n",
      "Epoch: 12159 Training Loss: 0.10687123531765408 Test Loss: 0.3614587131076389\n",
      "Epoch: 12160 Training Loss: 0.10691958194308811 Test Loss: 0.36123741319444447\n",
      "Epoch: 12161 Training Loss: 0.10697361246744792 Test Loss: 0.36113237847222224\n",
      "Epoch: 12162 Training Loss: 0.10704870944552951 Test Loss: 0.3613786349826389\n",
      "Epoch: 12163 Training Loss: 0.10714671156141493 Test Loss: 0.36178342013888887\n",
      "Epoch: 12164 Training Loss: 0.1072599843343099 Test Loss: 0.36215611436631945\n",
      "Epoch: 12165 Training Loss: 0.10737845442030165 Test Loss: 0.36264463975694444\n",
      "Epoch: 12166 Training Loss: 0.1075033671061198 Test Loss: 0.36329549153645835\n",
      "Epoch: 12167 Training Loss: 0.10764460923936632 Test Loss: 0.3637446017795139\n",
      "Epoch: 12168 Training Loss: 0.10780426194932725 Test Loss: 0.3642003580729167\n",
      "Epoch: 12169 Training Loss: 0.10799774593777127 Test Loss: 0.364634765625\n",
      "Epoch: 12170 Training Loss: 0.10822842407226563 Test Loss: 0.36531279839409725\n",
      "Epoch: 12171 Training Loss: 0.10848724195692275 Test Loss: 0.3657900390625\n",
      "Epoch: 12172 Training Loss: 0.10877348073323567 Test Loss: 0.36619620768229166\n",
      "Epoch: 12173 Training Loss: 0.10909326595730252 Test Loss: 0.3669096950954861\n",
      "Epoch: 12174 Training Loss: 0.10943227810329861 Test Loss: 0.36755045572916667\n",
      "Epoch: 12175 Training Loss: 0.10980928039550782 Test Loss: 0.3681078559027778\n",
      "Epoch: 12176 Training Loss: 0.11017506578233507 Test Loss: 0.36858200412326386\n",
      "Epoch: 12177 Training Loss: 0.11051842244466145 Test Loss: 0.36862744140625\n",
      "Epoch: 12178 Training Loss: 0.11083072238498264 Test Loss: 0.36882489691840276\n",
      "Epoch: 12179 Training Loss: 0.11110763210720487 Test Loss: 0.3692455512152778\n",
      "Epoch: 12180 Training Loss: 0.11138928646511502 Test Loss: 0.3691880967881944\n",
      "Epoch: 12181 Training Loss: 0.11172831386990018 Test Loss: 0.36917784288194444\n",
      "Epoch: 12182 Training Loss: 0.11208653004964193 Test Loss: 0.3686341959635417\n",
      "Epoch: 12183 Training Loss: 0.1124945288764106 Test Loss: 0.36791259765625\n",
      "Epoch: 12184 Training Loss: 0.11290298292371961 Test Loss: 0.36706410047743054\n",
      "Epoch: 12185 Training Loss: 0.11328323788113064 Test Loss: 0.36630875651041667\n",
      "Epoch: 12186 Training Loss: 0.11364786529541016 Test Loss: 0.3654336208767361\n",
      "Epoch: 12187 Training Loss: 0.11400938839382596 Test Loss: 0.36462353515625\n",
      "Epoch: 12188 Training Loss: 0.11432509019639757 Test Loss: 0.3640082465277778\n",
      "Epoch: 12189 Training Loss: 0.11454976230197482 Test Loss: 0.3634513346354167\n",
      "Epoch: 12190 Training Loss: 0.11474988301595052 Test Loss: 0.36366886393229164\n",
      "Epoch: 12191 Training Loss: 0.11498279147677952 Test Loss: 0.3651088053385417\n",
      "Epoch: 12192 Training Loss: 0.115235230339898 Test Loss: 0.36709841579861113\n",
      "Epoch: 12193 Training Loss: 0.11546436479356553 Test Loss: 0.369323974609375\n",
      "Epoch: 12194 Training Loss: 0.11562592485215929 Test Loss: 0.3706795247395833\n",
      "Epoch: 12195 Training Loss: 0.11568861219618055 Test Loss: 0.37030333116319447\n",
      "Epoch: 12196 Training Loss: 0.11561666954888238 Test Loss: 0.3688142632378472\n",
      "Epoch: 12197 Training Loss: 0.11540010579427083 Test Loss: 0.3673280707465278\n",
      "Epoch: 12198 Training Loss: 0.11505819702148437 Test Loss: 0.36704432508680557\n",
      "Epoch: 12199 Training Loss: 0.1145900132921007 Test Loss: 0.36744102647569443\n",
      "Epoch: 12200 Training Loss: 0.11410064951578776 Test Loss: 0.3687568088107639\n",
      "Epoch: 12201 Training Loss: 0.11369699944390191 Test Loss: 0.3695302734375\n",
      "Epoch: 12202 Training Loss: 0.11345747375488281 Test Loss: 0.3694030490451389\n",
      "Epoch: 12203 Training Loss: 0.11336255476209853 Test Loss: 0.36877916124131943\n",
      "Epoch: 12204 Training Loss: 0.11333050960964627 Test Loss: 0.36735701497395834\n",
      "Epoch: 12205 Training Loss: 0.11326915486653646 Test Loss: 0.3655870225694444\n",
      "Epoch: 12206 Training Loss: 0.1131705542670356 Test Loss: 0.36386355251736113\n",
      "Epoch: 12207 Training Loss: 0.11299432627360026 Test Loss: 0.3625856662326389\n",
      "Epoch: 12208 Training Loss: 0.11270975155300564 Test Loss: 0.3618895941840278\n",
      "Epoch: 12209 Training Loss: 0.11235057322184244 Test Loss: 0.36252490234375\n",
      "Epoch: 12210 Training Loss: 0.11194303046332466 Test Loss: 0.36359453667534725\n",
      "Epoch: 12211 Training Loss: 0.11146606614854601 Test Loss: 0.3648958875868056\n",
      "Epoch: 12212 Training Loss: 0.11097321404351128 Test Loss: 0.3656634114583333\n",
      "Epoch: 12213 Training Loss: 0.11045290713840061 Test Loss: 0.3655072970920139\n",
      "Epoch: 12214 Training Loss: 0.1099519271850586 Test Loss: 0.3650800509982639\n",
      "Epoch: 12215 Training Loss: 0.10947740173339844 Test Loss: 0.3641650390625\n",
      "Epoch: 12216 Training Loss: 0.10903318871392144 Test Loss: 0.3632146809895833\n",
      "Epoch: 12217 Training Loss: 0.1086180402967665 Test Loss: 0.36243391927083335\n",
      "Epoch: 12218 Training Loss: 0.10823338996039497 Test Loss: 0.36171980794270836\n",
      "Epoch: 12219 Training Loss: 0.10789848327636718 Test Loss: 0.3614497612847222\n",
      "Epoch: 12220 Training Loss: 0.10762055121527778 Test Loss: 0.36116240776909725\n",
      "Epoch: 12221 Training Loss: 0.10741063012017144 Test Loss: 0.3612199164496528\n",
      "Epoch: 12222 Training Loss: 0.1072629860772027 Test Loss: 0.3614833713107639\n",
      "Epoch: 12223 Training Loss: 0.10715491400824653 Test Loss: 0.36194319661458335\n",
      "Epoch: 12224 Training Loss: 0.1070837894015842 Test Loss: 0.36241493055555557\n",
      "Epoch: 12225 Training Loss: 0.10704489390055338 Test Loss: 0.36277010091145834\n",
      "Epoch: 12226 Training Loss: 0.10702840423583984 Test Loss: 0.3628732638888889\n",
      "Epoch: 12227 Training Loss: 0.10703014119466146 Test Loss: 0.3627776150173611\n",
      "Epoch: 12228 Training Loss: 0.10703536054823133 Test Loss: 0.3626727430555556\n",
      "Epoch: 12229 Training Loss: 0.10704777103000217 Test Loss: 0.3623646647135417\n",
      "Epoch: 12230 Training Loss: 0.10705879211425781 Test Loss: 0.3616828342013889\n",
      "Epoch: 12231 Training Loss: 0.10708135732014974 Test Loss: 0.3610761176215278\n",
      "Epoch: 12232 Training Loss: 0.10710448540581598 Test Loss: 0.36052134874131947\n",
      "Epoch: 12233 Training Loss: 0.10712514580620659 Test Loss: 0.36007484266493056\n",
      "Epoch: 12234 Training Loss: 0.10716057671440972 Test Loss: 0.3598225911458333\n",
      "Epoch: 12235 Training Loss: 0.1071864725748698 Test Loss: 0.3597211642795139\n",
      "Epoch: 12236 Training Loss: 0.10721331108940972 Test Loss: 0.35982801649305557\n",
      "Epoch: 12237 Training Loss: 0.107248169793023 Test Loss: 0.36004999457465275\n",
      "Epoch: 12238 Training Loss: 0.10728643714057075 Test Loss: 0.36033797200520834\n",
      "Epoch: 12239 Training Loss: 0.10735535939534505 Test Loss: 0.360613525390625\n",
      "Epoch: 12240 Training Loss: 0.1074496349758572 Test Loss: 0.3611080186631944\n",
      "Epoch: 12241 Training Loss: 0.10754255930582682 Test Loss: 0.36151171875\n",
      "Epoch: 12242 Training Loss: 0.10762413957383897 Test Loss: 0.361751220703125\n",
      "Epoch: 12243 Training Loss: 0.10773301357693142 Test Loss: 0.36163926866319446\n",
      "Epoch: 12244 Training Loss: 0.10789358520507812 Test Loss: 0.36121912977430554\n",
      "Epoch: 12245 Training Loss: 0.10808971744113498 Test Loss: 0.3606915961371528\n",
      "Epoch: 12246 Training Loss: 0.10829768371582031 Test Loss: 0.3598130154079861\n",
      "Epoch: 12247 Training Loss: 0.10851185353597005 Test Loss: 0.35923280164930554\n",
      "Epoch: 12248 Training Loss: 0.10873507012261285 Test Loss: 0.35845741102430556\n",
      "Epoch: 12249 Training Loss: 0.10896969180636935 Test Loss: 0.3578335232204861\n",
      "Epoch: 12250 Training Loss: 0.10922017161051432 Test Loss: 0.3574619140625\n",
      "Epoch: 12251 Training Loss: 0.10950101301405164 Test Loss: 0.3578162163628472\n",
      "Epoch: 12252 Training Loss: 0.10979740651448568 Test Loss: 0.3586982150607639\n",
      "Epoch: 12253 Training Loss: 0.11009353552924263 Test Loss: 0.3598757052951389\n",
      "Epoch: 12254 Training Loss: 0.11038849131266276 Test Loss: 0.3613103841145833\n",
      "Epoch: 12255 Training Loss: 0.11070918358696831 Test Loss: 0.36262969292534725\n",
      "Epoch: 12256 Training Loss: 0.1110380130343967 Test Loss: 0.36304500325520833\n",
      "Epoch: 12257 Training Loss: 0.11133678690592448 Test Loss: 0.3633499620225694\n",
      "Epoch: 12258 Training Loss: 0.11150190311008029 Test Loss: 0.36316465928819447\n",
      "Epoch: 12259 Training Loss: 0.11153549109564888 Test Loss: 0.36335194227430556\n",
      "Epoch: 12260 Training Loss: 0.11147698805067274 Test Loss: 0.3634067111545139\n",
      "Epoch: 12261 Training Loss: 0.1113395750257704 Test Loss: 0.36440174696180555\n",
      "Epoch: 12262 Training Loss: 0.11119195387098524 Test Loss: 0.3655034993489583\n",
      "Epoch: 12263 Training Loss: 0.11105175272623698 Test Loss: 0.36638585069444446\n",
      "Epoch: 12264 Training Loss: 0.1108790062798394 Test Loss: 0.3668062337239583\n",
      "Epoch: 12265 Training Loss: 0.11067936282687717 Test Loss: 0.36704402669270836\n",
      "Epoch: 12266 Training Loss: 0.11040604061550564 Test Loss: 0.36672835286458333\n",
      "Epoch: 12267 Training Loss: 0.11008850606282553 Test Loss: 0.3666315646701389\n",
      "Epoch: 12268 Training Loss: 0.10976684146457248 Test Loss: 0.366517578125\n",
      "Epoch: 12269 Training Loss: 0.10946148935953776 Test Loss: 0.36623375108506945\n",
      "Epoch: 12270 Training Loss: 0.10916009267171224 Test Loss: 0.36577864583333336\n",
      "Epoch: 12271 Training Loss: 0.10886968909369575 Test Loss: 0.36530729166666664\n",
      "Epoch: 12272 Training Loss: 0.10860988108317057 Test Loss: 0.3645000542534722\n",
      "Epoch: 12273 Training Loss: 0.10841125149197049 Test Loss: 0.3635468207465278\n",
      "Epoch: 12274 Training Loss: 0.1082593527899848 Test Loss: 0.36233024088541665\n",
      "Epoch: 12275 Training Loss: 0.10814693874782987 Test Loss: 0.3606967230902778\n",
      "Epoch: 12276 Training Loss: 0.10807196129692925 Test Loss: 0.35918448893229166\n",
      "Epoch: 12277 Training Loss: 0.10802038913302951 Test Loss: 0.35791097005208333\n",
      "Epoch: 12278 Training Loss: 0.10798582543267145 Test Loss: 0.3570686848958333\n",
      "Epoch: 12279 Training Loss: 0.10797221967909071 Test Loss: 0.3567322319878472\n",
      "Epoch: 12280 Training Loss: 0.10797551812065972 Test Loss: 0.3568789876302083\n",
      "Epoch: 12281 Training Loss: 0.10799237908257378 Test Loss: 0.357259765625\n",
      "Epoch: 12282 Training Loss: 0.10803515455457899 Test Loss: 0.35826038953993056\n",
      "Epoch: 12283 Training Loss: 0.10812667422824436 Test Loss: 0.3594715440538194\n",
      "Epoch: 12284 Training Loss: 0.10823025682237414 Test Loss: 0.36050726996527777\n",
      "Epoch: 12285 Training Loss: 0.10834625328911675 Test Loss: 0.36132221137152776\n",
      "Epoch: 12286 Training Loss: 0.10845994228786893 Test Loss: 0.3619660915798611\n",
      "Epoch: 12287 Training Loss: 0.10856721750895182 Test Loss: 0.36229964192708336\n",
      "Epoch: 12288 Training Loss: 0.10866261800130209 Test Loss: 0.3628237033420139\n",
      "Epoch: 12289 Training Loss: 0.10876248168945313 Test Loss: 0.3629699978298611\n",
      "Epoch: 12290 Training Loss: 0.10884648895263672 Test Loss: 0.36312589518229166\n",
      "Epoch: 12291 Training Loss: 0.10891211700439453 Test Loss: 0.3629623480902778\n",
      "Epoch: 12292 Training Loss: 0.10897085401746961 Test Loss: 0.3629280056423611\n",
      "Epoch: 12293 Training Loss: 0.10904310099283854 Test Loss: 0.36238096788194446\n",
      "Epoch: 12294 Training Loss: 0.10913584221733941 Test Loss: 0.36165616861979166\n",
      "Epoch: 12295 Training Loss: 0.10921703169080946 Test Loss: 0.36053914388020836\n",
      "Epoch: 12296 Training Loss: 0.10927987331814236 Test Loss: 0.3594287651909722\n",
      "Epoch: 12297 Training Loss: 0.10934671274820963 Test Loss: 0.3585536566840278\n",
      "Epoch: 12298 Training Loss: 0.10940648820665147 Test Loss: 0.3579448784722222\n",
      "Epoch: 12299 Training Loss: 0.10944789462619357 Test Loss: 0.3579949273003472\n",
      "Epoch: 12300 Training Loss: 0.10951431104871961 Test Loss: 0.3589356011284722\n",
      "Epoch: 12301 Training Loss: 0.10960462782118055 Test Loss: 0.36075689019097223\n",
      "Epoch: 12302 Training Loss: 0.10973866102430556 Test Loss: 0.3626875542534722\n",
      "Epoch: 12303 Training Loss: 0.10994442155626086 Test Loss: 0.36456800672743056\n",
      "Epoch: 12304 Training Loss: 0.11019304911295573 Test Loss: 0.366076416015625\n",
      "Epoch: 12305 Training Loss: 0.11047774759928386 Test Loss: 0.3670964626736111\n",
      "Epoch: 12306 Training Loss: 0.1107517573038737 Test Loss: 0.36740345594618057\n",
      "Epoch: 12307 Training Loss: 0.11093813069661458 Test Loss: 0.3670900607638889\n",
      "Epoch: 12308 Training Loss: 0.11098938327365451 Test Loss: 0.36662638346354165\n",
      "Epoch: 12309 Training Loss: 0.11090463172064888 Test Loss: 0.366175048828125\n",
      "Epoch: 12310 Training Loss: 0.11072177632649739 Test Loss: 0.36623963758680556\n",
      "Epoch: 12311 Training Loss: 0.11047422536214192 Test Loss: 0.3665179036458333\n",
      "Epoch: 12312 Training Loss: 0.1102167485555013 Test Loss: 0.36651844618055557\n",
      "Epoch: 12313 Training Loss: 0.11001551564534505 Test Loss: 0.3663779296875\n",
      "Epoch: 12314 Training Loss: 0.10984224361843534 Test Loss: 0.36607967122395835\n",
      "Epoch: 12315 Training Loss: 0.10966072421603733 Test Loss: 0.36591840277777776\n",
      "Epoch: 12316 Training Loss: 0.10948064846462674 Test Loss: 0.36648475477430553\n",
      "Epoch: 12317 Training Loss: 0.10933415730794271 Test Loss: 0.36691441514756945\n",
      "Epoch: 12318 Training Loss: 0.10922895389133029 Test Loss: 0.36716075303819445\n",
      "Epoch: 12319 Training Loss: 0.10917386457655165 Test Loss: 0.3672678493923611\n",
      "Epoch: 12320 Training Loss: 0.10916993374294705 Test Loss: 0.366802001953125\n",
      "Epoch: 12321 Training Loss: 0.10924803246392144 Test Loss: 0.36593155924479165\n",
      "Epoch: 12322 Training Loss: 0.10936121198866103 Test Loss: 0.3652809787326389\n",
      "Epoch: 12323 Training Loss: 0.10946659342447916 Test Loss: 0.3646528862847222\n",
      "Epoch: 12324 Training Loss: 0.10953475613064236 Test Loss: 0.3637079535590278\n",
      "Epoch: 12325 Training Loss: 0.10955905066596137 Test Loss: 0.362958740234375\n",
      "Epoch: 12326 Training Loss: 0.10956250423855252 Test Loss: 0.3624189453125\n",
      "Epoch: 12327 Training Loss: 0.10958448028564453 Test Loss: 0.3623785807291667\n",
      "Epoch: 12328 Training Loss: 0.10962227461073133 Test Loss: 0.36268250868055557\n",
      "Epoch: 12329 Training Loss: 0.10965636274549696 Test Loss: 0.36319981553819447\n",
      "Epoch: 12330 Training Loss: 0.10966593339708117 Test Loss: 0.36380585394965276\n",
      "Epoch: 12331 Training Loss: 0.10965125783284506 Test Loss: 0.36413351779513886\n",
      "Epoch: 12332 Training Loss: 0.10963091532389323 Test Loss: 0.3643512098524306\n",
      "Epoch: 12333 Training Loss: 0.10958679453531901 Test Loss: 0.36457237413194443\n",
      "Epoch: 12334 Training Loss: 0.10952762942843967 Test Loss: 0.36472588433159725\n",
      "Epoch: 12335 Training Loss: 0.10945316992865668 Test Loss: 0.3647395290798611\n",
      "Epoch: 12336 Training Loss: 0.10937811194525825 Test Loss: 0.3648415798611111\n",
      "Epoch: 12337 Training Loss: 0.10932692379421657 Test Loss: 0.3648420952690972\n",
      "Epoch: 12338 Training Loss: 0.10928087022569444 Test Loss: 0.3648568793402778\n",
      "Epoch: 12339 Training Loss: 0.10925859663221571 Test Loss: 0.36478917100694447\n",
      "Epoch: 12340 Training Loss: 0.10923072136773003 Test Loss: 0.36470526801215275\n",
      "Epoch: 12341 Training Loss: 0.10923705715603299 Test Loss: 0.36458878580729165\n",
      "Epoch: 12342 Training Loss: 0.10927695549858941 Test Loss: 0.36477186414930557\n",
      "Epoch: 12343 Training Loss: 0.10934545220269097 Test Loss: 0.3649920247395833\n",
      "Epoch: 12344 Training Loss: 0.10944013383653428 Test Loss: 0.3648415798611111\n",
      "Epoch: 12345 Training Loss: 0.10957424757215711 Test Loss: 0.36486046006944445\n",
      "Epoch: 12346 Training Loss: 0.10970096842447917 Test Loss: 0.3642301432291667\n",
      "Epoch: 12347 Training Loss: 0.1098556416829427 Test Loss: 0.36335617404513887\n",
      "Epoch: 12348 Training Loss: 0.1100621066623264 Test Loss: 0.362306396484375\n",
      "Epoch: 12349 Training Loss: 0.11029768286810981 Test Loss: 0.3609435763888889\n",
      "Epoch: 12350 Training Loss: 0.11055604553222656 Test Loss: 0.359767578125\n",
      "Epoch: 12351 Training Loss: 0.11080893198649089 Test Loss: 0.35905045572916666\n",
      "Epoch: 12352 Training Loss: 0.11105482482910156 Test Loss: 0.3591808810763889\n",
      "Epoch: 12353 Training Loss: 0.11131309848361545 Test Loss: 0.35997322591145836\n",
      "Epoch: 12354 Training Loss: 0.11159168582492404 Test Loss: 0.3614001736111111\n",
      "Epoch: 12355 Training Loss: 0.11193458896213107 Test Loss: 0.3626132541232639\n",
      "Epoch: 12356 Training Loss: 0.11230066511366102 Test Loss: 0.3632830403645833\n",
      "Epoch: 12357 Training Loss: 0.11265051354302301 Test Loss: 0.36327020941840277\n",
      "Epoch: 12358 Training Loss: 0.11297974989149305 Test Loss: 0.3624199761284722\n",
      "Epoch: 12359 Training Loss: 0.11329077741834852 Test Loss: 0.3619157443576389\n",
      "Epoch: 12360 Training Loss: 0.11349858686659071 Test Loss: 0.3622776692708333\n",
      "Epoch: 12361 Training Loss: 0.11354183366563585 Test Loss: 0.36322846137152776\n",
      "Epoch: 12362 Training Loss: 0.11351344129774306 Test Loss: 0.3649595269097222\n",
      "Epoch: 12363 Training Loss: 0.11345271301269531 Test Loss: 0.366056640625\n",
      "Epoch: 12364 Training Loss: 0.11335543060302734 Test Loss: 0.36652232530381945\n",
      "Epoch: 12365 Training Loss: 0.11322332085503473 Test Loss: 0.3654828016493056\n",
      "Epoch: 12366 Training Loss: 0.11304597473144531 Test Loss: 0.3634043240017361\n",
      "Epoch: 12367 Training Loss: 0.11278152974446615 Test Loss: 0.3611863064236111\n",
      "Epoch: 12368 Training Loss: 0.11247512478298612 Test Loss: 0.35960530598958335\n",
      "Epoch: 12369 Training Loss: 0.11214678361680773 Test Loss: 0.35902769639756943\n",
      "Epoch: 12370 Training Loss: 0.11178823937310113 Test Loss: 0.3592108832465278\n",
      "Epoch: 12371 Training Loss: 0.11140156724717883 Test Loss: 0.3596587456597222\n",
      "Epoch: 12372 Training Loss: 0.11103354983859592 Test Loss: 0.36050602213541666\n",
      "Epoch: 12373 Training Loss: 0.11069439527723525 Test Loss: 0.3608884548611111\n",
      "Epoch: 12374 Training Loss: 0.110314453125 Test Loss: 0.36113088650173614\n",
      "Epoch: 12375 Training Loss: 0.10991693284776476 Test Loss: 0.3610094943576389\n",
      "Epoch: 12376 Training Loss: 0.10951635996500651 Test Loss: 0.360800537109375\n",
      "Epoch: 12377 Training Loss: 0.10912205844455296 Test Loss: 0.36047059461805553\n",
      "Epoch: 12378 Training Loss: 0.10876401180691189 Test Loss: 0.35977403428819443\n",
      "Epoch: 12379 Training Loss: 0.10845407274034288 Test Loss: 0.3590962456597222\n",
      "Epoch: 12380 Training Loss: 0.10818195597330729 Test Loss: 0.3580830891927083\n",
      "Epoch: 12381 Training Loss: 0.10797548166910807 Test Loss: 0.3571468098958333\n",
      "Epoch: 12382 Training Loss: 0.10781285264756944 Test Loss: 0.35634700520833335\n",
      "Epoch: 12383 Training Loss: 0.10769684685601129 Test Loss: 0.3558502604166667\n",
      "Epoch: 12384 Training Loss: 0.10764857398139106 Test Loss: 0.3557572699652778\n",
      "Epoch: 12385 Training Loss: 0.10765660942925347 Test Loss: 0.3558854166666667\n",
      "Epoch: 12386 Training Loss: 0.10769417656792535 Test Loss: 0.35650398763020835\n",
      "Epoch: 12387 Training Loss: 0.10775229305691189 Test Loss: 0.3572097439236111\n",
      "Epoch: 12388 Training Loss: 0.10784066687689887 Test Loss: 0.3581585286458333\n",
      "Epoch: 12389 Training Loss: 0.10796222601996527 Test Loss: 0.3588767361111111\n",
      "Epoch: 12390 Training Loss: 0.10808699289957682 Test Loss: 0.35946522352430554\n",
      "Epoch: 12391 Training Loss: 0.1082145021226671 Test Loss: 0.3596092122395833\n",
      "Epoch: 12392 Training Loss: 0.10832510291205512 Test Loss: 0.3591937391493056\n",
      "Epoch: 12393 Training Loss: 0.10840194617377387 Test Loss: 0.35819417317708335\n",
      "Epoch: 12394 Training Loss: 0.10841351233588324 Test Loss: 0.3573134223090278\n",
      "Epoch: 12395 Training Loss: 0.1083531739976671 Test Loss: 0.3565268283420139\n",
      "Epoch: 12396 Training Loss: 0.10823942396375869 Test Loss: 0.3563714192708333\n",
      "Epoch: 12397 Training Loss: 0.10808468119303385 Test Loss: 0.3567043185763889\n",
      "Epoch: 12398 Training Loss: 0.10791638098822699 Test Loss: 0.35761143663194445\n",
      "Epoch: 12399 Training Loss: 0.1077320539686415 Test Loss: 0.3586469184027778\n",
      "Epoch: 12400 Training Loss: 0.10751322852240669 Test Loss: 0.3596620279947917\n",
      "Epoch: 12401 Training Loss: 0.10727794223361545 Test Loss: 0.3604777560763889\n",
      "Epoch: 12402 Training Loss: 0.10703243255615234 Test Loss: 0.36097455512152776\n",
      "Epoch: 12403 Training Loss: 0.10678555891248914 Test Loss: 0.36118343098958333\n",
      "Epoch: 12404 Training Loss: 0.10653037431504991 Test Loss: 0.3611499565972222\n",
      "Epoch: 12405 Training Loss: 0.10627005174424914 Test Loss: 0.36062055121527775\n",
      "Epoch: 12406 Training Loss: 0.10602080874972873 Test Loss: 0.35985139973958336\n",
      "Epoch: 12407 Training Loss: 0.10579554239908855 Test Loss: 0.359041259765625\n",
      "Epoch: 12408 Training Loss: 0.10559927876790365 Test Loss: 0.3583213161892361\n",
      "Epoch: 12409 Training Loss: 0.10543902926974827 Test Loss: 0.35762017144097225\n",
      "Epoch: 12410 Training Loss: 0.10531484900580512 Test Loss: 0.3571656629774306\n",
      "Epoch: 12411 Training Loss: 0.10522231038411459 Test Loss: 0.3570331217447917\n",
      "Epoch: 12412 Training Loss: 0.10515641699896919 Test Loss: 0.35694053819444443\n",
      "Epoch: 12413 Training Loss: 0.10511405012342664 Test Loss: 0.35714708116319444\n",
      "Epoch: 12414 Training Loss: 0.105085817972819 Test Loss: 0.35746641710069443\n",
      "Epoch: 12415 Training Loss: 0.10507818518744574 Test Loss: 0.3577729763454861\n",
      "Epoch: 12416 Training Loss: 0.10509195539686415 Test Loss: 0.35814992947048613\n",
      "Epoch: 12417 Training Loss: 0.10513026004367404 Test Loss: 0.3587234700520833\n",
      "Epoch: 12418 Training Loss: 0.10518627421061198 Test Loss: 0.3591807725694444\n",
      "Epoch: 12419 Training Loss: 0.10523776414659289 Test Loss: 0.35947710503472224\n",
      "Epoch: 12420 Training Loss: 0.10529711490207248 Test Loss: 0.36006689453125\n",
      "Epoch: 12421 Training Loss: 0.10535220167371961 Test Loss: 0.3604842122395833\n",
      "Epoch: 12422 Training Loss: 0.10541551971435546 Test Loss: 0.360820068359375\n",
      "Epoch: 12423 Training Loss: 0.10551236724853516 Test Loss: 0.3612244194878472\n",
      "Epoch: 12424 Training Loss: 0.10562522379557292 Test Loss: 0.3616174045138889\n",
      "Epoch: 12425 Training Loss: 0.10574301571316189 Test Loss: 0.3618802897135417\n",
      "Epoch: 12426 Training Loss: 0.10585604773627387 Test Loss: 0.3619193250868056\n",
      "Epoch: 12427 Training Loss: 0.10594400956895617 Test Loss: 0.36178602430555556\n",
      "Epoch: 12428 Training Loss: 0.10602489132351345 Test Loss: 0.36170865885416664\n",
      "Epoch: 12429 Training Loss: 0.10611735449896918 Test Loss: 0.36144398328993055\n",
      "Epoch: 12430 Training Loss: 0.1062370376586914 Test Loss: 0.361386962890625\n",
      "Epoch: 12431 Training Loss: 0.10636470201280382 Test Loss: 0.361546875\n",
      "Epoch: 12432 Training Loss: 0.10649633026123047 Test Loss: 0.36176985677083334\n",
      "Epoch: 12433 Training Loss: 0.10662021976047092 Test Loss: 0.3620187717013889\n",
      "Epoch: 12434 Training Loss: 0.10674663289388021 Test Loss: 0.3622664930555556\n",
      "Epoch: 12435 Training Loss: 0.10687620713975694 Test Loss: 0.362521728515625\n",
      "Epoch: 12436 Training Loss: 0.10703032769097222 Test Loss: 0.36243950737847225\n",
      "Epoch: 12437 Training Loss: 0.10720807817247179 Test Loss: 0.3620921223958333\n",
      "Epoch: 12438 Training Loss: 0.10739224582248263 Test Loss: 0.3614803059895833\n",
      "Epoch: 12439 Training Loss: 0.10756717851426867 Test Loss: 0.36077704535590277\n",
      "Epoch: 12440 Training Loss: 0.10773586781819662 Test Loss: 0.3602468532986111\n",
      "Epoch: 12441 Training Loss: 0.10789946577284071 Test Loss: 0.3599779730902778\n",
      "Epoch: 12442 Training Loss: 0.1080599602593316 Test Loss: 0.35982845052083334\n",
      "Epoch: 12443 Training Loss: 0.108209108988444 Test Loss: 0.3601021321614583\n",
      "Epoch: 12444 Training Loss: 0.10836067114935981 Test Loss: 0.3604637044270833\n",
      "Epoch: 12445 Training Loss: 0.1085214580959744 Test Loss: 0.36093983289930553\n",
      "Epoch: 12446 Training Loss: 0.10871784549289279 Test Loss: 0.3612404242621528\n",
      "Epoch: 12447 Training Loss: 0.10896039157443577 Test Loss: 0.36157731119791664\n",
      "Epoch: 12448 Training Loss: 0.10924027676052517 Test Loss: 0.36163096788194443\n",
      "Epoch: 12449 Training Loss: 0.10955555809868707 Test Loss: 0.36151161024305556\n",
      "Epoch: 12450 Training Loss: 0.10992469363742405 Test Loss: 0.36121305338541665\n",
      "Epoch: 12451 Training Loss: 0.1103279529147678 Test Loss: 0.36113981119791666\n",
      "Epoch: 12452 Training Loss: 0.1107215822007921 Test Loss: 0.3614724934895833\n",
      "Epoch: 12453 Training Loss: 0.11108910454644097 Test Loss: 0.3625198567708333\n",
      "Epoch: 12454 Training Loss: 0.11139297824435763 Test Loss: 0.36343543836805553\n",
      "Epoch: 12455 Training Loss: 0.11164879353841146 Test Loss: 0.36419525824652776\n",
      "Epoch: 12456 Training Loss: 0.11189906480577257 Test Loss: 0.3641971842447917\n",
      "Epoch: 12457 Training Loss: 0.11213438669840495 Test Loss: 0.36343397352430556\n",
      "Epoch: 12458 Training Loss: 0.11232145182291667 Test Loss: 0.36297007921006946\n",
      "Epoch: 12459 Training Loss: 0.11247327338324653 Test Loss: 0.3623908420138889\n",
      "Epoch: 12460 Training Loss: 0.11255962965223525 Test Loss: 0.3629040798611111\n",
      "Epoch: 12461 Training Loss: 0.11254696655273437 Test Loss: 0.36478805881076387\n",
      "Epoch: 12462 Training Loss: 0.11244653913709853 Test Loss: 0.3677028537326389\n",
      "Epoch: 12463 Training Loss: 0.11229601372612848 Test Loss: 0.36971470811631946\n",
      "Epoch: 12464 Training Loss: 0.1121383539835612 Test Loss: 0.3704348415798611\n",
      "Epoch: 12465 Training Loss: 0.11191610717773437 Test Loss: 0.36932305230034723\n",
      "Epoch: 12466 Training Loss: 0.11153490871853299 Test Loss: 0.36643405490451386\n",
      "Epoch: 12467 Training Loss: 0.1109603491889106 Test Loss: 0.3635866427951389\n",
      "Epoch: 12468 Training Loss: 0.11026200697157118 Test Loss: 0.36136271158854166\n",
      "Epoch: 12469 Training Loss: 0.10953244272867839 Test Loss: 0.36022737630208335\n",
      "Epoch: 12470 Training Loss: 0.10881957923041449 Test Loss: 0.36007264539930556\n",
      "Epoch: 12471 Training Loss: 0.10823216417100695 Test Loss: 0.36075873480902776\n",
      "Epoch: 12472 Training Loss: 0.10778404490152994 Test Loss: 0.3611251627604167\n",
      "Epoch: 12473 Training Loss: 0.10745431942409939 Test Loss: 0.36099720594618057\n",
      "Epoch: 12474 Training Loss: 0.10719646114773221 Test Loss: 0.36079150390625\n",
      "Epoch: 12475 Training Loss: 0.10698629506429036 Test Loss: 0.3605331759982639\n",
      "Epoch: 12476 Training Loss: 0.1068041254679362 Test Loss: 0.36043986002604167\n",
      "Epoch: 12477 Training Loss: 0.10665158759223091 Test Loss: 0.3608083767361111\n",
      "Epoch: 12478 Training Loss: 0.10653668891059027 Test Loss: 0.3609170193142361\n",
      "Epoch: 12479 Training Loss: 0.10648798200819228 Test Loss: 0.36118587239583333\n",
      "Epoch: 12480 Training Loss: 0.10649702877468532 Test Loss: 0.3617937282986111\n",
      "Epoch: 12481 Training Loss: 0.10653897772894966 Test Loss: 0.3622236056857639\n",
      "Epoch: 12482 Training Loss: 0.1065871590508355 Test Loss: 0.3631280653211806\n",
      "Epoch: 12483 Training Loss: 0.10660679456922743 Test Loss: 0.36383626302083333\n",
      "Epoch: 12484 Training Loss: 0.10660986412896051 Test Loss: 0.36446587456597224\n",
      "Epoch: 12485 Training Loss: 0.10659798600938585 Test Loss: 0.3650082465277778\n",
      "Epoch: 12486 Training Loss: 0.10657512495252822 Test Loss: 0.3649447699652778\n",
      "Epoch: 12487 Training Loss: 0.10654258812798394 Test Loss: 0.364767578125\n",
      "Epoch: 12488 Training Loss: 0.10651660664876302 Test Loss: 0.3644016655815972\n",
      "Epoch: 12489 Training Loss: 0.10649860551622178 Test Loss: 0.3638380533854167\n",
      "Epoch: 12490 Training Loss: 0.10648292880588107 Test Loss: 0.3632639973958333\n",
      "Epoch: 12491 Training Loss: 0.1064750747680664 Test Loss: 0.3624441189236111\n",
      "Epoch: 12492 Training Loss: 0.1064773161146376 Test Loss: 0.3616103515625\n",
      "Epoch: 12493 Training Loss: 0.10648672400580513 Test Loss: 0.36087432183159723\n",
      "Epoch: 12494 Training Loss: 0.10651108042399089 Test Loss: 0.36018465169270836\n",
      "Epoch: 12495 Training Loss: 0.10655011579725478 Test Loss: 0.3597938639322917\n",
      "Epoch: 12496 Training Loss: 0.1066222644382053 Test Loss: 0.3593384060329861\n",
      "Epoch: 12497 Training Loss: 0.10671348317464192 Test Loss: 0.3593310546875\n",
      "Epoch: 12498 Training Loss: 0.10682608540852864 Test Loss: 0.3592969563802083\n",
      "Epoch: 12499 Training Loss: 0.106962036980523 Test Loss: 0.3595678439670139\n",
      "Epoch: 12500 Training Loss: 0.10712805853949653 Test Loss: 0.3600228949652778\n",
      "Epoch: 12501 Training Loss: 0.10732955169677734 Test Loss: 0.3603242730034722\n",
      "Epoch: 12502 Training Loss: 0.10755784183078342 Test Loss: 0.3606291775173611\n",
      "Epoch: 12503 Training Loss: 0.10779530334472656 Test Loss: 0.3607794596354167\n",
      "Epoch: 12504 Training Loss: 0.1080187742445204 Test Loss: 0.36091606987847225\n",
      "Epoch: 12505 Training Loss: 0.10825010087754991 Test Loss: 0.3611522894965278\n",
      "Epoch: 12506 Training Loss: 0.10851019117567275 Test Loss: 0.3610254991319444\n",
      "Epoch: 12507 Training Loss: 0.10875815158420139 Test Loss: 0.361106201171875\n",
      "Epoch: 12508 Training Loss: 0.10901186964246962 Test Loss: 0.36115456814236113\n",
      "Epoch: 12509 Training Loss: 0.10926850043402778 Test Loss: 0.36159288194444444\n",
      "Epoch: 12510 Training Loss: 0.10945180850558811 Test Loss: 0.36202964952256944\n",
      "Epoch: 12511 Training Loss: 0.10958623504638672 Test Loss: 0.3630553927951389\n",
      "Epoch: 12512 Training Loss: 0.10963121710883246 Test Loss: 0.3641951226128472\n",
      "Epoch: 12513 Training Loss: 0.10959386274549696 Test Loss: 0.36534309895833333\n",
      "Epoch: 12514 Training Loss: 0.10953663635253906 Test Loss: 0.3658258734809028\n",
      "Epoch: 12515 Training Loss: 0.10944673665364583 Test Loss: 0.3657322319878472\n",
      "Epoch: 12516 Training Loss: 0.10929325527615018 Test Loss: 0.364746337890625\n",
      "Epoch: 12517 Training Loss: 0.10908310275607638 Test Loss: 0.3635478244357639\n",
      "Epoch: 12518 Training Loss: 0.10885090806749131 Test Loss: 0.3621875\n",
      "Epoch: 12519 Training Loss: 0.10859009212917752 Test Loss: 0.3607626953125\n",
      "Epoch: 12520 Training Loss: 0.10834762403700086 Test Loss: 0.35965125868055553\n",
      "Epoch: 12521 Training Loss: 0.10811575232611763 Test Loss: 0.35879239908854166\n",
      "Epoch: 12522 Training Loss: 0.10791246795654297 Test Loss: 0.3580501302083333\n",
      "Epoch: 12523 Training Loss: 0.10773451571994358 Test Loss: 0.35746514214409725\n",
      "Epoch: 12524 Training Loss: 0.10756051211886936 Test Loss: 0.3567528754340278\n",
      "Epoch: 12525 Training Loss: 0.10738748084174263 Test Loss: 0.3560185818142361\n",
      "Epoch: 12526 Training Loss: 0.10723586951361762 Test Loss: 0.3554093967013889\n",
      "Epoch: 12527 Training Loss: 0.10710371568467882 Test Loss: 0.35495166015625\n",
      "Epoch: 12528 Training Loss: 0.10699001651340061 Test Loss: 0.354658447265625\n",
      "Epoch: 12529 Training Loss: 0.10688693576388889 Test Loss: 0.3548004557291667\n",
      "Epoch: 12530 Training Loss: 0.10680901336669922 Test Loss: 0.35533241102430557\n",
      "Epoch: 12531 Training Loss: 0.10678922695583767 Test Loss: 0.35605289713541666\n",
      "Epoch: 12532 Training Loss: 0.1068302239312066 Test Loss: 0.35691422526041666\n",
      "Epoch: 12533 Training Loss: 0.10692153676350911 Test Loss: 0.3579088541666667\n",
      "Epoch: 12534 Training Loss: 0.10704759216308593 Test Loss: 0.3582817111545139\n",
      "Epoch: 12535 Training Loss: 0.10718599955240886 Test Loss: 0.35850439453125\n",
      "Epoch: 12536 Training Loss: 0.10731484900580512 Test Loss: 0.35841167534722224\n",
      "Epoch: 12537 Training Loss: 0.10742447323269315 Test Loss: 0.3585802951388889\n",
      "Epoch: 12538 Training Loss: 0.10752351972791883 Test Loss: 0.35873415798611114\n",
      "Epoch: 12539 Training Loss: 0.10763602956136067 Test Loss: 0.35998187934027776\n",
      "Epoch: 12540 Training Loss: 0.10776883443196615 Test Loss: 0.36132524956597223\n",
      "Epoch: 12541 Training Loss: 0.10790284305148655 Test Loss: 0.3631982150607639\n",
      "Epoch: 12542 Training Loss: 0.10806021881103516 Test Loss: 0.3644981011284722\n",
      "Epoch: 12543 Training Loss: 0.10825940195719401 Test Loss: 0.36523109266493053\n",
      "Epoch: 12544 Training Loss: 0.1084914796617296 Test Loss: 0.36475230577256945\n",
      "Epoch: 12545 Training Loss: 0.1087560272216797 Test Loss: 0.3635020073784722\n",
      "Epoch: 12546 Training Loss: 0.10901178232828776 Test Loss: 0.3624325358072917\n",
      "Epoch: 12547 Training Loss: 0.1092118437025282 Test Loss: 0.3611954752604167\n",
      "Epoch: 12548 Training Loss: 0.1093281750149197 Test Loss: 0.3608942599826389\n",
      "Epoch: 12549 Training Loss: 0.10940390777587891 Test Loss: 0.36089935980902776\n",
      "Epoch: 12550 Training Loss: 0.10945058102077908 Test Loss: 0.3607746310763889\n",
      "Epoch: 12551 Training Loss: 0.1095183851453993 Test Loss: 0.3608050944010417\n",
      "Epoch: 12552 Training Loss: 0.10962867652045356 Test Loss: 0.3601884765625\n",
      "Epoch: 12553 Training Loss: 0.10972359127468533 Test Loss: 0.35880620659722223\n",
      "Epoch: 12554 Training Loss: 0.10978642188178168 Test Loss: 0.35745719401041665\n",
      "Epoch: 12555 Training Loss: 0.10982052188449436 Test Loss: 0.3567366265190972\n",
      "Epoch: 12556 Training Loss: 0.10985448964436849 Test Loss: 0.3567919379340278\n",
      "Epoch: 12557 Training Loss: 0.10988098568386502 Test Loss: 0.35695472547743057\n",
      "Epoch: 12558 Training Loss: 0.1098851318359375 Test Loss: 0.35716718207465276\n",
      "Epoch: 12559 Training Loss: 0.10984534539116754 Test Loss: 0.35707774522569447\n",
      "Epoch: 12560 Training Loss: 0.10976631927490234 Test Loss: 0.3573108723958333\n",
      "Epoch: 12561 Training Loss: 0.10962217288547092 Test Loss: 0.357822021484375\n",
      "Epoch: 12562 Training Loss: 0.10939859602186415 Test Loss: 0.35856141493055554\n",
      "Epoch: 12563 Training Loss: 0.10916549767388238 Test Loss: 0.35920458984375\n",
      "Epoch: 12564 Training Loss: 0.10896167924669053 Test Loss: 0.3593904079861111\n",
      "Epoch: 12565 Training Loss: 0.10878868442111544 Test Loss: 0.35922349717881946\n",
      "Epoch: 12566 Training Loss: 0.10862879265679254 Test Loss: 0.35891796875\n",
      "Epoch: 12567 Training Loss: 0.10841980912950304 Test Loss: 0.3581996799045139\n",
      "Epoch: 12568 Training Loss: 0.1081068861219618 Test Loss: 0.3578109809027778\n",
      "Epoch: 12569 Training Loss: 0.10769721306694878 Test Loss: 0.3575654568142361\n",
      "Epoch: 12570 Training Loss: 0.10725452507866753 Test Loss: 0.35748863389756946\n",
      "Epoch: 12571 Training Loss: 0.10681153869628907 Test Loss: 0.3578141547309028\n",
      "Epoch: 12572 Training Loss: 0.10640067036946614 Test Loss: 0.35835796440972223\n",
      "Epoch: 12573 Training Loss: 0.10604258643256294 Test Loss: 0.35899066840277777\n",
      "Epoch: 12574 Training Loss: 0.105748658074273 Test Loss: 0.359544921875\n",
      "Epoch: 12575 Training Loss: 0.10552311028374566 Test Loss: 0.3600348849826389\n",
      "Epoch: 12576 Training Loss: 0.10535047234429254 Test Loss: 0.3604509006076389\n",
      "Epoch: 12577 Training Loss: 0.10521042972140841 Test Loss: 0.36049245876736113\n",
      "Epoch: 12578 Training Loss: 0.1050803968641493 Test Loss: 0.36032042100694445\n",
      "Epoch: 12579 Training Loss: 0.10497604454888237 Test Loss: 0.36028461371527776\n",
      "Epoch: 12580 Training Loss: 0.10490412055121527 Test Loss: 0.3602240939670139\n",
      "Epoch: 12581 Training Loss: 0.10486721547444662 Test Loss: 0.3602790256076389\n",
      "Epoch: 12582 Training Loss: 0.10484799109564887 Test Loss: 0.3603353949652778\n",
      "Epoch: 12583 Training Loss: 0.10485391235351563 Test Loss: 0.36073320855034724\n",
      "Epoch: 12584 Training Loss: 0.10488036092122396 Test Loss: 0.36139407009548613\n",
      "Epoch: 12585 Training Loss: 0.10493994734022352 Test Loss: 0.36194816080729164\n",
      "Epoch: 12586 Training Loss: 0.10502467007107205 Test Loss: 0.36263001844618054\n",
      "Epoch: 12587 Training Loss: 0.10510414632161458 Test Loss: 0.36324169921875\n",
      "Epoch: 12588 Training Loss: 0.10517259979248048 Test Loss: 0.3639262966579861\n",
      "Epoch: 12589 Training Loss: 0.10525769382052952 Test Loss: 0.36484315321180555\n",
      "Epoch: 12590 Training Loss: 0.10535884433322483 Test Loss: 0.36557698567708335\n",
      "Epoch: 12591 Training Loss: 0.10546593475341796 Test Loss: 0.3666066623263889\n",
      "Epoch: 12592 Training Loss: 0.10557815975613065 Test Loss: 0.3674609103732639\n",
      "Epoch: 12593 Training Loss: 0.10569058312310113 Test Loss: 0.3680405002170139\n",
      "Epoch: 12594 Training Loss: 0.10580109066433377 Test Loss: 0.3683145616319444\n",
      "Epoch: 12595 Training Loss: 0.1059155527750651 Test Loss: 0.36867111545138886\n",
      "Epoch: 12596 Training Loss: 0.10601453823513454 Test Loss: 0.3687807074652778\n",
      "Epoch: 12597 Training Loss: 0.10612721930609809 Test Loss: 0.3686277669270833\n",
      "Epoch: 12598 Training Loss: 0.10626737382676867 Test Loss: 0.36829532877604165\n",
      "Epoch: 12599 Training Loss: 0.1064087863498264 Test Loss: 0.36756450737847224\n",
      "Epoch: 12600 Training Loss: 0.1065529285007053 Test Loss: 0.3667671440972222\n",
      "Epoch: 12601 Training Loss: 0.10669982825385199 Test Loss: 0.36601692708333333\n",
      "Epoch: 12602 Training Loss: 0.10684674750434028 Test Loss: 0.36518543836805556\n",
      "Epoch: 12603 Training Loss: 0.10701760609944662 Test Loss: 0.36455997721354166\n",
      "Epoch: 12604 Training Loss: 0.10719896104600694 Test Loss: 0.36422007921006944\n",
      "Epoch: 12605 Training Loss: 0.10741421932644314 Test Loss: 0.36410826280381947\n",
      "Epoch: 12606 Training Loss: 0.10764932759602865 Test Loss: 0.3642344292534722\n",
      "Epoch: 12607 Training Loss: 0.10787973870171441 Test Loss: 0.36470049370659724\n",
      "Epoch: 12608 Training Loss: 0.10808913082546658 Test Loss: 0.36510970052083336\n",
      "Epoch: 12609 Training Loss: 0.10828827836778429 Test Loss: 0.3656814778645833\n",
      "Epoch: 12610 Training Loss: 0.1084559792412652 Test Loss: 0.3662437065972222\n",
      "Epoch: 12611 Training Loss: 0.10862544928656684 Test Loss: 0.36706746419270836\n",
      "Epoch: 12612 Training Loss: 0.10877973853217231 Test Loss: 0.3679696723090278\n",
      "Epoch: 12613 Training Loss: 0.10893636152479384 Test Loss: 0.369266357421875\n",
      "Epoch: 12614 Training Loss: 0.10907661522759332 Test Loss: 0.37005753580729167\n",
      "Epoch: 12615 Training Loss: 0.10921728176540799 Test Loss: 0.3703473849826389\n",
      "Epoch: 12616 Training Loss: 0.10932529534233941 Test Loss: 0.3701950412326389\n",
      "Epoch: 12617 Training Loss: 0.10938933478461371 Test Loss: 0.3693617892795139\n",
      "Epoch: 12618 Training Loss: 0.10944622039794921 Test Loss: 0.36822119140625\n",
      "Epoch: 12619 Training Loss: 0.10950877719455294 Test Loss: 0.3669051649305556\n",
      "Epoch: 12620 Training Loss: 0.10955307006835938 Test Loss: 0.366037841796875\n",
      "Epoch: 12621 Training Loss: 0.1096227043999566 Test Loss: 0.36569930013020835\n",
      "Epoch: 12622 Training Loss: 0.10973384179009332 Test Loss: 0.36562339952256945\n",
      "Epoch: 12623 Training Loss: 0.10991087171766493 Test Loss: 0.3658478732638889\n",
      "Epoch: 12624 Training Loss: 0.11015358140733507 Test Loss: 0.3663134494357639\n",
      "Epoch: 12625 Training Loss: 0.11042092641194662 Test Loss: 0.36665606011284724\n",
      "Epoch: 12626 Training Loss: 0.11068226114908854 Test Loss: 0.36688104926215276\n",
      "Epoch: 12627 Training Loss: 0.1109445037841797 Test Loss: 0.36649500868055557\n",
      "Epoch: 12628 Training Loss: 0.11114418114556207 Test Loss: 0.36572216796875\n",
      "Epoch: 12629 Training Loss: 0.11132324727376303 Test Loss: 0.3647734375\n",
      "Epoch: 12630 Training Loss: 0.11150507354736328 Test Loss: 0.36360069444444443\n",
      "Epoch: 12631 Training Loss: 0.11163723415798611 Test Loss: 0.36206610785590276\n",
      "Epoch: 12632 Training Loss: 0.11173258124457465 Test Loss: 0.3608178168402778\n",
      "Epoch: 12633 Training Loss: 0.11178063201904297 Test Loss: 0.36048432074652775\n",
      "Epoch: 12634 Training Loss: 0.11174880811903212 Test Loss: 0.36109109157986113\n",
      "Epoch: 12635 Training Loss: 0.11161067114935981 Test Loss: 0.36201416015625\n",
      "Epoch: 12636 Training Loss: 0.11133716922336155 Test Loss: 0.3626597764756944\n",
      "Epoch: 12637 Training Loss: 0.11101223924424913 Test Loss: 0.3620857204861111\n",
      "Epoch: 12638 Training Loss: 0.11073012797037761 Test Loss: 0.3604440646701389\n",
      "Epoch: 12639 Training Loss: 0.11052040439181858 Test Loss: 0.3580188259548611\n",
      "Epoch: 12640 Training Loss: 0.11036788007948134 Test Loss: 0.35554603407118057\n",
      "Epoch: 12641 Training Loss: 0.11026170772976346 Test Loss: 0.3534753146701389\n",
      "Epoch: 12642 Training Loss: 0.11018697187635634 Test Loss: 0.35216197374131947\n",
      "Epoch: 12643 Training Loss: 0.11012259250217014 Test Loss: 0.3515655924479167\n",
      "Epoch: 12644 Training Loss: 0.11006357828776042 Test Loss: 0.3515971408420139\n",
      "Epoch: 12645 Training Loss: 0.10994500393337674 Test Loss: 0.3535147298177083\n",
      "Epoch: 12646 Training Loss: 0.10989984978569879 Test Loss: 0.35604861111111114\n",
      "Epoch: 12647 Training Loss: 0.10990583207872179 Test Loss: 0.3585544162326389\n",
      "Epoch: 12648 Training Loss: 0.1099050793117947 Test Loss: 0.3617259114583333\n",
      "Epoch: 12649 Training Loss: 0.10988585238986545 Test Loss: 0.3653656684027778\n",
      "Epoch: 12650 Training Loss: 0.10985321807861329 Test Loss: 0.3691626519097222\n",
      "Epoch: 12651 Training Loss: 0.10989447869194878 Test Loss: 0.3720358615451389\n",
      "Epoch: 12652 Training Loss: 0.11002474890814887 Test Loss: 0.37179112413194443\n",
      "Epoch: 12653 Training Loss: 0.11016129302978515 Test Loss: 0.36533821614583334\n",
      "Epoch: 12654 Training Loss: 0.11027366044786241 Test Loss: 0.356339111328125\n",
      "Epoch: 12655 Training Loss: 0.11008522288004557 Test Loss: 0.35500059678819446\n",
      "Epoch: 12656 Training Loss: 0.10940483601888021 Test Loss: 0.3625505642361111\n",
      "Epoch: 12657 Training Loss: 0.1083905029296875 Test Loss: 0.3643918185763889\n",
      "Epoch: 12658 Training Loss: 0.10726476796468098 Test Loss: 0.35995231119791665\n",
      "Epoch: 12659 Training Loss: 0.10639951409233941 Test Loss: 0.35611528862847225\n",
      "Epoch: 12660 Training Loss: 0.10599085235595704 Test Loss: 0.35491219075520836\n",
      "Epoch: 12661 Training Loss: 0.10585855865478516 Test Loss: 0.3549835611979167\n",
      "Epoch: 12662 Training Loss: 0.10582349819607205 Test Loss: 0.35517936197916666\n",
      "Epoch: 12663 Training Loss: 0.10579852634006076 Test Loss: 0.35557438151041665\n",
      "Epoch: 12664 Training Loss: 0.10575209130181207 Test Loss: 0.35634071180555554\n",
      "Epoch: 12665 Training Loss: 0.10568747965494792 Test Loss: 0.35731477864583333\n",
      "Epoch: 12666 Training Loss: 0.10561142137315538 Test Loss: 0.35815402560763887\n",
      "Epoch: 12667 Training Loss: 0.10554090881347657 Test Loss: 0.3585339626736111\n",
      "Epoch: 12668 Training Loss: 0.10549709065755208 Test Loss: 0.3582165798611111\n",
      "Epoch: 12669 Training Loss: 0.10548422156439888 Test Loss: 0.3577859429253472\n",
      "Epoch: 12670 Training Loss: 0.10551087612575955 Test Loss: 0.3574293619791667\n",
      "Epoch: 12671 Training Loss: 0.10558756256103516 Test Loss: 0.3571094021267361\n",
      "Epoch: 12672 Training Loss: 0.10569010077582465 Test Loss: 0.357041259765625\n",
      "Epoch: 12673 Training Loss: 0.1058136935763889 Test Loss: 0.35730224609375\n",
      "Epoch: 12674 Training Loss: 0.10593920474582248 Test Loss: 0.35763338216145835\n",
      "Epoch: 12675 Training Loss: 0.1060788548787435 Test Loss: 0.35830029296875\n",
      "Epoch: 12676 Training Loss: 0.10623115793863933 Test Loss: 0.358959228515625\n",
      "Epoch: 12677 Training Loss: 0.10638639068603516 Test Loss: 0.3598669704861111\n",
      "Epoch: 12678 Training Loss: 0.1065520757039388 Test Loss: 0.36041162109375\n",
      "Epoch: 12679 Training Loss: 0.10672047000461155 Test Loss: 0.3610649685329861\n",
      "Epoch: 12680 Training Loss: 0.10689078776041666 Test Loss: 0.3614346245659722\n",
      "Epoch: 12681 Training Loss: 0.10707823859320746 Test Loss: 0.3617697482638889\n",
      "Epoch: 12682 Training Loss: 0.10727104610866971 Test Loss: 0.36157123480902775\n",
      "Epoch: 12683 Training Loss: 0.10745714230007596 Test Loss: 0.36162679036458334\n",
      "Epoch: 12684 Training Loss: 0.10762498474121093 Test Loss: 0.3616723361545139\n",
      "Epoch: 12685 Training Loss: 0.10776380496554905 Test Loss: 0.361879150390625\n",
      "Epoch: 12686 Training Loss: 0.10786771477593315 Test Loss: 0.36239990234375\n",
      "Epoch: 12687 Training Loss: 0.10793898688422308 Test Loss: 0.36317556423611114\n",
      "Epoch: 12688 Training Loss: 0.10800791168212891 Test Loss: 0.3638075629340278\n",
      "Epoch: 12689 Training Loss: 0.1080870844523112 Test Loss: 0.36426765950520834\n",
      "Epoch: 12690 Training Loss: 0.10812977854410807 Test Loss: 0.3642379557291667\n",
      "Epoch: 12691 Training Loss: 0.10815842098659939 Test Loss: 0.36398046875\n",
      "Epoch: 12692 Training Loss: 0.10816947174072265 Test Loss: 0.3630075412326389\n",
      "Epoch: 12693 Training Loss: 0.1081421873304579 Test Loss: 0.36182188585069447\n",
      "Epoch: 12694 Training Loss: 0.1080704837375217 Test Loss: 0.36118153211805554\n",
      "Epoch: 12695 Training Loss: 0.10796829562717014 Test Loss: 0.36069609917534723\n",
      "Epoch: 12696 Training Loss: 0.10784060584174263 Test Loss: 0.36053504774305556\n",
      "Epoch: 12697 Training Loss: 0.10767703247070312 Test Loss: 0.36093440755208334\n",
      "Epoch: 12698 Training Loss: 0.10750842624240452 Test Loss: 0.3611814778645833\n",
      "Epoch: 12699 Training Loss: 0.10734256235758463 Test Loss: 0.36162809244791666\n",
      "Epoch: 12700 Training Loss: 0.10718551466200087 Test Loss: 0.3620697699652778\n",
      "Epoch: 12701 Training Loss: 0.10702714029947917 Test Loss: 0.36219574652777775\n",
      "Epoch: 12702 Training Loss: 0.10688663821750218 Test Loss: 0.36195160590277775\n",
      "Epoch: 12703 Training Loss: 0.10676077100965711 Test Loss: 0.36178591579861114\n",
      "Epoch: 12704 Training Loss: 0.10663791995578342 Test Loss: 0.3614909396701389\n",
      "Epoch: 12705 Training Loss: 0.10653464084201389 Test Loss: 0.36114588758680555\n",
      "Epoch: 12706 Training Loss: 0.1064560309516059 Test Loss: 0.36078466796875\n",
      "Epoch: 12707 Training Loss: 0.10640663740370009 Test Loss: 0.3610004611545139\n",
      "Epoch: 12708 Training Loss: 0.10641078609890409 Test Loss: 0.3613248969184028\n",
      "Epoch: 12709 Training Loss: 0.10644891611735026 Test Loss: 0.36174457465277776\n",
      "Epoch: 12710 Training Loss: 0.10649830881754557 Test Loss: 0.3621148546006944\n",
      "Epoch: 12711 Training Loss: 0.10655745697021485 Test Loss: 0.3623701171875\n",
      "Epoch: 12712 Training Loss: 0.10664030371771918 Test Loss: 0.3626111653645833\n",
      "Epoch: 12713 Training Loss: 0.1067456063164605 Test Loss: 0.3625681423611111\n",
      "Epoch: 12714 Training Loss: 0.10685318077935113 Test Loss: 0.3626936306423611\n",
      "Epoch: 12715 Training Loss: 0.10698541259765625 Test Loss: 0.36308970811631947\n",
      "Epoch: 12716 Training Loss: 0.1071150368584527 Test Loss: 0.3631703830295139\n",
      "Epoch: 12717 Training Loss: 0.1072383321126302 Test Loss: 0.3635107693142361\n",
      "Epoch: 12718 Training Loss: 0.1073581568400065 Test Loss: 0.3640685221354167\n",
      "Epoch: 12719 Training Loss: 0.10745873345269097 Test Loss: 0.3642571072048611\n",
      "Epoch: 12720 Training Loss: 0.10757728915744358 Test Loss: 0.36450916883680556\n",
      "Epoch: 12721 Training Loss: 0.1077047593858507 Test Loss: 0.3644162868923611\n",
      "Epoch: 12722 Training Loss: 0.10780885654025607 Test Loss: 0.3642017144097222\n",
      "Epoch: 12723 Training Loss: 0.10791667768690322 Test Loss: 0.3635917154947917\n",
      "Epoch: 12724 Training Loss: 0.10806338246663412 Test Loss: 0.36221793619791665\n",
      "Epoch: 12725 Training Loss: 0.10822563171386719 Test Loss: 0.36052517361111114\n",
      "Epoch: 12726 Training Loss: 0.10836549970838759 Test Loss: 0.35859339735243057\n",
      "Epoch: 12727 Training Loss: 0.10845005120171441 Test Loss: 0.3569907769097222\n",
      "Epoch: 12728 Training Loss: 0.10844347551133898 Test Loss: 0.35584033203125\n",
      "Epoch: 12729 Training Loss: 0.10837191518147786 Test Loss: 0.3559460991753472\n",
      "Epoch: 12730 Training Loss: 0.1082048611111111 Test Loss: 0.3570693359375\n",
      "Epoch: 12731 Training Loss: 0.10799290720621744 Test Loss: 0.3589175889756944\n",
      "Epoch: 12732 Training Loss: 0.10779560852050782 Test Loss: 0.36088454861111113\n",
      "Epoch: 12733 Training Loss: 0.1076224382188585 Test Loss: 0.36243739149305554\n",
      "Epoch: 12734 Training Loss: 0.10748448774549696 Test Loss: 0.3631675075954861\n",
      "Epoch: 12735 Training Loss: 0.10737950219048394 Test Loss: 0.3631727973090278\n",
      "Epoch: 12736 Training Loss: 0.10727870856391059 Test Loss: 0.3622545844184028\n",
      "Epoch: 12737 Training Loss: 0.10717310926649305 Test Loss: 0.3607685818142361\n",
      "Epoch: 12738 Training Loss: 0.10706529320610894 Test Loss: 0.3592722439236111\n",
      "Epoch: 12739 Training Loss: 0.10695183732774523 Test Loss: 0.3583263346354167\n",
      "Epoch: 12740 Training Loss: 0.10682994418674045 Test Loss: 0.35816015625\n",
      "Epoch: 12741 Training Loss: 0.10669639587402344 Test Loss: 0.35844379340277777\n",
      "Epoch: 12742 Training Loss: 0.10656919182671441 Test Loss: 0.35925537109375\n",
      "Epoch: 12743 Training Loss: 0.10646143764919705 Test Loss: 0.36022786458333333\n",
      "Epoch: 12744 Training Loss: 0.1063865737915039 Test Loss: 0.36098695203993053\n",
      "Epoch: 12745 Training Loss: 0.10632930416531033 Test Loss: 0.3611483561197917\n",
      "Epoch: 12746 Training Loss: 0.10624900817871094 Test Loss: 0.36108186848958335\n",
      "Epoch: 12747 Training Loss: 0.10615038130018446 Test Loss: 0.3606935763888889\n",
      "Epoch: 12748 Training Loss: 0.1060541966756185 Test Loss: 0.3601467013888889\n",
      "Epoch: 12749 Training Loss: 0.1059698020087348 Test Loss: 0.3595774197048611\n",
      "Epoch: 12750 Training Loss: 0.1059013163248698 Test Loss: 0.35899989149305556\n",
      "Epoch: 12751 Training Loss: 0.10586312527126736 Test Loss: 0.35869436306423613\n",
      "Epoch: 12752 Training Loss: 0.10583718956841363 Test Loss: 0.3585613064236111\n",
      "Epoch: 12753 Training Loss: 0.1058284674750434 Test Loss: 0.3583936089409722\n",
      "Epoch: 12754 Training Loss: 0.10585725826687283 Test Loss: 0.3584821506076389\n",
      "Epoch: 12755 Training Loss: 0.10592659505208334 Test Loss: 0.35880680338541665\n",
      "Epoch: 12756 Training Loss: 0.10603150431315105 Test Loss: 0.35916533745659723\n",
      "Epoch: 12757 Training Loss: 0.10617976548936632 Test Loss: 0.35909228515625\n",
      "Epoch: 12758 Training Loss: 0.10636210632324218 Test Loss: 0.35907210286458335\n",
      "Epoch: 12759 Training Loss: 0.10656928083631727 Test Loss: 0.35906865776909724\n",
      "Epoch: 12760 Training Loss: 0.10680069139268664 Test Loss: 0.35895665147569444\n",
      "Epoch: 12761 Training Loss: 0.10705124749077691 Test Loss: 0.3590971137152778\n",
      "Epoch: 12762 Training Loss: 0.10732027604844835 Test Loss: 0.3595716688368056\n",
      "Epoch: 12763 Training Loss: 0.10758717854817708 Test Loss: 0.36024213324652776\n",
      "Epoch: 12764 Training Loss: 0.10782727474636501 Test Loss: 0.36125016276041666\n",
      "Epoch: 12765 Training Loss: 0.10804494137234158 Test Loss: 0.36233723958333336\n",
      "Epoch: 12766 Training Loss: 0.10822884368896485 Test Loss: 0.3632887641059028\n",
      "Epoch: 12767 Training Loss: 0.10841198221842448 Test Loss: 0.36401478407118054\n",
      "Epoch: 12768 Training Loss: 0.10859039900037977 Test Loss: 0.3648222927517361\n",
      "Epoch: 12769 Training Loss: 0.10872835964626736 Test Loss: 0.3650095486111111\n",
      "Epoch: 12770 Training Loss: 0.10880582682291666 Test Loss: 0.3650046657986111\n",
      "Epoch: 12771 Training Loss: 0.10885238901774089 Test Loss: 0.3650329047309028\n",
      "Epoch: 12772 Training Loss: 0.1088518312242296 Test Loss: 0.3648698459201389\n",
      "Epoch: 12773 Training Loss: 0.10880254787868923 Test Loss: 0.36482595486111113\n",
      "Epoch: 12774 Training Loss: 0.10869768778483073 Test Loss: 0.36478062608506945\n",
      "Epoch: 12775 Training Loss: 0.10858412339952257 Test Loss: 0.36463042534722223\n",
      "Epoch: 12776 Training Loss: 0.10847839609781901 Test Loss: 0.36404790581597224\n",
      "Epoch: 12777 Training Loss: 0.1083545905219184 Test Loss: 0.3634684787326389\n",
      "Epoch: 12778 Training Loss: 0.10820582071940105 Test Loss: 0.3631433648003472\n",
      "Epoch: 12779 Training Loss: 0.10802064683702257 Test Loss: 0.3631742350260417\n",
      "Epoch: 12780 Training Loss: 0.10784759521484374 Test Loss: 0.3631538899739583\n",
      "Epoch: 12781 Training Loss: 0.10770308007134331 Test Loss: 0.36295982530381943\n",
      "Epoch: 12782 Training Loss: 0.10757151540120442 Test Loss: 0.3622196723090278\n",
      "Epoch: 12783 Training Loss: 0.10746698337131076 Test Loss: 0.3613432888454861\n",
      "Epoch: 12784 Training Loss: 0.10739589097764757 Test Loss: 0.36035609266493057\n",
      "Epoch: 12785 Training Loss: 0.10735959794786241 Test Loss: 0.359443359375\n",
      "Epoch: 12786 Training Loss: 0.10734930080837674 Test Loss: 0.3587409939236111\n",
      "Epoch: 12787 Training Loss: 0.10732770114474827 Test Loss: 0.35868787977430555\n",
      "Epoch: 12788 Training Loss: 0.10726978047688802 Test Loss: 0.35873057725694446\n",
      "Epoch: 12789 Training Loss: 0.1071589838663737 Test Loss: 0.3592971462673611\n",
      "Epoch: 12790 Training Loss: 0.10703786892361111 Test Loss: 0.3596356879340278\n",
      "Epoch: 12791 Training Loss: 0.10692705196804471 Test Loss: 0.3597259114583333\n",
      "Epoch: 12792 Training Loss: 0.10684331681993273 Test Loss: 0.3591321614583333\n",
      "Epoch: 12793 Training Loss: 0.10677047305636936 Test Loss: 0.3582862955729167\n",
      "Epoch: 12794 Training Loss: 0.10669000752766927 Test Loss: 0.3572043999565972\n",
      "Epoch: 12795 Training Loss: 0.10659816741943359 Test Loss: 0.3559292534722222\n",
      "Epoch: 12796 Training Loss: 0.10650798628065321 Test Loss: 0.354834228515625\n",
      "Epoch: 12797 Training Loss: 0.10642137824164496 Test Loss: 0.35406342230902776\n",
      "Epoch: 12798 Training Loss: 0.10632792409261067 Test Loss: 0.3537117513020833\n",
      "Epoch: 12799 Training Loss: 0.10624966769748263 Test Loss: 0.3540231391059028\n",
      "Epoch: 12800 Training Loss: 0.10617807176378039 Test Loss: 0.3545639919704861\n",
      "Epoch: 12801 Training Loss: 0.10614456346299914 Test Loss: 0.35554123263888887\n",
      "Epoch: 12802 Training Loss: 0.10613355000813803 Test Loss: 0.35617960611979166\n",
      "Epoch: 12803 Training Loss: 0.10611869896782769 Test Loss: 0.35656070963541664\n",
      "Epoch: 12804 Training Loss: 0.1060897462632921 Test Loss: 0.35668006727430557\n",
      "Epoch: 12805 Training Loss: 0.10602406565348307 Test Loss: 0.35667252604166666\n",
      "Epoch: 12806 Training Loss: 0.10592365010579427 Test Loss: 0.3566994357638889\n",
      "Epoch: 12807 Training Loss: 0.10581463792588976 Test Loss: 0.35657503255208334\n",
      "Epoch: 12808 Training Loss: 0.10569682227240669 Test Loss: 0.3562586263020833\n",
      "Epoch: 12809 Training Loss: 0.10559660847981771 Test Loss: 0.355877197265625\n",
      "Epoch: 12810 Training Loss: 0.10549640570746528 Test Loss: 0.35543421766493055\n",
      "Epoch: 12811 Training Loss: 0.10539164733886719 Test Loss: 0.3547727322048611\n",
      "Epoch: 12812 Training Loss: 0.10528900824652777 Test Loss: 0.3540854220920139\n",
      "Epoch: 12813 Training Loss: 0.10518258243136935 Test Loss: 0.35346912977430556\n",
      "Epoch: 12814 Training Loss: 0.10507203759087456 Test Loss: 0.3529425726996528\n",
      "Epoch: 12815 Training Loss: 0.10495829349093967 Test Loss: 0.3527064615885417\n",
      "Epoch: 12816 Training Loss: 0.10487635887993707 Test Loss: 0.35281483289930554\n",
      "Epoch: 12817 Training Loss: 0.10484123229980469 Test Loss: 0.35321728515625\n",
      "Epoch: 12818 Training Loss: 0.1048496593899197 Test Loss: 0.35394748263888887\n",
      "Epoch: 12819 Training Loss: 0.10488601515028212 Test Loss: 0.3546945529513889\n",
      "Epoch: 12820 Training Loss: 0.10493279012044271 Test Loss: 0.3557571072048611\n",
      "Epoch: 12821 Training Loss: 0.10499174499511718 Test Loss: 0.3568758951822917\n",
      "Epoch: 12822 Training Loss: 0.10506462944878472 Test Loss: 0.35803534613715277\n",
      "Epoch: 12823 Training Loss: 0.10513248019748264 Test Loss: 0.3592502170138889\n",
      "Epoch: 12824 Training Loss: 0.10518579440646701 Test Loss: 0.3603914116753472\n",
      "Epoch: 12825 Training Loss: 0.10522698042127822 Test Loss: 0.3615977105034722\n",
      "Epoch: 12826 Training Loss: 0.10525249735514323 Test Loss: 0.3625329861111111\n",
      "Epoch: 12827 Training Loss: 0.10527099609375 Test Loss: 0.36295415581597223\n",
      "Epoch: 12828 Training Loss: 0.10528152635362413 Test Loss: 0.36282552083333336\n",
      "Epoch: 12829 Training Loss: 0.10530369144015841 Test Loss: 0.36235045030381946\n",
      "Epoch: 12830 Training Loss: 0.10535091400146485 Test Loss: 0.36166688368055555\n",
      "Epoch: 12831 Training Loss: 0.10539355807834201 Test Loss: 0.3608954535590278\n",
      "Epoch: 12832 Training Loss: 0.1053819842868381 Test Loss: 0.36003998480902777\n",
      "Epoch: 12833 Training Loss: 0.10536659579806858 Test Loss: 0.3591178927951389\n",
      "Epoch: 12834 Training Loss: 0.10535091230604383 Test Loss: 0.35850390625\n",
      "Epoch: 12835 Training Loss: 0.10534461127387153 Test Loss: 0.3582761501736111\n",
      "Epoch: 12836 Training Loss: 0.10533835940890841 Test Loss: 0.358281982421875\n",
      "Epoch: 12837 Training Loss: 0.10532626088460287 Test Loss: 0.3584268120659722\n",
      "Epoch: 12838 Training Loss: 0.10533255343967014 Test Loss: 0.3588251953125\n",
      "Epoch: 12839 Training Loss: 0.10537474314371745 Test Loss: 0.3592510850694444\n",
      "Epoch: 12840 Training Loss: 0.10545839182535807 Test Loss: 0.35987033420138886\n",
      "Epoch: 12841 Training Loss: 0.10559155358208551 Test Loss: 0.36036848958333334\n",
      "Epoch: 12842 Training Loss: 0.10573807101779514 Test Loss: 0.36092179361979165\n",
      "Epoch: 12843 Training Loss: 0.10590457238091362 Test Loss: 0.3613971896701389\n",
      "Epoch: 12844 Training Loss: 0.10608695136176215 Test Loss: 0.36177796766493053\n",
      "Epoch: 12845 Training Loss: 0.10626641676161025 Test Loss: 0.3621930338541667\n",
      "Epoch: 12846 Training Loss: 0.10644159020317925 Test Loss: 0.36263720703125\n",
      "Epoch: 12847 Training Loss: 0.10661235555013021 Test Loss: 0.3625962185329861\n",
      "Epoch: 12848 Training Loss: 0.10677557542588975 Test Loss: 0.3625538194444444\n",
      "Epoch: 12849 Training Loss: 0.10691871643066406 Test Loss: 0.3621906467013889\n",
      "Epoch: 12850 Training Loss: 0.10705002848307292 Test Loss: 0.3619233669704861\n",
      "Epoch: 12851 Training Loss: 0.10716332583957248 Test Loss: 0.36120301649305553\n",
      "Epoch: 12852 Training Loss: 0.10729051208496093 Test Loss: 0.36033165147569446\n",
      "Epoch: 12853 Training Loss: 0.10745834435356988 Test Loss: 0.35942236328125\n",
      "Epoch: 12854 Training Loss: 0.10765994262695312 Test Loss: 0.35900282118055554\n",
      "Epoch: 12855 Training Loss: 0.10790230390760634 Test Loss: 0.3587537163628472\n",
      "Epoch: 12856 Training Loss: 0.10819161309136285 Test Loss: 0.35896034071180555\n",
      "Epoch: 12857 Training Loss: 0.10852603742811415 Test Loss: 0.35907557508680554\n",
      "Epoch: 12858 Training Loss: 0.10889371914333767 Test Loss: 0.35918177625868053\n",
      "Epoch: 12859 Training Loss: 0.10931101396348741 Test Loss: 0.35918853081597224\n",
      "Epoch: 12860 Training Loss: 0.1097671635945638 Test Loss: 0.35898795572916664\n",
      "Epoch: 12861 Training Loss: 0.11023904927571615 Test Loss: 0.3582333984375\n",
      "Epoch: 12862 Training Loss: 0.1107082994249132 Test Loss: 0.35751185438368055\n",
      "Epoch: 12863 Training Loss: 0.11112136840820312 Test Loss: 0.3559900987413194\n",
      "Epoch: 12864 Training Loss: 0.11138818698459202 Test Loss: 0.3545857476128472\n",
      "Epoch: 12865 Training Loss: 0.11146915266248915 Test Loss: 0.3536891004774306\n",
      "Epoch: 12866 Training Loss: 0.11132704755995008 Test Loss: 0.35363755967881944\n",
      "Epoch: 12867 Training Loss: 0.11093467373318143 Test Loss: 0.3549262966579861\n",
      "Epoch: 12868 Training Loss: 0.11034975941975911 Test Loss: 0.356646240234375\n",
      "Epoch: 12869 Training Loss: 0.10967974260118273 Test Loss: 0.3580200737847222\n",
      "Epoch: 12870 Training Loss: 0.10894852277967665 Test Loss: 0.35859098307291665\n",
      "Epoch: 12871 Training Loss: 0.10818059624565972 Test Loss: 0.3587735731336806\n",
      "Epoch: 12872 Training Loss: 0.10744931115044488 Test Loss: 0.3585649685329861\n",
      "Epoch: 12873 Training Loss: 0.10679480828179254 Test Loss: 0.3582556423611111\n",
      "Epoch: 12874 Training Loss: 0.10625346967909072 Test Loss: 0.35807269965277777\n",
      "Epoch: 12875 Training Loss: 0.10582132805718315 Test Loss: 0.35768785264756947\n",
      "Epoch: 12876 Training Loss: 0.10546433342827691 Test Loss: 0.35747884114583334\n",
      "Epoch: 12877 Training Loss: 0.10515596601698134 Test Loss: 0.35714610460069446\n",
      "Epoch: 12878 Training Loss: 0.10487733205159505 Test Loss: 0.3570369194878472\n",
      "Epoch: 12879 Training Loss: 0.10462252129448785 Test Loss: 0.3567929416232639\n",
      "Epoch: 12880 Training Loss: 0.10439499833848741 Test Loss: 0.35651131184895835\n",
      "Epoch: 12881 Training Loss: 0.1042000478108724 Test Loss: 0.3562297634548611\n",
      "Epoch: 12882 Training Loss: 0.1040159929063585 Test Loss: 0.3559452582465278\n",
      "Epoch: 12883 Training Loss: 0.10385227796766493 Test Loss: 0.3554601236979167\n",
      "Epoch: 12884 Training Loss: 0.10370390828450521 Test Loss: 0.35514618598090275\n",
      "Epoch: 12885 Training Loss: 0.10356456671820746 Test Loss: 0.3550692274305556\n",
      "Epoch: 12886 Training Loss: 0.10344129265679254 Test Loss: 0.35502671983506945\n",
      "Epoch: 12887 Training Loss: 0.10333841535780165 Test Loss: 0.35502023654513887\n",
      "Epoch: 12888 Training Loss: 0.10326353115505642 Test Loss: 0.3551061197916667\n",
      "Epoch: 12889 Training Loss: 0.10320869021945529 Test Loss: 0.35514949544270835\n",
      "Epoch: 12890 Training Loss: 0.10317152998182509 Test Loss: 0.35535139973958335\n",
      "Epoch: 12891 Training Loss: 0.10315262264675565 Test Loss: 0.3553891059027778\n",
      "Epoch: 12892 Training Loss: 0.10315254635281033 Test Loss: 0.35542404513888887\n",
      "Epoch: 12893 Training Loss: 0.10317214796278212 Test Loss: 0.35527056206597224\n",
      "Epoch: 12894 Training Loss: 0.10321493615044487 Test Loss: 0.3552185872395833\n",
      "Epoch: 12895 Training Loss: 0.10326171281602647 Test Loss: 0.35520665147569447\n",
      "Epoch: 12896 Training Loss: 0.10333028496636285 Test Loss: 0.35540174696180554\n",
      "Epoch: 12897 Training Loss: 0.10341965738932292 Test Loss: 0.35600830078125\n",
      "Epoch: 12898 Training Loss: 0.10352230750189888 Test Loss: 0.3563729926215278\n",
      "Epoch: 12899 Training Loss: 0.1036367670694987 Test Loss: 0.3569132486979167\n",
      "Epoch: 12900 Training Loss: 0.10378130933973524 Test Loss: 0.3574455295138889\n",
      "Epoch: 12901 Training Loss: 0.10393373701307508 Test Loss: 0.3579558376736111\n",
      "Epoch: 12902 Training Loss: 0.10409831153021919 Test Loss: 0.3582798122829861\n",
      "Epoch: 12903 Training Loss: 0.10428066762288411 Test Loss: 0.35825873480902776\n",
      "Epoch: 12904 Training Loss: 0.10448903316921658 Test Loss: 0.3580768500434028\n",
      "Epoch: 12905 Training Loss: 0.10471077643500434 Test Loss: 0.3577124294704861\n",
      "Epoch: 12906 Training Loss: 0.10493761952718099 Test Loss: 0.3571353081597222\n",
      "Epoch: 12907 Training Loss: 0.10518454064263238 Test Loss: 0.35684629991319444\n",
      "Epoch: 12908 Training Loss: 0.10546194966634115 Test Loss: 0.3564128146701389\n",
      "Epoch: 12909 Training Loss: 0.10576795366075303 Test Loss: 0.3561755099826389\n",
      "Epoch: 12910 Training Loss: 0.10609949408637152 Test Loss: 0.3560416937934028\n",
      "Epoch: 12911 Training Loss: 0.1064458499484592 Test Loss: 0.3559034830729167\n",
      "Epoch: 12912 Training Loss: 0.10677407582600912 Test Loss: 0.35598149956597225\n",
      "Epoch: 12913 Training Loss: 0.10708911556667752 Test Loss: 0.356037841796875\n",
      "Epoch: 12914 Training Loss: 0.10739749399820964 Test Loss: 0.3563249240451389\n",
      "Epoch: 12915 Training Loss: 0.10766035885281033 Test Loss: 0.35658726671006946\n",
      "Epoch: 12916 Training Loss: 0.10788092125786676 Test Loss: 0.35692160373263887\n",
      "Epoch: 12917 Training Loss: 0.108096558464898 Test Loss: 0.3573959418402778\n",
      "Epoch: 12918 Training Loss: 0.10828788079155816 Test Loss: 0.35788959418402777\n",
      "Epoch: 12919 Training Loss: 0.10839955139160157 Test Loss: 0.3583037923177083\n",
      "Epoch: 12920 Training Loss: 0.1084776136610243 Test Loss: 0.3587526312934028\n",
      "Epoch: 12921 Training Loss: 0.10852628156873916 Test Loss: 0.3592233615451389\n",
      "Epoch: 12922 Training Loss: 0.10852567376030815 Test Loss: 0.35931190321180556\n",
      "Epoch: 12923 Training Loss: 0.1084687508477105 Test Loss: 0.35931287977430554\n",
      "Epoch: 12924 Training Loss: 0.10834213680691189 Test Loss: 0.3594751247829861\n",
      "Epoch: 12925 Training Loss: 0.10815075344509549 Test Loss: 0.35965196397569443\n",
      "Epoch: 12926 Training Loss: 0.10790664842393663 Test Loss: 0.35937483723958336\n",
      "Epoch: 12927 Training Loss: 0.10765507083468967 Test Loss: 0.35862171766493056\n",
      "Epoch: 12928 Training Loss: 0.10742927890353733 Test Loss: 0.35750623914930557\n",
      "Epoch: 12929 Training Loss: 0.10723001268174913 Test Loss: 0.35594371202256947\n",
      "Epoch: 12930 Training Loss: 0.10700886959499784 Test Loss: 0.35418828667534724\n",
      "Epoch: 12931 Training Loss: 0.10672542148166232 Test Loss: 0.3529186740451389\n",
      "Epoch: 12932 Training Loss: 0.10641731940375435 Test Loss: 0.35263498263888887\n",
      "Epoch: 12933 Training Loss: 0.10610892317030166 Test Loss: 0.35319032118055554\n",
      "Epoch: 12934 Training Loss: 0.1057862311469184 Test Loss: 0.3539226345486111\n",
      "Epoch: 12935 Training Loss: 0.10549947696261935 Test Loss: 0.3547265353732639\n",
      "Epoch: 12936 Training Loss: 0.10527794901529948 Test Loss: 0.3552028266059028\n",
      "Epoch: 12937 Training Loss: 0.1051280780368381 Test Loss: 0.35545133463541667\n",
      "Epoch: 12938 Training Loss: 0.10501130761040582 Test Loss: 0.35530013020833334\n",
      "Epoch: 12939 Training Loss: 0.10487689548068577 Test Loss: 0.35510880533854167\n",
      "Epoch: 12940 Training Loss: 0.1047510002983941 Test Loss: 0.3549103732638889\n",
      "Epoch: 12941 Training Loss: 0.1046269310845269 Test Loss: 0.3548460286458333\n",
      "Epoch: 12942 Training Loss: 0.10449410841200087 Test Loss: 0.35498187934027775\n",
      "Epoch: 12943 Training Loss: 0.10434994422064887 Test Loss: 0.3554979654947917\n",
      "Epoch: 12944 Training Loss: 0.10418645307752822 Test Loss: 0.3559505479600694\n",
      "Epoch: 12945 Training Loss: 0.10404218546549479 Test Loss: 0.35610392252604167\n",
      "Epoch: 12946 Training Loss: 0.10391207885742187 Test Loss: 0.355933837890625\n",
      "Epoch: 12947 Training Loss: 0.1037964350382487 Test Loss: 0.35561911349826386\n",
      "Epoch: 12948 Training Loss: 0.10371285671657986 Test Loss: 0.3554011773003472\n",
      "Epoch: 12949 Training Loss: 0.10366866217719184 Test Loss: 0.35522303602430555\n",
      "Epoch: 12950 Training Loss: 0.10366397857666015 Test Loss: 0.35508072916666666\n",
      "Epoch: 12951 Training Loss: 0.10371411810980903 Test Loss: 0.3551171061197917\n",
      "Epoch: 12952 Training Loss: 0.10382333119710287 Test Loss: 0.35510989040798613\n",
      "Epoch: 12953 Training Loss: 0.10396691301133898 Test Loss: 0.3551142578125\n",
      "Epoch: 12954 Training Loss: 0.10414366658528645 Test Loss: 0.35516140407986113\n",
      "Epoch: 12955 Training Loss: 0.10434713745117187 Test Loss: 0.35503917100694443\n",
      "Epoch: 12956 Training Loss: 0.10455755106608072 Test Loss: 0.3547917751736111\n",
      "Epoch: 12957 Training Loss: 0.10478682539198134 Test Loss: 0.3543965115017361\n",
      "Epoch: 12958 Training Loss: 0.10504649013943142 Test Loss: 0.3539794650607639\n",
      "Epoch: 12959 Training Loss: 0.10533621046278212 Test Loss: 0.35372791883680554\n",
      "Epoch: 12960 Training Loss: 0.10566382175021702 Test Loss: 0.3535933702256944\n",
      "Epoch: 12961 Training Loss: 0.10606260596381294 Test Loss: 0.3536913519965278\n",
      "Epoch: 12962 Training Loss: 0.10649797142876519 Test Loss: 0.35404475911458333\n",
      "Epoch: 12963 Training Loss: 0.10693378109402127 Test Loss: 0.3546647677951389\n",
      "Epoch: 12964 Training Loss: 0.10736664157443576 Test Loss: 0.3555788845486111\n",
      "Epoch: 12965 Training Loss: 0.10776400926378038 Test Loss: 0.3565343695746528\n",
      "Epoch: 12966 Training Loss: 0.10810607486300998 Test Loss: 0.3576007486979167\n",
      "Epoch: 12967 Training Loss: 0.1083953391181098 Test Loss: 0.3589367947048611\n",
      "Epoch: 12968 Training Loss: 0.10864293840196397 Test Loss: 0.36032137044270834\n",
      "Epoch: 12969 Training Loss: 0.10882843102349175 Test Loss: 0.3613783908420139\n",
      "Epoch: 12970 Training Loss: 0.10898629930284288 Test Loss: 0.3615636393229167\n",
      "Epoch: 12971 Training Loss: 0.109078367445204 Test Loss: 0.36083265516493057\n",
      "Epoch: 12972 Training Loss: 0.10906155904134114 Test Loss: 0.3592049153645833\n",
      "Epoch: 12973 Training Loss: 0.10885894097222222 Test Loss: 0.35739876302083334\n",
      "Epoch: 12974 Training Loss: 0.10845140753851996 Test Loss: 0.3557993706597222\n",
      "Epoch: 12975 Training Loss: 0.107888185289171 Test Loss: 0.3549440646701389\n",
      "Epoch: 12976 Training Loss: 0.10723903147379557 Test Loss: 0.35507801649305554\n",
      "Epoch: 12977 Training Loss: 0.10656972927517361 Test Loss: 0.35588916015625\n",
      "Epoch: 12978 Training Loss: 0.10596756320529514 Test Loss: 0.3567473958333333\n",
      "Epoch: 12979 Training Loss: 0.10541368611653645 Test Loss: 0.3569934353298611\n",
      "Epoch: 12980 Training Loss: 0.1049116219414605 Test Loss: 0.3567708333333333\n",
      "Epoch: 12981 Training Loss: 0.10447586059570313 Test Loss: 0.3559643012152778\n",
      "Epoch: 12982 Training Loss: 0.10412374962700738 Test Loss: 0.35505034722222223\n",
      "Epoch: 12983 Training Loss: 0.10385836537679037 Test Loss: 0.3541723361545139\n",
      "Epoch: 12984 Training Loss: 0.10363194359673394 Test Loss: 0.3534118923611111\n",
      "Epoch: 12985 Training Loss: 0.1034358647664388 Test Loss: 0.3531481119791667\n",
      "Epoch: 12986 Training Loss: 0.10327452002631293 Test Loss: 0.3531216362847222\n",
      "Epoch: 12987 Training Loss: 0.10315523105197483 Test Loss: 0.3533830295138889\n",
      "Epoch: 12988 Training Loss: 0.1030951182047526 Test Loss: 0.35369368489583336\n",
      "Epoch: 12989 Training Loss: 0.10307103814019097 Test Loss: 0.35404820421006944\n",
      "Epoch: 12990 Training Loss: 0.1030784674750434 Test Loss: 0.3542540690104167\n",
      "Epoch: 12991 Training Loss: 0.10311830732557509 Test Loss: 0.35451915147569446\n",
      "Epoch: 12992 Training Loss: 0.10317823537190755 Test Loss: 0.3545926649305556\n",
      "Epoch: 12993 Training Loss: 0.1032362764146593 Test Loss: 0.35483512369791664\n",
      "Epoch: 12994 Training Loss: 0.10330209604899089 Test Loss: 0.35517306857638886\n",
      "Epoch: 12995 Training Loss: 0.10337708112928602 Test Loss: 0.3556857096354167\n",
      "Epoch: 12996 Training Loss: 0.10347686174180773 Test Loss: 0.35624815538194443\n",
      "Epoch: 12997 Training Loss: 0.10359847429063584 Test Loss: 0.3569075249565972\n",
      "Epoch: 12998 Training Loss: 0.10377302805582682 Test Loss: 0.35765185546875\n",
      "Epoch: 12999 Training Loss: 0.10401832326253255 Test Loss: 0.3581831597222222\n",
      "Epoch: 13000 Training Loss: 0.10432534959581163 Test Loss: 0.3583933376736111\n",
      "Epoch: 13001 Training Loss: 0.10470379130045573 Test Loss: 0.3584706488715278\n",
      "Epoch: 13002 Training Loss: 0.10512097761366103 Test Loss: 0.3581372884114583\n",
      "Epoch: 13003 Training Loss: 0.10555651516384548 Test Loss: 0.3571321072048611\n",
      "Epoch: 13004 Training Loss: 0.1059751959906684 Test Loss: 0.3559567328559028\n",
      "Epoch: 13005 Training Loss: 0.10633214908175999 Test Loss: 0.3551039767795139\n",
      "Epoch: 13006 Training Loss: 0.10659489440917969 Test Loss: 0.3544933810763889\n",
      "Epoch: 13007 Training Loss: 0.10678392537434896 Test Loss: 0.3547958984375\n",
      "Epoch: 13008 Training Loss: 0.1069261695014106 Test Loss: 0.3551818576388889\n",
      "Epoch: 13009 Training Loss: 0.10701807064480252 Test Loss: 0.35539306640625\n",
      "Epoch: 13010 Training Loss: 0.10710722181532117 Test Loss: 0.355722412109375\n",
      "Epoch: 13011 Training Loss: 0.10717118072509765 Test Loss: 0.35606949869791665\n",
      "Epoch: 13012 Training Loss: 0.10718733469645182 Test Loss: 0.35643296983506945\n",
      "Epoch: 13013 Training Loss: 0.10715945095486111 Test Loss: 0.357263427734375\n",
      "Epoch: 13014 Training Loss: 0.10711305152045356 Test Loss: 0.35827943250868055\n",
      "Epoch: 13015 Training Loss: 0.10709366268581814 Test Loss: 0.3594345974392361\n",
      "Epoch: 13016 Training Loss: 0.1071235843234592 Test Loss: 0.3598554416232639\n",
      "Epoch: 13017 Training Loss: 0.10720658789740668 Test Loss: 0.3596594509548611\n",
      "Epoch: 13018 Training Loss: 0.1072977049085829 Test Loss: 0.35925770399305557\n",
      "Epoch: 13019 Training Loss: 0.10737720998128256 Test Loss: 0.3589565158420139\n",
      "Epoch: 13020 Training Loss: 0.10742270067003039 Test Loss: 0.3587400716145833\n",
      "Epoch: 13021 Training Loss: 0.10748162163628472 Test Loss: 0.3587021484375\n",
      "Epoch: 13022 Training Loss: 0.10755542585584853 Test Loss: 0.3584206271701389\n",
      "Epoch: 13023 Training Loss: 0.10765430535210503 Test Loss: 0.35792643229166665\n",
      "Epoch: 13024 Training Loss: 0.10778002590603299 Test Loss: 0.356951171875\n",
      "Epoch: 13025 Training Loss: 0.10790612030029297 Test Loss: 0.3557779134114583\n",
      "Epoch: 13026 Training Loss: 0.10801102108425564 Test Loss: 0.35547789171006944\n",
      "Epoch: 13027 Training Loss: 0.10808194647894966 Test Loss: 0.3563396538628472\n",
      "Epoch: 13028 Training Loss: 0.10814581976996528 Test Loss: 0.35771440972222224\n",
      "Epoch: 13029 Training Loss: 0.10820236714680989 Test Loss: 0.35917925347222224\n",
      "Epoch: 13030 Training Loss: 0.10829791768391928 Test Loss: 0.36072737630208335\n",
      "Epoch: 13031 Training Loss: 0.10837811957465278 Test Loss: 0.3617274848090278\n",
      "Epoch: 13032 Training Loss: 0.1082986594306098 Test Loss: 0.36242702907986113\n",
      "Epoch: 13033 Training Loss: 0.10809369828965928 Test Loss: 0.36287109375\n",
      "Epoch: 13034 Training Loss: 0.10782418145073784 Test Loss: 0.36364005533854166\n",
      "Epoch: 13035 Training Loss: 0.10750447506374783 Test Loss: 0.36411238606770835\n",
      "Epoch: 13036 Training Loss: 0.1071688478257921 Test Loss: 0.36406461588541666\n",
      "Epoch: 13037 Training Loss: 0.10691860792371961 Test Loss: 0.3637813042534722\n",
      "Epoch: 13038 Training Loss: 0.10679833475748698 Test Loss: 0.3635671929253472\n",
      "Epoch: 13039 Training Loss: 0.10677075619167752 Test Loss: 0.3638391384548611\n",
      "Epoch: 13040 Training Loss: 0.10680408308241103 Test Loss: 0.364538330078125\n",
      "Epoch: 13041 Training Loss: 0.10682303449842664 Test Loss: 0.3650132378472222\n",
      "Epoch: 13042 Training Loss: 0.10683921390109592 Test Loss: 0.36561024305555556\n",
      "Epoch: 13043 Training Loss: 0.1068562757703993 Test Loss: 0.3660306803385417\n",
      "Epoch: 13044 Training Loss: 0.10689247385660808 Test Loss: 0.36628358289930557\n",
      "Epoch: 13045 Training Loss: 0.10694176059299045 Test Loss: 0.366151611328125\n",
      "Epoch: 13046 Training Loss: 0.10699837578667534 Test Loss: 0.3659273003472222\n",
      "Epoch: 13047 Training Loss: 0.10706793297661675 Test Loss: 0.36522401258680554\n",
      "Epoch: 13048 Training Loss: 0.10715847269694011 Test Loss: 0.36403792317708333\n",
      "Epoch: 13049 Training Loss: 0.10726681179470486 Test Loss: 0.36282606336805556\n",
      "Epoch: 13050 Training Loss: 0.10739923095703124 Test Loss: 0.361767578125\n",
      "Epoch: 13051 Training Loss: 0.10748169199625651 Test Loss: 0.36060267469618057\n",
      "Epoch: 13052 Training Loss: 0.1074769032796224 Test Loss: 0.35958794487847223\n",
      "Epoch: 13053 Training Loss: 0.107370607164171 Test Loss: 0.35859781901041665\n",
      "Epoch: 13054 Training Loss: 0.10722342003716363 Test Loss: 0.35788123914930553\n",
      "Epoch: 13055 Training Loss: 0.10702880011664496 Test Loss: 0.3575902777777778\n",
      "Epoch: 13056 Training Loss: 0.10679178365071615 Test Loss: 0.3575896809895833\n",
      "Epoch: 13057 Training Loss: 0.10652675120035807 Test Loss: 0.35787242296006944\n",
      "Epoch: 13058 Training Loss: 0.10627638075086805 Test Loss: 0.3581876085069444\n",
      "Epoch: 13059 Training Loss: 0.10605683983696831 Test Loss: 0.35854291449652775\n",
      "Epoch: 13060 Training Loss: 0.10585211690266927 Test Loss: 0.35858257378472225\n",
      "Epoch: 13061 Training Loss: 0.10564450073242188 Test Loss: 0.3584352756076389\n",
      "Epoch: 13062 Training Loss: 0.1054070561726888 Test Loss: 0.358037109375\n",
      "Epoch: 13063 Training Loss: 0.1051857901679145 Test Loss: 0.3574966362847222\n",
      "Epoch: 13064 Training Loss: 0.10496762508816189 Test Loss: 0.35723404947916665\n",
      "Epoch: 13065 Training Loss: 0.1047503907945421 Test Loss: 0.35705110677083335\n",
      "Epoch: 13066 Training Loss: 0.10454783799913195 Test Loss: 0.3568189019097222\n",
      "Epoch: 13067 Training Loss: 0.10439551968044705 Test Loss: 0.3566879069010417\n",
      "Epoch: 13068 Training Loss: 0.10428909979926215 Test Loss: 0.3566874728732639\n",
      "Epoch: 13069 Training Loss: 0.10421497938368056 Test Loss: 0.35647330729166665\n",
      "Epoch: 13070 Training Loss: 0.1041886740790473 Test Loss: 0.3562045084635417\n",
      "Epoch: 13071 Training Loss: 0.10420539093017578 Test Loss: 0.35586336263020835\n",
      "Epoch: 13072 Training Loss: 0.10423129950629341 Test Loss: 0.35508865017361113\n",
      "Epoch: 13073 Training Loss: 0.10426331159803602 Test Loss: 0.3544418674045139\n",
      "Epoch: 13074 Training Loss: 0.104299925910102 Test Loss: 0.3536889105902778\n",
      "Epoch: 13075 Training Loss: 0.10433852132161459 Test Loss: 0.35333902994791666\n",
      "Epoch: 13076 Training Loss: 0.10438697984483507 Test Loss: 0.35340630425347225\n",
      "Epoch: 13077 Training Loss: 0.10446003723144531 Test Loss: 0.35383951822916665\n",
      "Epoch: 13078 Training Loss: 0.10454622734917535 Test Loss: 0.35476212565104165\n",
      "Epoch: 13079 Training Loss: 0.1046582022772895 Test Loss: 0.35569661458333335\n",
      "Epoch: 13080 Training Loss: 0.10481976148817275 Test Loss: 0.35678700086805554\n",
      "Epoch: 13081 Training Loss: 0.10502300940619574 Test Loss: 0.3574811197916667\n",
      "Epoch: 13082 Training Loss: 0.10526727973090277 Test Loss: 0.3580466851128472\n",
      "Epoch: 13083 Training Loss: 0.105528322007921 Test Loss: 0.3580963541666667\n",
      "Epoch: 13084 Training Loss: 0.1057937995062934 Test Loss: 0.35793117947048614\n",
      "Epoch: 13085 Training Loss: 0.10605972205268012 Test Loss: 0.35769514973958333\n",
      "Epoch: 13086 Training Loss: 0.1063109639485677 Test Loss: 0.3578576931423611\n",
      "Epoch: 13087 Training Loss: 0.10657511308458116 Test Loss: 0.3584862467447917\n",
      "Epoch: 13088 Training Loss: 0.10687523312038845 Test Loss: 0.35986924913194446\n",
      "Epoch: 13089 Training Loss: 0.10721840413411458 Test Loss: 0.3611711154513889\n",
      "Epoch: 13090 Training Loss: 0.10765117730034722 Test Loss: 0.36241216362847223\n",
      "Epoch: 13091 Training Loss: 0.10812730662027994 Test Loss: 0.3628028971354167\n",
      "Epoch: 13092 Training Loss: 0.10857538859049479 Test Loss: 0.3627243923611111\n",
      "Epoch: 13093 Training Loss: 0.10895955488416884 Test Loss: 0.3621727430555556\n",
      "Epoch: 13094 Training Loss: 0.1091946758694119 Test Loss: 0.3611535915798611\n",
      "Epoch: 13095 Training Loss: 0.10922730000813802 Test Loss: 0.36052227105034723\n",
      "Epoch: 13096 Training Loss: 0.10903057861328125 Test Loss: 0.36026253255208335\n",
      "Epoch: 13097 Training Loss: 0.10864031897650825 Test Loss: 0.3604120551215278\n",
      "Epoch: 13098 Training Loss: 0.10809484015570746 Test Loss: 0.36016628689236113\n",
      "Epoch: 13099 Training Loss: 0.1074981926812066 Test Loss: 0.3588818630642361\n",
      "Epoch: 13100 Training Loss: 0.10687812211778429 Test Loss: 0.3576337890625\n",
      "Epoch: 13101 Training Loss: 0.10626747809516059 Test Loss: 0.3565920138888889\n",
      "Epoch: 13102 Training Loss: 0.10569210984971789 Test Loss: 0.35588720703125\n",
      "Epoch: 13103 Training Loss: 0.10515198601616753 Test Loss: 0.3550693901909722\n",
      "Epoch: 13104 Training Loss: 0.10464162784152561 Test Loss: 0.35468804253472225\n",
      "Epoch: 13105 Training Loss: 0.10416898176405165 Test Loss: 0.3543591851128472\n",
      "Epoch: 13106 Training Loss: 0.10376148223876953 Test Loss: 0.3541950412326389\n",
      "Epoch: 13107 Training Loss: 0.10342881605360243 Test Loss: 0.35388804796006945\n",
      "Epoch: 13108 Training Loss: 0.10316748725043402 Test Loss: 0.3535247124565972\n",
      "Epoch: 13109 Training Loss: 0.10297220950656467 Test Loss: 0.35283943684895835\n",
      "Epoch: 13110 Training Loss: 0.10282342698838975 Test Loss: 0.35233612738715275\n",
      "Epoch: 13111 Training Loss: 0.10270155927870009 Test Loss: 0.35194970703125\n",
      "Epoch: 13112 Training Loss: 0.10259430948893229 Test Loss: 0.3515137261284722\n",
      "Epoch: 13113 Training Loss: 0.10251475185818143 Test Loss: 0.35146525065104167\n",
      "Epoch: 13114 Training Loss: 0.10244338734944662 Test Loss: 0.35164664713541666\n",
      "Epoch: 13115 Training Loss: 0.10239393191867405 Test Loss: 0.35190131293402777\n",
      "Epoch: 13116 Training Loss: 0.10237277052137587 Test Loss: 0.35228306749131943\n",
      "Epoch: 13117 Training Loss: 0.10236559295654298 Test Loss: 0.35248746744791665\n",
      "Epoch: 13118 Training Loss: 0.1023622072007921 Test Loss: 0.3526856282552083\n",
      "Epoch: 13119 Training Loss: 0.10235964626736112 Test Loss: 0.3528505316840278\n",
      "Epoch: 13120 Training Loss: 0.10237012057834201 Test Loss: 0.35313218858506945\n",
      "Epoch: 13121 Training Loss: 0.10239761013454861 Test Loss: 0.3534236111111111\n",
      "Epoch: 13122 Training Loss: 0.10242612626817492 Test Loss: 0.35369590928819444\n",
      "Epoch: 13123 Training Loss: 0.1024417241414388 Test Loss: 0.3537919650607639\n",
      "Epoch: 13124 Training Loss: 0.1024504903157552 Test Loss: 0.35401844618055556\n",
      "Epoch: 13125 Training Loss: 0.10246343655056424 Test Loss: 0.3540236545138889\n",
      "Epoch: 13126 Training Loss: 0.10249325985378689 Test Loss: 0.3541286349826389\n",
      "Epoch: 13127 Training Loss: 0.10254732259114584 Test Loss: 0.3541155598958333\n",
      "Epoch: 13128 Training Loss: 0.10262306552463107 Test Loss: 0.3542485894097222\n",
      "Epoch: 13129 Training Loss: 0.10272216118706597 Test Loss: 0.35432017686631945\n",
      "Epoch: 13130 Training Loss: 0.10281069183349609 Test Loss: 0.35443359375\n",
      "Epoch: 13131 Training Loss: 0.1028838136461046 Test Loss: 0.3545348849826389\n",
      "Epoch: 13132 Training Loss: 0.10294709184434679 Test Loss: 0.3546450737847222\n",
      "Epoch: 13133 Training Loss: 0.1029972635904948 Test Loss: 0.3549880099826389\n",
      "Epoch: 13134 Training Loss: 0.10303753323025173 Test Loss: 0.3552068142361111\n",
      "Epoch: 13135 Training Loss: 0.10306655968560113 Test Loss: 0.35560020616319443\n",
      "Epoch: 13136 Training Loss: 0.10310222032335069 Test Loss: 0.35605853949652777\n",
      "Epoch: 13137 Training Loss: 0.10315141889784071 Test Loss: 0.35612136501736114\n",
      "Epoch: 13138 Training Loss: 0.10320301055908203 Test Loss: 0.35581553819444445\n",
      "Epoch: 13139 Training Loss: 0.10327604505750868 Test Loss: 0.3552779676649306\n",
      "Epoch: 13140 Training Loss: 0.10336695183648004 Test Loss: 0.35430691189236113\n",
      "Epoch: 13141 Training Loss: 0.10349706776936848 Test Loss: 0.3529378797743056\n",
      "Epoch: 13142 Training Loss: 0.1036499743991428 Test Loss: 0.35169981553819446\n",
      "Epoch: 13143 Training Loss: 0.10381279585096571 Test Loss: 0.3507109375\n",
      "Epoch: 13144 Training Loss: 0.10397387101915148 Test Loss: 0.35028572591145835\n",
      "Epoch: 13145 Training Loss: 0.10415714348687066 Test Loss: 0.3506208767361111\n",
      "Epoch: 13146 Training Loss: 0.10433072492811415 Test Loss: 0.35160026041666664\n",
      "Epoch: 13147 Training Loss: 0.10451229604085287 Test Loss: 0.3528417426215278\n",
      "Epoch: 13148 Training Loss: 0.10471031782362196 Test Loss: 0.3541247829861111\n",
      "Epoch: 13149 Training Loss: 0.10492063649495442 Test Loss: 0.3552658962673611\n",
      "Epoch: 13150 Training Loss: 0.10512591552734375 Test Loss: 0.35607044813368055\n",
      "Epoch: 13151 Training Loss: 0.10526711866590711 Test Loss: 0.3560340711805556\n",
      "Epoch: 13152 Training Loss: 0.1053408432006836 Test Loss: 0.35555948893229167\n",
      "Epoch: 13153 Training Loss: 0.10535190073649088 Test Loss: 0.3540452473958333\n",
      "Epoch: 13154 Training Loss: 0.10530617438422309 Test Loss: 0.35318958875868056\n",
      "Epoch: 13155 Training Loss: 0.10519855329725478 Test Loss: 0.35327864583333335\n",
      "Epoch: 13156 Training Loss: 0.1050888180202908 Test Loss: 0.35416617838541664\n",
      "Epoch: 13157 Training Loss: 0.10499474843343098 Test Loss: 0.35521302625868056\n",
      "Epoch: 13158 Training Loss: 0.10491689809163411 Test Loss: 0.35592488606770833\n",
      "Epoch: 13159 Training Loss: 0.10484207068549262 Test Loss: 0.35627189127604164\n",
      "Epoch: 13160 Training Loss: 0.10473382907443576 Test Loss: 0.3562290581597222\n",
      "Epoch: 13161 Training Loss: 0.10460826619466146 Test Loss: 0.356248046875\n",
      "Epoch: 13162 Training Loss: 0.10447670067681207 Test Loss: 0.35657074652777776\n",
      "Epoch: 13163 Training Loss: 0.10435919867621528 Test Loss: 0.35677205403645834\n",
      "Epoch: 13164 Training Loss: 0.10428830210367838 Test Loss: 0.3568156195746528\n",
      "Epoch: 13165 Training Loss: 0.10425704956054688 Test Loss: 0.3568811848958333\n",
      "Epoch: 13166 Training Loss: 0.10429265848795573 Test Loss: 0.3571670193142361\n",
      "Epoch: 13167 Training Loss: 0.1043825946384006 Test Loss: 0.3574235297309028\n",
      "Epoch: 13168 Training Loss: 0.10450179968939888 Test Loss: 0.3581751844618056\n",
      "Epoch: 13169 Training Loss: 0.10464397345648871 Test Loss: 0.35896367730034723\n",
      "Epoch: 13170 Training Loss: 0.10475738355848524 Test Loss: 0.36022477213541665\n",
      "Epoch: 13171 Training Loss: 0.1048304706149631 Test Loss: 0.36122197808159723\n",
      "Epoch: 13172 Training Loss: 0.10485591803656684 Test Loss: 0.36219542100694446\n",
      "Epoch: 13173 Training Loss: 0.10482645585801867 Test Loss: 0.36276182725694445\n",
      "Epoch: 13174 Training Loss: 0.10475114356146918 Test Loss: 0.3626432834201389\n",
      "Epoch: 13175 Training Loss: 0.10463283030192057 Test Loss: 0.362090576171875\n",
      "Epoch: 13176 Training Loss: 0.10450844913058811 Test Loss: 0.36118543836805556\n",
      "Epoch: 13177 Training Loss: 0.10439382934570313 Test Loss: 0.3601399197048611\n",
      "Epoch: 13178 Training Loss: 0.10427943250868056 Test Loss: 0.35901888020833334\n",
      "Epoch: 13179 Training Loss: 0.10418060217963325 Test Loss: 0.35808430989583334\n",
      "Epoch: 13180 Training Loss: 0.10411232588026259 Test Loss: 0.35734052191840276\n",
      "Epoch: 13181 Training Loss: 0.10407509273952908 Test Loss: 0.35673171657986114\n",
      "Epoch: 13182 Training Loss: 0.10408013576931424 Test Loss: 0.35619352213541666\n",
      "Epoch: 13183 Training Loss: 0.10410910288492839 Test Loss: 0.35548752170138886\n",
      "Epoch: 13184 Training Loss: 0.10416502210828993 Test Loss: 0.35482850477430555\n",
      "Epoch: 13185 Training Loss: 0.10425729031032986 Test Loss: 0.3545857204861111\n",
      "Epoch: 13186 Training Loss: 0.10440956200493706 Test Loss: 0.35458192274305556\n",
      "Epoch: 13187 Training Loss: 0.10463037787543403 Test Loss: 0.35495062934027777\n",
      "Epoch: 13188 Training Loss: 0.10485943857828776 Test Loss: 0.35560582139756947\n",
      "Epoch: 13189 Training Loss: 0.10513474019368489 Test Loss: 0.35658186848958334\n",
      "Epoch: 13190 Training Loss: 0.10546707577175564 Test Loss: 0.3581382378472222\n",
      "Epoch: 13191 Training Loss: 0.10576511552598741 Test Loss: 0.36030525716145834\n",
      "Epoch: 13192 Training Loss: 0.1060668224758572 Test Loss: 0.3631399468315972\n",
      "Epoch: 13193 Training Loss: 0.10639963192409939 Test Loss: 0.3660945095486111\n",
      "Epoch: 13194 Training Loss: 0.10671834733751084 Test Loss: 0.3685731608072917\n",
      "Epoch: 13195 Training Loss: 0.10702170562744141 Test Loss: 0.36977734375\n",
      "Epoch: 13196 Training Loss: 0.10728877173529731 Test Loss: 0.36967296006944445\n",
      "Epoch: 13197 Training Loss: 0.10753809102376302 Test Loss: 0.36791788736979164\n",
      "Epoch: 13198 Training Loss: 0.1077881368001302 Test Loss: 0.3648845757378472\n",
      "Epoch: 13199 Training Loss: 0.10797791883680556 Test Loss: 0.36200059678819446\n",
      "Epoch: 13200 Training Loss: 0.10805211808946398 Test Loss: 0.3596356879340278\n",
      "Epoch: 13201 Training Loss: 0.10801365068223741 Test Loss: 0.35873030598958333\n",
      "Epoch: 13202 Training Loss: 0.1079387215508355 Test Loss: 0.35856331380208334\n",
      "Epoch: 13203 Training Loss: 0.10786212582058377 Test Loss: 0.35935343424479166\n",
      "Epoch: 13204 Training Loss: 0.1078233396742079 Test Loss: 0.35958639865451386\n",
      "Epoch: 13205 Training Loss: 0.10782689836290148 Test Loss: 0.35977587890625\n",
      "Epoch: 13206 Training Loss: 0.1078029073079427 Test Loss: 0.35976806640625\n",
      "Epoch: 13207 Training Loss: 0.10773932054307726 Test Loss: 0.35988709852430556\n",
      "Epoch: 13208 Training Loss: 0.1076354480319553 Test Loss: 0.3601162109375\n",
      "Epoch: 13209 Training Loss: 0.10748228878445096 Test Loss: 0.36067366536458334\n",
      "Epoch: 13210 Training Loss: 0.10727868906656901 Test Loss: 0.36140435112847225\n",
      "Epoch: 13211 Training Loss: 0.1070097418891059 Test Loss: 0.36215760633680555\n",
      "Epoch: 13212 Training Loss: 0.1066880382961697 Test Loss: 0.3626452365451389\n",
      "Epoch: 13213 Training Loss: 0.10630364651150173 Test Loss: 0.3625823025173611\n",
      "Epoch: 13214 Training Loss: 0.10588293711344401 Test Loss: 0.3618827853732639\n",
      "Epoch: 13215 Training Loss: 0.10546314578586155 Test Loss: 0.36092936197916664\n",
      "Epoch: 13216 Training Loss: 0.10504838562011719 Test Loss: 0.35965673828125\n",
      "Epoch: 13217 Training Loss: 0.10463269636366103 Test Loss: 0.35850526258680554\n",
      "Epoch: 13218 Training Loss: 0.10421843889024522 Test Loss: 0.35738343641493053\n",
      "Epoch: 13219 Training Loss: 0.10381583658854167 Test Loss: 0.3565119900173611\n",
      "Epoch: 13220 Training Loss: 0.10344594404432508 Test Loss: 0.35599001736111113\n",
      "Epoch: 13221 Training Loss: 0.1031288833618164 Test Loss: 0.3556579318576389\n",
      "Epoch: 13222 Training Loss: 0.10287277136908637 Test Loss: 0.35591148546006945\n",
      "Epoch: 13223 Training Loss: 0.10265789794921874 Test Loss: 0.35619352213541666\n",
      "Epoch: 13224 Training Loss: 0.10248790486653646 Test Loss: 0.35643839518229165\n",
      "Epoch: 13225 Training Loss: 0.1023743421766493 Test Loss: 0.35657758246527776\n",
      "Epoch: 13226 Training Loss: 0.1023154805501302 Test Loss: 0.35686979166666666\n",
      "Epoch: 13227 Training Loss: 0.10230999247233073 Test Loss: 0.35665836588541666\n",
      "Epoch: 13228 Training Loss: 0.10234472147623698 Test Loss: 0.35632750108506944\n",
      "Epoch: 13229 Training Loss: 0.10237560187445746 Test Loss: 0.35590280490451387\n",
      "Epoch: 13230 Training Loss: 0.10236446465386284 Test Loss: 0.35559079318576386\n",
      "Epoch: 13231 Training Loss: 0.10233504994710287 Test Loss: 0.3552333984375\n",
      "Epoch: 13232 Training Loss: 0.1023044670952691 Test Loss: 0.35499755859375\n",
      "Epoch: 13233 Training Loss: 0.10228538682725695 Test Loss: 0.35524544270833336\n",
      "Epoch: 13234 Training Loss: 0.10226815202501086 Test Loss: 0.35549639214409723\n",
      "Epoch: 13235 Training Loss: 0.1022566163804796 Test Loss: 0.3558715549045139\n",
      "Epoch: 13236 Training Loss: 0.10226841227213541 Test Loss: 0.35603944227430556\n",
      "Epoch: 13237 Training Loss: 0.10230946943495009 Test Loss: 0.3561475694444444\n",
      "Epoch: 13238 Training Loss: 0.10239602915445964 Test Loss: 0.3559467502170139\n",
      "Epoch: 13239 Training Loss: 0.10249961259629992 Test Loss: 0.35548578559027777\n",
      "Epoch: 13240 Training Loss: 0.10261131032307942 Test Loss: 0.35505455186631946\n",
      "Epoch: 13241 Training Loss: 0.10272832573784722 Test Loss: 0.3546918674045139\n",
      "Epoch: 13242 Training Loss: 0.10284025828043619 Test Loss: 0.35421625434027776\n",
      "Epoch: 13243 Training Loss: 0.10295383622911242 Test Loss: 0.354021484375\n",
      "Epoch: 13244 Training Loss: 0.10306359354654948 Test Loss: 0.35400697157118055\n",
      "Epoch: 13245 Training Loss: 0.1031760737101237 Test Loss: 0.35413319227430556\n",
      "Epoch: 13246 Training Loss: 0.10330804951985677 Test Loss: 0.354637939453125\n",
      "Epoch: 13247 Training Loss: 0.10346964518229167 Test Loss: 0.35495619032118053\n",
      "Epoch: 13248 Training Loss: 0.10368102603488498 Test Loss: 0.35508157009548613\n",
      "Epoch: 13249 Training Loss: 0.10391951497395834 Test Loss: 0.35510959201388886\n",
      "Epoch: 13250 Training Loss: 0.10415857272677952 Test Loss: 0.35481380208333335\n",
      "Epoch: 13251 Training Loss: 0.10439675649007162 Test Loss: 0.35415641276041665\n",
      "Epoch: 13252 Training Loss: 0.10464568413628472 Test Loss: 0.3541674262152778\n",
      "Epoch: 13253 Training Loss: 0.1048872578938802 Test Loss: 0.3541636284722222\n",
      "Epoch: 13254 Training Loss: 0.10510013580322265 Test Loss: 0.35483718532986114\n",
      "Epoch: 13255 Training Loss: 0.10530990176730685 Test Loss: 0.35567740885416665\n",
      "Epoch: 13256 Training Loss: 0.10552675035264757 Test Loss: 0.3568381076388889\n",
      "Epoch: 13257 Training Loss: 0.10573204210069445 Test Loss: 0.35782750108506944\n",
      "Epoch: 13258 Training Loss: 0.10590543874104817 Test Loss: 0.3584504937065972\n",
      "Epoch: 13259 Training Loss: 0.10601674482557509 Test Loss: 0.35909483506944445\n",
      "Epoch: 13260 Training Loss: 0.10607571834988064 Test Loss: 0.3592302788628472\n",
      "Epoch: 13261 Training Loss: 0.10606802113850912 Test Loss: 0.3591400282118056\n",
      "Epoch: 13262 Training Loss: 0.10598490142822266 Test Loss: 0.35932107204861113\n",
      "Epoch: 13263 Training Loss: 0.10581831359863281 Test Loss: 0.3596086154513889\n",
      "Epoch: 13264 Training Loss: 0.10560835689968533 Test Loss: 0.36028276909722223\n",
      "Epoch: 13265 Training Loss: 0.10536931525336371 Test Loss: 0.3605812717013889\n",
      "Epoch: 13266 Training Loss: 0.10513596089680989 Test Loss: 0.3603879123263889\n",
      "Epoch: 13267 Training Loss: 0.1048878682454427 Test Loss: 0.35928122287326386\n",
      "Epoch: 13268 Training Loss: 0.10460381147596572 Test Loss: 0.35805186631944447\n",
      "Epoch: 13269 Training Loss: 0.10431438785129123 Test Loss: 0.3565236002604167\n",
      "Epoch: 13270 Training Loss: 0.10402935875786676 Test Loss: 0.355227294921875\n",
      "Epoch: 13271 Training Loss: 0.10378191630045573 Test Loss: 0.3544151475694444\n",
      "Epoch: 13272 Training Loss: 0.10359771813286675 Test Loss: 0.35408336046006944\n",
      "Epoch: 13273 Training Loss: 0.10347976854112413 Test Loss: 0.3540115017361111\n",
      "Epoch: 13274 Training Loss: 0.10341501617431641 Test Loss: 0.35401774088541665\n",
      "Epoch: 13275 Training Loss: 0.10339183044433593 Test Loss: 0.3541554904513889\n",
      "Epoch: 13276 Training Loss: 0.10341056399875218 Test Loss: 0.35434456380208335\n",
      "Epoch: 13277 Training Loss: 0.10343254937065972 Test Loss: 0.35441935221354165\n",
      "Epoch: 13278 Training Loss: 0.10344811503092448 Test Loss: 0.35432731119791666\n",
      "Epoch: 13279 Training Loss: 0.10345062764485677 Test Loss: 0.35413053385416665\n",
      "Epoch: 13280 Training Loss: 0.10346669599745009 Test Loss: 0.35410091145833333\n",
      "Epoch: 13281 Training Loss: 0.10352644517686632 Test Loss: 0.3540456814236111\n",
      "Epoch: 13282 Training Loss: 0.10362074279785156 Test Loss: 0.35415546332465275\n",
      "Epoch: 13283 Training Loss: 0.10376605733235678 Test Loss: 0.35423890516493056\n",
      "Epoch: 13284 Training Loss: 0.10393250020345052 Test Loss: 0.3545444878472222\n",
      "Epoch: 13285 Training Loss: 0.10410144721137153 Test Loss: 0.35459342447916664\n",
      "Epoch: 13286 Training Loss: 0.10421512858072916 Test Loss: 0.354588623046875\n",
      "Epoch: 13287 Training Loss: 0.10427249908447266 Test Loss: 0.354703857421875\n",
      "Epoch: 13288 Training Loss: 0.1042873543633355 Test Loss: 0.35469921875\n",
      "Epoch: 13289 Training Loss: 0.10425521511501736 Test Loss: 0.3547605251736111\n",
      "Epoch: 13290 Training Loss: 0.10419643317328559 Test Loss: 0.3545846896701389\n",
      "Epoch: 13291 Training Loss: 0.10411389838324653 Test Loss: 0.3547089572482639\n",
      "Epoch: 13292 Training Loss: 0.10405686526828342 Test Loss: 0.3548681098090278\n",
      "Epoch: 13293 Training Loss: 0.10401551394992405 Test Loss: 0.35529296875\n",
      "Epoch: 13294 Training Loss: 0.1040566889444987 Test Loss: 0.355540283203125\n",
      "Epoch: 13295 Training Loss: 0.10413487328423394 Test Loss: 0.35568912760416665\n",
      "Epoch: 13296 Training Loss: 0.10417032792833117 Test Loss: 0.355822998046875\n",
      "Epoch: 13297 Training Loss: 0.10412832895914713 Test Loss: 0.35639019097222224\n",
      "Epoch: 13298 Training Loss: 0.1040586157904731 Test Loss: 0.3572973090277778\n",
      "Epoch: 13299 Training Loss: 0.10395674472384983 Test Loss: 0.3581243760850694\n",
      "Epoch: 13300 Training Loss: 0.10380379486083985 Test Loss: 0.35855170355902777\n",
      "Epoch: 13301 Training Loss: 0.10360279591878255 Test Loss: 0.358320556640625\n",
      "Epoch: 13302 Training Loss: 0.10338246663411459 Test Loss: 0.3577726779513889\n",
      "Epoch: 13303 Training Loss: 0.10317344580756294 Test Loss: 0.35680441623263887\n",
      "Epoch: 13304 Training Loss: 0.10298279317220052 Test Loss: 0.35588311089409724\n",
      "Epoch: 13305 Training Loss: 0.10281039513481988 Test Loss: 0.3553015407986111\n",
      "Epoch: 13306 Training Loss: 0.10266948106553819 Test Loss: 0.3554008517795139\n",
      "Epoch: 13307 Training Loss: 0.10256271277533636 Test Loss: 0.3560785590277778\n",
      "Epoch: 13308 Training Loss: 0.10250792185465495 Test Loss: 0.3566460774739583\n",
      "Epoch: 13309 Training Loss: 0.10250851440429687 Test Loss: 0.35692464192708334\n",
      "Epoch: 13310 Training Loss: 0.10256674448649089 Test Loss: 0.3569965277777778\n",
      "Epoch: 13311 Training Loss: 0.10266801367865669 Test Loss: 0.3567486436631944\n",
      "Epoch: 13312 Training Loss: 0.10278973727756076 Test Loss: 0.356701416015625\n",
      "Epoch: 13313 Training Loss: 0.10289954460991753 Test Loss: 0.3566328125\n",
      "Epoch: 13314 Training Loss: 0.10300178358289931 Test Loss: 0.35707703993055556\n",
      "Epoch: 13315 Training Loss: 0.10309683481852214 Test Loss: 0.35752875434027775\n",
      "Epoch: 13316 Training Loss: 0.10318328264024522 Test Loss: 0.3581344401041667\n",
      "Epoch: 13317 Training Loss: 0.10327124447292751 Test Loss: 0.35872911241319444\n",
      "Epoch: 13318 Training Loss: 0.10336414082845052 Test Loss: 0.35887657335069445\n",
      "Epoch: 13319 Training Loss: 0.10343673536512586 Test Loss: 0.3590469292534722\n",
      "Epoch: 13320 Training Loss: 0.10352025773790148 Test Loss: 0.35897395833333334\n",
      "Epoch: 13321 Training Loss: 0.10362699551052518 Test Loss: 0.3589379340277778\n",
      "Epoch: 13322 Training Loss: 0.10376831309000652 Test Loss: 0.3586349555121528\n",
      "Epoch: 13323 Training Loss: 0.10392165798611111 Test Loss: 0.35823209635416664\n",
      "Epoch: 13324 Training Loss: 0.10407050323486328 Test Loss: 0.35790562608506943\n",
      "Epoch: 13325 Training Loss: 0.10422711435953776 Test Loss: 0.3578229437934028\n",
      "Epoch: 13326 Training Loss: 0.10441632673475477 Test Loss: 0.35795665147569444\n",
      "Epoch: 13327 Training Loss: 0.10465849134657118 Test Loss: 0.357953125\n",
      "Epoch: 13328 Training Loss: 0.1049170905219184 Test Loss: 0.35778377278645834\n",
      "Epoch: 13329 Training Loss: 0.1051774402194553 Test Loss: 0.3581148546006944\n",
      "Epoch: 13330 Training Loss: 0.10547653537326389 Test Loss: 0.3587196180555556\n",
      "Epoch: 13331 Training Loss: 0.10583606889512803 Test Loss: 0.35965049913194447\n",
      "Epoch: 13332 Training Loss: 0.10625162251790364 Test Loss: 0.3613341471354167\n",
      "Epoch: 13333 Training Loss: 0.10672122022840712 Test Loss: 0.36321297200520836\n",
      "Epoch: 13334 Training Loss: 0.10722339884440105 Test Loss: 0.3658048773871528\n",
      "Epoch: 13335 Training Loss: 0.10778647020128038 Test Loss: 0.36759073893229166\n",
      "Epoch: 13336 Training Loss: 0.10840682220458984 Test Loss: 0.3689144965277778\n",
      "Epoch: 13337 Training Loss: 0.10910809071858724 Test Loss: 0.36909966362847224\n",
      "Epoch: 13338 Training Loss: 0.10988410356309679 Test Loss: 0.3674159342447917\n",
      "Epoch: 13339 Training Loss: 0.1106776106092665 Test Loss: 0.36415825737847224\n",
      "Epoch: 13340 Training Loss: 0.11137275865342881 Test Loss: 0.36011138237847223\n",
      "Epoch: 13341 Training Loss: 0.1117112053765191 Test Loss: 0.35645865885416667\n",
      "Epoch: 13342 Training Loss: 0.11160044182671441 Test Loss: 0.3550479329427083\n",
      "Epoch: 13343 Training Loss: 0.11109722052680121 Test Loss: 0.3553114691840278\n",
      "Epoch: 13344 Training Loss: 0.11029981231689454 Test Loss: 0.3562912326388889\n",
      "Epoch: 13345 Training Loss: 0.10933136579725478 Test Loss: 0.355603515625\n",
      "Epoch: 13346 Training Loss: 0.10834316677517361 Test Loss: 0.353467041015625\n",
      "Epoch: 13347 Training Loss: 0.1074088609483507 Test Loss: 0.3514817165798611\n",
      "Epoch: 13348 Training Loss: 0.1065490214029948 Test Loss: 0.35046538628472224\n",
      "Epoch: 13349 Training Loss: 0.1058265855577257 Test Loss: 0.35016566297743057\n",
      "Epoch: 13350 Training Loss: 0.10528987630208334 Test Loss: 0.35051893446180554\n",
      "Epoch: 13351 Training Loss: 0.10492720794677735 Test Loss: 0.3509033745659722\n",
      "Epoch: 13352 Training Loss: 0.10471804724799262 Test Loss: 0.35133338758680555\n",
      "Epoch: 13353 Training Loss: 0.1046034656100803 Test Loss: 0.35186376953125\n",
      "Epoch: 13354 Training Loss: 0.10454343583848741 Test Loss: 0.3525354275173611\n",
      "Epoch: 13355 Training Loss: 0.10449759589301215 Test Loss: 0.3533567165798611\n",
      "Epoch: 13356 Training Loss: 0.10445423719618055 Test Loss: 0.35405409071180555\n",
      "Epoch: 13357 Training Loss: 0.10444449361165364 Test Loss: 0.35468364800347224\n",
      "Epoch: 13358 Training Loss: 0.10448796929253472 Test Loss: 0.3557302517361111\n",
      "Epoch: 13359 Training Loss: 0.10457381015353732 Test Loss: 0.35689466688368054\n",
      "Epoch: 13360 Training Loss: 0.10471583133273654 Test Loss: 0.3580569118923611\n",
      "Epoch: 13361 Training Loss: 0.10489523061116536 Test Loss: 0.3591061740451389\n",
      "Epoch: 13362 Training Loss: 0.10510750410291883 Test Loss: 0.3597275933159722\n",
      "Epoch: 13363 Training Loss: 0.10532234615749783 Test Loss: 0.36037022569444443\n",
      "Epoch: 13364 Training Loss: 0.10549962531195746 Test Loss: 0.36058924696180555\n",
      "Epoch: 13365 Training Loss: 0.1056381352742513 Test Loss: 0.36074273003472224\n",
      "Epoch: 13366 Training Loss: 0.10577061038547092 Test Loss: 0.3609181857638889\n",
      "Epoch: 13367 Training Loss: 0.10593258243136935 Test Loss: 0.3612200249565972\n",
      "Epoch: 13368 Training Loss: 0.10611539459228515 Test Loss: 0.36150740559895833\n",
      "Epoch: 13369 Training Loss: 0.10630620235866971 Test Loss: 0.3620304633246528\n",
      "Epoch: 13370 Training Loss: 0.10650303480360243 Test Loss: 0.3623420952690972\n",
      "Epoch: 13371 Training Loss: 0.1066908221774631 Test Loss: 0.3622869466145833\n",
      "Epoch: 13372 Training Loss: 0.10684418572319879 Test Loss: 0.3619563259548611\n",
      "Epoch: 13373 Training Loss: 0.10696539560953776 Test Loss: 0.3615116644965278\n",
      "Epoch: 13374 Training Loss: 0.10705230458577474 Test Loss: 0.36110316297743056\n",
      "Epoch: 13375 Training Loss: 0.10706777530246311 Test Loss: 0.36101947699652776\n",
      "Epoch: 13376 Training Loss: 0.10699490356445313 Test Loss: 0.3610087348090278\n",
      "Epoch: 13377 Training Loss: 0.10682620493570963 Test Loss: 0.3608996853298611\n",
      "Epoch: 13378 Training Loss: 0.1066075931125217 Test Loss: 0.36105712890625\n",
      "Epoch: 13379 Training Loss: 0.10637090640597874 Test Loss: 0.3608918728298611\n",
      "Epoch: 13380 Training Loss: 0.1061475092569987 Test Loss: 0.3606609157986111\n",
      "Epoch: 13381 Training Loss: 0.1059343982272678 Test Loss: 0.36057123480902775\n",
      "Epoch: 13382 Training Loss: 0.10568817816840277 Test Loss: 0.36046050347222225\n",
      "Epoch: 13383 Training Loss: 0.10542500474717882 Test Loss: 0.3606776258680556\n",
      "Epoch: 13384 Training Loss: 0.10515965864393446 Test Loss: 0.3608118218315972\n",
      "Epoch: 13385 Training Loss: 0.10488867102728949 Test Loss: 0.36116764322916667\n",
      "Epoch: 13386 Training Loss: 0.1046014149983724 Test Loss: 0.36142119683159724\n",
      "Epoch: 13387 Training Loss: 0.10430355919731987 Test Loss: 0.3614711371527778\n",
      "Epoch: 13388 Training Loss: 0.10402725728352864 Test Loss: 0.36110101996527777\n",
      "Epoch: 13389 Training Loss: 0.10381073421902126 Test Loss: 0.36041243489583336\n",
      "Epoch: 13390 Training Loss: 0.10363426378038194 Test Loss: 0.35944807942708334\n",
      "Epoch: 13391 Training Loss: 0.10346874152289497 Test Loss: 0.3585881890190972\n",
      "Epoch: 13392 Training Loss: 0.1033187255859375 Test Loss: 0.3576950412326389\n",
      "Epoch: 13393 Training Loss: 0.10318511199951172 Test Loss: 0.35702387152777776\n",
      "Epoch: 13394 Training Loss: 0.10302477688259548 Test Loss: 0.356587890625\n",
      "Epoch: 13395 Training Loss: 0.10283491516113281 Test Loss: 0.35626616753472223\n",
      "Epoch: 13396 Training Loss: 0.10264847479926215 Test Loss: 0.35610460069444444\n",
      "Epoch: 13397 Training Loss: 0.10248196072048611 Test Loss: 0.35616259765625\n",
      "Epoch: 13398 Training Loss: 0.10233823649088541 Test Loss: 0.35614558919270833\n",
      "Epoch: 13399 Training Loss: 0.10220984564887153 Test Loss: 0.3564760199652778\n",
      "Epoch: 13400 Training Loss: 0.10208732350667318 Test Loss: 0.3568055013020833\n",
      "Epoch: 13401 Training Loss: 0.10197550116644966 Test Loss: 0.357063232421875\n",
      "Epoch: 13402 Training Loss: 0.10189549933539496 Test Loss: 0.3570056423611111\n",
      "Epoch: 13403 Training Loss: 0.10184559037950304 Test Loss: 0.35682386610243055\n",
      "Epoch: 13404 Training Loss: 0.10180571916368272 Test Loss: 0.3565865342881944\n",
      "Epoch: 13405 Training Loss: 0.1017892091539171 Test Loss: 0.35640863715277776\n",
      "Epoch: 13406 Training Loss: 0.10179509904649522 Test Loss: 0.3561739908854167\n",
      "Epoch: 13407 Training Loss: 0.10179415384928385 Test Loss: 0.35587722439236114\n",
      "Epoch: 13408 Training Loss: 0.10176886410183376 Test Loss: 0.35548182508680554\n",
      "Epoch: 13409 Training Loss: 0.10172211625840928 Test Loss: 0.3550859646267361\n",
      "Epoch: 13410 Training Loss: 0.1016993162367079 Test Loss: 0.35449354383680554\n",
      "Epoch: 13411 Training Loss: 0.10172549692789713 Test Loss: 0.3542763671875\n",
      "Epoch: 13412 Training Loss: 0.10178027937147352 Test Loss: 0.3542206217447917\n",
      "Epoch: 13413 Training Loss: 0.1018494389851888 Test Loss: 0.3544509548611111\n",
      "Epoch: 13414 Training Loss: 0.1019428007337782 Test Loss: 0.35472450086805557\n",
      "Epoch: 13415 Training Loss: 0.10206098090277778 Test Loss: 0.35524408637152777\n",
      "Epoch: 13416 Training Loss: 0.10220118289523654 Test Loss: 0.3558000217013889\n",
      "Epoch: 13417 Training Loss: 0.1023508792453342 Test Loss: 0.35641026475694443\n",
      "Epoch: 13418 Training Loss: 0.10250215911865235 Test Loss: 0.35712017144097224\n",
      "Epoch: 13419 Training Loss: 0.10265509117974175 Test Loss: 0.3578623046875\n",
      "Epoch: 13420 Training Loss: 0.10280622863769531 Test Loss: 0.3584699978298611\n",
      "Epoch: 13421 Training Loss: 0.1029634280734592 Test Loss: 0.35933797200520834\n",
      "Epoch: 13422 Training Loss: 0.10311737145317926 Test Loss: 0.3602911241319444\n",
      "Epoch: 13423 Training Loss: 0.10324571990966797 Test Loss: 0.36116484917534725\n",
      "Epoch: 13424 Training Loss: 0.10335563659667969 Test Loss: 0.36198079427083335\n",
      "Epoch: 13425 Training Loss: 0.10346336449517145 Test Loss: 0.36275130208333334\n",
      "Epoch: 13426 Training Loss: 0.10354816012912327 Test Loss: 0.36348562282986113\n",
      "Epoch: 13427 Training Loss: 0.10359138827853732 Test Loss: 0.36408723958333333\n",
      "Epoch: 13428 Training Loss: 0.10359058380126954 Test Loss: 0.36432389322916664\n",
      "Epoch: 13429 Training Loss: 0.10352620273166233 Test Loss: 0.36455303276909723\n",
      "Epoch: 13430 Training Loss: 0.10341805521647135 Test Loss: 0.3644959309895833\n",
      "Epoch: 13431 Training Loss: 0.10329231346978082 Test Loss: 0.364037109375\n",
      "Epoch: 13432 Training Loss: 0.10315505133734809 Test Loss: 0.36319769965277776\n",
      "Epoch: 13433 Training Loss: 0.103017579820421 Test Loss: 0.36176581488715276\n",
      "Epoch: 13434 Training Loss: 0.10288597785101997 Test Loss: 0.36018741861979164\n",
      "Epoch: 13435 Training Loss: 0.10277187771267361 Test Loss: 0.3586681586371528\n",
      "Epoch: 13436 Training Loss: 0.10267136467827691 Test Loss: 0.3570685763888889\n",
      "Epoch: 13437 Training Loss: 0.10258263312445746 Test Loss: 0.35553662109375\n",
      "Epoch: 13438 Training Loss: 0.10248481157090929 Test Loss: 0.3547826877170139\n",
      "Epoch: 13439 Training Loss: 0.10238091701931423 Test Loss: 0.35434969075520834\n",
      "Epoch: 13440 Training Loss: 0.1022879172431098 Test Loss: 0.35420133463541664\n",
      "Epoch: 13441 Training Loss: 0.10218064965142144 Test Loss: 0.35419197591145835\n",
      "Epoch: 13442 Training Loss: 0.10208453369140626 Test Loss: 0.35424433051215276\n",
      "Epoch: 13443 Training Loss: 0.10205200873480903 Test Loss: 0.35429310438368056\n",
      "Epoch: 13444 Training Loss: 0.10208427005343967 Test Loss: 0.35430376519097223\n",
      "Epoch: 13445 Training Loss: 0.10219010162353516 Test Loss: 0.35465521918402776\n",
      "Epoch: 13446 Training Loss: 0.10239890882703993 Test Loss: 0.35504353841145836\n",
      "Epoch: 13447 Training Loss: 0.10264704047309028 Test Loss: 0.3556962890625\n",
      "Epoch: 13448 Training Loss: 0.10290831671820747 Test Loss: 0.35655658637152776\n",
      "Epoch: 13449 Training Loss: 0.10313910166422526 Test Loss: 0.35744807942708334\n",
      "Epoch: 13450 Training Loss: 0.10331606886121962 Test Loss: 0.3578225368923611\n",
      "Epoch: 13451 Training Loss: 0.10341602325439453 Test Loss: 0.3579495985243056\n",
      "Epoch: 13452 Training Loss: 0.10345111931694878 Test Loss: 0.35765782335069446\n",
      "Epoch: 13453 Training Loss: 0.10348006947835287 Test Loss: 0.35711903211805557\n",
      "Epoch: 13454 Training Loss: 0.1035021743774414 Test Loss: 0.3563291286892361\n",
      "Epoch: 13455 Training Loss: 0.10353197564019097 Test Loss: 0.3559964192708333\n",
      "Epoch: 13456 Training Loss: 0.1035537846883138 Test Loss: 0.3555207248263889\n",
      "Epoch: 13457 Training Loss: 0.10353292761908638 Test Loss: 0.3553759765625\n",
      "Epoch: 13458 Training Loss: 0.10346519046359592 Test Loss: 0.35562898763020834\n",
      "Epoch: 13459 Training Loss: 0.10334597863091363 Test Loss: 0.35598293728298613\n",
      "Epoch: 13460 Training Loss: 0.10321423763699002 Test Loss: 0.3566740451388889\n",
      "Epoch: 13461 Training Loss: 0.10308604770236546 Test Loss: 0.35707042100694447\n",
      "Epoch: 13462 Training Loss: 0.10293328687879774 Test Loss: 0.3571156955295139\n",
      "Epoch: 13463 Training Loss: 0.10278761206732856 Test Loss: 0.3571103515625\n",
      "Epoch: 13464 Training Loss: 0.10264706929524739 Test Loss: 0.3571427951388889\n",
      "Epoch: 13465 Training Loss: 0.10253100840250651 Test Loss: 0.35705213758680554\n",
      "Epoch: 13466 Training Loss: 0.10246311781141493 Test Loss: 0.35692689344618056\n",
      "Epoch: 13467 Training Loss: 0.102429567972819 Test Loss: 0.3569067111545139\n",
      "Epoch: 13468 Training Loss: 0.10241558074951172 Test Loss: 0.3568112250434028\n",
      "Epoch: 13469 Training Loss: 0.10239080047607421 Test Loss: 0.3570350206163194\n",
      "Epoch: 13470 Training Loss: 0.10237999979654948 Test Loss: 0.3570951605902778\n",
      "Epoch: 13471 Training Loss: 0.10241319868299696 Test Loss: 0.35708072916666667\n",
      "Epoch: 13472 Training Loss: 0.10250838131374783 Test Loss: 0.3568005099826389\n",
      "Epoch: 13473 Training Loss: 0.10266955142550999 Test Loss: 0.35646194118923613\n",
      "Epoch: 13474 Training Loss: 0.10289168379041884 Test Loss: 0.35633577473958333\n",
      "Epoch: 13475 Training Loss: 0.1031501456366645 Test Loss: 0.3567008192274306\n",
      "Epoch: 13476 Training Loss: 0.10341858673095704 Test Loss: 0.3570999891493056\n",
      "Epoch: 13477 Training Loss: 0.10368250613742405 Test Loss: 0.3575432942708333\n",
      "Epoch: 13478 Training Loss: 0.10394713083902994 Test Loss: 0.35802582465277777\n",
      "Epoch: 13479 Training Loss: 0.10422217475043402 Test Loss: 0.3582424587673611\n",
      "Epoch: 13480 Training Loss: 0.10453016238742405 Test Loss: 0.35842567274305553\n",
      "Epoch: 13481 Training Loss: 0.1048614968193902 Test Loss: 0.35884564887152776\n",
      "Epoch: 13482 Training Loss: 0.10524195437961155 Test Loss: 0.3596835666232639\n",
      "Epoch: 13483 Training Loss: 0.1056801978217231 Test Loss: 0.36080631510416666\n",
      "Epoch: 13484 Training Loss: 0.10621556006537543 Test Loss: 0.3623403862847222\n",
      "Epoch: 13485 Training Loss: 0.1069072036743164 Test Loss: 0.3641071506076389\n",
      "Epoch: 13486 Training Loss: 0.10777720303005642 Test Loss: 0.36594962565104167\n",
      "Epoch: 13487 Training Loss: 0.10879652743869357 Test Loss: 0.3671338433159722\n",
      "Epoch: 13488 Training Loss: 0.10992010498046875 Test Loss: 0.3672865939670139\n",
      "Epoch: 13489 Training Loss: 0.11106358591715494 Test Loss: 0.36636268446180553\n",
      "Epoch: 13490 Training Loss: 0.11200467597113715 Test Loss: 0.3648406304253472\n",
      "Epoch: 13491 Training Loss: 0.1125634774102105 Test Loss: 0.3634780815972222\n",
      "Epoch: 13492 Training Loss: 0.11258504909939236 Test Loss: 0.36325672743055554\n",
      "Epoch: 13493 Training Loss: 0.1120944578382704 Test Loss: 0.36335411241319443\n",
      "Epoch: 13494 Training Loss: 0.1112285376654731 Test Loss: 0.3640627983940972\n",
      "Epoch: 13495 Training Loss: 0.11020572153727214 Test Loss: 0.3642150336371528\n",
      "Epoch: 13496 Training Loss: 0.10909332190619575 Test Loss: 0.36334912109375\n",
      "Epoch: 13497 Training Loss: 0.10792602369520399 Test Loss: 0.3615042046440972\n",
      "Epoch: 13498 Training Loss: 0.10676356336805555 Test Loss: 0.3597245008680556\n",
      "Epoch: 13499 Training Loss: 0.10570132446289063 Test Loss: 0.35878656684027777\n",
      "Epoch: 13500 Training Loss: 0.10480391099717883 Test Loss: 0.35851231553819446\n",
      "Epoch: 13501 Training Loss: 0.10413206142849392 Test Loss: 0.3585217556423611\n",
      "Epoch: 13502 Training Loss: 0.10367509375678169 Test Loss: 0.3583656684027778\n",
      "Epoch: 13503 Training Loss: 0.10334360843234593 Test Loss: 0.3578872341579861\n",
      "Epoch: 13504 Training Loss: 0.10306920623779296 Test Loss: 0.35727864583333335\n",
      "Epoch: 13505 Training Loss: 0.10283742099338107 Test Loss: 0.35658978949652775\n",
      "Epoch: 13506 Training Loss: 0.10260725572374133 Test Loss: 0.35562196180555555\n",
      "Epoch: 13507 Training Loss: 0.10240087212456597 Test Loss: 0.3550422634548611\n",
      "Epoch: 13508 Training Loss: 0.10222762213812935 Test Loss: 0.35485633680555556\n",
      "Epoch: 13509 Training Loss: 0.10209251064724392 Test Loss: 0.35502642144097224\n",
      "Epoch: 13510 Training Loss: 0.10199755265977647 Test Loss: 0.35547002495659724\n",
      "Epoch: 13511 Training Loss: 0.10196317291259765 Test Loss: 0.35626985677083334\n",
      "Epoch: 13512 Training Loss: 0.10197485690646702 Test Loss: 0.35704191080729164\n",
      "Epoch: 13513 Training Loss: 0.1019794913397895 Test Loss: 0.3573862033420139\n",
      "Epoch: 13514 Training Loss: 0.1019769032796224 Test Loss: 0.35745323350694447\n",
      "Epoch: 13515 Training Loss: 0.10197688208685982 Test Loss: 0.3574172905815972\n",
      "Epoch: 13516 Training Loss: 0.10198779635959201 Test Loss: 0.3571432834201389\n",
      "Epoch: 13517 Training Loss: 0.10200408003065321 Test Loss: 0.35697032335069445\n",
      "Epoch: 13518 Training Loss: 0.10202724372016059 Test Loss: 0.3567560763888889\n",
      "Epoch: 13519 Training Loss: 0.10207409074571397 Test Loss: 0.35659388563368055\n",
      "Epoch: 13520 Training Loss: 0.10214414808485243 Test Loss: 0.3563461371527778\n",
      "Epoch: 13521 Training Loss: 0.10224423133002387 Test Loss: 0.3558611924913194\n",
      "Epoch: 13522 Training Loss: 0.10235255347357856 Test Loss: 0.35532576497395835\n",
      "Epoch: 13523 Training Loss: 0.10247681766086154 Test Loss: 0.35455759006076387\n",
      "Epoch: 13524 Training Loss: 0.1026272481282552 Test Loss: 0.3536721462673611\n",
      "Epoch: 13525 Training Loss: 0.10279429117838541 Test Loss: 0.35323828125\n",
      "Epoch: 13526 Training Loss: 0.10296946292453342 Test Loss: 0.35318077256944447\n",
      "Epoch: 13527 Training Loss: 0.1031422848171658 Test Loss: 0.3537040744357639\n",
      "Epoch: 13528 Training Loss: 0.10331905195448134 Test Loss: 0.35470849609375\n",
      "Epoch: 13529 Training Loss: 0.10344101884629991 Test Loss: 0.35574842664930556\n",
      "Epoch: 13530 Training Loss: 0.10355990261501737 Test Loss: 0.35670741102430553\n",
      "Epoch: 13531 Training Loss: 0.10366214667426216 Test Loss: 0.35727294921875\n",
      "Epoch: 13532 Training Loss: 0.10370277913411459 Test Loss: 0.35742203776041664\n",
      "Epoch: 13533 Training Loss: 0.10369893476698133 Test Loss: 0.3574167209201389\n",
      "Epoch: 13534 Training Loss: 0.10369917805989583 Test Loss: 0.35763023546006945\n",
      "Epoch: 13535 Training Loss: 0.10372943284776476 Test Loss: 0.35776871744791666\n",
      "Epoch: 13536 Training Loss: 0.10377473534478082 Test Loss: 0.35733997938368056\n",
      "Epoch: 13537 Training Loss: 0.1037831539577908 Test Loss: 0.3570112033420139\n",
      "Epoch: 13538 Training Loss: 0.10379233890109592 Test Loss: 0.35688783094618054\n",
      "Epoch: 13539 Training Loss: 0.10380561828613281 Test Loss: 0.3572907172309028\n",
      "Epoch: 13540 Training Loss: 0.10383539581298828 Test Loss: 0.35826497395833334\n",
      "Epoch: 13541 Training Loss: 0.10387739817301432 Test Loss: 0.35981404622395835\n",
      "Epoch: 13542 Training Loss: 0.10390975104437934 Test Loss: 0.36162513563368054\n",
      "Epoch: 13543 Training Loss: 0.10392342885335286 Test Loss: 0.363466796875\n",
      "Epoch: 13544 Training Loss: 0.1039274402194553 Test Loss: 0.3649460991753472\n",
      "Epoch: 13545 Training Loss: 0.10394423505995008 Test Loss: 0.36629020182291666\n",
      "Epoch: 13546 Training Loss: 0.10398527696397569 Test Loss: 0.3665254448784722\n",
      "Epoch: 13547 Training Loss: 0.10404078420003256 Test Loss: 0.36559418402777777\n",
      "Epoch: 13548 Training Loss: 0.10411358557807075 Test Loss: 0.36453184678819445\n",
      "Epoch: 13549 Training Loss: 0.10422582160101997 Test Loss: 0.3629667426215278\n",
      "Epoch: 13550 Training Loss: 0.10437755245632596 Test Loss: 0.3625555555555556\n",
      "Epoch: 13551 Training Loss: 0.10452754720052083 Test Loss: 0.36486480034722224\n",
      "Epoch: 13552 Training Loss: 0.10468703036838108 Test Loss: 0.36735861545138887\n",
      "Epoch: 13553 Training Loss: 0.10475105285644531 Test Loss: 0.36377284071180555\n",
      "Epoch: 13554 Training Loss: 0.10468141004774306 Test Loss: 0.3606109212239583\n",
      "Epoch: 13555 Training Loss: 0.10460372755262587 Test Loss: 0.3589421657986111\n",
      "Epoch: 13556 Training Loss: 0.10459995439317492 Test Loss: 0.35956591796875\n",
      "Epoch: 13557 Training Loss: 0.1047198986477322 Test Loss: 0.36008314344618053\n",
      "Epoch: 13558 Training Loss: 0.10492972564697266 Test Loss: 0.36138324652777776\n",
      "Epoch: 13559 Training Loss: 0.10521908823649088 Test Loss: 0.36355360243055557\n",
      "Epoch: 13560 Training Loss: 0.10558877309163411 Test Loss: 0.36644951714409724\n",
      "Epoch: 13561 Training Loss: 0.1060210444132487 Test Loss: 0.36991419813368054\n",
      "Epoch: 13562 Training Loss: 0.10645591566297744 Test Loss: 0.37271630859375\n",
      "Epoch: 13563 Training Loss: 0.10676673295762804 Test Loss: 0.37194685872395833\n",
      "Epoch: 13564 Training Loss: 0.10687091827392578 Test Loss: 0.36881233723958334\n",
      "Epoch: 13565 Training Loss: 0.10687147352430555 Test Loss: 0.3680401475694444\n",
      "Epoch: 13566 Training Loss: 0.10678654395209418 Test Loss: 0.36395301649305556\n",
      "Epoch: 13567 Training Loss: 0.10656510162353515 Test Loss: 0.3604927300347222\n",
      "Epoch: 13568 Training Loss: 0.10633580610487196 Test Loss: 0.36236474609375\n",
      "Epoch: 13569 Training Loss: 0.1062046610514323 Test Loss: 0.36556081814236113\n",
      "Epoch: 13570 Training Loss: 0.10610601552327474 Test Loss: 0.36616859266493057\n",
      "Epoch: 13571 Training Loss: 0.10599971686469184 Test Loss: 0.3644846462673611\n",
      "Epoch: 13572 Training Loss: 0.10585907236735026 Test Loss: 0.3623322482638889\n",
      "Epoch: 13573 Training Loss: 0.10567696126302083 Test Loss: 0.36101578776041665\n",
      "Epoch: 13574 Training Loss: 0.10546125115288628 Test Loss: 0.36101595052083335\n",
      "Epoch: 13575 Training Loss: 0.10528532155354818 Test Loss: 0.36147146267361113\n",
      "Epoch: 13576 Training Loss: 0.10517936706542969 Test Loss: 0.3616627604166667\n",
      "Epoch: 13577 Training Loss: 0.10514239501953125 Test Loss: 0.3612688259548611\n",
      "Epoch: 13578 Training Loss: 0.10514544169108073 Test Loss: 0.36044791666666665\n",
      "Epoch: 13579 Training Loss: 0.1051642328898112 Test Loss: 0.35941132269965276\n",
      "Epoch: 13580 Training Loss: 0.10517264641655816 Test Loss: 0.3587682834201389\n",
      "Epoch: 13581 Training Loss: 0.1051757312350803 Test Loss: 0.35863416883680554\n",
      "Epoch: 13582 Training Loss: 0.10515479956732855 Test Loss: 0.3588539496527778\n",
      "Epoch: 13583 Training Loss: 0.10516813744439019 Test Loss: 0.3590236545138889\n",
      "Epoch: 13584 Training Loss: 0.10520263163248698 Test Loss: 0.3587619900173611\n",
      "Epoch: 13585 Training Loss: 0.10525611707899306 Test Loss: 0.3582346462673611\n",
      "Epoch: 13586 Training Loss: 0.10527251180013021 Test Loss: 0.3573911675347222\n",
      "Epoch: 13587 Training Loss: 0.10517648569742838 Test Loss: 0.35676698133680557\n",
      "Epoch: 13588 Training Loss: 0.10499715847439237 Test Loss: 0.35663677300347224\n",
      "Epoch: 13589 Training Loss: 0.10473647477891711 Test Loss: 0.3568256293402778\n",
      "Epoch: 13590 Training Loss: 0.10445863766140408 Test Loss: 0.3575387098524306\n",
      "Epoch: 13591 Training Loss: 0.10420038689507378 Test Loss: 0.3585352105034722\n",
      "Epoch: 13592 Training Loss: 0.10395050642225477 Test Loss: 0.359511962890625\n",
      "Epoch: 13593 Training Loss: 0.10372935824924046 Test Loss: 0.36032484266493053\n",
      "Epoch: 13594 Training Loss: 0.10352740817599826 Test Loss: 0.36103325737847225\n",
      "Epoch: 13595 Training Loss: 0.10334033626980252 Test Loss: 0.36132584635416665\n",
      "Epoch: 13596 Training Loss: 0.10317598385281034 Test Loss: 0.36109022352430553\n",
      "Epoch: 13597 Training Loss: 0.10302094184027778 Test Loss: 0.3606303168402778\n",
      "Epoch: 13598 Training Loss: 0.10288966200086806 Test Loss: 0.3601640625\n",
      "Epoch: 13599 Training Loss: 0.1027801759507921 Test Loss: 0.35957080078125\n",
      "Epoch: 13600 Training Loss: 0.10267083062065972 Test Loss: 0.35910975477430557\n",
      "Epoch: 13601 Training Loss: 0.10256200663248698 Test Loss: 0.3585286458333333\n",
      "Epoch: 13602 Training Loss: 0.10244358571370443 Test Loss: 0.3581591796875\n",
      "Epoch: 13603 Training Loss: 0.10228194766574436 Test Loss: 0.3579130859375\n",
      "Epoch: 13604 Training Loss: 0.10207904476589627 Test Loss: 0.3576302625868056\n",
      "Epoch: 13605 Training Loss: 0.10185034518771702 Test Loss: 0.3575325249565972\n",
      "Epoch: 13606 Training Loss: 0.1016073735555013 Test Loss: 0.3573555501302083\n",
      "Epoch: 13607 Training Loss: 0.10137330203586155 Test Loss: 0.35705992296006944\n",
      "Epoch: 13608 Training Loss: 0.10115977308485243 Test Loss: 0.3566714409722222\n",
      "Epoch: 13609 Training Loss: 0.10097344716389973 Test Loss: 0.3562031792534722\n",
      "Epoch: 13610 Training Loss: 0.10081664276123047 Test Loss: 0.35578114149305556\n",
      "Epoch: 13611 Training Loss: 0.10068986002604166 Test Loss: 0.3554494357638889\n",
      "Epoch: 13612 Training Loss: 0.10059748077392579 Test Loss: 0.3553275282118056\n",
      "Epoch: 13613 Training Loss: 0.10052206336127387 Test Loss: 0.3551255696614583\n",
      "Epoch: 13614 Training Loss: 0.10046632978651258 Test Loss: 0.35492523871527776\n",
      "Epoch: 13615 Training Loss: 0.10043185340033638 Test Loss: 0.35498079427083334\n",
      "Epoch: 13616 Training Loss: 0.10042172495524089 Test Loss: 0.3550930718315972\n",
      "Epoch: 13617 Training Loss: 0.100426513671875 Test Loss: 0.35510953776041665\n",
      "Epoch: 13618 Training Loss: 0.10045589362250434 Test Loss: 0.3552782118055556\n",
      "Epoch: 13619 Training Loss: 0.10051805708143446 Test Loss: 0.35546847873263887\n",
      "Epoch: 13620 Training Loss: 0.1005994398328993 Test Loss: 0.35570225694444446\n",
      "Epoch: 13621 Training Loss: 0.10070633443196615 Test Loss: 0.35615616861979166\n",
      "Epoch: 13622 Training Loss: 0.10083711836073134 Test Loss: 0.3565608452690972\n",
      "Epoch: 13623 Training Loss: 0.10096690538194444 Test Loss: 0.35715717230902777\n",
      "Epoch: 13624 Training Loss: 0.10110817464192709 Test Loss: 0.35766612413194443\n",
      "Epoch: 13625 Training Loss: 0.10125890096028646 Test Loss: 0.358173828125\n",
      "Epoch: 13626 Training Loss: 0.1014011959499783 Test Loss: 0.35883208550347223\n",
      "Epoch: 13627 Training Loss: 0.10152899678548177 Test Loss: 0.35931060112847224\n",
      "Epoch: 13628 Training Loss: 0.10164896477593316 Test Loss: 0.35939371744791665\n",
      "Epoch: 13629 Training Loss: 0.10177332051595052 Test Loss: 0.3590394965277778\n",
      "Epoch: 13630 Training Loss: 0.10190667385525173 Test Loss: 0.3582197536892361\n",
      "Epoch: 13631 Training Loss: 0.1020416480170356 Test Loss: 0.3571152615017361\n",
      "Epoch: 13632 Training Loss: 0.10216891394721136 Test Loss: 0.35588251410590277\n",
      "Epoch: 13633 Training Loss: 0.1022958263821072 Test Loss: 0.35504302300347224\n",
      "Epoch: 13634 Training Loss: 0.10244842020670573 Test Loss: 0.35463807508680556\n",
      "Epoch: 13635 Training Loss: 0.10261163330078125 Test Loss: 0.3549816623263889\n",
      "Epoch: 13636 Training Loss: 0.10279287804497612 Test Loss: 0.35586325412326386\n",
      "Epoch: 13637 Training Loss: 0.10302396562364366 Test Loss: 0.35729161241319446\n",
      "Epoch: 13638 Training Loss: 0.10328927866617839 Test Loss: 0.35870399305555556\n",
      "Epoch: 13639 Training Loss: 0.10359827338324652 Test Loss: 0.3598724500868056\n",
      "Epoch: 13640 Training Loss: 0.10390394931369358 Test Loss: 0.360415771484375\n",
      "Epoch: 13641 Training Loss: 0.10419600847032336 Test Loss: 0.36051947699652775\n",
      "Epoch: 13642 Training Loss: 0.1044448267618815 Test Loss: 0.3607360568576389\n",
      "Epoch: 13643 Training Loss: 0.10461836412217881 Test Loss: 0.36109337022569443\n",
      "Epoch: 13644 Training Loss: 0.10469971042209202 Test Loss: 0.3620485297309028\n",
      "Epoch: 13645 Training Loss: 0.1046812989976671 Test Loss: 0.36294327799479165\n",
      "Epoch: 13646 Training Loss: 0.10462340545654297 Test Loss: 0.36350699869791664\n",
      "Epoch: 13647 Training Loss: 0.10456019931369358 Test Loss: 0.36340120442708335\n",
      "Epoch: 13648 Training Loss: 0.10456083679199218 Test Loss: 0.3626247287326389\n",
      "Epoch: 13649 Training Loss: 0.10457089487711589 Test Loss: 0.36089740668402776\n",
      "Epoch: 13650 Training Loss: 0.10455871327718098 Test Loss: 0.359291748046875\n",
      "Epoch: 13651 Training Loss: 0.10450853983561198 Test Loss: 0.35821392144097225\n",
      "Epoch: 13652 Training Loss: 0.10439746771918403 Test Loss: 0.3585334201388889\n",
      "Epoch: 13653 Training Loss: 0.104243775261773 Test Loss: 0.3595427517361111\n",
      "Epoch: 13654 Training Loss: 0.10405545043945312 Test Loss: 0.36127240668402777\n",
      "Epoch: 13655 Training Loss: 0.10390614573160807 Test Loss: 0.361904296875\n",
      "Epoch: 13656 Training Loss: 0.10383819580078125 Test Loss: 0.36190565321180557\n",
      "Epoch: 13657 Training Loss: 0.1038381822374132 Test Loss: 0.36107237413194443\n",
      "Epoch: 13658 Training Loss: 0.10384872182210286 Test Loss: 0.3602289496527778\n",
      "Epoch: 13659 Training Loss: 0.10388068389892578 Test Loss: 0.36022243923611114\n",
      "Epoch: 13660 Training Loss: 0.10395055135091145 Test Loss: 0.36057335069444446\n",
      "Epoch: 13661 Training Loss: 0.1040752207438151 Test Loss: 0.36103778754340277\n",
      "Epoch: 13662 Training Loss: 0.10425915951199002 Test Loss: 0.3609909396701389\n",
      "Epoch: 13663 Training Loss: 0.10452351294623481 Test Loss: 0.36056336805555556\n",
      "Epoch: 13664 Training Loss: 0.1047907969156901 Test Loss: 0.35980506727430556\n",
      "Epoch: 13665 Training Loss: 0.10504951392279731 Test Loss: 0.35842214626736113\n",
      "Epoch: 13666 Training Loss: 0.10524412451850043 Test Loss: 0.35701833767361113\n",
      "Epoch: 13667 Training Loss: 0.1053536131117079 Test Loss: 0.35519981553819446\n",
      "Epoch: 13668 Training Loss: 0.10541470591227213 Test Loss: 0.3536043836805556\n",
      "Epoch: 13669 Training Loss: 0.1054721688164605 Test Loss: 0.3525619032118056\n",
      "Epoch: 13670 Training Loss: 0.1055912356906467 Test Loss: 0.35276453993055557\n",
      "Epoch: 13671 Training Loss: 0.10573482089572482 Test Loss: 0.35348394097222224\n",
      "Epoch: 13672 Training Loss: 0.10584422726101346 Test Loss: 0.35436789279513886\n",
      "Epoch: 13673 Training Loss: 0.10588716888427735 Test Loss: 0.3551865505642361\n",
      "Epoch: 13674 Training Loss: 0.10590527767605251 Test Loss: 0.3560370551215278\n",
      "Epoch: 13675 Training Loss: 0.10594112226698134 Test Loss: 0.35644677734375\n",
      "Epoch: 13676 Training Loss: 0.10600100199381511 Test Loss: 0.35643006727430554\n",
      "Epoch: 13677 Training Loss: 0.10608699205186632 Test Loss: 0.3561752387152778\n",
      "Epoch: 13678 Training Loss: 0.10617579650878907 Test Loss: 0.35620906575520833\n",
      "Epoch: 13679 Training Loss: 0.10614673021104601 Test Loss: 0.35650048828125\n",
      "Epoch: 13680 Training Loss: 0.10591678110758464 Test Loss: 0.35734624565972223\n",
      "Epoch: 13681 Training Loss: 0.10551104312472873 Test Loss: 0.3570391167534722\n",
      "Epoch: 13682 Training Loss: 0.10491573757595486 Test Loss: 0.3553085394965278\n",
      "Epoch: 13683 Training Loss: 0.10428900485568576 Test Loss: 0.3529507378472222\n",
      "Epoch: 13684 Training Loss: 0.1037217305501302 Test Loss: 0.35149405924479166\n",
      "Epoch: 13685 Training Loss: 0.10321458011203342 Test Loss: 0.3511192491319444\n",
      "Epoch: 13686 Training Loss: 0.10275238206651476 Test Loss: 0.3515777180989583\n",
      "Epoch: 13687 Training Loss: 0.10232826826307509 Test Loss: 0.35222520616319447\n",
      "Epoch: 13688 Training Loss: 0.10195100402832032 Test Loss: 0.35300059678819445\n",
      "Epoch: 13689 Training Loss: 0.10159115431043837 Test Loss: 0.353486083984375\n",
      "Epoch: 13690 Training Loss: 0.10124562327067058 Test Loss: 0.3537214084201389\n",
      "Epoch: 13691 Training Loss: 0.10092154693603515 Test Loss: 0.3537929144965278\n",
      "Epoch: 13692 Training Loss: 0.10062984381781684 Test Loss: 0.35383390299479167\n",
      "Epoch: 13693 Training Loss: 0.1003543463812934 Test Loss: 0.35405194769965276\n",
      "Epoch: 13694 Training Loss: 0.10010159725613064 Test Loss: 0.3542598198784722\n",
      "Epoch: 13695 Training Loss: 0.09989188554551866 Test Loss: 0.35416579861111114\n",
      "Epoch: 13696 Training Loss: 0.09972151607937282 Test Loss: 0.35405194769965276\n",
      "Epoch: 13697 Training Loss: 0.09958622063530816 Test Loss: 0.35369870334201386\n",
      "Epoch: 13698 Training Loss: 0.09946912892659505 Test Loss: 0.35324544270833336\n",
      "Epoch: 13699 Training Loss: 0.09935861206054687 Test Loss: 0.35263362630208334\n",
      "Epoch: 13700 Training Loss: 0.0992666015625 Test Loss: 0.35217632378472224\n",
      "Epoch: 13701 Training Loss: 0.09918714057074653 Test Loss: 0.35165283203125\n",
      "Epoch: 13702 Training Loss: 0.09912686157226562 Test Loss: 0.351248779296875\n",
      "Epoch: 13703 Training Loss: 0.09909381357828775 Test Loss: 0.3509081217447917\n",
      "Epoch: 13704 Training Loss: 0.09906918589274089 Test Loss: 0.3507272135416667\n",
      "Epoch: 13705 Training Loss: 0.09904967753092447 Test Loss: 0.3505456271701389\n",
      "Epoch: 13706 Training Loss: 0.09905044725206164 Test Loss: 0.35021809895833333\n",
      "Epoch: 13707 Training Loss: 0.09906575096978082 Test Loss: 0.35028065321180557\n",
      "Epoch: 13708 Training Loss: 0.09910055711534288 Test Loss: 0.35035367838541664\n",
      "Epoch: 13709 Training Loss: 0.09915473175048828 Test Loss: 0.35047037760416666\n",
      "Epoch: 13710 Training Loss: 0.09921873643663194 Test Loss: 0.35092654079861113\n",
      "Epoch: 13711 Training Loss: 0.09930387369791667 Test Loss: 0.35159412977430554\n",
      "Epoch: 13712 Training Loss: 0.09939515940348308 Test Loss: 0.35221497938368057\n",
      "Epoch: 13713 Training Loss: 0.09950150468614366 Test Loss: 0.35276567925347224\n",
      "Epoch: 13714 Training Loss: 0.09963468085394965 Test Loss: 0.3534773220486111\n",
      "Epoch: 13715 Training Loss: 0.09979780748155381 Test Loss: 0.35411585828993053\n",
      "Epoch: 13716 Training Loss: 0.09998127916124132 Test Loss: 0.35488834635416666\n",
      "Epoch: 13717 Training Loss: 0.10017642890082465 Test Loss: 0.3555762532552083\n",
      "Epoch: 13718 Training Loss: 0.10038776143391927 Test Loss: 0.35637537977430556\n",
      "Epoch: 13719 Training Loss: 0.10061387803819444 Test Loss: 0.3572099880642361\n",
      "Epoch: 13720 Training Loss: 0.1008603990342882 Test Loss: 0.3583264702690972\n",
      "Epoch: 13721 Training Loss: 0.1011442108154297 Test Loss: 0.3593335503472222\n",
      "Epoch: 13722 Training Loss: 0.10144445292154948 Test Loss: 0.36044618055555555\n",
      "Epoch: 13723 Training Loss: 0.1017665557861328 Test Loss: 0.3612006293402778\n",
      "Epoch: 13724 Training Loss: 0.1020995356241862 Test Loss: 0.36196229383680556\n",
      "Epoch: 13725 Training Loss: 0.10243437703450521 Test Loss: 0.36284266493055556\n",
      "Epoch: 13726 Training Loss: 0.1027873297797309 Test Loss: 0.3636203884548611\n",
      "Epoch: 13727 Training Loss: 0.10313496992323133 Test Loss: 0.36411219618055557\n",
      "Epoch: 13728 Training Loss: 0.10348211500379774 Test Loss: 0.3641395941840278\n",
      "Epoch: 13729 Training Loss: 0.10384510887993706 Test Loss: 0.36434852430555553\n",
      "Epoch: 13730 Training Loss: 0.10420955488416883 Test Loss: 0.3641331922743056\n",
      "Epoch: 13731 Training Loss: 0.10457739427354601 Test Loss: 0.3633825141059028\n",
      "Epoch: 13732 Training Loss: 0.10492182498508029 Test Loss: 0.36218929036458336\n",
      "Epoch: 13733 Training Loss: 0.10519117567274305 Test Loss: 0.3607631022135417\n",
      "Epoch: 13734 Training Loss: 0.10537392255995008 Test Loss: 0.35934421115451387\n",
      "Epoch: 13735 Training Loss: 0.10545773484971788 Test Loss: 0.35784898546006944\n",
      "Epoch: 13736 Training Loss: 0.10547318861219618 Test Loss: 0.3568650716145833\n",
      "Epoch: 13737 Training Loss: 0.10550374603271484 Test Loss: 0.3563857421875\n",
      "Epoch: 13738 Training Loss: 0.10556121487087673 Test Loss: 0.3567487521701389\n",
      "Epoch: 13739 Training Loss: 0.1056578352186415 Test Loss: 0.35713861762152777\n",
      "Epoch: 13740 Training Loss: 0.10580386945936415 Test Loss: 0.35780512152777777\n",
      "Epoch: 13741 Training Loss: 0.10593140750461154 Test Loss: 0.358487548828125\n",
      "Epoch: 13742 Training Loss: 0.105984864976671 Test Loss: 0.35883506944444443\n",
      "Epoch: 13743 Training Loss: 0.10595623270670573 Test Loss: 0.3591486002604167\n",
      "Epoch: 13744 Training Loss: 0.1058812493218316 Test Loss: 0.35988194444444443\n",
      "Epoch: 13745 Training Loss: 0.10580621507432726 Test Loss: 0.36066498480902776\n",
      "Epoch: 13746 Training Loss: 0.10576407368977865 Test Loss: 0.36158116319444444\n",
      "Epoch: 13747 Training Loss: 0.10576532236735026 Test Loss: 0.36189366319444444\n",
      "Epoch: 13748 Training Loss: 0.10580167897542318 Test Loss: 0.36160546875\n",
      "Epoch: 13749 Training Loss: 0.10581316969129774 Test Loss: 0.36096058485243054\n",
      "Epoch: 13750 Training Loss: 0.10577224477132162 Test Loss: 0.36085546875\n",
      "Epoch: 13751 Training Loss: 0.10569034576416016 Test Loss: 0.3604465603298611\n",
      "Epoch: 13752 Training Loss: 0.10548683081732856 Test Loss: 0.36004337565104166\n",
      "Epoch: 13753 Training Loss: 0.10517660776774089 Test Loss: 0.35971885850694446\n",
      "Epoch: 13754 Training Loss: 0.10483419121636285 Test Loss: 0.3591541069878472\n",
      "Epoch: 13755 Training Loss: 0.10448832024468316 Test Loss: 0.358125732421875\n",
      "Epoch: 13756 Training Loss: 0.1041423077053494 Test Loss: 0.3567905815972222\n",
      "Epoch: 13757 Training Loss: 0.10381183454725477 Test Loss: 0.3554246148003472\n",
      "Epoch: 13758 Training Loss: 0.10347567325168186 Test Loss: 0.3544784613715278\n",
      "Epoch: 13759 Training Loss: 0.10315811326768663 Test Loss: 0.3543200141059028\n",
      "Epoch: 13760 Training Loss: 0.10282342614067926 Test Loss: 0.35468956163194443\n",
      "Epoch: 13761 Training Loss: 0.10247374640570747 Test Loss: 0.3552572157118056\n",
      "Epoch: 13762 Training Loss: 0.10216247304280598 Test Loss: 0.35576437717013887\n",
      "Epoch: 13763 Training Loss: 0.10190667131212022 Test Loss: 0.3562682834201389\n",
      "Epoch: 13764 Training Loss: 0.10172175004747179 Test Loss: 0.35633875868055553\n",
      "Epoch: 13765 Training Loss: 0.101619751824273 Test Loss: 0.3563830295138889\n",
      "Epoch: 13766 Training Loss: 0.10158805847167969 Test Loss: 0.35643885633680555\n",
      "Epoch: 13767 Training Loss: 0.10158839416503906 Test Loss: 0.35639976671006945\n",
      "Epoch: 13768 Training Loss: 0.10161378733317057 Test Loss: 0.35644761827256943\n",
      "Epoch: 13769 Training Loss: 0.10163375939263238 Test Loss: 0.3565685763888889\n",
      "Epoch: 13770 Training Loss: 0.10166009775797526 Test Loss: 0.35654410807291664\n",
      "Epoch: 13771 Training Loss: 0.10172261386447483 Test Loss: 0.3567940266927083\n",
      "Epoch: 13772 Training Loss: 0.10184745025634766 Test Loss: 0.35708837890625\n",
      "Epoch: 13773 Training Loss: 0.10200391727023654 Test Loss: 0.3575031467013889\n",
      "Epoch: 13774 Training Loss: 0.1022010031806098 Test Loss: 0.3580247124565972\n",
      "Epoch: 13775 Training Loss: 0.10243747880723741 Test Loss: 0.35868489583333335\n",
      "Epoch: 13776 Training Loss: 0.10269696892632378 Test Loss: 0.3593393012152778\n",
      "Epoch: 13777 Training Loss: 0.10295864529079861 Test Loss: 0.3601520453559028\n",
      "Epoch: 13778 Training Loss: 0.1031999274359809 Test Loss: 0.36098763020833335\n",
      "Epoch: 13779 Training Loss: 0.10340437316894531 Test Loss: 0.36195724826388886\n",
      "Epoch: 13780 Training Loss: 0.10359072367350261 Test Loss: 0.362858154296875\n",
      "Epoch: 13781 Training Loss: 0.10375291019015842 Test Loss: 0.3638073459201389\n",
      "Epoch: 13782 Training Loss: 0.10390396457248263 Test Loss: 0.36420225694444447\n",
      "Epoch: 13783 Training Loss: 0.10409292432996962 Test Loss: 0.3643798285590278\n",
      "Epoch: 13784 Training Loss: 0.10429894934760199 Test Loss: 0.36459233940972224\n",
      "Epoch: 13785 Training Loss: 0.10452169799804688 Test Loss: 0.36458935546875\n",
      "Epoch: 13786 Training Loss: 0.1047317123413086 Test Loss: 0.36498014322916666\n",
      "Epoch: 13787 Training Loss: 0.10495574103461372 Test Loss: 0.36595041232638886\n",
      "Epoch: 13788 Training Loss: 0.10524753824869791 Test Loss: 0.36697005208333333\n",
      "Epoch: 13789 Training Loss: 0.1056390643649631 Test Loss: 0.36814838324652777\n",
      "Epoch: 13790 Training Loss: 0.10608144632975261 Test Loss: 0.36915825737847224\n",
      "Epoch: 13791 Training Loss: 0.10651847076416016 Test Loss: 0.36986349826388887\n",
      "Epoch: 13792 Training Loss: 0.10692091284857856 Test Loss: 0.37043929036458334\n",
      "Epoch: 13793 Training Loss: 0.10726361846923828 Test Loss: 0.37043516710069446\n",
      "Epoch: 13794 Training Loss: 0.10746882290310329 Test Loss: 0.3696248101128472\n",
      "Epoch: 13795 Training Loss: 0.10750799560546875 Test Loss: 0.36864903428819445\n",
      "Epoch: 13796 Training Loss: 0.10742596350775825 Test Loss: 0.3674264051649306\n",
      "Epoch: 13797 Training Loss: 0.10722249772813586 Test Loss: 0.3664871419270833\n",
      "Epoch: 13798 Training Loss: 0.1069226320054796 Test Loss: 0.36546617296006945\n",
      "Epoch: 13799 Training Loss: 0.10654875776502822 Test Loss: 0.36439827473958336\n",
      "Epoch: 13800 Training Loss: 0.10611070421006945 Test Loss: 0.36356879340277776\n",
      "Epoch: 13801 Training Loss: 0.10555864630805122 Test Loss: 0.36316867404513886\n",
      "Epoch: 13802 Training Loss: 0.10494197591145833 Test Loss: 0.3626159939236111\n",
      "Epoch: 13803 Training Loss: 0.10433652750651042 Test Loss: 0.36226622178819445\n",
      "Epoch: 13804 Training Loss: 0.10378040907118055 Test Loss: 0.36164398871527775\n",
      "Epoch: 13805 Training Loss: 0.10328539360894097 Test Loss: 0.36108110894097223\n",
      "Epoch: 13806 Training Loss: 0.10288368818495008 Test Loss: 0.36018009440104165\n",
      "Epoch: 13807 Training Loss: 0.10260399627685547 Test Loss: 0.35950851779513887\n",
      "Epoch: 13808 Training Loss: 0.10241532135009766 Test Loss: 0.3586799858940972\n",
      "Epoch: 13809 Training Loss: 0.10230682542588976 Test Loss: 0.3583851725260417\n",
      "Epoch: 13810 Training Loss: 0.10223106638590494 Test Loss: 0.35851155598958334\n",
      "Epoch: 13811 Training Loss: 0.10219542439778646 Test Loss: 0.35917854817708333\n",
      "Epoch: 13812 Training Loss: 0.10220577833387587 Test Loss: 0.3600987955729167\n",
      "Epoch: 13813 Training Loss: 0.10228364647759332 Test Loss: 0.36119656032986114\n",
      "Epoch: 13814 Training Loss: 0.10242337375217014 Test Loss: 0.3625288357204861\n",
      "Epoch: 13815 Training Loss: 0.1026218736436632 Test Loss: 0.3639850802951389\n",
      "Epoch: 13816 Training Loss: 0.1028670179578993 Test Loss: 0.36548111979166664\n",
      "Epoch: 13817 Training Loss: 0.10314280192057292 Test Loss: 0.36710061306423614\n",
      "Epoch: 13818 Training Loss: 0.10345757802327474 Test Loss: 0.3686488986545139\n",
      "Epoch: 13819 Training Loss: 0.10383270772298177 Test Loss: 0.37014767795138886\n",
      "Epoch: 13820 Training Loss: 0.10426639726426866 Test Loss: 0.37080940755208336\n",
      "Epoch: 13821 Training Loss: 0.10474286397298177 Test Loss: 0.371035400390625\n",
      "Epoch: 13822 Training Loss: 0.10524175262451171 Test Loss: 0.3707192654079861\n",
      "Epoch: 13823 Training Loss: 0.10570397440592448 Test Loss: 0.36942903645833336\n",
      "Epoch: 13824 Training Loss: 0.10609769185384114 Test Loss: 0.3683498806423611\n",
      "Epoch: 13825 Training Loss: 0.10633842044406468 Test Loss: 0.366899658203125\n",
      "Epoch: 13826 Training Loss: 0.10640206824408636 Test Loss: 0.36582245551215276\n",
      "Epoch: 13827 Training Loss: 0.106262085808648 Test Loss: 0.3655974934895833\n",
      "Epoch: 13828 Training Loss: 0.10599159325493707 Test Loss: 0.3657136501736111\n",
      "Epoch: 13829 Training Loss: 0.10563652716742622 Test Loss: 0.365616455078125\n",
      "Epoch: 13830 Training Loss: 0.10525183529324002 Test Loss: 0.36525276692708336\n",
      "Epoch: 13831 Training Loss: 0.10481516689724392 Test Loss: 0.36416357421875\n",
      "Epoch: 13832 Training Loss: 0.10436003536648221 Test Loss: 0.3623274197048611\n",
      "Epoch: 13833 Training Loss: 0.10389527214898003 Test Loss: 0.3600269911024306\n",
      "Epoch: 13834 Training Loss: 0.1034733140733507 Test Loss: 0.3579290093315972\n",
      "Epoch: 13835 Training Loss: 0.10311404418945312 Test Loss: 0.3566584201388889\n",
      "Epoch: 13836 Training Loss: 0.10285984717475044 Test Loss: 0.35626817491319446\n",
      "Epoch: 13837 Training Loss: 0.10274735514322916 Test Loss: 0.35660739474826386\n",
      "Epoch: 13838 Training Loss: 0.10274850718180338 Test Loss: 0.35729573567708334\n",
      "Epoch: 13839 Training Loss: 0.102872314453125 Test Loss: 0.3577719455295139\n",
      "Epoch: 13840 Training Loss: 0.1030785403781467 Test Loss: 0.3577326931423611\n",
      "Epoch: 13841 Training Loss: 0.10331893666585286 Test Loss: 0.35740321180555557\n",
      "Epoch: 13842 Training Loss: 0.10360229153103298 Test Loss: 0.3569309353298611\n",
      "Epoch: 13843 Training Loss: 0.1039537370469835 Test Loss: 0.35648912217881945\n",
      "Epoch: 13844 Training Loss: 0.10437555864122179 Test Loss: 0.356302734375\n",
      "Epoch: 13845 Training Loss: 0.10486482662624783 Test Loss: 0.3557334255642361\n",
      "Epoch: 13846 Training Loss: 0.10544712914360894 Test Loss: 0.35513916015625\n",
      "Epoch: 13847 Training Loss: 0.10614437018500435 Test Loss: 0.35450737847222225\n",
      "Epoch: 13848 Training Loss: 0.1069244876437717 Test Loss: 0.35452891710069445\n",
      "Epoch: 13849 Training Loss: 0.10771645016140408 Test Loss: 0.3553398980034722\n",
      "Epoch: 13850 Training Loss: 0.10852278221978082 Test Loss: 0.35810481770833336\n",
      "Epoch: 13851 Training Loss: 0.10933875105116102 Test Loss: 0.36156117078993055\n",
      "Epoch: 13852 Training Loss: 0.11006754302978515 Test Loss: 0.3645049099392361\n",
      "Epoch: 13853 Training Loss: 0.11064813995361328 Test Loss: 0.36551063368055553\n",
      "Epoch: 13854 Training Loss: 0.11096075015597873 Test Loss: 0.3648224826388889\n",
      "Epoch: 13855 Training Loss: 0.11092174190945095 Test Loss: 0.3628594292534722\n",
      "Epoch: 13856 Training Loss: 0.11042129516601562 Test Loss: 0.3605666232638889\n",
      "Epoch: 13857 Training Loss: 0.1094852049085829 Test Loss: 0.358481689453125\n",
      "Epoch: 13858 Training Loss: 0.1082898457845052 Test Loss: 0.3567196180555556\n",
      "Epoch: 13859 Training Loss: 0.10702891286214193 Test Loss: 0.35511507161458333\n",
      "Epoch: 13860 Training Loss: 0.10582598283555772 Test Loss: 0.35405997721354165\n",
      "Epoch: 13861 Training Loss: 0.1047567155626085 Test Loss: 0.3535316297743056\n",
      "Epoch: 13862 Training Loss: 0.10385554589165581 Test Loss: 0.3533329535590278\n",
      "Epoch: 13863 Training Loss: 0.10314811791314019 Test Loss: 0.35356038411458335\n",
      "Epoch: 13864 Training Loss: 0.10261095343695746 Test Loss: 0.353707275390625\n",
      "Epoch: 13865 Training Loss: 0.10222715335422092 Test Loss: 0.3540056423611111\n",
      "Epoch: 13866 Training Loss: 0.10199029286702474 Test Loss: 0.3540559353298611\n",
      "Epoch: 13867 Training Loss: 0.10184810299343533 Test Loss: 0.3538320041232639\n",
      "Epoch: 13868 Training Loss: 0.10176772138807509 Test Loss: 0.3537225206163194\n",
      "Epoch: 13869 Training Loss: 0.10173751152886285 Test Loss: 0.35360576714409725\n",
      "Epoch: 13870 Training Loss: 0.10173722161187065 Test Loss: 0.35387847222222224\n",
      "Epoch: 13871 Training Loss: 0.10175157080756293 Test Loss: 0.3539644639756944\n",
      "Epoch: 13872 Training Loss: 0.10175550503200954 Test Loss: 0.35394504123263887\n",
      "Epoch: 13873 Training Loss: 0.10175203111436631 Test Loss: 0.353923828125\n",
      "Epoch: 13874 Training Loss: 0.1017514419555664 Test Loss: 0.35391536458333334\n",
      "Epoch: 13875 Training Loss: 0.10174803246392145 Test Loss: 0.35363972981770836\n",
      "Epoch: 13876 Training Loss: 0.10172779846191406 Test Loss: 0.35323936631944447\n",
      "Epoch: 13877 Training Loss: 0.10171513960096572 Test Loss: 0.3529253743489583\n",
      "Epoch: 13878 Training Loss: 0.10169312116834853 Test Loss: 0.3524675564236111\n",
      "Epoch: 13879 Training Loss: 0.10166750844319661 Test Loss: 0.35203160264756944\n",
      "Epoch: 13880 Training Loss: 0.10164958953857423 Test Loss: 0.3517890896267361\n",
      "Epoch: 13881 Training Loss: 0.10164635891384549 Test Loss: 0.3519443630642361\n",
      "Epoch: 13882 Training Loss: 0.10167519887288412 Test Loss: 0.35249055989583333\n",
      "Epoch: 13883 Training Loss: 0.1017282231648763 Test Loss: 0.3533371039496528\n",
      "Epoch: 13884 Training Loss: 0.10180946350097657 Test Loss: 0.3542165798611111\n",
      "Epoch: 13885 Training Loss: 0.10190133751763238 Test Loss: 0.3550463324652778\n",
      "Epoch: 13886 Training Loss: 0.10197432454427083 Test Loss: 0.3555927734375\n",
      "Epoch: 13887 Training Loss: 0.10202659946017795 Test Loss: 0.35578841145833334\n",
      "Epoch: 13888 Training Loss: 0.1020719977484809 Test Loss: 0.3557327473958333\n",
      "Epoch: 13889 Training Loss: 0.10209315490722656 Test Loss: 0.35532850477430555\n",
      "Epoch: 13890 Training Loss: 0.102068968878852 Test Loss: 0.3548071831597222\n",
      "Epoch: 13891 Training Loss: 0.10199110582139757 Test Loss: 0.35434461805555556\n",
      "Epoch: 13892 Training Loss: 0.10188958316379124 Test Loss: 0.35448752170138886\n",
      "Epoch: 13893 Training Loss: 0.10178546057807075 Test Loss: 0.3550848795572917\n",
      "Epoch: 13894 Training Loss: 0.10168934631347656 Test Loss: 0.3560710720486111\n",
      "Epoch: 13895 Training Loss: 0.10161634487575955 Test Loss: 0.35737339952256947\n",
      "Epoch: 13896 Training Loss: 0.10158103434244792 Test Loss: 0.35829069010416664\n",
      "Epoch: 13897 Training Loss: 0.10159022013346354 Test Loss: 0.35910975477430557\n",
      "Epoch: 13898 Training Loss: 0.10162810516357422 Test Loss: 0.3598157009548611\n",
      "Epoch: 13899 Training Loss: 0.10169690110948351 Test Loss: 0.36028369140625\n",
      "Epoch: 13900 Training Loss: 0.1017860616048177 Test Loss: 0.36063487413194445\n",
      "Epoch: 13901 Training Loss: 0.10187819163004558 Test Loss: 0.36087939453125\n",
      "Epoch: 13902 Training Loss: 0.10196402740478516 Test Loss: 0.36086973741319445\n",
      "Epoch: 13903 Training Loss: 0.10205086093478732 Test Loss: 0.36067645941840276\n",
      "Epoch: 13904 Training Loss: 0.10213717651367188 Test Loss: 0.36047205946180555\n",
      "Epoch: 13905 Training Loss: 0.10223433176676432 Test Loss: 0.36052756076388887\n",
      "Epoch: 13906 Training Loss: 0.10234613884819878 Test Loss: 0.3604462890625\n",
      "Epoch: 13907 Training Loss: 0.10245960066053603 Test Loss: 0.3603738606770833\n",
      "Epoch: 13908 Training Loss: 0.10256445058186849 Test Loss: 0.3604801974826389\n",
      "Epoch: 13909 Training Loss: 0.10266332414415147 Test Loss: 0.3605771213107639\n",
      "Epoch: 13910 Training Loss: 0.10277070532904731 Test Loss: 0.3607570529513889\n",
      "Epoch: 13911 Training Loss: 0.1028874265882704 Test Loss: 0.3611628146701389\n",
      "Epoch: 13912 Training Loss: 0.10297079637315538 Test Loss: 0.36156057400173613\n",
      "Epoch: 13913 Training Loss: 0.10304086303710938 Test Loss: 0.36178759765625\n",
      "Epoch: 13914 Training Loss: 0.10309386020236545 Test Loss: 0.36156949869791666\n",
      "Epoch: 13915 Training Loss: 0.10313041347927518 Test Loss: 0.3608626844618056\n",
      "Epoch: 13916 Training Loss: 0.10313379245334202 Test Loss: 0.3594907769097222\n",
      "Epoch: 13917 Training Loss: 0.10308664872911241 Test Loss: 0.35843763563368053\n",
      "Epoch: 13918 Training Loss: 0.1029836179945204 Test Loss: 0.3577236328125\n",
      "Epoch: 13919 Training Loss: 0.10285172271728515 Test Loss: 0.3578722059461806\n",
      "Epoch: 13920 Training Loss: 0.10270257398817274 Test Loss: 0.3582272677951389\n",
      "Epoch: 13921 Training Loss: 0.10255731879340278 Test Loss: 0.35894487847222223\n",
      "Epoch: 13922 Training Loss: 0.10244158342149523 Test Loss: 0.35965234375\n",
      "Epoch: 13923 Training Loss: 0.10235730573866103 Test Loss: 0.36025374348958333\n",
      "Epoch: 13924 Training Loss: 0.10228441111246744 Test Loss: 0.3604768337673611\n",
      "Epoch: 13925 Training Loss: 0.1022611567179362 Test Loss: 0.3604873318142361\n",
      "Epoch: 13926 Training Loss: 0.10229765828450521 Test Loss: 0.3602484809027778\n",
      "Epoch: 13927 Training Loss: 0.10241398620605469 Test Loss: 0.35957598198784724\n",
      "Epoch: 13928 Training Loss: 0.10257768334282769 Test Loss: 0.35889043511284724\n",
      "Epoch: 13929 Training Loss: 0.10276879543728298 Test Loss: 0.3578166232638889\n",
      "Epoch: 13930 Training Loss: 0.1029451658460829 Test Loss: 0.3570328776041667\n",
      "Epoch: 13931 Training Loss: 0.1030744849310981 Test Loss: 0.35684231228298613\n",
      "Epoch: 13932 Training Loss: 0.1031535144382053 Test Loss: 0.35731304253472224\n",
      "Epoch: 13933 Training Loss: 0.10320211198594835 Test Loss: 0.3584069552951389\n",
      "Epoch: 13934 Training Loss: 0.10318829430474176 Test Loss: 0.3601110568576389\n",
      "Epoch: 13935 Training Loss: 0.10312092759874132 Test Loss: 0.3621887478298611\n",
      "Epoch: 13936 Training Loss: 0.10301444583468967 Test Loss: 0.3638070746527778\n",
      "Epoch: 13937 Training Loss: 0.10287448374430339 Test Loss: 0.3645507269965278\n",
      "Epoch: 13938 Training Loss: 0.10270188988579644 Test Loss: 0.3642521701388889\n",
      "Epoch: 13939 Training Loss: 0.10249487898084852 Test Loss: 0.362865966796875\n",
      "Epoch: 13940 Training Loss: 0.10226688554551866 Test Loss: 0.36109798177083335\n",
      "Epoch: 13941 Training Loss: 0.10204413519965277 Test Loss: 0.35934586588541667\n",
      "Epoch: 13942 Training Loss: 0.10182747565375434 Test Loss: 0.35775664605034724\n",
      "Epoch: 13943 Training Loss: 0.10165147145589193 Test Loss: 0.35668199327256944\n",
      "Epoch: 13944 Training Loss: 0.10153175862630208 Test Loss: 0.3562407769097222\n",
      "Epoch: 13945 Training Loss: 0.10148526763916016 Test Loss: 0.35631043836805554\n",
      "Epoch: 13946 Training Loss: 0.10149348534478081 Test Loss: 0.35630989583333333\n",
      "Epoch: 13947 Training Loss: 0.10153422970241971 Test Loss: 0.3563930121527778\n",
      "Epoch: 13948 Training Loss: 0.1015993406507704 Test Loss: 0.3566432834201389\n",
      "Epoch: 13949 Training Loss: 0.1016916275024414 Test Loss: 0.35673828125\n",
      "Epoch: 13950 Training Loss: 0.10182965257432726 Test Loss: 0.35714735243055556\n",
      "Epoch: 13951 Training Loss: 0.1020168702867296 Test Loss: 0.3574121365017361\n",
      "Epoch: 13952 Training Loss: 0.10225240580240885 Test Loss: 0.35806496853298614\n",
      "Epoch: 13953 Training Loss: 0.10255689409044054 Test Loss: 0.35914884440104167\n",
      "Epoch: 13954 Training Loss: 0.10293638865152994 Test Loss: 0.36066078016493053\n",
      "Epoch: 13955 Training Loss: 0.10337078603108724 Test Loss: 0.36223589409722223\n",
      "Epoch: 13956 Training Loss: 0.10386332787407769 Test Loss: 0.36388419596354166\n",
      "Epoch: 13957 Training Loss: 0.1043660176595052 Test Loss: 0.3658194986979167\n",
      "Epoch: 13958 Training Loss: 0.10484329817030165 Test Loss: 0.36812483723958334\n",
      "Epoch: 13959 Training Loss: 0.10528366258409289 Test Loss: 0.3699472927517361\n",
      "Epoch: 13960 Training Loss: 0.10569508022732205 Test Loss: 0.37174934895833334\n",
      "Epoch: 13961 Training Loss: 0.10601148817274306 Test Loss: 0.37252525499131944\n",
      "Epoch: 13962 Training Loss: 0.106301634894477 Test Loss: 0.3726951768663194\n",
      "Epoch: 13963 Training Loss: 0.10657193247477213 Test Loss: 0.3719921332465278\n",
      "Epoch: 13964 Training Loss: 0.10677959611680772 Test Loss: 0.37092518446180556\n",
      "Epoch: 13965 Training Loss: 0.10688162146674263 Test Loss: 0.36951323784722223\n",
      "Epoch: 13966 Training Loss: 0.10687965393066406 Test Loss: 0.368529541015625\n",
      "Epoch: 13967 Training Loss: 0.10674463229709201 Test Loss: 0.3678153483072917\n",
      "Epoch: 13968 Training Loss: 0.10648689863416884 Test Loss: 0.36768419053819446\n",
      "Epoch: 13969 Training Loss: 0.10613799201117621 Test Loss: 0.3671117892795139\n",
      "Epoch: 13970 Training Loss: 0.10570962863498264 Test Loss: 0.36613818359375\n",
      "Epoch: 13971 Training Loss: 0.1052246568467882 Test Loss: 0.365044189453125\n",
      "Epoch: 13972 Training Loss: 0.1047176496717665 Test Loss: 0.3639970160590278\n",
      "Epoch: 13973 Training Loss: 0.10422555202907986 Test Loss: 0.36258997938368054\n",
      "Epoch: 13974 Training Loss: 0.10377242024739583 Test Loss: 0.3612887369791667\n",
      "Epoch: 13975 Training Loss: 0.10336461469862196 Test Loss: 0.3597983940972222\n",
      "Epoch: 13976 Training Loss: 0.10299502393934462 Test Loss: 0.3588910590277778\n",
      "Epoch: 13977 Training Loss: 0.10265034739176432 Test Loss: 0.3580451388888889\n",
      "Epoch: 13978 Training Loss: 0.10235879262288411 Test Loss: 0.35752918836805553\n",
      "Epoch: 13979 Training Loss: 0.10213597276475694 Test Loss: 0.35693684895833333\n",
      "Epoch: 13980 Training Loss: 0.1020023447672526 Test Loss: 0.3569482421875\n",
      "Epoch: 13981 Training Loss: 0.10197607760959201 Test Loss: 0.3570739474826389\n",
      "Epoch: 13982 Training Loss: 0.10201830376519097 Test Loss: 0.357501220703125\n",
      "Epoch: 13983 Training Loss: 0.102122681511773 Test Loss: 0.35800244140625\n",
      "Epoch: 13984 Training Loss: 0.10226641252305772 Test Loss: 0.3584143880208333\n",
      "Epoch: 13985 Training Loss: 0.10241947343614366 Test Loss: 0.35859521484375\n",
      "Epoch: 13986 Training Loss: 0.10259166632758246 Test Loss: 0.35860530598958335\n",
      "Epoch: 13987 Training Loss: 0.1027643797132704 Test Loss: 0.3583364529079861\n",
      "Epoch: 13988 Training Loss: 0.10289468977186415 Test Loss: 0.35788623046875\n",
      "Epoch: 13989 Training Loss: 0.10300289238823784 Test Loss: 0.3574013671875\n",
      "Epoch: 13990 Training Loss: 0.10309020487467448 Test Loss: 0.3571068793402778\n",
      "Epoch: 13991 Training Loss: 0.10317369333902995 Test Loss: 0.3564177517361111\n",
      "Epoch: 13992 Training Loss: 0.1032613033718533 Test Loss: 0.35612586805555557\n",
      "Epoch: 13993 Training Loss: 0.10333519066704644 Test Loss: 0.35594254557291666\n",
      "Epoch: 13994 Training Loss: 0.10338533528645834 Test Loss: 0.35597479926215275\n",
      "Epoch: 13995 Training Loss: 0.10344761488172743 Test Loss: 0.35606920030381944\n",
      "Epoch: 13996 Training Loss: 0.10352874077690973 Test Loss: 0.3561547037760417\n",
      "Epoch: 13997 Training Loss: 0.10361679416232639 Test Loss: 0.3564408637152778\n",
      "Epoch: 13998 Training Loss: 0.10370203569200304 Test Loss: 0.35685123697916665\n",
      "Epoch: 13999 Training Loss: 0.10377389950222439 Test Loss: 0.3575085720486111\n",
      "Epoch: 14000 Training Loss: 0.10387371571858724 Test Loss: 0.3580285915798611\n",
      "Epoch: 14001 Training Loss: 0.10398946719699435 Test Loss: 0.35858745659722224\n",
      "Epoch: 14002 Training Loss: 0.10410320536295573 Test Loss: 0.3588805881076389\n",
      "Epoch: 14003 Training Loss: 0.1042123294406467 Test Loss: 0.3588595377604167\n",
      "Epoch: 14004 Training Loss: 0.1042802225748698 Test Loss: 0.3587981228298611\n",
      "Epoch: 14005 Training Loss: 0.10427374437120225 Test Loss: 0.35852528211805557\n",
      "Epoch: 14006 Training Loss: 0.10417653232150607 Test Loss: 0.35815706380208334\n",
      "Epoch: 14007 Training Loss: 0.1040299292670356 Test Loss: 0.35764244249131943\n",
      "Epoch: 14008 Training Loss: 0.1038617426554362 Test Loss: 0.35721373155381947\n",
      "Epoch: 14009 Training Loss: 0.10371757253011067 Test Loss: 0.3569786783854167\n",
      "Epoch: 14010 Training Loss: 0.10361497921413845 Test Loss: 0.35698442925347224\n",
      "Epoch: 14011 Training Loss: 0.10355737474229601 Test Loss: 0.35754576280381944\n",
      "Epoch: 14012 Training Loss: 0.10353599209255643 Test Loss: 0.35837472873263887\n",
      "Epoch: 14013 Training Loss: 0.10350134870741103 Test Loss: 0.3587366807725694\n",
      "Epoch: 14014 Training Loss: 0.10343660142686632 Test Loss: 0.35919108072916667\n",
      "Epoch: 14015 Training Loss: 0.10334325578477647 Test Loss: 0.3593849826388889\n",
      "Epoch: 14016 Training Loss: 0.10323614586724175 Test Loss: 0.35895765516493056\n",
      "Epoch: 14017 Training Loss: 0.10314272816975911 Test Loss: 0.3580859375\n",
      "Epoch: 14018 Training Loss: 0.10306752268473307 Test Loss: 0.35725998263888886\n",
      "Epoch: 14019 Training Loss: 0.10302377743191189 Test Loss: 0.35620005967881946\n",
      "Epoch: 14020 Training Loss: 0.1029507556491428 Test Loss: 0.35538056098090276\n",
      "Epoch: 14021 Training Loss: 0.1028410890367296 Test Loss: 0.35498063151041664\n",
      "Epoch: 14022 Training Loss: 0.10266231875949436 Test Loss: 0.35508824327256944\n",
      "Epoch: 14023 Training Loss: 0.10243119557698567 Test Loss: 0.3557277560763889\n",
      "Epoch: 14024 Training Loss: 0.10213824208577474 Test Loss: 0.35673980034722225\n",
      "Epoch: 14025 Training Loss: 0.10182828352186415 Test Loss: 0.35752652994791667\n",
      "Epoch: 14026 Training Loss: 0.10153602684868707 Test Loss: 0.35874169921875\n",
      "Epoch: 14027 Training Loss: 0.10128962029351128 Test Loss: 0.3594388834635417\n",
      "Epoch: 14028 Training Loss: 0.10109495120578342 Test Loss: 0.36009852430555556\n",
      "Epoch: 14029 Training Loss: 0.10092882283528645 Test Loss: 0.36077159288194444\n",
      "Epoch: 14030 Training Loss: 0.10080261400010851 Test Loss: 0.3616233723958333\n",
      "Epoch: 14031 Training Loss: 0.10069774881998698 Test Loss: 0.3621316189236111\n",
      "Epoch: 14032 Training Loss: 0.10059314558241103 Test Loss: 0.3623448350694444\n",
      "Epoch: 14033 Training Loss: 0.10048977406819662 Test Loss: 0.36188704427083335\n",
      "Epoch: 14034 Training Loss: 0.10036836581759982 Test Loss: 0.3606653103298611\n",
      "Epoch: 14035 Training Loss: 0.10025586615668403 Test Loss: 0.35899102105034725\n",
      "Epoch: 14036 Training Loss: 0.1001626697116428 Test Loss: 0.3570207248263889\n",
      "Epoch: 14037 Training Loss: 0.10008925289577908 Test Loss: 0.35538427734375\n",
      "Epoch: 14038 Training Loss: 0.09999869283040365 Test Loss: 0.35420583767361113\n",
      "Epoch: 14039 Training Loss: 0.09989271799723308 Test Loss: 0.3537290581597222\n",
      "Epoch: 14040 Training Loss: 0.09978216467963325 Test Loss: 0.35394688585069445\n",
      "Epoch: 14041 Training Loss: 0.09966689215766059 Test Loss: 0.35455354817708334\n",
      "Epoch: 14042 Training Loss: 0.09954320610894098 Test Loss: 0.35561962890625\n",
      "Epoch: 14043 Training Loss: 0.09944591013590495 Test Loss: 0.35705485026041667\n",
      "Epoch: 14044 Training Loss: 0.09938551923963759 Test Loss: 0.35839246961805554\n",
      "Epoch: 14045 Training Loss: 0.09932804446750217 Test Loss: 0.35953325737847225\n",
      "Epoch: 14046 Training Loss: 0.0992769775390625 Test Loss: 0.35966986762152775\n",
      "Epoch: 14047 Training Loss: 0.09924151950412326 Test Loss: 0.36002284071180557\n",
      "Epoch: 14048 Training Loss: 0.09922289445665147 Test Loss: 0.36006182183159724\n",
      "Epoch: 14049 Training Loss: 0.09921868387858072 Test Loss: 0.3598825412326389\n",
      "Epoch: 14050 Training Loss: 0.09921966637505425 Test Loss: 0.3604437662760417\n",
      "Epoch: 14051 Training Loss: 0.09922450680202909 Test Loss: 0.36093885633680556\n",
      "Epoch: 14052 Training Loss: 0.09922567155626084 Test Loss: 0.36148402235243055\n",
      "Epoch: 14053 Training Loss: 0.09922034200032552 Test Loss: 0.36185167100694443\n",
      "Epoch: 14054 Training Loss: 0.0992126710679796 Test Loss: 0.3617007107204861\n",
      "Epoch: 14055 Training Loss: 0.09921322886149088 Test Loss: 0.3612251519097222\n",
      "Epoch: 14056 Training Loss: 0.09921307966444227 Test Loss: 0.36026318359375\n",
      "Epoch: 14057 Training Loss: 0.09923080952962239 Test Loss: 0.35907828776041667\n",
      "Epoch: 14058 Training Loss: 0.09924422030978733 Test Loss: 0.35801085069444444\n",
      "Epoch: 14059 Training Loss: 0.0992586915757921 Test Loss: 0.35711208767361113\n",
      "Epoch: 14060 Training Loss: 0.09926239437527126 Test Loss: 0.35639078776041666\n",
      "Epoch: 14061 Training Loss: 0.09925631035698784 Test Loss: 0.3559113498263889\n",
      "Epoch: 14062 Training Loss: 0.09925355360243056 Test Loss: 0.3561560872395833\n",
      "Epoch: 14063 Training Loss: 0.09925863308376737 Test Loss: 0.35625504557291665\n",
      "Epoch: 14064 Training Loss: 0.09926317935519749 Test Loss: 0.3566640082465278\n",
      "Epoch: 14065 Training Loss: 0.09924336581759982 Test Loss: 0.35668020290798613\n",
      "Epoch: 14066 Training Loss: 0.09924219767252604 Test Loss: 0.3564224175347222\n",
      "Epoch: 14067 Training Loss: 0.09925531853569879 Test Loss: 0.35626725260416664\n",
      "Epoch: 14068 Training Loss: 0.09927389017740886 Test Loss: 0.3559833984375\n",
      "Epoch: 14069 Training Loss: 0.09929136657714843 Test Loss: 0.3558127712673611\n",
      "Epoch: 14070 Training Loss: 0.09931306881374782 Test Loss: 0.3560354275173611\n",
      "Epoch: 14071 Training Loss: 0.09933472951253255 Test Loss: 0.35691761610243056\n",
      "Epoch: 14072 Training Loss: 0.09936828274197049 Test Loss: 0.35784065755208333\n",
      "Epoch: 14073 Training Loss: 0.09940242513020833 Test Loss: 0.35861168077256944\n",
      "Epoch: 14074 Training Loss: 0.0994744644165039 Test Loss: 0.35896251085069447\n",
      "Epoch: 14075 Training Loss: 0.0996072040134006 Test Loss: 0.3589975043402778\n",
      "Epoch: 14076 Training Loss: 0.09979568481445313 Test Loss: 0.35821525065104165\n",
      "Epoch: 14077 Training Loss: 0.10002567291259766 Test Loss: 0.3570030110677083\n",
      "Epoch: 14078 Training Loss: 0.10025211588541667 Test Loss: 0.35611702473958334\n",
      "Epoch: 14079 Training Loss: 0.10046900346544053 Test Loss: 0.3556783854166667\n",
      "Epoch: 14080 Training Loss: 0.1006374003092448 Test Loss: 0.3557615559895833\n",
      "Epoch: 14081 Training Loss: 0.10075138854980469 Test Loss: 0.35633976236979165\n",
      "Epoch: 14082 Training Loss: 0.10084584723578559 Test Loss: 0.35713726128472223\n",
      "Epoch: 14083 Training Loss: 0.10094155799018012 Test Loss: 0.3577179904513889\n",
      "Epoch: 14084 Training Loss: 0.10102156490749784 Test Loss: 0.3576364474826389\n",
      "Epoch: 14085 Training Loss: 0.10107867346869574 Test Loss: 0.3568565538194444\n",
      "Epoch: 14086 Training Loss: 0.10109635501437716 Test Loss: 0.3558366427951389\n",
      "Epoch: 14087 Training Loss: 0.10104130469428169 Test Loss: 0.35508900282118055\n",
      "Epoch: 14088 Training Loss: 0.10092538536919488 Test Loss: 0.35471845160590276\n",
      "Epoch: 14089 Training Loss: 0.10075252787272135 Test Loss: 0.3549896375868056\n",
      "Epoch: 14090 Training Loss: 0.10056187693277994 Test Loss: 0.3554259982638889\n",
      "Epoch: 14091 Training Loss: 0.10040955268012153 Test Loss: 0.3556804470486111\n",
      "Epoch: 14092 Training Loss: 0.10033156500922309 Test Loss: 0.3553059353298611\n",
      "Epoch: 14093 Training Loss: 0.10032423994276259 Test Loss: 0.35453434244791665\n",
      "Epoch: 14094 Training Loss: 0.10037694125705295 Test Loss: 0.3538009982638889\n",
      "Epoch: 14095 Training Loss: 0.10044003041585287 Test Loss: 0.353155517578125\n",
      "Epoch: 14096 Training Loss: 0.1005069842868381 Test Loss: 0.3529736328125\n",
      "Epoch: 14097 Training Loss: 0.1005694113837348 Test Loss: 0.3533139105902778\n",
      "Epoch: 14098 Training Loss: 0.1006143781873915 Test Loss: 0.35434901258680557\n",
      "Epoch: 14099 Training Loss: 0.10065548706054687 Test Loss: 0.35588788519965275\n",
      "Epoch: 14100 Training Loss: 0.10071681891547309 Test Loss: 0.35737033420138886\n",
      "Epoch: 14101 Training Loss: 0.10080711195203992 Test Loss: 0.3583945855034722\n",
      "Epoch: 14102 Training Loss: 0.10096488867865669 Test Loss: 0.35848323567708335\n",
      "Epoch: 14103 Training Loss: 0.10114524586995442 Test Loss: 0.35814737955729165\n",
      "Epoch: 14104 Training Loss: 0.10130554623074002 Test Loss: 0.3579799262152778\n",
      "Epoch: 14105 Training Loss: 0.10144283040364584 Test Loss: 0.35789111328125\n",
      "Epoch: 14106 Training Loss: 0.10153360493977864 Test Loss: 0.35766221788194447\n",
      "Epoch: 14107 Training Loss: 0.10156337483723958 Test Loss: 0.35715093315972224\n",
      "Epoch: 14108 Training Loss: 0.10160901896158854 Test Loss: 0.3567746853298611\n",
      "Epoch: 14109 Training Loss: 0.10168548753526475 Test Loss: 0.35645939127604165\n",
      "Epoch: 14110 Training Loss: 0.10178710683186849 Test Loss: 0.3563922526041667\n",
      "Epoch: 14111 Training Loss: 0.1019266357421875 Test Loss: 0.35609288194444444\n",
      "Epoch: 14112 Training Loss: 0.10208159637451172 Test Loss: 0.3563306206597222\n",
      "Epoch: 14113 Training Loss: 0.10217788611518012 Test Loss: 0.3566015353732639\n",
      "Epoch: 14114 Training Loss: 0.10219843122694228 Test Loss: 0.3566885036892361\n",
      "Epoch: 14115 Training Loss: 0.10219923061794704 Test Loss: 0.35611173502604165\n",
      "Epoch: 14116 Training Loss: 0.10217962816026475 Test Loss: 0.3553307834201389\n",
      "Epoch: 14117 Training Loss: 0.10210959625244141 Test Loss: 0.3542112087673611\n",
      "Epoch: 14118 Training Loss: 0.10199474249945746 Test Loss: 0.35372059461805555\n",
      "Epoch: 14119 Training Loss: 0.10187627834743923 Test Loss: 0.35366248914930554\n",
      "Epoch: 14120 Training Loss: 0.10178573438856336 Test Loss: 0.3543713650173611\n",
      "Epoch: 14121 Training Loss: 0.10174771711561414 Test Loss: 0.35538731553819447\n",
      "Epoch: 14122 Training Loss: 0.10173322041829427 Test Loss: 0.3561970486111111\n",
      "Epoch: 14123 Training Loss: 0.10172854190402561 Test Loss: 0.35708930121527777\n",
      "Epoch: 14124 Training Loss: 0.10173891279432509 Test Loss: 0.35766134982638886\n",
      "Epoch: 14125 Training Loss: 0.10176770697699652 Test Loss: 0.3579074164496528\n",
      "Epoch: 14126 Training Loss: 0.1018555925157335 Test Loss: 0.35749555121527776\n",
      "Epoch: 14127 Training Loss: 0.10198199886745876 Test Loss: 0.3571306423611111\n",
      "Epoch: 14128 Training Loss: 0.10211396874321832 Test Loss: 0.35719666883680556\n",
      "Epoch: 14129 Training Loss: 0.10226147715250651 Test Loss: 0.3577070041232639\n",
      "Epoch: 14130 Training Loss: 0.10245012919108074 Test Loss: 0.35882850477430556\n",
      "Epoch: 14131 Training Loss: 0.10267809295654297 Test Loss: 0.3595826822916667\n",
      "Epoch: 14132 Training Loss: 0.10284990861680772 Test Loss: 0.35946082899305554\n",
      "Epoch: 14133 Training Loss: 0.10295683203803169 Test Loss: 0.35872010633680557\n",
      "Epoch: 14134 Training Loss: 0.10298957655164931 Test Loss: 0.3573416883680556\n",
      "Epoch: 14135 Training Loss: 0.10293969048394097 Test Loss: 0.35626671006944444\n",
      "Epoch: 14136 Training Loss: 0.10281981658935548 Test Loss: 0.3562086046006944\n",
      "Epoch: 14137 Training Loss: 0.10261799791124132 Test Loss: 0.3569475368923611\n",
      "Epoch: 14138 Training Loss: 0.10237476942274305 Test Loss: 0.35840714518229166\n",
      "Epoch: 14139 Training Loss: 0.10208437008327909 Test Loss: 0.35997837999131943\n",
      "Epoch: 14140 Training Loss: 0.10179895782470703 Test Loss: 0.361093017578125\n",
      "Epoch: 14141 Training Loss: 0.10148534393310547 Test Loss: 0.361388427734375\n",
      "Epoch: 14142 Training Loss: 0.10116139729817708 Test Loss: 0.36130875651041666\n",
      "Epoch: 14143 Training Loss: 0.1008808568318685 Test Loss: 0.3603847113715278\n",
      "Epoch: 14144 Training Loss: 0.10064955563015408 Test Loss: 0.3592333713107639\n",
      "Epoch: 14145 Training Loss: 0.10045013597276475 Test Loss: 0.35850637478298614\n",
      "Epoch: 14146 Training Loss: 0.10025113423665365 Test Loss: 0.3577368706597222\n",
      "Epoch: 14147 Training Loss: 0.10008022901746962 Test Loss: 0.3575039605034722\n",
      "Epoch: 14148 Training Loss: 0.09996438429090712 Test Loss: 0.3574118923611111\n",
      "Epoch: 14149 Training Loss: 0.09993023003472222 Test Loss: 0.35748502604166665\n",
      "Epoch: 14150 Training Loss: 0.09998705715603298 Test Loss: 0.3575784505208333\n",
      "Epoch: 14151 Training Loss: 0.10009813096788195 Test Loss: 0.3578978949652778\n",
      "Epoch: 14152 Training Loss: 0.10024116770426432 Test Loss: 0.3584511990017361\n",
      "Epoch: 14153 Training Loss: 0.10039035627577039 Test Loss: 0.35890928819444445\n",
      "Epoch: 14154 Training Loss: 0.10053998141818576 Test Loss: 0.35940459526909724\n",
      "Epoch: 14155 Training Loss: 0.10069290161132813 Test Loss: 0.36011322699652776\n",
      "Epoch: 14156 Training Loss: 0.10085898844401042 Test Loss: 0.36123738606770833\n",
      "Epoch: 14157 Training Loss: 0.10104946560329861 Test Loss: 0.36260411241319446\n",
      "Epoch: 14158 Training Loss: 0.10126904042561849 Test Loss: 0.3637558322482639\n",
      "Epoch: 14159 Training Loss: 0.10148270077175564 Test Loss: 0.36490852864583334\n",
      "Epoch: 14160 Training Loss: 0.10164231872558593 Test Loss: 0.36557253689236113\n",
      "Epoch: 14161 Training Loss: 0.10175724453396268 Test Loss: 0.3659688042534722\n",
      "Epoch: 14162 Training Loss: 0.10184446970621745 Test Loss: 0.36574492730034724\n",
      "Epoch: 14163 Training Loss: 0.10190697818332248 Test Loss: 0.36512646484375\n",
      "Epoch: 14164 Training Loss: 0.10192249467637804 Test Loss: 0.3645204535590278\n",
      "Epoch: 14165 Training Loss: 0.10188628980848524 Test Loss: 0.3641721462673611\n",
      "Epoch: 14166 Training Loss: 0.10182738833957249 Test Loss: 0.36351041666666667\n",
      "Epoch: 14167 Training Loss: 0.10174254438612196 Test Loss: 0.3629812282986111\n",
      "Epoch: 14168 Training Loss: 0.10163894738091363 Test Loss: 0.36147794596354166\n",
      "Epoch: 14169 Training Loss: 0.10153568013509115 Test Loss: 0.36033289930555557\n",
      "Epoch: 14170 Training Loss: 0.10144341956244575 Test Loss: 0.3586234809027778\n",
      "Epoch: 14171 Training Loss: 0.10135384453667534 Test Loss: 0.3571394314236111\n",
      "Epoch: 14172 Training Loss: 0.10127952829996745 Test Loss: 0.35599818250868054\n",
      "Epoch: 14173 Training Loss: 0.10121463012695313 Test Loss: 0.3551656629774306\n",
      "Epoch: 14174 Training Loss: 0.10114292483859592 Test Loss: 0.35473497178819446\n",
      "Epoch: 14175 Training Loss: 0.10107319895426432 Test Loss: 0.3550266927083333\n",
      "Epoch: 14176 Training Loss: 0.10103163401285807 Test Loss: 0.3557183159722222\n",
      "Epoch: 14177 Training Loss: 0.10102237616644966 Test Loss: 0.3563942057291667\n",
      "Epoch: 14178 Training Loss: 0.10105858018663194 Test Loss: 0.3571039767795139\n",
      "Epoch: 14179 Training Loss: 0.10111176469590928 Test Loss: 0.3574463161892361\n",
      "Epoch: 14180 Training Loss: 0.10118528662787543 Test Loss: 0.35725504557291665\n",
      "Epoch: 14181 Training Loss: 0.1012271474202474 Test Loss: 0.35673027886284725\n",
      "Epoch: 14182 Training Loss: 0.10124024709065756 Test Loss: 0.3562579752604167\n",
      "Epoch: 14183 Training Loss: 0.10123545328776042 Test Loss: 0.35597116427951386\n",
      "Epoch: 14184 Training Loss: 0.10122094133165148 Test Loss: 0.3563807779947917\n",
      "Epoch: 14185 Training Loss: 0.10121162584092883 Test Loss: 0.356793701171875\n",
      "Epoch: 14186 Training Loss: 0.10121820831298828 Test Loss: 0.35738395182291666\n",
      "Epoch: 14187 Training Loss: 0.10129065450032552 Test Loss: 0.3578386501736111\n",
      "Epoch: 14188 Training Loss: 0.10141541459825304 Test Loss: 0.35810888671875\n",
      "Epoch: 14189 Training Loss: 0.1015773951212565 Test Loss: 0.35835921223958334\n",
      "Epoch: 14190 Training Loss: 0.10177815500895182 Test Loss: 0.3586831325954861\n",
      "Epoch: 14191 Training Loss: 0.10201017167833117 Test Loss: 0.3583153754340278\n",
      "Epoch: 14192 Training Loss: 0.10227955118815105 Test Loss: 0.3575363498263889\n",
      "Epoch: 14193 Training Loss: 0.10261807081434461 Test Loss: 0.35722184244791666\n",
      "Epoch: 14194 Training Loss: 0.10300365617540147 Test Loss: 0.3569168023003472\n",
      "Epoch: 14195 Training Loss: 0.10336860148111979 Test Loss: 0.3576117078993056\n",
      "Epoch: 14196 Training Loss: 0.10367598046196831 Test Loss: 0.3589835611979167\n",
      "Epoch: 14197 Training Loss: 0.1039320788913303 Test Loss: 0.3612398003472222\n",
      "Epoch: 14198 Training Loss: 0.10417487250434028 Test Loss: 0.36435498046875\n",
      "Epoch: 14199 Training Loss: 0.1043680665757921 Test Loss: 0.3675468207465278\n",
      "Epoch: 14200 Training Loss: 0.10451624891493055 Test Loss: 0.3703045247395833\n",
      "Epoch: 14201 Training Loss: 0.10461383480495877 Test Loss: 0.3717710774739583\n",
      "Epoch: 14202 Training Loss: 0.10465969933403863 Test Loss: 0.3722973090277778\n",
      "Epoch: 14203 Training Loss: 0.1046727549235026 Test Loss: 0.37152864583333334\n",
      "Epoch: 14204 Training Loss: 0.10462831370035808 Test Loss: 0.3698273111979167\n",
      "Epoch: 14205 Training Loss: 0.10448816341824002 Test Loss: 0.3675111219618056\n",
      "Epoch: 14206 Training Loss: 0.10422806040445963 Test Loss: 0.3651115180121528\n",
      "Epoch: 14207 Training Loss: 0.1038769768608941 Test Loss: 0.36286260308159723\n",
      "Epoch: 14208 Training Loss: 0.10344796668158637 Test Loss: 0.3610743543836806\n",
      "Epoch: 14209 Training Loss: 0.1030142084757487 Test Loss: 0.3594226345486111\n",
      "Epoch: 14210 Training Loss: 0.10260347578260634 Test Loss: 0.3584049750434028\n",
      "Epoch: 14211 Training Loss: 0.10222206454806858 Test Loss: 0.3573351779513889\n",
      "Epoch: 14212 Training Loss: 0.10185153367784289 Test Loss: 0.35636235894097223\n",
      "Epoch: 14213 Training Loss: 0.10149292924669054 Test Loss: 0.3555400390625\n",
      "Epoch: 14214 Training Loss: 0.10115337880452474 Test Loss: 0.35461238606770834\n",
      "Epoch: 14215 Training Loss: 0.1008410424126519 Test Loss: 0.35366748046875\n",
      "Epoch: 14216 Training Loss: 0.10055355665418837 Test Loss: 0.353133544921875\n",
      "Epoch: 14217 Training Loss: 0.10030392710367839 Test Loss: 0.35285460069444446\n",
      "Epoch: 14218 Training Loss: 0.10013001251220703 Test Loss: 0.3530660807291667\n",
      "Epoch: 14219 Training Loss: 0.10001293521457248 Test Loss: 0.3537311740451389\n",
      "Epoch: 14220 Training Loss: 0.09994629584418403 Test Loss: 0.3545877278645833\n",
      "Epoch: 14221 Training Loss: 0.09994068823920356 Test Loss: 0.35512350802951387\n",
      "Epoch: 14222 Training Loss: 0.10000441402859157 Test Loss: 0.3557813042534722\n",
      "Epoch: 14223 Training Loss: 0.10013294559054904 Test Loss: 0.35640283203125\n",
      "Epoch: 14224 Training Loss: 0.10029630449083116 Test Loss: 0.35720692274305554\n",
      "Epoch: 14225 Training Loss: 0.10048387824164497 Test Loss: 0.35791615125868054\n",
      "Epoch: 14226 Training Loss: 0.10069998253716363 Test Loss: 0.35872482638888886\n",
      "Epoch: 14227 Training Loss: 0.10091370222303603 Test Loss: 0.3593042534722222\n",
      "Epoch: 14228 Training Loss: 0.10110178120930989 Test Loss: 0.3600153537326389\n",
      "Epoch: 14229 Training Loss: 0.1012428699069553 Test Loss: 0.36067564561631943\n",
      "Epoch: 14230 Training Loss: 0.10134342532687717 Test Loss: 0.36134296332465277\n",
      "Epoch: 14231 Training Loss: 0.10140561930338542 Test Loss: 0.36215966796875\n",
      "Epoch: 14232 Training Loss: 0.10144566853841146 Test Loss: 0.3626115180121528\n",
      "Epoch: 14233 Training Loss: 0.10149278344048394 Test Loss: 0.36247770182291666\n",
      "Epoch: 14234 Training Loss: 0.10157685852050781 Test Loss: 0.36184434678819444\n",
      "Epoch: 14235 Training Loss: 0.10168694983588325 Test Loss: 0.36095985243055556\n",
      "Epoch: 14236 Training Loss: 0.1018089328342014 Test Loss: 0.3599748263888889\n",
      "Epoch: 14237 Training Loss: 0.10193948109944662 Test Loss: 0.3591528862847222\n",
      "Epoch: 14238 Training Loss: 0.10202748616536458 Test Loss: 0.3591714409722222\n",
      "Epoch: 14239 Training Loss: 0.10205919308132595 Test Loss: 0.35979448784722223\n",
      "Epoch: 14240 Training Loss: 0.10205506727430555 Test Loss: 0.36097623697916664\n",
      "Epoch: 14241 Training Loss: 0.10203739505343967 Test Loss: 0.3625024956597222\n",
      "Epoch: 14242 Training Loss: 0.10201847161187066 Test Loss: 0.3633137749565972\n",
      "Epoch: 14243 Training Loss: 0.10202149539523654 Test Loss: 0.36341167534722224\n",
      "Epoch: 14244 Training Loss: 0.10202081468370226 Test Loss: 0.36287163628472224\n",
      "Epoch: 14245 Training Loss: 0.10200441572401259 Test Loss: 0.362086181640625\n",
      "Epoch: 14246 Training Loss: 0.1019552001953125 Test Loss: 0.36128819444444443\n",
      "Epoch: 14247 Training Loss: 0.10184981536865234 Test Loss: 0.361161865234375\n",
      "Epoch: 14248 Training Loss: 0.10170584275987413 Test Loss: 0.3619426812065972\n",
      "Epoch: 14249 Training Loss: 0.10153933037651909 Test Loss: 0.3630153537326389\n",
      "Epoch: 14250 Training Loss: 0.10141469319661459 Test Loss: 0.3635259874131944\n",
      "Epoch: 14251 Training Loss: 0.10136613294813368 Test Loss: 0.3634232584635417\n",
      "Epoch: 14252 Training Loss: 0.10135959201388889 Test Loss: 0.3621448025173611\n",
      "Epoch: 14253 Training Loss: 0.10133980560302734 Test Loss: 0.36091606987847225\n",
      "Epoch: 14254 Training Loss: 0.10127776675754123 Test Loss: 0.35974981011284723\n",
      "Epoch: 14255 Training Loss: 0.10119086117214626 Test Loss: 0.35952311197916664\n",
      "Epoch: 14256 Training Loss: 0.10112207709418403 Test Loss: 0.35933957248263887\n",
      "Epoch: 14257 Training Loss: 0.10106956821017796 Test Loss: 0.3593254665798611\n",
      "Epoch: 14258 Training Loss: 0.10107877010769314 Test Loss: 0.35921571180555556\n",
      "Epoch: 14259 Training Loss: 0.10117208777533637 Test Loss: 0.35891780598958334\n",
      "Epoch: 14260 Training Loss: 0.10134277258978949 Test Loss: 0.35854551866319445\n",
      "Epoch: 14261 Training Loss: 0.1015802976820204 Test Loss: 0.35817222764756945\n",
      "Epoch: 14262 Training Loss: 0.10185918172200521 Test Loss: 0.35785923936631947\n",
      "Epoch: 14263 Training Loss: 0.10217476654052735 Test Loss: 0.35757676866319443\n",
      "Epoch: 14264 Training Loss: 0.10245826721191406 Test Loss: 0.3577123480902778\n",
      "Epoch: 14265 Training Loss: 0.10267374250623915 Test Loss: 0.3580832790798611\n",
      "Epoch: 14266 Training Loss: 0.10292023383246528 Test Loss: 0.35889352756076387\n",
      "Epoch: 14267 Training Loss: 0.10324056413438586 Test Loss: 0.35994108072916664\n",
      "Epoch: 14268 Training Loss: 0.10358318159315322 Test Loss: 0.36088384331597223\n",
      "Epoch: 14269 Training Loss: 0.10383467441134983 Test Loss: 0.36138151041666666\n",
      "Epoch: 14270 Training Loss: 0.10395509677463108 Test Loss: 0.3614237738715278\n",
      "Epoch: 14271 Training Loss: 0.10396265326605902 Test Loss: 0.36103428819444444\n",
      "Epoch: 14272 Training Loss: 0.10385774909125434 Test Loss: 0.3599558919270833\n",
      "Epoch: 14273 Training Loss: 0.1036994383070204 Test Loss: 0.35841861979166667\n",
      "Epoch: 14274 Training Loss: 0.10348644341362848 Test Loss: 0.357097900390625\n",
      "Epoch: 14275 Training Loss: 0.10324887847900391 Test Loss: 0.3555300021701389\n",
      "Epoch: 14276 Training Loss: 0.10300368499755859 Test Loss: 0.3537382269965278\n",
      "Epoch: 14277 Training Loss: 0.10275862375895183 Test Loss: 0.3525139431423611\n",
      "Epoch: 14278 Training Loss: 0.10253939565022786 Test Loss: 0.35179996744791664\n",
      "Epoch: 14279 Training Loss: 0.10233298746744791 Test Loss: 0.35154844835069443\n",
      "Epoch: 14280 Training Loss: 0.10210162014431423 Test Loss: 0.3521629774305556\n",
      "Epoch: 14281 Training Loss: 0.10189204491509332 Test Loss: 0.35293522135416666\n",
      "Epoch: 14282 Training Loss: 0.10172289615207249 Test Loss: 0.3535699327256944\n",
      "Epoch: 14283 Training Loss: 0.10160553571912978 Test Loss: 0.353716064453125\n",
      "Epoch: 14284 Training Loss: 0.10152926974826389 Test Loss: 0.35372743055555556\n",
      "Epoch: 14285 Training Loss: 0.10147665998670789 Test Loss: 0.3536786838107639\n",
      "Epoch: 14286 Training Loss: 0.10145173560248481 Test Loss: 0.35367561848958334\n",
      "Epoch: 14287 Training Loss: 0.10147165764702691 Test Loss: 0.3533865288628472\n",
      "Epoch: 14288 Training Loss: 0.1015398957994249 Test Loss: 0.35375260416666665\n",
      "Epoch: 14289 Training Loss: 0.10163711123996311 Test Loss: 0.3543640950520833\n",
      "Epoch: 14290 Training Loss: 0.10173521338568793 Test Loss: 0.3553349609375\n",
      "Epoch: 14291 Training Loss: 0.10185301208496093 Test Loss: 0.3572107476128472\n",
      "Epoch: 14292 Training Loss: 0.10199100833468967 Test Loss: 0.3590976833767361\n",
      "Epoch: 14293 Training Loss: 0.10214226277669271 Test Loss: 0.36028599717881943\n",
      "Epoch: 14294 Training Loss: 0.10232316335042317 Test Loss: 0.36056081814236113\n",
      "Epoch: 14295 Training Loss: 0.10252759552001953 Test Loss: 0.35931504991319446\n",
      "Epoch: 14296 Training Loss: 0.10276105414496528 Test Loss: 0.35627842881944444\n",
      "Epoch: 14297 Training Loss: 0.10296722920735676 Test Loss: 0.3533494737413194\n",
      "Epoch: 14298 Training Loss: 0.10309427134195963 Test Loss: 0.35099400499131944\n",
      "Epoch: 14299 Training Loss: 0.10309810977511935 Test Loss: 0.3503845757378472\n",
      "Epoch: 14300 Training Loss: 0.10299527316623264 Test Loss: 0.3510232204861111\n",
      "Epoch: 14301 Training Loss: 0.10286653730604384 Test Loss: 0.35282164171006947\n",
      "Epoch: 14302 Training Loss: 0.1027381829155816 Test Loss: 0.35457896592881943\n",
      "Epoch: 14303 Training Loss: 0.1025682610405816 Test Loss: 0.3555915256076389\n",
      "Epoch: 14304 Training Loss: 0.10231984117296007 Test Loss: 0.3557529839409722\n",
      "Epoch: 14305 Training Loss: 0.10198939429389106 Test Loss: 0.35535384114583335\n",
      "Epoch: 14306 Training Loss: 0.10159232754177518 Test Loss: 0.3550910373263889\n",
      "Epoch: 14307 Training Loss: 0.10118202379014757 Test Loss: 0.35505726453993053\n",
      "Epoch: 14308 Training Loss: 0.10078593190511068 Test Loss: 0.35523404947916665\n",
      "Epoch: 14309 Training Loss: 0.10042907375759548 Test Loss: 0.35563617621527777\n",
      "Epoch: 14310 Training Loss: 0.10012400224473741 Test Loss: 0.3560428059895833\n",
      "Epoch: 14311 Training Loss: 0.09988241746690538 Test Loss: 0.35624772135416666\n",
      "Epoch: 14312 Training Loss: 0.0997165027194553 Test Loss: 0.35593351236979165\n",
      "Epoch: 14313 Training Loss: 0.09959546491834853 Test Loss: 0.3553602973090278\n",
      "Epoch: 14314 Training Loss: 0.099497923956977 Test Loss: 0.35461577690972224\n",
      "Epoch: 14315 Training Loss: 0.09942584821912978 Test Loss: 0.3540690646701389\n",
      "Epoch: 14316 Training Loss: 0.09939033677842882 Test Loss: 0.35364111328125\n",
      "Epoch: 14317 Training Loss: 0.09939098612467448 Test Loss: 0.35351795789930557\n",
      "Epoch: 14318 Training Loss: 0.09941646406385633 Test Loss: 0.35370952690972224\n",
      "Epoch: 14319 Training Loss: 0.09943853081597222 Test Loss: 0.35382476128472223\n",
      "Epoch: 14320 Training Loss: 0.09947564273410374 Test Loss: 0.3542990993923611\n",
      "Epoch: 14321 Training Loss: 0.09949380238850912 Test Loss: 0.3546202799479167\n",
      "Epoch: 14322 Training Loss: 0.09951508076985677 Test Loss: 0.35520429144965276\n",
      "Epoch: 14323 Training Loss: 0.09954930114746094 Test Loss: 0.355595947265625\n",
      "Epoch: 14324 Training Loss: 0.09958895111083985 Test Loss: 0.3561933051215278\n",
      "Epoch: 14325 Training Loss: 0.09962773895263671 Test Loss: 0.35673396809895835\n",
      "Epoch: 14326 Training Loss: 0.09966566043429904 Test Loss: 0.3573234320746528\n",
      "Epoch: 14327 Training Loss: 0.09968665059407553 Test Loss: 0.35777596028645836\n",
      "Epoch: 14328 Training Loss: 0.09970160675048828 Test Loss: 0.3578800455729167\n",
      "Epoch: 14329 Training Loss: 0.09972580973307292 Test Loss: 0.35811846245659723\n",
      "Epoch: 14330 Training Loss: 0.09976682281494141 Test Loss: 0.35776220703125\n",
      "Epoch: 14331 Training Loss: 0.09984656100802951 Test Loss: 0.35746435546875\n",
      "Epoch: 14332 Training Loss: 0.09995419226752388 Test Loss: 0.3569712456597222\n",
      "Epoch: 14333 Training Loss: 0.10007151794433594 Test Loss: 0.35623887803819443\n",
      "Epoch: 14334 Training Loss: 0.10019242519802517 Test Loss: 0.35556979709201386\n",
      "Epoch: 14335 Training Loss: 0.10032513597276475 Test Loss: 0.35534407552083336\n",
      "Epoch: 14336 Training Loss: 0.10047217305501302 Test Loss: 0.3555219184027778\n",
      "Epoch: 14337 Training Loss: 0.10060058339436849 Test Loss: 0.3564977213541667\n",
      "Epoch: 14338 Training Loss: 0.10071025170220269 Test Loss: 0.3576533203125\n",
      "Epoch: 14339 Training Loss: 0.10079503207736545 Test Loss: 0.35927794053819445\n",
      "Epoch: 14340 Training Loss: 0.10085523393419053 Test Loss: 0.36070301649305553\n",
      "Epoch: 14341 Training Loss: 0.10090982903374567 Test Loss: 0.3616964789496528\n",
      "Epoch: 14342 Training Loss: 0.10096175299750434 Test Loss: 0.36176600477430554\n",
      "Epoch: 14343 Training Loss: 0.10099505021837023 Test Loss: 0.36124207899305555\n",
      "Epoch: 14344 Training Loss: 0.10099190266927083 Test Loss: 0.3603350151909722\n",
      "Epoch: 14345 Training Loss: 0.10097950744628906 Test Loss: 0.359623046875\n",
      "Epoch: 14346 Training Loss: 0.10095736016167535 Test Loss: 0.35948280164930557\n",
      "Epoch: 14347 Training Loss: 0.10093036821153428 Test Loss: 0.35946055772569446\n",
      "Epoch: 14348 Training Loss: 0.1009273935953776 Test Loss: 0.3597141927083333\n",
      "Epoch: 14349 Training Loss: 0.10097969902886285 Test Loss: 0.3602366265190972\n",
      "Epoch: 14350 Training Loss: 0.1010978283352322 Test Loss: 0.3608372938368056\n",
      "Epoch: 14351 Training Loss: 0.10127012210422091 Test Loss: 0.3612881401909722\n",
      "Epoch: 14352 Training Loss: 0.10149617597791884 Test Loss: 0.36201548936631944\n",
      "Epoch: 14353 Training Loss: 0.10177259233262803 Test Loss: 0.36281876627604165\n",
      "Epoch: 14354 Training Loss: 0.10210340118408204 Test Loss: 0.36369186740451387\n",
      "Epoch: 14355 Training Loss: 0.10249214935302735 Test Loss: 0.3646267632378472\n",
      "Epoch: 14356 Training Loss: 0.10291062842475043 Test Loss: 0.3656476779513889\n",
      "Epoch: 14357 Training Loss: 0.1034199210272895 Test Loss: 0.36675873480902776\n",
      "Epoch: 14358 Training Loss: 0.10406691148546007 Test Loss: 0.36746082899305554\n",
      "Epoch: 14359 Training Loss: 0.10486734263102214 Test Loss: 0.36738313802083333\n",
      "Epoch: 14360 Training Loss: 0.10578409915500217 Test Loss: 0.36643153211805557\n",
      "Epoch: 14361 Training Loss: 0.10681827884250217 Test Loss: 0.3639560546875\n",
      "Epoch: 14362 Training Loss: 0.10791378614637587 Test Loss: 0.36058536783854167\n",
      "Epoch: 14363 Training Loss: 0.10897135586208767 Test Loss: 0.35726627604166666\n",
      "Epoch: 14364 Training Loss: 0.10980885230170356 Test Loss: 0.35475651041666667\n",
      "Epoch: 14365 Training Loss: 0.11030941687689887 Test Loss: 0.35384583875868053\n",
      "Epoch: 14366 Training Loss: 0.11051900227864583 Test Loss: 0.35463248697916666\n",
      "Epoch: 14367 Training Loss: 0.11045088450113932 Test Loss: 0.35588509114583333\n",
      "Epoch: 14368 Training Loss: 0.11009323374430338 Test Loss: 0.356974365234375\n",
      "Epoch: 14369 Training Loss: 0.10950248887803819 Test Loss: 0.3578971354166667\n",
      "Epoch: 14370 Training Loss: 0.10870284949408637 Test Loss: 0.3589234212239583\n",
      "Epoch: 14371 Training Loss: 0.10779198116726345 Test Loss: 0.3592737087673611\n",
      "Epoch: 14372 Training Loss: 0.10675446912977431 Test Loss: 0.35872941080729165\n",
      "Epoch: 14373 Training Loss: 0.10568938700358073 Test Loss: 0.358048583984375\n",
      "Epoch: 14374 Training Loss: 0.10466252983940973 Test Loss: 0.35735956488715276\n",
      "Epoch: 14375 Training Loss: 0.10364697180853949 Test Loss: 0.35709779188368057\n",
      "Epoch: 14376 Training Loss: 0.10264122348361546 Test Loss: 0.35634879557291665\n",
      "Epoch: 14377 Training Loss: 0.10173715464274088 Test Loss: 0.3554101019965278\n",
      "Epoch: 14378 Training Loss: 0.10097495354546442 Test Loss: 0.35408062065972223\n",
      "Epoch: 14379 Training Loss: 0.10034219360351562 Test Loss: 0.35258346896701387\n",
      "Epoch: 14380 Training Loss: 0.09982892862955729 Test Loss: 0.3516742892795139\n",
      "Epoch: 14381 Training Loss: 0.09939650132921007 Test Loss: 0.35092898220486113\n",
      "Epoch: 14382 Training Loss: 0.09903094821506077 Test Loss: 0.3511136610243056\n",
      "Epoch: 14383 Training Loss: 0.09872689480251737 Test Loss: 0.35214217122395836\n",
      "Epoch: 14384 Training Loss: 0.09848266940646701 Test Loss: 0.35334456380208334\n",
      "Epoch: 14385 Training Loss: 0.0983003150092231 Test Loss: 0.3545620930989583\n",
      "Epoch: 14386 Training Loss: 0.09818117099338107 Test Loss: 0.35539008246527776\n",
      "Epoch: 14387 Training Loss: 0.09812515089246962 Test Loss: 0.3557531467013889\n",
      "Epoch: 14388 Training Loss: 0.09812453375922309 Test Loss: 0.355501953125\n",
      "Epoch: 14389 Training Loss: 0.09818004099527995 Test Loss: 0.35482568359375\n",
      "Epoch: 14390 Training Loss: 0.09826297760009765 Test Loss: 0.35374354383680556\n",
      "Epoch: 14391 Training Loss: 0.09837711758083767 Test Loss: 0.3526273871527778\n",
      "Epoch: 14392 Training Loss: 0.09850818549262153 Test Loss: 0.35169368489583336\n",
      "Epoch: 14393 Training Loss: 0.09866210089789497 Test Loss: 0.35108289930555553\n",
      "Epoch: 14394 Training Loss: 0.0988167978922526 Test Loss: 0.3506661783854167\n",
      "Epoch: 14395 Training Loss: 0.09899427625868056 Test Loss: 0.35054796006944444\n",
      "Epoch: 14396 Training Loss: 0.09918409729003906 Test Loss: 0.35039322916666665\n",
      "Epoch: 14397 Training Loss: 0.0993776846991645 Test Loss: 0.3499792751736111\n",
      "Epoch: 14398 Training Loss: 0.09956206681993272 Test Loss: 0.34975954861111114\n",
      "Epoch: 14399 Training Loss: 0.0997521023220486 Test Loss: 0.3497202690972222\n",
      "Epoch: 14400 Training Loss: 0.09994994354248046 Test Loss: 0.34985508897569445\n",
      "Epoch: 14401 Training Loss: 0.10013671112060547 Test Loss: 0.3504722493489583\n",
      "Epoch: 14402 Training Loss: 0.10030382283528645 Test Loss: 0.35148833550347225\n",
      "Epoch: 14403 Training Loss: 0.10046329413519965 Test Loss: 0.35303130425347223\n",
      "Epoch: 14404 Training Loss: 0.10060063510470921 Test Loss: 0.3546837836371528\n",
      "Epoch: 14405 Training Loss: 0.10068988121880425 Test Loss: 0.35654869249131943\n",
      "Epoch: 14406 Training Loss: 0.10074518500434028 Test Loss: 0.35799620225694445\n",
      "Epoch: 14407 Training Loss: 0.10079860517713758 Test Loss: 0.358718017578125\n",
      "Epoch: 14408 Training Loss: 0.10082271067301432 Test Loss: 0.3587325846354167\n",
      "Epoch: 14409 Training Loss: 0.10081647660997178 Test Loss: 0.35808127170138887\n",
      "Epoch: 14410 Training Loss: 0.10076953718397352 Test Loss: 0.3570611979166667\n",
      "Epoch: 14411 Training Loss: 0.10068277570936415 Test Loss: 0.35615752495659725\n",
      "Epoch: 14412 Training Loss: 0.1005323723687066 Test Loss: 0.35528238932291667\n",
      "Epoch: 14413 Training Loss: 0.10037313249376086 Test Loss: 0.35449565972222224\n",
      "Epoch: 14414 Training Loss: 0.10021521928575304 Test Loss: 0.35375390625\n",
      "Epoch: 14415 Training Loss: 0.10005909474690755 Test Loss: 0.35326817491319445\n",
      "Epoch: 14416 Training Loss: 0.0999010247124566 Test Loss: 0.3528684624565972\n",
      "Epoch: 14417 Training Loss: 0.09975003899468315 Test Loss: 0.3525384657118056\n",
      "Epoch: 14418 Training Loss: 0.09959553782145182 Test Loss: 0.35231732855902775\n",
      "Epoch: 14419 Training Loss: 0.09939960394965278 Test Loss: 0.3521955023871528\n",
      "Epoch: 14420 Training Loss: 0.09915853542751736 Test Loss: 0.3520747612847222\n",
      "Epoch: 14421 Training Loss: 0.09888331519232856 Test Loss: 0.3515592719184028\n",
      "Epoch: 14422 Training Loss: 0.09860921223958333 Test Loss: 0.35112565104166665\n",
      "Epoch: 14423 Training Loss: 0.09835563659667969 Test Loss: 0.35082991536458336\n",
      "Epoch: 14424 Training Loss: 0.0981444329155816 Test Loss: 0.35063853624131947\n",
      "Epoch: 14425 Training Loss: 0.09796348147922092 Test Loss: 0.3509831814236111\n",
      "Epoch: 14426 Training Loss: 0.09778263261583116 Test Loss: 0.35149452039930557\n",
      "Epoch: 14427 Training Loss: 0.09760464307996962 Test Loss: 0.35233544921875\n",
      "Epoch: 14428 Training Loss: 0.09745018683539497 Test Loss: 0.3530364040798611\n",
      "Epoch: 14429 Training Loss: 0.09732458157009549 Test Loss: 0.3533811306423611\n",
      "Epoch: 14430 Training Loss: 0.09721842363145616 Test Loss: 0.3534852973090278\n",
      "Epoch: 14431 Training Loss: 0.09711094326443143 Test Loss: 0.35343497721354167\n",
      "Epoch: 14432 Training Loss: 0.09700415547688802 Test Loss: 0.35335698784722225\n",
      "Epoch: 14433 Training Loss: 0.09691397518581814 Test Loss: 0.35325992838541664\n",
      "Epoch: 14434 Training Loss: 0.09683749220106337 Test Loss: 0.35326093207465276\n",
      "Epoch: 14435 Training Loss: 0.09677015516493055 Test Loss: 0.3534310980902778\n",
      "Epoch: 14436 Training Loss: 0.09670457288953993 Test Loss: 0.35364002821180557\n",
      "Epoch: 14437 Training Loss: 0.09663483428955078 Test Loss: 0.3538545464409722\n",
      "Epoch: 14438 Training Loss: 0.09656847296820746 Test Loss: 0.3539714626736111\n",
      "Epoch: 14439 Training Loss: 0.09652378675672743 Test Loss: 0.354015625\n",
      "Epoch: 14440 Training Loss: 0.09650397576226129 Test Loss: 0.354087890625\n",
      "Epoch: 14441 Training Loss: 0.09649757893880208 Test Loss: 0.3541490071614583\n",
      "Epoch: 14442 Training Loss: 0.09649530368381076 Test Loss: 0.35424180772569447\n",
      "Epoch: 14443 Training Loss: 0.09650656975640191 Test Loss: 0.35441786024305555\n",
      "Epoch: 14444 Training Loss: 0.09652937316894532 Test Loss: 0.3546339518229167\n",
      "Epoch: 14445 Training Loss: 0.09657911343044705 Test Loss: 0.35468340386284725\n",
      "Epoch: 14446 Training Loss: 0.09666051567925348 Test Loss: 0.3544453125\n",
      "Epoch: 14447 Training Loss: 0.09674681176079644 Test Loss: 0.35434182400173614\n",
      "Epoch: 14448 Training Loss: 0.09682433742947048 Test Loss: 0.3541876627604167\n",
      "Epoch: 14449 Training Loss: 0.0968902842203776 Test Loss: 0.35415855577256944\n",
      "Epoch: 14450 Training Loss: 0.09695621914333767 Test Loss: 0.3541217447916667\n",
      "Epoch: 14451 Training Loss: 0.09702506425645617 Test Loss: 0.35406022135416665\n",
      "Epoch: 14452 Training Loss: 0.09710765499538845 Test Loss: 0.3542788628472222\n",
      "Epoch: 14453 Training Loss: 0.0972189203898112 Test Loss: 0.3544331597222222\n",
      "Epoch: 14454 Training Loss: 0.0973559324476454 Test Loss: 0.3546008843315972\n",
      "Epoch: 14455 Training Loss: 0.09748539055718317 Test Loss: 0.3546648220486111\n",
      "Epoch: 14456 Training Loss: 0.09761661275227865 Test Loss: 0.3545878634982639\n",
      "Epoch: 14457 Training Loss: 0.0977512927585178 Test Loss: 0.3545132378472222\n",
      "Epoch: 14458 Training Loss: 0.09790119086371528 Test Loss: 0.3542666558159722\n",
      "Epoch: 14459 Training Loss: 0.09805067782931857 Test Loss: 0.35383718532986114\n",
      "Epoch: 14460 Training Loss: 0.09820377943250869 Test Loss: 0.35330604383680553\n",
      "Epoch: 14461 Training Loss: 0.09834712219238281 Test Loss: 0.3529917263454861\n",
      "Epoch: 14462 Training Loss: 0.0984925028483073 Test Loss: 0.35288121202256945\n",
      "Epoch: 14463 Training Loss: 0.09860095553927951 Test Loss: 0.3533661295572917\n",
      "Epoch: 14464 Training Loss: 0.09868978373209636 Test Loss: 0.35420477973090275\n",
      "Epoch: 14465 Training Loss: 0.09874327511257595 Test Loss: 0.35570233832465276\n",
      "Epoch: 14466 Training Loss: 0.09878034210205078 Test Loss: 0.3571459689670139\n",
      "Epoch: 14467 Training Loss: 0.09880058797200521 Test Loss: 0.3584810112847222\n",
      "Epoch: 14468 Training Loss: 0.09879478200276692 Test Loss: 0.35936640082465277\n",
      "Epoch: 14469 Training Loss: 0.09877679612901476 Test Loss: 0.35953599717881946\n",
      "Epoch: 14470 Training Loss: 0.09879610951741537 Test Loss: 0.3591016438802083\n",
      "Epoch: 14471 Training Loss: 0.09883707936604817 Test Loss: 0.358022705078125\n",
      "Epoch: 14472 Training Loss: 0.09890635850694444 Test Loss: 0.3565110405815972\n",
      "Epoch: 14473 Training Loss: 0.09896721988254123 Test Loss: 0.3553906521267361\n",
      "Epoch: 14474 Training Loss: 0.09898395538330078 Test Loss: 0.3545039605034722\n",
      "Epoch: 14475 Training Loss: 0.09895350223117405 Test Loss: 0.35470496961805553\n",
      "Epoch: 14476 Training Loss: 0.09889158884684245 Test Loss: 0.35560302734375\n",
      "Epoch: 14477 Training Loss: 0.09885762617323134 Test Loss: 0.3569583333333333\n",
      "Epoch: 14478 Training Loss: 0.09892072974310981 Test Loss: 0.35768120659722225\n",
      "Epoch: 14479 Training Loss: 0.09909910837809245 Test Loss: 0.35763552517361114\n",
      "Epoch: 14480 Training Loss: 0.09936482408311632 Test Loss: 0.35673961046006947\n",
      "Epoch: 14481 Training Loss: 0.09968408372667101 Test Loss: 0.35530430772569443\n",
      "Epoch: 14482 Training Loss: 0.10004708268907335 Test Loss: 0.35390923394097223\n",
      "Epoch: 14483 Training Loss: 0.10035375044080946 Test Loss: 0.3529015842013889\n",
      "Epoch: 14484 Training Loss: 0.10055027770996093 Test Loss: 0.3531655544704861\n",
      "Epoch: 14485 Training Loss: 0.10059311676025391 Test Loss: 0.35396959092881947\n",
      "Epoch: 14486 Training Loss: 0.10050180223253039 Test Loss: 0.3549996744791667\n",
      "Epoch: 14487 Training Loss: 0.10033307139078776 Test Loss: 0.355882568359375\n",
      "Epoch: 14488 Training Loss: 0.1001538814968533 Test Loss: 0.35684266493055555\n",
      "Epoch: 14489 Training Loss: 0.10000308481852213 Test Loss: 0.3568425021701389\n",
      "Epoch: 14490 Training Loss: 0.09992602115207248 Test Loss: 0.3560008138020833\n",
      "Epoch: 14491 Training Loss: 0.09987891133626302 Test Loss: 0.3545146484375\n",
      "Epoch: 14492 Training Loss: 0.09980610656738281 Test Loss: 0.35338834635416666\n",
      "Epoch: 14493 Training Loss: 0.09968021053738065 Test Loss: 0.35323914930555556\n",
      "Epoch: 14494 Training Loss: 0.09949403381347656 Test Loss: 0.35373920355902777\n",
      "Epoch: 14495 Training Loss: 0.09928943634033204 Test Loss: 0.35469734700520833\n",
      "Epoch: 14496 Training Loss: 0.099134644402398 Test Loss: 0.3550818142361111\n",
      "Epoch: 14497 Training Loss: 0.09907049730088975 Test Loss: 0.35538530815972225\n",
      "Epoch: 14498 Training Loss: 0.09907470109727648 Test Loss: 0.3556388346354167\n",
      "Epoch: 14499 Training Loss: 0.0991452162000868 Test Loss: 0.35657828776041667\n",
      "Epoch: 14500 Training Loss: 0.09926140424940322 Test Loss: 0.35716389973958335\n",
      "Epoch: 14501 Training Loss: 0.09941729482014974 Test Loss: 0.3575728081597222\n",
      "Epoch: 14502 Training Loss: 0.09956077660454644 Test Loss: 0.35800298394097224\n",
      "Epoch: 14503 Training Loss: 0.09969887373182508 Test Loss: 0.35867979600694444\n",
      "Epoch: 14504 Training Loss: 0.09979759979248047 Test Loss: 0.3602295464409722\n",
      "Epoch: 14505 Training Loss: 0.09986939748128255 Test Loss: 0.3619038357204861\n",
      "Epoch: 14506 Training Loss: 0.09998495992024739 Test Loss: 0.36392130533854167\n",
      "Epoch: 14507 Training Loss: 0.10010981580946181 Test Loss: 0.365732421875\n",
      "Epoch: 14508 Training Loss: 0.10026876576741536 Test Loss: 0.36705368381076386\n",
      "Epoch: 14509 Training Loss: 0.10042737409803602 Test Loss: 0.36768169487847224\n",
      "Epoch: 14510 Training Loss: 0.1005309575398763 Test Loss: 0.36715809461805554\n",
      "Epoch: 14511 Training Loss: 0.1005804689195421 Test Loss: 0.36566617838541665\n",
      "Epoch: 14512 Training Loss: 0.10068116505940755 Test Loss: 0.36310685221354166\n",
      "Epoch: 14513 Training Loss: 0.10089690314398872 Test Loss: 0.36018543836805555\n",
      "Epoch: 14514 Training Loss: 0.10119151984320747 Test Loss: 0.35695784505208333\n",
      "Epoch: 14515 Training Loss: 0.10155819447835286 Test Loss: 0.35503515625\n",
      "Epoch: 14516 Training Loss: 0.10199935065375434 Test Loss: 0.35489328342013887\n",
      "Epoch: 14517 Training Loss: 0.10245565287272135 Test Loss: 0.35624921332465276\n",
      "Epoch: 14518 Training Loss: 0.1028562511867947 Test Loss: 0.357507568359375\n",
      "Epoch: 14519 Training Loss: 0.1032031979031033 Test Loss: 0.35838194444444443\n",
      "Epoch: 14520 Training Loss: 0.10351767645941841 Test Loss: 0.35944523111979165\n",
      "Epoch: 14521 Training Loss: 0.10384844292534723 Test Loss: 0.3609466959635417\n",
      "Epoch: 14522 Training Loss: 0.10418504079182943 Test Loss: 0.36241845703125\n",
      "Epoch: 14523 Training Loss: 0.10452691226535374 Test Loss: 0.36262882486979164\n",
      "Epoch: 14524 Training Loss: 0.10484981452094184 Test Loss: 0.36118128797743054\n",
      "Epoch: 14525 Training Loss: 0.10510495842827691 Test Loss: 0.35900238715277777\n",
      "Epoch: 14526 Training Loss: 0.10522816891140407 Test Loss: 0.3570791558159722\n",
      "Epoch: 14527 Training Loss: 0.1052346928914388 Test Loss: 0.35664225260416665\n",
      "Epoch: 14528 Training Loss: 0.105201781378852 Test Loss: 0.3580537651909722\n",
      "Epoch: 14529 Training Loss: 0.10526046668158637 Test Loss: 0.3588832194010417\n",
      "Epoch: 14530 Training Loss: 0.10547006395128038 Test Loss: 0.35866853841145835\n",
      "Epoch: 14531 Training Loss: 0.10581478288438585 Test Loss: 0.3582283528645833\n",
      "Epoch: 14532 Training Loss: 0.10621558634440104 Test Loss: 0.3582757161458333\n",
      "Epoch: 14533 Training Loss: 0.1064841079711914 Test Loss: 0.35865576171875\n",
      "Epoch: 14534 Training Loss: 0.10650782691107856 Test Loss: 0.35967138671875\n",
      "Epoch: 14535 Training Loss: 0.10637169901529948 Test Loss: 0.36031656901041664\n",
      "Epoch: 14536 Training Loss: 0.1062556635538737 Test Loss: 0.3607746039496528\n",
      "Epoch: 14537 Training Loss: 0.1062428953382704 Test Loss: 0.3596851671006944\n",
      "Epoch: 14538 Training Loss: 0.10628939395480685 Test Loss: 0.3568718532986111\n",
      "Epoch: 14539 Training Loss: 0.1062019297281901 Test Loss: 0.35369091796875\n",
      "Epoch: 14540 Training Loss: 0.10591004096137152 Test Loss: 0.3520898708767361\n",
      "Epoch: 14541 Training Loss: 0.10547215779622396 Test Loss: 0.3528728298611111\n",
      "Epoch: 14542 Training Loss: 0.10494263288709853 Test Loss: 0.3535224066840278\n",
      "Epoch: 14543 Training Loss: 0.10435100894504123 Test Loss: 0.3543201497395833\n",
      "Epoch: 14544 Training Loss: 0.10373341878255209 Test Loss: 0.3542827962239583\n",
      "Epoch: 14545 Training Loss: 0.10308112928602431 Test Loss: 0.35368988715277777\n",
      "Epoch: 14546 Training Loss: 0.10242580074734157 Test Loss: 0.35326220703125\n",
      "Epoch: 14547 Training Loss: 0.10176318698459201 Test Loss: 0.35345054796006947\n",
      "Epoch: 14548 Training Loss: 0.10111530643039279 Test Loss: 0.3539989149305556\n",
      "Epoch: 14549 Training Loss: 0.10050059170193143 Test Loss: 0.35488864474826387\n",
      "Epoch: 14550 Training Loss: 0.09994461483425564 Test Loss: 0.35567532009548614\n",
      "Epoch: 14551 Training Loss: 0.09945327250162761 Test Loss: 0.3563393825954861\n",
      "Epoch: 14552 Training Loss: 0.09901942019992405 Test Loss: 0.35647062174479166\n",
      "Epoch: 14553 Training Loss: 0.0986378894382053 Test Loss: 0.35603602430555553\n",
      "Epoch: 14554 Training Loss: 0.09832554880777995 Test Loss: 0.35536615668402777\n",
      "Epoch: 14555 Training Loss: 0.0980628916422526 Test Loss: 0.35446834309895836\n",
      "Epoch: 14556 Training Loss: 0.09786370764838324 Test Loss: 0.35373844401041665\n",
      "Epoch: 14557 Training Loss: 0.09771371120876736 Test Loss: 0.35307722981770834\n",
      "Epoch: 14558 Training Loss: 0.09760121070014106 Test Loss: 0.35254747178819446\n",
      "Epoch: 14559 Training Loss: 0.09754034762912327 Test Loss: 0.3520942654079861\n",
      "Epoch: 14560 Training Loss: 0.0975041512383355 Test Loss: 0.3517287868923611\n",
      "Epoch: 14561 Training Loss: 0.0975102996826172 Test Loss: 0.3511974826388889\n",
      "Epoch: 14562 Training Loss: 0.09756449890136719 Test Loss: 0.35086631944444446\n",
      "Epoch: 14563 Training Loss: 0.09766967519124349 Test Loss: 0.3502744140625\n",
      "Epoch: 14564 Training Loss: 0.09778231726752387 Test Loss: 0.34985443793402776\n",
      "Epoch: 14565 Training Loss: 0.09791847144232856 Test Loss: 0.34938750542534724\n",
      "Epoch: 14566 Training Loss: 0.09808422936333551 Test Loss: 0.3490793999565972\n",
      "Epoch: 14567 Training Loss: 0.09828831058078343 Test Loss: 0.3491521538628472\n",
      "Epoch: 14568 Training Loss: 0.09853546227349175 Test Loss: 0.3495326877170139\n",
      "Epoch: 14569 Training Loss: 0.09881605105929904 Test Loss: 0.35032676866319445\n",
      "Epoch: 14570 Training Loss: 0.0991400646633572 Test Loss: 0.35182462565104167\n",
      "Epoch: 14571 Training Loss: 0.09951674313015409 Test Loss: 0.3534879557291667\n",
      "Epoch: 14572 Training Loss: 0.0999391589694553 Test Loss: 0.35548139105902776\n",
      "Epoch: 14573 Training Loss: 0.1004026133219401 Test Loss: 0.35713850911458334\n",
      "Epoch: 14574 Training Loss: 0.10089832984076606 Test Loss: 0.3581634657118056\n",
      "Epoch: 14575 Training Loss: 0.10141012912326389 Test Loss: 0.35809619140625\n",
      "Epoch: 14576 Training Loss: 0.10191708543565538 Test Loss: 0.3568535427517361\n",
      "Epoch: 14577 Training Loss: 0.1023973371717665 Test Loss: 0.3547898491753472\n",
      "Epoch: 14578 Training Loss: 0.10278027937147352 Test Loss: 0.3524238823784722\n",
      "Epoch: 14579 Training Loss: 0.10300197347005208 Test Loss: 0.3513059353298611\n",
      "Epoch: 14580 Training Loss: 0.10300404781765408 Test Loss: 0.35152606879340276\n",
      "Epoch: 14581 Training Loss: 0.10280427975124783 Test Loss: 0.3531327582465278\n",
      "Epoch: 14582 Training Loss: 0.10249208492702908 Test Loss: 0.3549202202690972\n",
      "Epoch: 14583 Training Loss: 0.1021278559366862 Test Loss: 0.3558639322916667\n",
      "Epoch: 14584 Training Loss: 0.10172320895724826 Test Loss: 0.3556616753472222\n",
      "Epoch: 14585 Training Loss: 0.10128720092773437 Test Loss: 0.3542696397569444\n",
      "Epoch: 14586 Training Loss: 0.10078589121500652 Test Loss: 0.3525112847222222\n",
      "Epoch: 14587 Training Loss: 0.10025646294487847 Test Loss: 0.3506041937934028\n",
      "Epoch: 14588 Training Loss: 0.09973067559136285 Test Loss: 0.3493779568142361\n",
      "Epoch: 14589 Training Loss: 0.0992773657904731 Test Loss: 0.349064697265625\n",
      "Epoch: 14590 Training Loss: 0.09890853203667535 Test Loss: 0.34955794270833335\n",
      "Epoch: 14591 Training Loss: 0.09861504109700521 Test Loss: 0.35066834852430556\n",
      "Epoch: 14592 Training Loss: 0.09841087086995443 Test Loss: 0.35229356553819446\n",
      "Epoch: 14593 Training Loss: 0.09828358629014756 Test Loss: 0.35411713324652777\n",
      "Epoch: 14594 Training Loss: 0.09821233367919922 Test Loss: 0.3558573404947917\n",
      "Epoch: 14595 Training Loss: 0.09818908945719401 Test Loss: 0.3570476345486111\n",
      "Epoch: 14596 Training Loss: 0.09819168429904514 Test Loss: 0.35746647135416665\n",
      "Epoch: 14597 Training Loss: 0.09820746782090929 Test Loss: 0.3570988498263889\n",
      "Epoch: 14598 Training Loss: 0.09825806427001953 Test Loss: 0.35613777669270835\n",
      "Epoch: 14599 Training Loss: 0.09836240895589193 Test Loss: 0.35500618489583335\n",
      "Epoch: 14600 Training Loss: 0.09852716403537326 Test Loss: 0.3538578559027778\n",
      "Epoch: 14601 Training Loss: 0.09874481794569227 Test Loss: 0.35312660047743055\n",
      "Epoch: 14602 Training Loss: 0.09899392954508464 Test Loss: 0.35284993489583333\n",
      "Epoch: 14603 Training Loss: 0.09924870893690321 Test Loss: 0.3534965277777778\n",
      "Epoch: 14604 Training Loss: 0.09950410546196832 Test Loss: 0.35493831380208335\n",
      "Epoch: 14605 Training Loss: 0.09975559912787543 Test Loss: 0.35677289496527775\n",
      "Epoch: 14606 Training Loss: 0.09996005079481336 Test Loss: 0.35899001736111114\n",
      "Epoch: 14607 Training Loss: 0.10013927120632596 Test Loss: 0.36086295572916666\n",
      "Epoch: 14608 Training Loss: 0.10030156792534722 Test Loss: 0.36217496744791666\n",
      "Epoch: 14609 Training Loss: 0.10045033264160157 Test Loss: 0.3628754340277778\n",
      "Epoch: 14610 Training Loss: 0.10058587307400174 Test Loss: 0.36301573350694444\n",
      "Epoch: 14611 Training Loss: 0.1007151862250434 Test Loss: 0.36265361870659724\n",
      "Epoch: 14612 Training Loss: 0.10085695987277561 Test Loss: 0.3618274197048611\n",
      "Epoch: 14613 Training Loss: 0.10102830505371094 Test Loss: 0.36084635416666666\n",
      "Epoch: 14614 Training Loss: 0.10124719662136501 Test Loss: 0.3597797037760417\n",
      "Epoch: 14615 Training Loss: 0.10147604200575087 Test Loss: 0.35844487847222223\n",
      "Epoch: 14616 Training Loss: 0.10166791110568577 Test Loss: 0.3569432237413194\n",
      "Epoch: 14617 Training Loss: 0.10179493882921006 Test Loss: 0.35512613932291665\n",
      "Epoch: 14618 Training Loss: 0.10185582054985894 Test Loss: 0.35331863064236113\n",
      "Epoch: 14619 Training Loss: 0.10189092339409722 Test Loss: 0.3518750542534722\n",
      "Epoch: 14620 Training Loss: 0.10192581261528863 Test Loss: 0.3511437717013889\n",
      "Epoch: 14621 Training Loss: 0.1019567133585612 Test Loss: 0.3508779568142361\n",
      "Epoch: 14622 Training Loss: 0.10203093634711372 Test Loss: 0.35139008246527775\n",
      "Epoch: 14623 Training Loss: 0.10219530571831598 Test Loss: 0.3526245930989583\n",
      "Epoch: 14624 Training Loss: 0.10242451477050782 Test Loss: 0.3544326443142361\n",
      "Epoch: 14625 Training Loss: 0.10263261074490017 Test Loss: 0.3564940863715278\n",
      "Epoch: 14626 Training Loss: 0.10274142964680989 Test Loss: 0.3582682834201389\n",
      "Epoch: 14627 Training Loss: 0.10273922051323785 Test Loss: 0.35946912977430556\n",
      "Epoch: 14628 Training Loss: 0.10265227678087023 Test Loss: 0.36022786458333333\n",
      "Epoch: 14629 Training Loss: 0.102513793097602 Test Loss: 0.3608746473524306\n",
      "Epoch: 14630 Training Loss: 0.10230895657009549 Test Loss: 0.36149951171875\n",
      "Epoch: 14631 Training Loss: 0.10205663045247396 Test Loss: 0.3620553656684028\n",
      "Epoch: 14632 Training Loss: 0.10177734375 Test Loss: 0.3625978461371528\n",
      "Epoch: 14633 Training Loss: 0.10149207814534505 Test Loss: 0.3625659450954861\n",
      "Epoch: 14634 Training Loss: 0.10119076283772786 Test Loss: 0.3615750054253472\n",
      "Epoch: 14635 Training Loss: 0.10090541924370659 Test Loss: 0.36061263020833334\n",
      "Epoch: 14636 Training Loss: 0.10063812001546224 Test Loss: 0.3594704318576389\n",
      "Epoch: 14637 Training Loss: 0.10039615885416667 Test Loss: 0.3586877712673611\n",
      "Epoch: 14638 Training Loss: 0.10015759531656901 Test Loss: 0.3578781467013889\n",
      "Epoch: 14639 Training Loss: 0.09993185424804688 Test Loss: 0.3571252712673611\n",
      "Epoch: 14640 Training Loss: 0.09972465345594618 Test Loss: 0.3562399631076389\n",
      "Epoch: 14641 Training Loss: 0.09952628750271267 Test Loss: 0.3551348470052083\n",
      "Epoch: 14642 Training Loss: 0.09935740746392144 Test Loss: 0.35459369574652777\n",
      "Epoch: 14643 Training Loss: 0.09922040642632378 Test Loss: 0.35395600043402775\n",
      "Epoch: 14644 Training Loss: 0.09914403449164497 Test Loss: 0.3535150282118056\n",
      "Epoch: 14645 Training Loss: 0.09911481730143229 Test Loss: 0.35361409505208335\n",
      "Epoch: 14646 Training Loss: 0.09914545271131728 Test Loss: 0.35372520616319447\n",
      "Epoch: 14647 Training Loss: 0.09924608272976346 Test Loss: 0.35424807400173614\n",
      "Epoch: 14648 Training Loss: 0.09939687940809462 Test Loss: 0.35465752495659725\n",
      "Epoch: 14649 Training Loss: 0.09960179053412543 Test Loss: 0.3549062771267361\n",
      "Epoch: 14650 Training Loss: 0.0998532689412435 Test Loss: 0.35521885850694446\n",
      "Epoch: 14651 Training Loss: 0.10013746303982204 Test Loss: 0.3551227756076389\n",
      "Epoch: 14652 Training Loss: 0.1004367667304145 Test Loss: 0.3552282443576389\n",
      "Epoch: 14653 Training Loss: 0.10075206587049697 Test Loss: 0.35537174479166667\n",
      "Epoch: 14654 Training Loss: 0.1011041742960612 Test Loss: 0.35599310980902776\n",
      "Epoch: 14655 Training Loss: 0.1015016606648763 Test Loss: 0.35715565321180553\n",
      "Epoch: 14656 Training Loss: 0.10194674682617187 Test Loss: 0.35964255099826387\n",
      "Epoch: 14657 Training Loss: 0.10245032585991753 Test Loss: 0.36259556749131944\n",
      "Epoch: 14658 Training Loss: 0.10292873721652561 Test Loss: 0.3651251627604167\n",
      "Epoch: 14659 Training Loss: 0.10334495883517796 Test Loss: 0.36644178602430555\n",
      "Epoch: 14660 Training Loss: 0.10371342807345921 Test Loss: 0.36558251953125\n",
      "Epoch: 14661 Training Loss: 0.10400994449191624 Test Loss: 0.3618559841579861\n",
      "Epoch: 14662 Training Loss: 0.10418209754096137 Test Loss: 0.3574553765190972\n",
      "Epoch: 14663 Training Loss: 0.10423261091444228 Test Loss: 0.35282666015625\n",
      "Epoch: 14664 Training Loss: 0.10408255428738064 Test Loss: 0.34960213216145836\n",
      "Epoch: 14665 Training Loss: 0.10370664808485243 Test Loss: 0.3476650661892361\n",
      "Epoch: 14666 Training Loss: 0.10311076185438368 Test Loss: 0.3477541232638889\n",
      "Epoch: 14667 Training Loss: 0.10236972893608941 Test Loss: 0.3488735080295139\n",
      "Epoch: 14668 Training Loss: 0.1015782470703125 Test Loss: 0.35048228624131944\n",
      "Epoch: 14669 Training Loss: 0.10080028788248697 Test Loss: 0.35201402452256947\n",
      "Epoch: 14670 Training Loss: 0.10012277306450738 Test Loss: 0.353277587890625\n",
      "Epoch: 14671 Training Loss: 0.09954788208007813 Test Loss: 0.35405186631944446\n",
      "Epoch: 14672 Training Loss: 0.09908468543158637 Test Loss: 0.3542505967881944\n",
      "Epoch: 14673 Training Loss: 0.0987066896226671 Test Loss: 0.3533460286458333\n",
      "Epoch: 14674 Training Loss: 0.09839703623453776 Test Loss: 0.3519677191840278\n",
      "Epoch: 14675 Training Loss: 0.0981637691921658 Test Loss: 0.3505041232638889\n",
      "Epoch: 14676 Training Loss: 0.09796352301703559 Test Loss: 0.34949077690972224\n",
      "Epoch: 14677 Training Loss: 0.09780103895399306 Test Loss: 0.34912396918402777\n",
      "Epoch: 14678 Training Loss: 0.09767442660861544 Test Loss: 0.3496100802951389\n",
      "Epoch: 14679 Training Loss: 0.09759144846598307 Test Loss: 0.35027845594618057\n",
      "Epoch: 14680 Training Loss: 0.09752523040771484 Test Loss: 0.3509363335503472\n",
      "Epoch: 14681 Training Loss: 0.09748174794514974 Test Loss: 0.35113416883680554\n",
      "Epoch: 14682 Training Loss: 0.09746110365125868 Test Loss: 0.35104182942708334\n",
      "Epoch: 14683 Training Loss: 0.09748495483398438 Test Loss: 0.35063511827256943\n",
      "Epoch: 14684 Training Loss: 0.09752019755045573 Test Loss: 0.35015638563368057\n",
      "Epoch: 14685 Training Loss: 0.0975626220703125 Test Loss: 0.3496414388020833\n",
      "Epoch: 14686 Training Loss: 0.09759331766764323 Test Loss: 0.3496063910590278\n",
      "Epoch: 14687 Training Loss: 0.09764134555392795 Test Loss: 0.349755615234375\n",
      "Epoch: 14688 Training Loss: 0.09771483951144748 Test Loss: 0.35024004448784724\n",
      "Epoch: 14689 Training Loss: 0.09781973859998915 Test Loss: 0.3509245876736111\n",
      "Epoch: 14690 Training Loss: 0.09794784291585286 Test Loss: 0.35165516493055554\n",
      "Epoch: 14691 Training Loss: 0.09810178714328342 Test Loss: 0.35232655164930554\n",
      "Epoch: 14692 Training Loss: 0.09826413133409288 Test Loss: 0.3526142035590278\n",
      "Epoch: 14693 Training Loss: 0.09843057590060764 Test Loss: 0.3523613009982639\n",
      "Epoch: 14694 Training Loss: 0.09857250298394098 Test Loss: 0.35192583550347223\n",
      "Epoch: 14695 Training Loss: 0.09869173770480685 Test Loss: 0.3510837944878472\n",
      "Epoch: 14696 Training Loss: 0.09877945793999567 Test Loss: 0.3502520887586806\n",
      "Epoch: 14697 Training Loss: 0.09882058291965061 Test Loss: 0.349630126953125\n",
      "Epoch: 14698 Training Loss: 0.09881300523546006 Test Loss: 0.34934776475694446\n",
      "Epoch: 14699 Training Loss: 0.09876461113823785 Test Loss: 0.3493034125434028\n",
      "Epoch: 14700 Training Loss: 0.09870388624403212 Test Loss: 0.34929155815972224\n",
      "Epoch: 14701 Training Loss: 0.09864921400282119 Test Loss: 0.34929654947916666\n",
      "Epoch: 14702 Training Loss: 0.0985788336859809 Test Loss: 0.3494873046875\n",
      "Epoch: 14703 Training Loss: 0.09851091935899522 Test Loss: 0.34982435438368054\n",
      "Epoch: 14704 Training Loss: 0.09848490736219619 Test Loss: 0.3504113498263889\n",
      "Epoch: 14705 Training Loss: 0.09851338789198133 Test Loss: 0.35110142686631945\n",
      "Epoch: 14706 Training Loss: 0.0985573238796658 Test Loss: 0.3516477322048611\n",
      "Epoch: 14707 Training Loss: 0.09860954962836371 Test Loss: 0.3519345703125\n",
      "Epoch: 14708 Training Loss: 0.09869271765814887 Test Loss: 0.35156700303819444\n",
      "Epoch: 14709 Training Loss: 0.09878827752007378 Test Loss: 0.3507787814670139\n",
      "Epoch: 14710 Training Loss: 0.09884049309624567 Test Loss: 0.34941650390625\n",
      "Epoch: 14711 Training Loss: 0.09881510755750868 Test Loss: 0.3479695095486111\n",
      "Epoch: 14712 Training Loss: 0.09871854146321614 Test Loss: 0.34711637369791665\n",
      "Epoch: 14713 Training Loss: 0.0985679202609592 Test Loss: 0.3467160373263889\n",
      "Epoch: 14714 Training Loss: 0.09841947767469618 Test Loss: 0.3468066948784722\n",
      "Epoch: 14715 Training Loss: 0.098277956644694 Test Loss: 0.34704893663194447\n",
      "Epoch: 14716 Training Loss: 0.09814553493923611 Test Loss: 0.347373291015625\n",
      "Epoch: 14717 Training Loss: 0.09803084903293185 Test Loss: 0.34792041015625\n",
      "Epoch: 14718 Training Loss: 0.09792042371961805 Test Loss: 0.3483127170138889\n",
      "Epoch: 14719 Training Loss: 0.0977926517062717 Test Loss: 0.3491636284722222\n",
      "Epoch: 14720 Training Loss: 0.09764003499348958 Test Loss: 0.3501012912326389\n",
      "Epoch: 14721 Training Loss: 0.09745098029242621 Test Loss: 0.35130815972222224\n",
      "Epoch: 14722 Training Loss: 0.09726090494791667 Test Loss: 0.35236330837673613\n",
      "Epoch: 14723 Training Loss: 0.09708769056532118 Test Loss: 0.35300634765625\n",
      "Epoch: 14724 Training Loss: 0.09696887885199652 Test Loss: 0.3538774142795139\n",
      "Epoch: 14725 Training Loss: 0.09692462327745226 Test Loss: 0.3543342013888889\n",
      "Epoch: 14726 Training Loss: 0.09693835364447699 Test Loss: 0.3548620876736111\n",
      "Epoch: 14727 Training Loss: 0.09697279018825955 Test Loss: 0.35557847764756945\n",
      "Epoch: 14728 Training Loss: 0.09699586232503256 Test Loss: 0.35620225694444446\n",
      "Epoch: 14729 Training Loss: 0.09697096167670356 Test Loss: 0.35624671766493055\n",
      "Epoch: 14730 Training Loss: 0.09691854434543186 Test Loss: 0.35593885633680555\n",
      "Epoch: 14731 Training Loss: 0.0968552983601888 Test Loss: 0.3549822591145833\n",
      "Epoch: 14732 Training Loss: 0.09682458835177951 Test Loss: 0.35377495659722225\n",
      "Epoch: 14733 Training Loss: 0.09682915920681423 Test Loss: 0.35240760633680557\n",
      "Epoch: 14734 Training Loss: 0.09685490332709418 Test Loss: 0.3511240776909722\n",
      "Epoch: 14735 Training Loss: 0.09692757246229383 Test Loss: 0.35003765190972225\n",
      "Epoch: 14736 Training Loss: 0.09704877217610677 Test Loss: 0.34951866319444447\n",
      "Epoch: 14737 Training Loss: 0.09719557274712456 Test Loss: 0.3493347710503472\n",
      "Epoch: 14738 Training Loss: 0.09733267466227213 Test Loss: 0.34945458984375\n",
      "Epoch: 14739 Training Loss: 0.09743664381239149 Test Loss: 0.34970393880208334\n",
      "Epoch: 14740 Training Loss: 0.09749775356716579 Test Loss: 0.3503194173177083\n",
      "Epoch: 14741 Training Loss: 0.0975342771742079 Test Loss: 0.35076871744791666\n",
      "Epoch: 14742 Training Loss: 0.09755946350097656 Test Loss: 0.3509470757378472\n",
      "Epoch: 14743 Training Loss: 0.09758016883002388 Test Loss: 0.35074820963541664\n",
      "Epoch: 14744 Training Loss: 0.09761154683430989 Test Loss: 0.3510032009548611\n",
      "Epoch: 14745 Training Loss: 0.09763972557915582 Test Loss: 0.35111219618055556\n",
      "Epoch: 14746 Training Loss: 0.09764249081081815 Test Loss: 0.3516496310763889\n",
      "Epoch: 14747 Training Loss: 0.0976321055094401 Test Loss: 0.3527575412326389\n",
      "Epoch: 14748 Training Loss: 0.09761754184299046 Test Loss: 0.354443115234375\n",
      "Epoch: 14749 Training Loss: 0.0975731447007921 Test Loss: 0.35589122178819443\n",
      "Epoch: 14750 Training Loss: 0.09753363376193576 Test Loss: 0.35718234592013887\n",
      "Epoch: 14751 Training Loss: 0.09753101348876952 Test Loss: 0.357443359375\n",
      "Epoch: 14752 Training Loss: 0.09753992038302951 Test Loss: 0.3573534613715278\n",
      "Epoch: 14753 Training Loss: 0.09755936686197916 Test Loss: 0.35646150716145836\n",
      "Epoch: 14754 Training Loss: 0.09758396318223742 Test Loss: 0.3549174262152778\n",
      "Epoch: 14755 Training Loss: 0.097596556769477 Test Loss: 0.353625732421875\n",
      "Epoch: 14756 Training Loss: 0.0976285654703776 Test Loss: 0.35276765950520833\n",
      "Epoch: 14757 Training Loss: 0.09767474450005426 Test Loss: 0.35212369791666664\n",
      "Epoch: 14758 Training Loss: 0.09775059170193143 Test Loss: 0.35147037760416666\n",
      "Epoch: 14759 Training Loss: 0.09785894181993272 Test Loss: 0.35093359375\n",
      "Epoch: 14760 Training Loss: 0.09802635955810547 Test Loss: 0.3501526150173611\n",
      "Epoch: 14761 Training Loss: 0.09824952952067058 Test Loss: 0.3491920572916667\n",
      "Epoch: 14762 Training Loss: 0.09848792860243055 Test Loss: 0.34807025824652776\n",
      "Epoch: 14763 Training Loss: 0.09868421427408854 Test Loss: 0.34742485894097225\n",
      "Epoch: 14764 Training Loss: 0.0988379160563151 Test Loss: 0.3473841417100694\n",
      "Epoch: 14765 Training Loss: 0.09893053860134549 Test Loss: 0.3485058051215278\n",
      "Epoch: 14766 Training Loss: 0.09894011773003472 Test Loss: 0.35019189453125\n",
      "Epoch: 14767 Training Loss: 0.09889655388726129 Test Loss: 0.35212131076388886\n",
      "Epoch: 14768 Training Loss: 0.09881124708387587 Test Loss: 0.35315616861979165\n",
      "Epoch: 14769 Training Loss: 0.09869099850124782 Test Loss: 0.35300037977430554\n",
      "Epoch: 14770 Training Loss: 0.09854185401068793 Test Loss: 0.3516650390625\n",
      "Epoch: 14771 Training Loss: 0.09836956024169922 Test Loss: 0.35023749457465275\n",
      "Epoch: 14772 Training Loss: 0.0981825697157118 Test Loss: 0.3489852973090278\n",
      "Epoch: 14773 Training Loss: 0.09801899041069878 Test Loss: 0.3481642252604167\n",
      "Epoch: 14774 Training Loss: 0.09786667972140842 Test Loss: 0.34823253038194446\n",
      "Epoch: 14775 Training Loss: 0.0977409176296658 Test Loss: 0.34863533528645835\n",
      "Epoch: 14776 Training Loss: 0.09764558410644532 Test Loss: 0.3493026529947917\n",
      "Epoch: 14777 Training Loss: 0.09759448157416449 Test Loss: 0.34997105577256943\n",
      "Epoch: 14778 Training Loss: 0.09758562469482422 Test Loss: 0.35022783745659725\n",
      "Epoch: 14779 Training Loss: 0.09759795040554471 Test Loss: 0.35042279730902776\n",
      "Epoch: 14780 Training Loss: 0.09764243655734592 Test Loss: 0.3507575141059028\n",
      "Epoch: 14781 Training Loss: 0.09774502563476563 Test Loss: 0.3515526801215278\n",
      "Epoch: 14782 Training Loss: 0.09788023206922743 Test Loss: 0.3526867947048611\n",
      "Epoch: 14783 Training Loss: 0.0980598865085178 Test Loss: 0.35407020399305555\n",
      "Epoch: 14784 Training Loss: 0.0982667507595486 Test Loss: 0.35554736328125\n",
      "Epoch: 14785 Training Loss: 0.09849529266357422 Test Loss: 0.3566617838541667\n",
      "Epoch: 14786 Training Loss: 0.09874170769585504 Test Loss: 0.35725813802083334\n",
      "Epoch: 14787 Training Loss: 0.09895542314317492 Test Loss: 0.357447509765625\n",
      "Epoch: 14788 Training Loss: 0.0991201163397895 Test Loss: 0.35731005859375\n",
      "Epoch: 14789 Training Loss: 0.09921031443277994 Test Loss: 0.3573178168402778\n",
      "Epoch: 14790 Training Loss: 0.0992141613430447 Test Loss: 0.35733629014756946\n",
      "Epoch: 14791 Training Loss: 0.09914109378390842 Test Loss: 0.357724853515625\n",
      "Epoch: 14792 Training Loss: 0.0990383309258355 Test Loss: 0.35864274088541664\n",
      "Epoch: 14793 Training Loss: 0.09899142286512587 Test Loss: 0.3593298611111111\n",
      "Epoch: 14794 Training Loss: 0.09897918701171875 Test Loss: 0.35989854600694443\n",
      "Epoch: 14795 Training Loss: 0.09894897037082248 Test Loss: 0.35940625\n",
      "Epoch: 14796 Training Loss: 0.0988901629977756 Test Loss: 0.35852208116319445\n",
      "Epoch: 14797 Training Loss: 0.09878961520724826 Test Loss: 0.35715049913194447\n",
      "Epoch: 14798 Training Loss: 0.09866943105061848 Test Loss: 0.35618958875868056\n",
      "Epoch: 14799 Training Loss: 0.09855884891086154 Test Loss: 0.3552547200520833\n",
      "Epoch: 14800 Training Loss: 0.0984734115600586 Test Loss: 0.35456043836805556\n",
      "Epoch: 14801 Training Loss: 0.09843438042534722 Test Loss: 0.35395665147569444\n",
      "Epoch: 14802 Training Loss: 0.09845294952392578 Test Loss: 0.3536948784722222\n",
      "Epoch: 14803 Training Loss: 0.0985315187242296 Test Loss: 0.35375056966145835\n",
      "Epoch: 14804 Training Loss: 0.09864766947428386 Test Loss: 0.3539588758680556\n",
      "Epoch: 14805 Training Loss: 0.09873228030734592 Test Loss: 0.3544898003472222\n",
      "Epoch: 14806 Training Loss: 0.09878727976481119 Test Loss: 0.3554796820746528\n",
      "Epoch: 14807 Training Loss: 0.09882654232449002 Test Loss: 0.356575927734375\n",
      "Epoch: 14808 Training Loss: 0.09883583238389756 Test Loss: 0.35776462131076386\n",
      "Epoch: 14809 Training Loss: 0.09883175404866537 Test Loss: 0.3588183865017361\n",
      "Epoch: 14810 Training Loss: 0.09881212192111545 Test Loss: 0.3592501356336806\n",
      "Epoch: 14811 Training Loss: 0.09877286953396268 Test Loss: 0.3590542534722222\n",
      "Epoch: 14812 Training Loss: 0.09871947648790147 Test Loss: 0.3578972981770833\n",
      "Epoch: 14813 Training Loss: 0.09865958828396268 Test Loss: 0.35608930121527776\n",
      "Epoch: 14814 Training Loss: 0.09861352199978299 Test Loss: 0.35414599609375\n",
      "Epoch: 14815 Training Loss: 0.09856283315022786 Test Loss: 0.35244080946180556\n",
      "Epoch: 14816 Training Loss: 0.09851729753282334 Test Loss: 0.35149446614583335\n",
      "Epoch: 14817 Training Loss: 0.09853026665581598 Test Loss: 0.3509084743923611\n",
      "Epoch: 14818 Training Loss: 0.09860044521755643 Test Loss: 0.3502835015190972\n",
      "Epoch: 14819 Training Loss: 0.0987525397406684 Test Loss: 0.34926627604166666\n",
      "Epoch: 14820 Training Loss: 0.09895007917616103 Test Loss: 0.3478359375\n",
      "Epoch: 14821 Training Loss: 0.09919136471218533 Test Loss: 0.3466670193142361\n",
      "Epoch: 14822 Training Loss: 0.09942321523030599 Test Loss: 0.3463554958767361\n",
      "Epoch: 14823 Training Loss: 0.09964349195692274 Test Loss: 0.34700518120659724\n",
      "Epoch: 14824 Training Loss: 0.09979912736680772 Test Loss: 0.3487981770833333\n",
      "Epoch: 14825 Training Loss: 0.09991249254014757 Test Loss: 0.35127669270833334\n",
      "Epoch: 14826 Training Loss: 0.09996930355495877 Test Loss: 0.35381298828125\n",
      "Epoch: 14827 Training Loss: 0.09998042551676432 Test Loss: 0.3560945095486111\n",
      "Epoch: 14828 Training Loss: 0.09994811757405599 Test Loss: 0.35754625108506943\n",
      "Epoch: 14829 Training Loss: 0.09987850189208984 Test Loss: 0.35781407335069443\n",
      "Epoch: 14830 Training Loss: 0.09980635833740234 Test Loss: 0.35692447916666664\n",
      "Epoch: 14831 Training Loss: 0.09976428561740451 Test Loss: 0.35510245768229165\n",
      "Epoch: 14832 Training Loss: 0.09971332634819878 Test Loss: 0.35319835069444444\n",
      "Epoch: 14833 Training Loss: 0.09962007480197482 Test Loss: 0.35158390299479164\n",
      "Epoch: 14834 Training Loss: 0.0994647708468967 Test Loss: 0.35059461805555553\n",
      "Epoch: 14835 Training Loss: 0.09928586917453341 Test Loss: 0.35062828233506943\n",
      "Epoch: 14836 Training Loss: 0.09908482276068793 Test Loss: 0.351831298828125\n",
      "Epoch: 14837 Training Loss: 0.09881717003716363 Test Loss: 0.35406361219618054\n",
      "Epoch: 14838 Training Loss: 0.09851669565836589 Test Loss: 0.35650059678819446\n",
      "Epoch: 14839 Training Loss: 0.09821831258138021 Test Loss: 0.35862375217013887\n",
      "Epoch: 14840 Training Loss: 0.09792825147840711 Test Loss: 0.3598757052951389\n",
      "Epoch: 14841 Training Loss: 0.0976706059773763 Test Loss: 0.36050645616319443\n",
      "Epoch: 14842 Training Loss: 0.09744807942708333 Test Loss: 0.35986490885416667\n",
      "Epoch: 14843 Training Loss: 0.0972526363796658 Test Loss: 0.3588162163628472\n",
      "Epoch: 14844 Training Loss: 0.0970805180867513 Test Loss: 0.3578200954861111\n",
      "Epoch: 14845 Training Loss: 0.09692091200086805 Test Loss: 0.3570329861111111\n",
      "Epoch: 14846 Training Loss: 0.09679158782958984 Test Loss: 0.356451171875\n",
      "Epoch: 14847 Training Loss: 0.09670156775580512 Test Loss: 0.35622935655381943\n",
      "Epoch: 14848 Training Loss: 0.09664846801757812 Test Loss: 0.35625914171006945\n",
      "Epoch: 14849 Training Loss: 0.09661645253499349 Test Loss: 0.356304443359375\n",
      "Epoch: 14850 Training Loss: 0.09662628258599175 Test Loss: 0.35645252821180556\n",
      "Epoch: 14851 Training Loss: 0.09669194963243273 Test Loss: 0.35682872178819447\n",
      "Epoch: 14852 Training Loss: 0.09680138566758897 Test Loss: 0.35689995659722223\n",
      "Epoch: 14853 Training Loss: 0.09692863718668619 Test Loss: 0.3566222330729167\n",
      "Epoch: 14854 Training Loss: 0.09706396145290799 Test Loss: 0.35603087022569446\n",
      "Epoch: 14855 Training Loss: 0.09719962226019965 Test Loss: 0.3550823025173611\n",
      "Epoch: 14856 Training Loss: 0.09732937367757162 Test Loss: 0.3541349826388889\n",
      "Epoch: 14857 Training Loss: 0.0974420632256402 Test Loss: 0.3531680230034722\n",
      "Epoch: 14858 Training Loss: 0.09758152770996094 Test Loss: 0.3525885687934028\n",
      "Epoch: 14859 Training Loss: 0.09775565677218967 Test Loss: 0.35239371744791664\n",
      "Epoch: 14860 Training Loss: 0.09795292409261068 Test Loss: 0.35244349500868055\n",
      "Epoch: 14861 Training Loss: 0.09816134558783637 Test Loss: 0.3523269314236111\n",
      "Epoch: 14862 Training Loss: 0.09842503950330946 Test Loss: 0.3521593424479167\n",
      "Epoch: 14863 Training Loss: 0.09868955485026042 Test Loss: 0.35268733723958334\n",
      "Epoch: 14864 Training Loss: 0.0989206288655599 Test Loss: 0.35327240668402776\n",
      "Epoch: 14865 Training Loss: 0.09912379031711155 Test Loss: 0.3538286675347222\n",
      "Epoch: 14866 Training Loss: 0.09936151292588975 Test Loss: 0.35455135091145834\n",
      "Epoch: 14867 Training Loss: 0.09964783732096354 Test Loss: 0.3540288899739583\n",
      "Epoch: 14868 Training Loss: 0.09996054416232639 Test Loss: 0.35239680989583333\n",
      "Epoch: 14869 Training Loss: 0.1002294659084744 Test Loss: 0.3507887369791667\n",
      "Epoch: 14870 Training Loss: 0.10036927032470704 Test Loss: 0.3492264539930556\n",
      "Epoch: 14871 Training Loss: 0.1002797139485677 Test Loss: 0.3498696017795139\n",
      "Epoch: 14872 Training Loss: 0.10001132202148437 Test Loss: 0.351741943359375\n",
      "Epoch: 14873 Training Loss: 0.09964801279703776 Test Loss: 0.3545050998263889\n",
      "Epoch: 14874 Training Loss: 0.09931585184733073 Test Loss: 0.35619761827256946\n",
      "Epoch: 14875 Training Loss: 0.09903246222601997 Test Loss: 0.35633827039930555\n",
      "Epoch: 14876 Training Loss: 0.09876482645670573 Test Loss: 0.35524582248263886\n",
      "Epoch: 14877 Training Loss: 0.09847180006239149 Test Loss: 0.35376700846354164\n",
      "Epoch: 14878 Training Loss: 0.09815287950303819 Test Loss: 0.352773681640625\n",
      "Epoch: 14879 Training Loss: 0.09785350375705296 Test Loss: 0.35204861111111113\n",
      "Epoch: 14880 Training Loss: 0.09764196099175347 Test Loss: 0.35189263237847224\n",
      "Epoch: 14881 Training Loss: 0.09756280941433376 Test Loss: 0.3516767578125\n",
      "Epoch: 14882 Training Loss: 0.09762712605794271 Test Loss: 0.3513956976996528\n",
      "Epoch: 14883 Training Loss: 0.09778406609429253 Test Loss: 0.3515750054253472\n",
      "Epoch: 14884 Training Loss: 0.09794135199652777 Test Loss: 0.3527415093315972\n",
      "Epoch: 14885 Training Loss: 0.0980629416571723 Test Loss: 0.35489124891493057\n",
      "Epoch: 14886 Training Loss: 0.09815593719482422 Test Loss: 0.3573085394965278\n",
      "Epoch: 14887 Training Loss: 0.09823181067572699 Test Loss: 0.35935622829861114\n",
      "Epoch: 14888 Training Loss: 0.09831262885199653 Test Loss: 0.3607417534722222\n",
      "Epoch: 14889 Training Loss: 0.09837393442789713 Test Loss: 0.3611424696180556\n",
      "Epoch: 14890 Training Loss: 0.09840967559814454 Test Loss: 0.36077400716145835\n",
      "Epoch: 14891 Training Loss: 0.0984131808810764 Test Loss: 0.35981887478298613\n",
      "Epoch: 14892 Training Loss: 0.09844463348388671 Test Loss: 0.35827734375\n",
      "Epoch: 14893 Training Loss: 0.09851827070448134 Test Loss: 0.35686919487847224\n",
      "Epoch: 14894 Training Loss: 0.09862075805664063 Test Loss: 0.35558753797743053\n",
      "Epoch: 14895 Training Loss: 0.098767212761773 Test Loss: 0.3549521484375\n",
      "Epoch: 14896 Training Loss: 0.098965454949273 Test Loss: 0.35476866319444444\n",
      "Epoch: 14897 Training Loss: 0.09918393113878038 Test Loss: 0.3547978515625\n",
      "Epoch: 14898 Training Loss: 0.0994165064493815 Test Loss: 0.3548350151909722\n",
      "Epoch: 14899 Training Loss: 0.09958933342827692 Test Loss: 0.3548387044270833\n",
      "Epoch: 14900 Training Loss: 0.09970317840576172 Test Loss: 0.3546769476996528\n",
      "Epoch: 14901 Training Loss: 0.09976802486843533 Test Loss: 0.35368196614583336\n",
      "Epoch: 14902 Training Loss: 0.09978887769911024 Test Loss: 0.35226497395833334\n",
      "Epoch: 14903 Training Loss: 0.09974614291720921 Test Loss: 0.35042784288194445\n",
      "Epoch: 14904 Training Loss: 0.0996787838406033 Test Loss: 0.34839097764756943\n",
      "Epoch: 14905 Training Loss: 0.09955621592203776 Test Loss: 0.3465373263888889\n",
      "Epoch: 14906 Training Loss: 0.09940445200602213 Test Loss: 0.34556651475694444\n",
      "Epoch: 14907 Training Loss: 0.09921145290798611 Test Loss: 0.34572450086805556\n",
      "Epoch: 14908 Training Loss: 0.09899670749240451 Test Loss: 0.3463437771267361\n",
      "Epoch: 14909 Training Loss: 0.09880249362521701 Test Loss: 0.34756032986111113\n",
      "Epoch: 14910 Training Loss: 0.0986245583428277 Test Loss: 0.34884616427951387\n",
      "Epoch: 14911 Training Loss: 0.0984795642428928 Test Loss: 0.34986311848958335\n",
      "Epoch: 14912 Training Loss: 0.0983549084133572 Test Loss: 0.35064341905381946\n",
      "Epoch: 14913 Training Loss: 0.09826015302870009 Test Loss: 0.35081654188368055\n",
      "Epoch: 14914 Training Loss: 0.09818106248643663 Test Loss: 0.3509138997395833\n",
      "Epoch: 14915 Training Loss: 0.09812755499945747 Test Loss: 0.3507805447048611\n",
      "Epoch: 14916 Training Loss: 0.09810282050238715 Test Loss: 0.35077107747395836\n",
      "Epoch: 14917 Training Loss: 0.09810711330837674 Test Loss: 0.350978515625\n",
      "Epoch: 14918 Training Loss: 0.09813923984103733 Test Loss: 0.35167274305555557\n",
      "Epoch: 14919 Training Loss: 0.09818163553873697 Test Loss: 0.3527532823350694\n",
      "Epoch: 14920 Training Loss: 0.09819345516628689 Test Loss: 0.35425032552083335\n",
      "Epoch: 14921 Training Loss: 0.09816525692409939 Test Loss: 0.3558167046440972\n",
      "Epoch: 14922 Training Loss: 0.0981306372748481 Test Loss: 0.35701890733506947\n",
      "Epoch: 14923 Training Loss: 0.0980899175008138 Test Loss: 0.3573004014756944\n",
      "Epoch: 14924 Training Loss: 0.09806140984429254 Test Loss: 0.35680989583333333\n",
      "Epoch: 14925 Training Loss: 0.09806264495849609 Test Loss: 0.35594444444444445\n",
      "Epoch: 14926 Training Loss: 0.0980987057156033 Test Loss: 0.35445147026909724\n",
      "Epoch: 14927 Training Loss: 0.09818982781304253 Test Loss: 0.35322219509548614\n",
      "Epoch: 14928 Training Loss: 0.09831923166910807 Test Loss: 0.35215825737847223\n",
      "Epoch: 14929 Training Loss: 0.09851404147677952 Test Loss: 0.3517550726996528\n",
      "Epoch: 14930 Training Loss: 0.09882050408257378 Test Loss: 0.35214350043402776\n",
      "Epoch: 14931 Training Loss: 0.09925939263237847 Test Loss: 0.3529404839409722\n",
      "Epoch: 14932 Training Loss: 0.09981695980495876 Test Loss: 0.35386783854166665\n",
      "Epoch: 14933 Training Loss: 0.10048367309570312 Test Loss: 0.35470564778645836\n",
      "Epoch: 14934 Training Loss: 0.10120533074273004 Test Loss: 0.35496668836805556\n",
      "Epoch: 14935 Training Loss: 0.10202588399251301 Test Loss: 0.35514518229166664\n",
      "Epoch: 14936 Training Loss: 0.10291952090793186 Test Loss: 0.3554363335503472\n",
      "Epoch: 14937 Training Loss: 0.10378207058376736 Test Loss: 0.356301513671875\n",
      "Epoch: 14938 Training Loss: 0.10446902974446615 Test Loss: 0.3587430013020833\n",
      "Epoch: 14939 Training Loss: 0.1049721671210395 Test Loss: 0.3628041449652778\n",
      "Epoch: 14940 Training Loss: 0.10533047400580513 Test Loss: 0.36732432725694447\n",
      "Epoch: 14941 Training Loss: 0.10553103892008464 Test Loss: 0.37107866753472224\n",
      "Epoch: 14942 Training Loss: 0.10559169091118707 Test Loss: 0.3733217502170139\n",
      "Epoch: 14943 Training Loss: 0.105484619140625 Test Loss: 0.3729228515625\n",
      "Epoch: 14944 Training Loss: 0.10522347089979384 Test Loss: 0.37038229709201387\n",
      "Epoch: 14945 Training Loss: 0.1048505359225803 Test Loss: 0.3666513671875\n",
      "Epoch: 14946 Training Loss: 0.10436386193169488 Test Loss: 0.3634990776909722\n",
      "Epoch: 14947 Training Loss: 0.10382928297254775 Test Loss: 0.361685546875\n",
      "Epoch: 14948 Training Loss: 0.1032998784383138 Test Loss: 0.36003363715277775\n",
      "Epoch: 14949 Training Loss: 0.10279776340060764 Test Loss: 0.35926497395833334\n",
      "Epoch: 14950 Training Loss: 0.10235213894314237 Test Loss: 0.3589922417534722\n",
      "Epoch: 14951 Training Loss: 0.10196939849853516 Test Loss: 0.35898187934027775\n",
      "Epoch: 14952 Training Loss: 0.10165919833713108 Test Loss: 0.3590822482638889\n",
      "Epoch: 14953 Training Loss: 0.1013788341946072 Test Loss: 0.35883897569444445\n",
      "Epoch: 14954 Training Loss: 0.10110972764756944 Test Loss: 0.35841975911458335\n",
      "Epoch: 14955 Training Loss: 0.10082420094807942 Test Loss: 0.35782725694444445\n",
      "Epoch: 14956 Training Loss: 0.10053591834174262 Test Loss: 0.35684342447916667\n",
      "Epoch: 14957 Training Loss: 0.1002580083211263 Test Loss: 0.355822265625\n",
      "Epoch: 14958 Training Loss: 0.09998721567789713 Test Loss: 0.3554104546440972\n",
      "Epoch: 14959 Training Loss: 0.09974251810709635 Test Loss: 0.3551558430989583\n",
      "Epoch: 14960 Training Loss: 0.09952764723036024 Test Loss: 0.3552272677951389\n",
      "Epoch: 14961 Training Loss: 0.09937195671929254 Test Loss: 0.3555380045572917\n",
      "Epoch: 14962 Training Loss: 0.09931779225667317 Test Loss: 0.35611436631944443\n",
      "Epoch: 14963 Training Loss: 0.09939232381184895 Test Loss: 0.35654039171006946\n",
      "Epoch: 14964 Training Loss: 0.09954591963026259 Test Loss: 0.35661946614583334\n",
      "Epoch: 14965 Training Loss: 0.09974083455403646 Test Loss: 0.35633189561631945\n",
      "Epoch: 14966 Training Loss: 0.09994705115424263 Test Loss: 0.35590171983506946\n",
      "Epoch: 14967 Training Loss: 0.10013654496934679 Test Loss: 0.35555582682291664\n",
      "Epoch: 14968 Training Loss: 0.10030569881863065 Test Loss: 0.3555662434895833\n",
      "Epoch: 14969 Training Loss: 0.1004490466647678 Test Loss: 0.3561992730034722\n",
      "Epoch: 14970 Training Loss: 0.1005867190890842 Test Loss: 0.35701451280381946\n",
      "Epoch: 14971 Training Loss: 0.1007230716281467 Test Loss: 0.3577412651909722\n",
      "Epoch: 14972 Training Loss: 0.10088117557101779 Test Loss: 0.3585061306423611\n",
      "Epoch: 14973 Training Loss: 0.10105371263292101 Test Loss: 0.35874134657118056\n",
      "Epoch: 14974 Training Loss: 0.10118800608317058 Test Loss: 0.35908241102430555\n",
      "Epoch: 14975 Training Loss: 0.10125779724121094 Test Loss: 0.35944099934895835\n",
      "Epoch: 14976 Training Loss: 0.10125136990017361 Test Loss: 0.3601808268229167\n",
      "Epoch: 14977 Training Loss: 0.10125347137451172 Test Loss: 0.3603454861111111\n",
      "Epoch: 14978 Training Loss: 0.10129253641764323 Test Loss: 0.3600833875868056\n",
      "Epoch: 14979 Training Loss: 0.1014394751654731 Test Loss: 0.3591284722222222\n",
      "Epoch: 14980 Training Loss: 0.10165482076009115 Test Loss: 0.35760904947916666\n",
      "Epoch: 14981 Training Loss: 0.10186602105034723 Test Loss: 0.356289306640625\n",
      "Epoch: 14982 Training Loss: 0.10196589830186632 Test Loss: 0.35547905815972225\n",
      "Epoch: 14983 Training Loss: 0.10188351864284939 Test Loss: 0.3557640516493056\n",
      "Epoch: 14984 Training Loss: 0.1016421898735894 Test Loss: 0.35588536241319446\n",
      "Epoch: 14985 Training Loss: 0.1012919913397895 Test Loss: 0.3564539388020833\n",
      "Epoch: 14986 Training Loss: 0.10089680311414931 Test Loss: 0.35608018663194446\n",
      "Epoch: 14987 Training Loss: 0.10049504004584418 Test Loss: 0.35496424696180556\n",
      "Epoch: 14988 Training Loss: 0.10004900275336372 Test Loss: 0.3537971462673611\n",
      "Epoch: 14989 Training Loss: 0.09954669358995226 Test Loss: 0.3529287651909722\n",
      "Epoch: 14990 Training Loss: 0.0990248786078559 Test Loss: 0.35246058485243054\n",
      "Epoch: 14991 Training Loss: 0.0985667487250434 Test Loss: 0.3528468967013889\n",
      "Epoch: 14992 Training Loss: 0.09822444407145182 Test Loss: 0.3534469943576389\n",
      "Epoch: 14993 Training Loss: 0.0980250498453776 Test Loss: 0.35412540690104166\n",
      "Epoch: 14994 Training Loss: 0.09799185095893012 Test Loss: 0.354434326171875\n",
      "Epoch: 14995 Training Loss: 0.0980651863945855 Test Loss: 0.35470464409722224\n",
      "Epoch: 14996 Training Loss: 0.09819330936008029 Test Loss: 0.35454606119791665\n",
      "Epoch: 14997 Training Loss: 0.09833570014105902 Test Loss: 0.354039306640625\n",
      "Epoch: 14998 Training Loss: 0.09847178141276042 Test Loss: 0.35326044379340277\n",
      "Epoch: 14999 Training Loss: 0.0986007334391276 Test Loss: 0.3527817111545139\n"
     ]
    }
   ],
   "source": [
    "nb_of_epochs=15000\n",
    "batch_size=int(x_variable.shape[0]/10)\n",
    "# Train the network #\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    \n",
    "    my_net.eval()\n",
    "    test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "    my_net.train()\n",
    "    print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save_network('4_neural_net.pkl',my_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#network9=load_network('9_neural_net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "network7=load_network('7_neural_net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (bn_input): BatchNorm1d(11, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc0): Linear(in_features=11, out_features=56, bias=True)\n",
       "  (bn0): BatchNorm1d(56, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (predict): Linear(in_features=56, out_features=49, bias=True)\n",
       "  (fc1): Linear(in_features=56, out_features=56, bias=True)\n",
       "  (bn1): BatchNorm1d(56, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc2): Linear(in_features=56, out_features=56, bias=True)\n",
       "  (bn2): BatchNorm1d(56, eps=1e-05, momentum=0.5, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network7.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'procrustes_contour' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-46f63bcba759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_contour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocrustes_contour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'procrustes_contour' is not defined"
     ]
    }
   ],
   "source": [
    "plot_contour(procrustes_contour)\n",
    "plt.axes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial set edges: {(0, 1), (1, 2), (4, 5), (5, 6), (6, 0), (2, 3), (3, 4)}\n",
      "Edge: (4, 5) targeting: 1\n",
      "(4, 5, 1)\n",
      "edges inserted: (4, 1)\n",
      "set of interior edges updated: {(4, 1)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (3, 4)}\n",
      "edges inserted: (5, 1)\n",
      "set of interior edges updated: {(5, 1), (4, 1)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (5, 1), (3, 4)}\n",
      "element inserted: (4, 5, 1)\n",
      "Edge: (0, 1) targeting: 5\n",
      "(0, 1, 5)\n",
      "edges inserted: (0, 5)\n",
      "set of interior edges updated: {(5, 1), (0, 5), (4, 1)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (0, 5), (5, 1), (3, 4)}\n",
      "element inserted: (0, 1, 5)\n",
      "Edge: (6, 0) targeting: 5\n",
      "(6, 0, 5)\n",
      "Vertex locked: 6\n",
      "Vertex locked: 0\n",
      "Vertex locked: 5\n",
      "element inserted: (6, 0, 5)\n",
      "Edge: (5, 6) targeting: 0\n",
      "(5, 6, 0)\n",
      "Element (6, 0, 5) already in set\n",
      "Edge: (1, 2) targeting: 4\n",
      "(1, 2, 4)\n",
      "Vertex locked: 1\n",
      "edges inserted: (2, 4)\n",
      "set of interior edges updated: {(5, 1), (0, 5), (4, 1), (2, 4)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (0, 5), (5, 1), (3, 4), (2, 4)}\n",
      "element inserted: (1, 2, 4)\n",
      "Edge: (3, 4) targeting: 2\n",
      "(3, 4, 2)\n",
      "Vertex locked: 3\n",
      "Vertex locked: 4\n",
      "Vertex locked: 2\n",
      "element inserted: (3, 4, 2)\n",
      "Edge: (2, 3) targeting: 4\n",
      "(2, 3, 4)\n",
      "Element (3, 4, 2) already in set\n",
      "Final edges: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (0, 5), (5, 1), (3, 4), (2, 4)}\n",
      "Elements created: {(6, 0, 5), (1, 2, 4), (4, 5, 1), (3, 4, 2), (0, 1, 5)}\n",
      "Set of locked vertices: {0, 1, 2, 3, 4, 5, 6}\n",
      "Set of open vertices: set()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADuCAYAAAAgJwuRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c10748c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%timeit\n",
    "procrustes_contour=apply_procrustes(generate_contour(7))\n",
    "procrustes_contour=procrustes_contour.reshape(2*procrustes_contour.shape[0]).reshape(1,-1)\n",
    "contour_transformed=pca.transform(procrustes_contour)\n",
    "\n",
    "input_contour=Variable(torch.from_numpy(contour_transformed).type(torch.FloatTensor)).cuda()\n",
    "predicted_quality_matrix=network7(input_contour)\n",
    "\n",
    "predicted_quality_matrix=predicted_quality_matrix.cpu()\n",
    "predicted_quality_matrix=predicted_quality_matrix.data[0].numpy().reshape(7,7)\n",
    "procrustes_contour=procrustes_contour.reshape(7,2)\n",
    "ordered_matrix=order_quality_matrix(predicted_quality_matrix,procrustes_contour)\n",
    "triangulate(procrustes_contour,ordered_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial set edges: {(0, 1), (1, 2), (4, 5), (5, 6), (6, 0), (2, 3), (3, 4)}\n",
      "Edge: (6, 0) targeting: 5\n",
      "(6, 0, 5)\n",
      "Vertex locked: 6\n",
      "edges inserted: (0, 5)\n",
      "set of interior edges updated: {(0, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 5), (5, 6), (6, 0), (2, 3), (0, 5), (3, 4)}\n",
      "element inserted: (6, 0, 5)\n",
      "Edge: (5, 6) targeting: 1\n",
      "found (5, 0) (6, 0) Canceling creation\n",
      "Edge: (5, 6) targeting: 0\n",
      "(5, 6, 0)\n",
      "Element (6, 0, 5) already in set\n",
      "Edge: (4, 5) targeting: 1\n",
      "(4, 5, 1)\n",
      "edges inserted: (4, 1)\n",
      "set of interior edges updated: {(4, 1), (0, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (0, 5), (3, 4)}\n",
      "edges inserted: (5, 1)\n",
      "set of interior edges updated: {(5, 1), (4, 1), (0, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (0, 5), (5, 1), (3, 4)}\n",
      "element inserted: (4, 5, 1)\n",
      "Edge: (3, 4) targeting: 2\n",
      "(3, 4, 2)\n",
      "Vertex locked: 3\n",
      "edges inserted: (4, 2)\n",
      "set of interior edges updated: {(4, 2), (5, 1), (4, 1), (0, 5)}\n",
      "set of edges updated: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (0, 5), (4, 2), (5, 1), (3, 4)}\n",
      "element inserted: (3, 4, 2)\n",
      "Edge: (2, 3) targeting: 4\n",
      "(2, 3, 4)\n",
      "Element (3, 4, 2) already in set\n",
      "Edge: (1, 2) targeting: 4\n",
      "Element inserted: (1, 2, 4)\n",
      "Edge: (1, 2) targeting: 5\n",
      "Element inserted: (1, 2, 4)\n",
      "Edge: (1, 2) targeting: 3\n",
      "Element inserted: (1, 2, 4)\n",
      "Edge: (1, 2) targeting: 6\n",
      "Element inserted: (1, 2, 4)\n",
      "Edge: (1, 2) targeting: 0\n",
      "Element inserted: (1, 2, 4)\n",
      "Edge: (0, 1) targeting: 6\n",
      "Element inserted: (0, 1, 5)\n",
      "Edge: (0, 1) targeting: 5\n",
      "Element inserted: (0, 1, 5)\n",
      "Edge: (0, 1) targeting: 4\n",
      "Element inserted: (0, 1, 5)\n",
      "Edge: (0, 1) targeting: 3\n",
      "Element inserted: (0, 1, 5)\n",
      "Edge: (0, 1) targeting: 2\n",
      "Element inserted: (0, 1, 5)\n",
      "Final edges: {(0, 1), (1, 2), (4, 1), (4, 5), (5, 6), (6, 0), (2, 3), (0, 5), (4, 2), (5, 1), (3, 4)}\n",
      "Elements created: {(6, 0, 5), (1, 2, 4), (4, 5, 1), (3, 4, 2), (0, 1, 5)}\n",
      "Set of locked vertices: {3, 6}\n",
      "Vertex locked: 0\n",
      "Vertex locked: 5\n",
      "Vertex locked: 1\n",
      "Vertex locked: 2\n",
      "Vertex locked: 4\n",
      "Set of open vertices: set()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADuCAYAAAAgJwuRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c09514390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_quality_matrix,_=quality_matrix(procrustes_contour)\n",
    "ordered_matrix=order_quality_matrix(original_quality_matrix,procrustes_contour)\n",
    "triangulate(procrustes_contour,ordered_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for applying procrustes --- 0.0009884834289550781 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:93: RuntimeWarning: invalid value encountered in true_divide\n",
      "D:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:94: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2f39c1d7673c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;31m# Project to space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtime2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcontour_transformed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocrustes_contour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprojection_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Elapsed time for projection --- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprojection_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mean_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'components_'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "# Perform procrustes\n",
    "time1=time.time()\n",
    "procrustes_contour=apply_procrustes(contour)\n",
    "procrustes_time=time.time() - time1\n",
    "print(\"Elapsed time for applying procrustes --- %s seconds ---\" % (procrustes_time))\n",
    "\n",
    "\n",
    "procrustes_contour=procrustes_contour.reshape(2*contour.shape[0]).reshape(1,-1)\n",
    "\n",
    "\n",
    "\n",
    "# Project to space \n",
    "time2=time.time()\n",
    "contour_transformed=pca.transform(procrustes_contour)\n",
    "projection_time=time.time() - time2\n",
    "print(\"Elapsed time for projection --- %s seconds ---\" % (projection_time))\n",
    "\n",
    "\n",
    "# Fire to network\n",
    "time3=time.time()\n",
    "input_contour=Variable(torch.from_numpy(contour_transformed).type(torch.FloatTensor)).cuda()\n",
    "predicted_quality_matrix=network9(input_contour)\n",
    "network_time=time.time() - time3\n",
    "print(\"Elapsed time for getting quality matrix from neural net --- %s seconds ---\" % (network_time))\n",
    "\n",
    "predicted_quality_matrix=predicted_quality_matrix.cpu()\n",
    "predicted_quality_matrix=predicted_quality_matrix.data[0].numpy().reshape(9,9)\n",
    "procrustes_contour=procrustes_contour.reshape(9,2)\n",
    "\n",
    "\n",
    "# Calculating quality matrix\n",
    "time4=time.time()\n",
    "original_quality_matrix,_=quality_matrix(procrustes_contour)\n",
    "calculation_quality_matrix_time=time.time() - time4\n",
    "print(\"Elapsed time for calculating original quality matrix  --- %s seconds ---\" % (calculation_quality_matrix_time))\n",
    "\n",
    "\n",
    "# Ordering quality matrix\n",
    "time5=time.time()\n",
    "ordered_matrix=order_quality_matrix(original_quality_matrix,procrustes_contour)\n",
    "ordering_matrix_time=time.time() - time5\n",
    "print(\"Elapsed time for ordering the quality matrix  --- %s seconds ---\" % (ordering_matrix_time))\n",
    "\n",
    "\n",
    "# Triangulation\n",
    "time6=time.time()\n",
    "triangulate(procrustes_contour,ordered_matrix)\n",
    "triangulation_time=time.time() - time6\n",
    "print(\"Elapsed time to triangulate according to matrix   --- %s seconds ---\" % (triangulation_time))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total time  of triangulation with calculation of matrix :   --- %s seconds ---\" % ( (procrustes_time+calculation_quality_matrix_time\n",
    "                                                                                      +ordering_matrix_time+triangulation_time)))\n",
    "\n",
    "print(\"Total time  of triangulation using neural network :   --- %s seconds ---\" % ( (procrustes_time+projection_time\n",
    "                                                                                      +network_time+triangulation_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must have at least three input vertices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-fbe4ed1d88c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtime7\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprocrustes_contour\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_contour_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocrustes_contour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtriangle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriangulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pq0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Elapsed time to triangulate using triangle module   --- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\triangle\\__init__.py\u001b[0m in \u001b[0;36mtriangulate\u001b[0;34m(tri, opts)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'vertices'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtri\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtri\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vertices'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input must have at least three input vertices.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTriangulateIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input must have at least three input vertices."
     ]
    }
   ],
   "source": [
    "# Triangulate using the library \n",
    "time7=time.time()\n",
    "shape=dict(vertices=procrustes_contour,segments=get_contour_edges(procrustes_contour))\n",
    "t = triangle.triangulate(shape, 'pq0')\n",
    "plot.plot(plt.axes(), **t)\n",
    "print(\"Elapsed time to triangulate using triangle module   --- %s seconds ---\" % (time.time() - time7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_time_test(start,finish):\n",
    "    \n",
    "    times=np.empty([int(((finish+1)-start)),9])\n",
    "\n",
    "    for index,i in enumerate(range(start,finish+1)):\n",
    "        \n",
    "        # train to project with pca\n",
    "        Polygons=load_dataset(str(i)+'_polygons.pkl')\n",
    "    \n",
    "        Polygons_reshaped=[]\n",
    "        for j in range(Polygons.shape[0]):\n",
    "            Polygons_reshaped.append(Polygons[j].reshape(2*i))\n",
    "\n",
    "        Polygons_reshaped=np.array(Polygons_reshaped) \n",
    "        \n",
    "                        # PCA   #\n",
    "        pca=PCA(.999)\n",
    "        pca.fit(Polygons_reshaped)\n",
    "        Polygons_projected=pca.transform(Polygons_reshaped)\n",
    "\n",
    "        \n",
    "        contour=generate_contour(i)\n",
    "        \n",
    "        network=load_network(str(i)+'_neural_net.pkl')\n",
    "        start_time=time.time()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Perform procrustes\n",
    "        time1=timer()\n",
    "        procrustes_contour=apply_procrustes(contour)\n",
    "        procrustes_time=timer() - time1\n",
    "        print(\"Elapsed time for applying procrustes --- %s seconds ---\" % (procrustes_time))\n",
    "        \n",
    "        \n",
    "        procrustes_contour=procrustes_contour.reshape(2*contour.shape[0]).reshape(1,-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Project to space \n",
    "        time2=timer()\n",
    "        contour_transformed=pca.transform(procrustes_contour)\n",
    "        projection_time=timer() - time2\n",
    "        print(\"Elapsed time for projection --- %s seconds ---\" % (projection_time))\n",
    "        \n",
    "        \n",
    "        # Fire to network\n",
    "        time3=timer()\n",
    "        input_contour=Variable(torch.from_numpy(contour_transformed).type(torch.FloatTensor)).cuda()\n",
    "        predicted_quality_matrix=network(input_contour)\n",
    "        network_time=timer() - time3\n",
    "        print(\"Elapsed time for getting quality matrix from neural net --- %s seconds ---\" % (network_time))\n",
    "        \n",
    "        predicted_quality_matrix=predicted_quality_matrix.cpu()\n",
    "        predicted_quality_matrix=predicted_quality_matrix.data[0].numpy().reshape(i,i)\n",
    "        procrustes_contour=procrustes_contour.reshape(i,2)\n",
    "        \n",
    "        \n",
    "        # Calculating quality matrix\n",
    "        time4=timer()\n",
    "        original_quality_matrix,_=quality_matrix(procrustes_contour)\n",
    "        calculation_quality_matrix_time=timer() - time4\n",
    "        print(\"Elapsed time for calculating original quality matrix  --- %s seconds ---\" % (calculation_quality_matrix_time))\n",
    "        \n",
    "        \n",
    "        # Ordering quality matrix\n",
    "        time5=timer()\n",
    "        ordered_matrix=order_quality_matrix(original_quality_matrix,procrustes_contour)\n",
    "        ordering_matrix_time=timer() - time5\n",
    "        print(\"Elapsed time for ordering the quality matrix  --- %s seconds ---\" % (ordering_matrix_time))\n",
    "        \n",
    "        \n",
    "        # Triangulation\n",
    "        time6=timer()\n",
    "        pure_triangulate(procrustes_contour,ordered_matrix,recursive=True)\n",
    "        triangulation_time=timer() - time6\n",
    "        print(\"Elapsed time to triangulate according to matrix   --- %s seconds ---\" % (triangulation_time))\n",
    "        \n",
    "        \n",
    "        total_time_with_calculation=(procrustes_time+calculation_quality_matrix_time+ordering_matrix_time+triangulation_time)\n",
    "        print(\"Total time  of triangulation with calculation of matrix :   --- %s seconds ---\" % (total_time_with_calculation ))\n",
    "        total_time_with_NN=(procrustes_time+projection_time+network_time+triangulation_time)\n",
    "        print(\"Total time  of triangulation using neural network :   --- %s seconds ---\" % ( total_time_with_NN))\n",
    "\n",
    "\n",
    "        \n",
    "        # Triangulate using the library \n",
    "        shape=dict(vertices=procrustes_contour,segments=get_contour_edges(procrustes_contour))\n",
    "        time7=timer()\n",
    "        for i in range(10000):\n",
    "            t = triangle.triangulate(shape, 'pq0')\n",
    "        remeshing_time=timer()-time7\n",
    "        remeshing_time/=10000\n",
    "\n",
    "        #plot.plot(plt.axes(), **t) \n",
    "        print(\"Elapsed time to triangulate using triangle module   --- %s seconds ---\" % (remeshing_time))\n",
    "        times[index]=[procrustes_time,projection_time,network_time,calculation_quality_matrix_time,ordering_matrix_time,triangulation_time,total_time_with_calculation,total_time_with_NN,remeshing_time]\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for applying procrustes --- 0.00032223501011685585 seconds ---\n",
      "Elapsed time for projection --- 0.0001631666941648291 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0006677795472569414 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.13363634011739123 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.0003150628472212702 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0010122994899575133 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.13528593746468687 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0021654807414961397 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 5.345530449285434e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.00037115940313015017 seconds ---\n",
      "Elapsed time for projection --- 0.0001813532489904901 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0007092756295605795 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.0481077914528214 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.0002730544679252489 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.00127254652488773 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.05002455184876453 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0025343348065689497 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 5.4032637949512715e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.00030865913095112774 seconds ---\n",
      "Elapsed time for projection --- 0.00012038986869811197 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0006652180609307834 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.08299190851994354 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.00028048277908965247 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0013066142960269644 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.08488766472601128 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0024008813566069875 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 5.3824798929690584e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.00028458115684770746 seconds ---\n",
      "Elapsed time for projection --- 0.00017136345104518114 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0006183428563417692 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.0765518190014518 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.0003160874421155313 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0010327913828405144 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.07818527898325556 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0021070788470751722 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 4.9365225207202455e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.00027843358930113027 seconds ---\n",
      "Elapsed time for projection --- 0.000162142099270568 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.0006670111010862456 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.08030644600648884 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.00037013480823588907 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0012927822690471658 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.08224779667307303 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.0024003690587051096 seconds ---\n",
      "Elapsed time to triangulate using triangle module   --- 4.7894829460074104e-05 seconds ---\n",
      "Elapsed time for applying procrustes --- 0.0002966201441267913 seconds ---\n",
      "Elapsed time for projection --- 0.00017136345104518114 seconds ---\n",
      "Elapsed time for getting quality matrix from neural net --- 0.000616549816186307 seconds ---\n",
      "Elapsed time for calculating original quality matrix  --- 0.08400446415862461 seconds ---\n",
      "Elapsed time for ordering the quality matrix  --- 0.0003268456853220414 seconds ---\n",
      "Elapsed time to triangulate according to matrix   --- 0.0013458050402732624 seconds ---\n",
      "Total time  of triangulation with calculation of matrix :   --- 0.08597373502834671 seconds ---\n",
      "Total time  of triangulation using neural network :   --- 0.002430338451631542 seconds ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-4263ba5acd95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtimes4\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtime_elapsed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_time_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtimes4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_elapsed\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtimes5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-e3d194d7e1a9>\u001b[0m in \u001b[0;36mrun_time_test\u001b[0;34m(start, finish)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtime7\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtriangle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtriangulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pq0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mremeshing_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime7\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mremeshing_time\u001b[0m\u001b[1;33m/=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\triangle\\__init__.py\u001b[0m in \u001b[0;36mtriangulate\u001b[0;34m(tri, opts)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn1\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtri\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtri\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "times4=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(4,4)\n",
    "    times4[j]=time_elapsed\n",
    "times5=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(5,5)\n",
    "    times5[j]=time_elapsed\n",
    "    \n",
    "times6=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(6,6)\n",
    "    times6[j]=time_elapsed\n",
    "    \n",
    "times7=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(7,7)\n",
    "    times7[j]=time_elapsed\n",
    "    \n",
    "times8=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(8,8)\n",
    "    times8[j]=time_elapsed\n",
    "    \n",
    "times9=np.empty([20,9])\n",
    "for j in range(20):\n",
    "    time_elapsed=run_time_test(9,9)\n",
    "    times9[j]=time_elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_elapsed_mean=np.array([times4.mean(0),times5.mean(0),times6.mean(0),times7.mean(0),times8.mean(0),times9.mean(0)])\n",
    "times_elapsed_std=np.array([times4.std(0),times5.std(0),times6.std(0),times7.std(0),times8.std(0),times9.std(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1c1f549f780>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEWCAYAAADGjIh1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1f5259b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "procrustes_time_graph=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,0],marker='o')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,0]-times_elapsed_std[:,0],times_elapsed_mean[:,0]+times_elapsed_std[:,0],alpha=.1)\n",
    "plt.title(' Procrustes')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x22332076c88>"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEWCAYAAAC5XZqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+/8BOYdelKZmucsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22333242748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projection_time_graph=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,1],marker='o')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,1]-times_elapsed_std[:,1],times_elapsed_mean[:,1]+times_elapsed_std[:,1],alpha=.1)\n",
    "\n",
    "plt.title(' Projection')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x22332097208>"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22333155400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network_time_graph=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,2],marker='o')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,2]-times_elapsed_std[:,2],times_elapsed_mean[:,2]+times_elapsed_std[:,2],alpha=.1)\n",
    "\n",
    "plt.title(' Network output')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x223331cb8d0>"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYKFgjDGmg4WCMcaYDv8fMScnfpOVh/0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x223320764e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calculation_matrix_graph=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,3],marker='o')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,3]-times_elapsed_std[:,3],times_elapsed_mean[:,3]+times_elapsed_std[:,3],alpha=.1)\n",
    "\n",
    "plt.title(' calculation of matrix')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x2232e27ed68>"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEWCAYAAADGjIh1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVlpOCmZmVlpOCmZmVlpOCmZmVlp/A9zekYBn7nQswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2232e1f50f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#triangulation_graph_with_calculation=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,6],marker='o',label='Calculation of matrix')\n",
    "#triangulation_graph_with_NN=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,5],marker='o',label='Our method')\n",
    "#plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,5]-times_elapsed_std[:,5],times_elapsed_mean[:,5]+times_elapsed_std[:,5],alpha=.1)\n",
    "\n",
    "triangulation_graph_with_library=plt.plot(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,8],marker='o',label='Using library')\n",
    "plt.fill_between(np.array([4,5,6,7,8,9]),times_elapsed_mean[:,8]-times_elapsed_std[:,8],times_elapsed_mean[:,8]+times_elapsed_std[:,8],alpha=.1)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(' Triangulations')\n",
    "plt.xlabel('number of points')\n",
    "plt.ylabel('time (seconds)')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o=3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(o)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_of_contours=100\n",
    "contours=[generate_contour(i) for i in range(4,nb_of_contours) ]\n",
    "processing_ctimes=np.empty([int(nb_of_contours-4)])\n",
    "processing_timer=np.empty([int(nb_of_contours-4)])\n",
    "processing_time=np.empty([int(nb_of_contours-4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for index,j in enumerate(range(4,nb_of_contours)):\n",
    "    contour=contours[index]\n",
    "    shape=dict(vertices=contour,segments=get_contour_edges(contour))\n",
    "    \n",
    "    time1=ctimer.start()\n",
    "    for i in range(1000):\n",
    "        triangulated=triangle.triangulate(shape,'pq0')  \n",
    "    time2=ctimer.stop()\n",
    "    remeshing_time=(ctimer.diff(time2,time1))*10**(-9)\n",
    "    \n",
    "    processing_ctimes[index]=remeshing_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for index,j in enumerate(range(4,nb_of_contours)):\n",
    "    contour=contours[index]\n",
    "\n",
    "    shape=dict(vertices=contour,segments=get_contour_edges(contour))\n",
    "    \n",
    "    time1=timer()\n",
    "    for i in range(1000):\n",
    "        triangulated=triangle.triangulate(shape,'pq0')  \n",
    "    time2=timer()\n",
    "    remeshing_time=time2-time1\n",
    "    \n",
    "    processing_timer[index]=remeshing_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index,j in enumerate(range(4,nb_of_contours)):\n",
    "    contour=contours[index]\n",
    "\n",
    "    \n",
    "    shape=dict(vertices=contour,segments=get_contour_edges(contour))\n",
    "    \n",
    "    time1=time.perf_counter()\n",
    "    for i in range(1000):\n",
    "        triangulated=triangle.triangulate(shape,'pq0')  \n",
    "    time2=time.perf_counter()\n",
    "    remeshing_time=time2-time1\n",
    "    \n",
    "    processing_time[index]=remeshing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.06760359,  0.05959082,  0.05308127,  0.05258083,  0.05258036,\n",
       "         0.04657149,  0.05558467,  0.05308199,  0.05758834,  0.05458331,\n",
       "         0.06159472,  0.05308104,  0.05308199,  0.0565865 ,  0.04506946,\n",
       "         0.06059241,  0.05308175,  0.05108547,  0.06309676,  0.06059265,\n",
       "         0.06309676,  0.063097  ,  0.06359696,  0.06710291,  0.07211065,\n",
       "         0.06810474,  0.08412838,  0.07261181,  0.0751152 ,  0.08112407,\n",
       "         0.07862043,  0.08863616,  0.1011548 ,  0.10365868,  0.07561564,\n",
       "         0.07811999,  0.07862091,  0.07361293,  0.07711792,  0.07962227,\n",
       "         0.076617  ,  0.07661748,  0.07661724,  0.0560863 ,  0.05458379,\n",
       "         0.08563089,  0.08563137,  0.08763433,  0.08162498,  0.07862115,\n",
       "         0.0826261 ,  0.08663273,  0.09564686,  0.08412933,  0.0906384 ,\n",
       "         0.0951457 ,  0.10365939,  0.09814978,  0.11016965,  0.10466003,\n",
       "         0.1076653 ,  0.11066961,  0.11016917,  0.12018418,  0.11116982,\n",
       "         0.10516119,  0.11317372,  0.11317348,  0.13070011,  0.12268806,\n",
       "         0.12018442,  0.13771081,  0.1266942 ,  0.13170171,  0.13270402,\n",
       "         0.13570738,  0.13570809,  0.14572358,  0.14872813,  0.14572334,\n",
       "         0.1477263 ,  0.14772654,  0.14922833,  0.1532352 ,  0.15123177,\n",
       "         0.14271879,  0.13821149,  0.15173316,  0.15173268,  0.13821149,\n",
       "         0.14371991,  0.1592443 ,  0.15673995,  0.16174793,  0.1657548 ,\n",
       "         0.1807766 ]),\n",
       " array([ 0.07337993,  0.0711271 ,  0.0560069 ,  0.0531447 ,  0.05541341,\n",
       "         0.0536196 ,  0.05326278,  0.04559728,  0.05776895,  0.05497949,\n",
       "         0.04778684,  0.05893494,  0.06098976,  0.05976537,  0.05792008,\n",
       "         0.05365264,  0.05507939,  0.05509578,  0.06265396,  0.06037706,\n",
       "         0.05214085,  0.06301052,  0.06427385,  0.06282456,  0.05809375,\n",
       "         0.0606227 ,  0.06911531,  0.06448081,  0.06504588,  0.06537452,\n",
       "         0.06130764,  0.06633405,  0.07603875,  0.06728718,  0.07121009,\n",
       "         0.07282178,  0.07420652,  0.0842204 ,  0.07689788,  0.08232823,\n",
       "         0.07738328,  0.07925239,  0.08792764,  0.08585309,  0.09093021,\n",
       "         0.08192095,  0.08860822,  0.09088667,  0.08970531,  0.09470072,\n",
       "         0.09944331,  0.1035358 ,  0.09803552,  0.09820637,  0.09040562,\n",
       "         0.10554938,  0.09366793,  0.09388847,  0.09877989,  0.10523355,\n",
       "         0.10140567,  0.10796154,  0.10666004,  0.11220515,  0.11306274,\n",
       "         0.09497044,  0.09183442,  0.11717239,  0.10019383,  0.11863935,\n",
       "         0.11420516,  0.11802946,  0.10938342,  0.12136246,  0.13039504,\n",
       "         0.12025129,  0.12055585,  0.13675213,  0.14345144,  0.1347575 ,\n",
       "         0.1480009 ,  0.14563511,  0.15072376,  0.14487384,  0.14721299,\n",
       "         0.15058903,  0.14279724,  0.14881187,  0.14136306,  0.14817303,\n",
       "         0.15477117,  0.16227709,  0.16143948,  0.15963876,  0.16323227,\n",
       "         0.16234984]),\n",
       " array([  5.60000000e-08,   8.57247520e-02,   8.93975040e-02,\n",
       "          8.93967840e-02,   8.93970720e-02,   9.42136800e-02,\n",
       "          8.94237280e-02,   8.94240880e-02,   8.94239440e-02,\n",
       "          8.94243760e-02,   8.94248080e-02,   8.94271120e-02,\n",
       "          8.94266800e-02,   8.94238720e-02,   8.94255280e-02,\n",
       "          8.94267520e-02,   8.94269680e-02,   8.94247360e-02,\n",
       "          8.94248800e-02,   8.94265360e-02,   8.94272560e-02,\n",
       "          8.94235840e-02,   8.94251680e-02,   8.94236560e-02,\n",
       "          8.94243040e-02,   8.93562560e-02,   8.93553920e-02,\n",
       "          8.93565440e-02,   8.93542400e-02,   8.93564000e-02,\n",
       "          8.93569040e-02,   8.93549600e-02,   8.93552480e-02,\n",
       "          8.93566880e-02,   8.93559680e-02,   8.93568320e-02,\n",
       "          8.93569760e-02,   8.93548880e-02,   8.93567600e-02,\n",
       "          8.93550320e-02,   8.93574080e-02,   8.93556800e-02,\n",
       "          8.93554640e-02,   8.93571920e-02,   8.93540240e-02,\n",
       "          8.93544560e-02,   8.93536640e-02,   8.93556080e-02,\n",
       "          8.93555360e-02,   8.93538080e-02,   8.93558960e-02,\n",
       "          8.93545280e-02,   8.93551040e-02,   8.93546000e-02,\n",
       "          8.93576240e-02,   8.93557520e-02,   8.93558240e-02,\n",
       "          8.93573360e-02,   8.93570480e-02,   8.93575520e-02,\n",
       "          8.93543120e-02,   8.93564720e-02,   8.93574800e-02,\n",
       "          8.93560400e-02,   8.93537360e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02,\n",
       "          8.93571200e-02,   8.93571200e-02,   8.93571200e-02]))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv3POJcz/AeCRoKpvZf1TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c1f562fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(processing_time,marker='o',label='time')\n",
    "#plt.plot(processing_ctimes,marker='x')\n",
    "plt.plot(processing_timer,marker='o',label='time_clock')\n",
    "plt.legend()\n",
    "processing_time,processing_timer,processing_ctimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remeshing time : 0.0022776704\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'triangulated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-42742607e414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"remeshing time : %.10f\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtriangulated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'triangulated' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16673614208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shape=dict(vertices=contour,segments=get_contour_edges(contour))\n",
    "\n",
    "time1=timer()\n",
    "Delaunay(contour)\n",
    "\n",
    "time2=timer()\n",
    "\n",
    "print(\"remeshing time : %.10f\"%((time2-time1)))\n",
    "plot.plot(plt.axes(), **triangulated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "from Delaunay_2d import Delaunay2d\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "\n",
    "qualities=[]\n",
    "for j in range(4,30):\n",
    "    print(j)\n",
    "    contour=get_reference_polygon(j)\n",
    "    quality,_=quality_matrix(contour)\n",
    "    qualities.append(quality)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 point polygon\n",
      "Elapsed time using Delaunay: 0.000068\n",
      "Elapsed time using my method: 0.000584\n",
      "5 point polygon\n",
      "Elapsed time using Delaunay: 0.000096\n",
      "Elapsed time using my method: 0.000880\n",
      "6 point polygon\n",
      "Elapsed time using Delaunay: 0.000164\n",
      "Elapsed time using my method: 0.001135\n",
      "7 point polygon\n",
      "Elapsed time using Delaunay: 0.000222\n",
      "Elapsed time using my method: 0.001502\n",
      "8 point polygon\n",
      "Elapsed time using Delaunay: 0.000295\n",
      "Elapsed time using my method: 0.002021\n",
      "9 point polygon\n",
      "Elapsed time using Delaunay: 0.000393\n",
      "Elapsed time using my method: 0.002831\n",
      "10 point polygon\n",
      "Elapsed time using Delaunay: 0.000508\n",
      "Elapsed time using my method: 0.003020\n",
      "11 point polygon\n",
      "Elapsed time using Delaunay: 0.000637\n",
      "Elapsed time using my method: 0.004245\n",
      "12 point polygon\n",
      "Elapsed time using Delaunay: 0.000751\n",
      "Elapsed time using my method: 0.004978\n",
      "13 point polygon\n",
      "Elapsed time using Delaunay: 0.000887\n",
      "Elapsed time using my method: 0.007775\n",
      "14 point polygon\n",
      "Elapsed time using Delaunay: 0.001031\n",
      "Elapsed time using my method: 0.006556\n",
      "15 point polygon\n",
      "Elapsed time using Delaunay: 0.001224\n",
      "Elapsed time using my method: 0.006633\n",
      "16 point polygon\n",
      "Elapsed time using Delaunay: 0.001413\n",
      "Elapsed time using my method: 0.011675\n",
      "17 point polygon\n",
      "Elapsed time using Delaunay: 0.001579\n",
      "Elapsed time using my method: 0.014115\n",
      "18 point polygon\n",
      "Elapsed time using Delaunay: 0.001827\n",
      "Elapsed time using my method: 0.023685\n",
      "19 point polygon\n",
      "Elapsed time using Delaunay: 0.001964\n",
      "Elapsed time using my method: 0.013547\n",
      "20 point polygon\n",
      "Elapsed time using Delaunay: 0.002231\n",
      "Elapsed time using my method: 0.015944\n",
      "21 point polygon\n",
      "Elapsed time using Delaunay: 0.002446\n",
      "Elapsed time using my method: 0.032731\n",
      "22 point polygon\n",
      "Elapsed time using Delaunay: 0.002705\n",
      "Elapsed time using my method: 0.019930\n",
      "23 point polygon\n",
      "Elapsed time using Delaunay: 0.002963\n",
      "Elapsed time using my method: 0.024289\n",
      "24 point polygon\n",
      "Elapsed time using Delaunay: 0.003266\n",
      "Elapsed time using my method: 0.042307\n",
      "25 point polygon\n",
      "Elapsed time using Delaunay: 0.003525\n",
      "Elapsed time using my method: 0.028974\n",
      "26 point polygon\n",
      "Elapsed time using Delaunay: 0.003791\n",
      "Elapsed time using my method: 0.036158\n",
      "27 point polygon\n",
      "Elapsed time using Delaunay: 0.004307\n",
      "Elapsed time using my method: 0.044887\n",
      "28 point polygon\n",
      "Elapsed time using Delaunay: 0.004461\n",
      "Elapsed time using my method: 0.045261\n",
      "29 point polygon\n",
      "Elapsed time using Delaunay: 0.004882\n",
      "Elapsed time using my method: 0.117225\n"
     ]
    }
   ],
   "source": [
    "time_delaunay=[]\n",
    "time_triangulation=[]\n",
    "\n",
    "\n",
    "for index,j in enumerate(range(4,30)):\n",
    "    nb_of_repetitions=100\n",
    "    print(\"{} point polygon\".format(j))\n",
    "    contour = [np.array([i[0],i[1]]) for i in get_reference_polygon(j)]\n",
    "    start=timer()\n",
    "    for i in range(nb_of_repetitions):\n",
    "        delaunay = Delaunay2d(contour)\n",
    "    stop=timer()\n",
    "    delaunay_remeshing_time=(stop-start)/nb_of_repetitions\n",
    "    print(\"Elapsed time using Delaunay: %f\"%(delaunay_remeshing_time))\n",
    "    time_delaunay.append(delaunay_remeshing_time)\n",
    "    \n",
    "    \n",
    "    contour=get_reference_polygon(j)\n",
    "    quality=qualities[index]\n",
    "    ordered_matrix=order_quality_matrix(quality,contour)\n",
    "    start=timer()\n",
    "    for i in range(nb_of_repetitions):\n",
    "        pure_triangulate(contour,ordered_matrix,recursive=False)\n",
    "    stop=timer()\n",
    "    construction_from_matrix_time=(stop-start)/nb_of_repetitions\n",
    "    print(\"Elapsed time using my method: %f\"%(construction_from_matrix_time))\n",
    "    time_triangulation.append(construction_from_matrix_time)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1667510b358>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x166750576d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_delaunay[:],label='Delaunay') \n",
    "#plt.plot(time_scipy_triangulation[:],label='Scipy  Delaunay')\n",
    "plt.plot(time_triangulation[:],label='Construction from matrix')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c209819e48>]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDX+Dzd7Jv6ajfm4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c209bdd438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(time_delaunay)/np.array(time_triangulation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ],\n",
       "       [ 0.9819287 ,  0.18925124],\n",
       "       [ 0.92836793,  0.37166246],\n",
       "       [ 0.84125353,  0.54064082],\n",
       "       [ 0.72373404,  0.69007901],\n",
       "       [ 0.58005691,  0.81457595],\n",
       "       [ 0.41541501,  0.909632  ],\n",
       "       [ 0.23575894,  0.97181157],\n",
       "       [ 0.04758192,  0.99886734],\n",
       "       [-0.14231484,  0.98982144],\n",
       "       [-0.32706796,  0.94500082],\n",
       "       [-0.5       ,  0.8660254 ],\n",
       "       [-0.65486073,  0.75574957],\n",
       "       [-0.78605309,  0.61815899],\n",
       "       [-0.88883545,  0.45822652],\n",
       "       [-0.95949297,  0.28173256],\n",
       "       [-0.99547192,  0.09505604],\n",
       "       [-0.99547192, -0.09505604],\n",
       "       [-0.95949297, -0.28173256],\n",
       "       [-0.88883545, -0.45822652],\n",
       "       [-0.78605309, -0.61815899],\n",
       "       [-0.65486073, -0.75574957],\n",
       "       [-0.5       , -0.8660254 ],\n",
       "       [-0.32706796, -0.94500082],\n",
       "       [-0.14231484, -0.98982144],\n",
       "       [ 0.04758192, -0.99886734],\n",
       "       [ 0.23575894, -0.97181157],\n",
       "       [ 0.41541501, -0.909632  ],\n",
       "       [ 0.58005691, -0.81457595],\n",
       "       [ 0.72373404, -0.69007901],\n",
       "       [ 0.84125353, -0.54064082],\n",
       "       [ 0.92836793, -0.37166246],\n",
       "       [ 0.9819287 , -0.18925124]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tri=Delaunay(contour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Triangulation import check_ordered_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_contour_edges' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b244250e9743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontour\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_contour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'23_polygon'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtriangulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontour\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcheck_ordered_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder_quality_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquality_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcontour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcontour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-b1763b8256f3>\u001b[0m in \u001b[0;36mquality_matrix\u001b[0;34m(polygon, compute_minimum, normalize, mean)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mpolygon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapply_procrustes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mcontour_connectivity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_contour_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolygon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_contour_edges' is not defined"
     ]
    }
   ],
   "source": [
    "contour=read_contour('23_polygon')\n",
    "triangulate(contour,check_ordered_matrix(order_quality_matrix(quality_matrix(contour)[0],contour),contour),recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_contour('23_polygon',contour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_contour' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-744-1bf40519fa33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_contour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontour\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_contour' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polygons=load_dataset('12_polygons.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
