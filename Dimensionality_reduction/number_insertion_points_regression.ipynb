{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Triangulation import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(net,x_variable,target_value):\n",
    "  \n",
    "    net.eval()\n",
    "    prediction=net(variable)\n",
    "    prediction=prediction.numpy()\n",
    "    count=np.sum(prediction==target_value.numpy())    \n",
    "    net.train()\n",
    "    return count/x_variable.numpy().shape[0]\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polygons=load_dataset('12_polygons.pkl')\n",
    "nb_of_points=load_dataset('12_nb_of_points.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_of_points=nb_of_points[:431350]\n",
    "polygons=polygons[:43135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polygons=[i for i in polygons for j in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(1,2*12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index,polygon in enumerate(polygons_reshaped):\n",
    "    if (index+1)%10==0:\n",
    "        polygons_reshaped[index]=np.append(polygon,1)\n",
    "    else:    \n",
    "        polygons_reshaped[index]=np.append(polygon,((0.1*((index+1)%10))))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_of_points=[i[0] for i in nb_of_points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "431350"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_of_points=np.array(nb_of_points)\n",
    "polygons_reshaped=np.array(polygons_reshaped)\n",
    "polygons_reshaped.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_of_test_data=int(len(polygons_reshaped)*0.2)\n",
    "nb_of_training_data=int(len(polygons_reshaped)-nb_of_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(polygons_reshaped[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "x_tensor_test=torch.from_numpy(polygons_reshaped[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "x_variable,x_variable_test=Variable(x_tensor),Variable(x_tensor_test)\n",
    "\n",
    "y_tensor=torch.from_numpy(nb_of_points[:nb_of_training_data]).type(torch.FloatTensor)\n",
    "y_tensor_test=torch.from_numpy(nb_of_points[nb_of_training_data:]).type(torch.FloatTensor)\n",
    "y_variable,y_variable_test=Variable(y_tensor),Variable(y_tensor_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B_INIT = -0.2 # use a bad bias constant initializer\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,in_features_dimension,out_features_dimension,nb_of_hidden_layers,nb_of_hidden_nodes,\n",
    "                 batch_normalization=False):\n",
    "        \n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.nb_hidden_layers=nb_of_hidden_layers\n",
    "        self.do_bn=batch_normalization\n",
    "        self.fcs=[]\n",
    "        self.bns=[]\n",
    "        self.bn_input=nn.BatchNorm1d(in_features_dimension,momentum=0.5) #for input data\n",
    "        \n",
    "        for i in range(nb_of_hidden_layers):                              # build hidden layers and BN layers\n",
    "            \n",
    "            input_size=in_features_dimension if i==0 else nb_of_hidden_nodes\n",
    "            fc=nn.Linear(input_size,nb_of_hidden_nodes)\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self._set_init(fc)                  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "            \n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(nb_of_hidden_nodes, momentum=0.5)\n",
    "                setattr(self, 'bn%i' % i, bn)                         # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "    \n",
    "            self.predict = nn.Linear(nb_of_hidden_nodes,out_features_dimension)         # output layer\n",
    "            self._set_init(self.predict)                                              # parameters initialization\n",
    "    \n",
    "    \n",
    "    def _set_init(self, layer):\n",
    "            init.normal(layer.weight, mean=0., std=.1)\n",
    "            init.constant(layer.bias, B_INIT)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ACTIVATION=F.relu\n",
    "        pre_activation = [x]\n",
    "        if self.do_bn: x = self.bn_input(x)     # input batch normalization\n",
    "        layer_input = [x]\n",
    "        for i in range(self.nb_hidden_layers):\n",
    "            x = self.fcs[i](x)\n",
    "            pre_activation.append(x)\n",
    "            if self.do_bn: x = self.bns[i](x)   # batch normalization\n",
    "            x = ACTIVATION(x)\n",
    "            layer_input.append(x)\n",
    "        out = self.predict(x)\n",
    "        return out\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "my_net=Net(25,1,nb_of_hidden_layers=5,\n",
    "           nb_of_hidden_nodes=50,batch_normalization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(my_net.parameters(), lr=1e-4,weight_decay=1e-4)\n",
    "#optimizer = torch.optim.SGD(my_net.parameters(), lr=1e-5,weight_decay=.5,momentum=0.9)\n",
    "\n",
    "loss_func = torch.nn.MSELoss(size_average=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda activated\n"
     ]
    }
   ],
   "source": [
    "if  torch.cuda.is_available():\n",
    "    my_net.cuda()\n",
    "    loss_func.cuda()\n",
    "    x_variable , y_variable,x_variable_test,y_variable_test= x_variable.cuda(), y_variable.cuda(),x_variable_test.cuda(),y_variable_test.cuda()\n",
    "    print(\"cuda activated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 162.31195411208995 Epoch: 0 Test Loss: 161.03154051234498\n",
      "Epoch: 10 Training Loss: 133.04454909006608 Epoch: 10 Test Loss: 130.99659209458676\n",
      "Epoch: 20 Training Loss: 109.28289925524516 Epoch: 20 Test Loss: 107.70055639272053\n",
      "Epoch: 30 Training Loss: 86.41315799959429 Epoch: 30 Test Loss: 84.56654688767821\n",
      "Epoch: 40 Training Loss: 60.91281469152081 Epoch: 40 Test Loss: 58.89485336733511\n",
      "Epoch: 50 Training Loss: 37.5614007983656 Epoch: 50 Test Loss: 35.6617885707662\n",
      "Epoch: 60 Training Loss: 21.74529456937522 Epoch: 60 Test Loss: 20.398907499710212\n",
      "Epoch: 70 Training Loss: 11.63598794120204 Epoch: 70 Test Loss: 10.827442911788571\n",
      "Epoch: 80 Training Loss: 5.926790266478063 Epoch: 80 Test Loss: 5.467788628723774\n",
      "Epoch: 90 Training Loss: 3.224053208676249 Epoch: 90 Test Loss: 3.100265880375565\n",
      "Epoch: 100 Training Loss: 2.456267268414208 Epoch: 100 Test Loss: 2.491518271125536\n",
      "Epoch: 110 Training Loss: 2.2742875689604585 Epoch: 110 Test Loss: 2.3653991465747075\n",
      "Epoch: 120 Training Loss: 2.2035328990741276 Epoch: 120 Test Loss: 2.3161339399559524\n",
      "Epoch: 130 Training Loss: 2.1485055322316566 Epoch: 130 Test Loss: 2.2750362234844093\n",
      "Epoch: 140 Training Loss: 2.1005518421452996 Epoch: 140 Test Loss: 2.230892655326301\n",
      "Epoch: 150 Training Loss: 2.056834199904732 Epoch: 150 Test Loss: 2.193129853946911\n",
      "Epoch: 160 Training Loss: 2.015633668166621 Epoch: 160 Test Loss: 2.1620673104787294\n",
      "Epoch: 170 Training Loss: 1.9761118543843095 Epoch: 170 Test Loss: 2.1255125623043933\n",
      "Epoch: 180 Training Loss: 1.9386976644229237 Epoch: 180 Test Loss: 2.0936163353425292\n",
      "Epoch: 190 Training Loss: 1.905464462611025 Epoch: 190 Test Loss: 2.0681081198562654\n",
      "Epoch: 200 Training Loss: 1.8750145941022733 Epoch: 200 Test Loss: 2.0330235017966847\n",
      "Epoch: 210 Training Loss: 1.8475538988468254 Epoch: 210 Test Loss: 2.0066989900892547\n",
      "Epoch: 220 Training Loss: 1.8225407678337269 Epoch: 220 Test Loss: 1.9813588515706504\n",
      "Epoch: 230 Training Loss: 1.7985383399546844 Epoch: 230 Test Loss: 1.9553179697461458\n",
      "Epoch: 240 Training Loss: 1.7755927548729642 Epoch: 240 Test Loss: 1.9325844731656427\n",
      "Epoch: 250 Training Loss: 1.75308696534137 Epoch: 250 Test Loss: 1.908967304682972\n",
      "Epoch: 260 Training Loss: 1.73144097417175 Epoch: 260 Test Loss: 1.8894174901472123\n",
      "Epoch: 270 Training Loss: 1.7102559013716023 Epoch: 270 Test Loss: 1.8740214225686798\n",
      "Epoch: 280 Training Loss: 1.6896888436648747 Epoch: 280 Test Loss: 1.8519069853367336\n",
      "Epoch: 290 Training Loss: 1.6697943711648386 Epoch: 290 Test Loss: 1.8361801973455432\n",
      "Epoch: 300 Training Loss: 1.650117086753434 Epoch: 300 Test Loss: 1.8166541091920714\n",
      "Epoch: 310 Training Loss: 1.6312447475947607 Epoch: 310 Test Loss: 1.8001488785209228\n",
      "Epoch: 320 Training Loss: 1.611313627116357 Epoch: 320 Test Loss: 1.7775532122985975\n",
      "Epoch: 330 Training Loss: 1.5903493647532818 Epoch: 330 Test Loss: 1.755283376318535\n",
      "Epoch: 340 Training Loss: 1.5695380164336894 Epoch: 340 Test Loss: 1.7344646531239134\n",
      "Epoch: 350 Training Loss: 1.550612782497899 Epoch: 350 Test Loss: 1.7165840167497393\n",
      "Epoch: 360 Training Loss: 1.533835329511454 Epoch: 360 Test Loss: 1.6985829372899037\n",
      "Epoch: 370 Training Loss: 1.5189338709728541 Epoch: 370 Test Loss: 1.685906710038252\n",
      "Epoch: 380 Training Loss: 1.5054369044632767 Epoch: 380 Test Loss: 1.6743626477918163\n",
      "Epoch: 390 Training Loss: 1.4934726101782558 Epoch: 390 Test Loss: 1.6641994247710676\n",
      "Epoch: 400 Training Loss: 1.4826963103437971 Epoch: 400 Test Loss: 1.6565729323635099\n",
      "Epoch: 410 Training Loss: 1.4724894634939725 Epoch: 410 Test Loss: 1.6518847078938217\n",
      "Epoch: 420 Training Loss: 1.4631506982982208 Epoch: 420 Test Loss: 1.646143466732352\n",
      "Epoch: 430 Training Loss: 1.4545698642343805 Epoch: 430 Test Loss: 1.6418866639619798\n",
      "Epoch: 440 Training Loss: 1.4462605280161485 Epoch: 440 Test Loss: 1.6347263677987713\n",
      "Epoch: 450 Training Loss: 1.438309144912955 Epoch: 450 Test Loss: 1.6259073982844559\n",
      "Epoch: 460 Training Loss: 1.430673930569542 Epoch: 460 Test Loss: 1.6229689492291643\n",
      "Epoch: 470 Training Loss: 1.423329030473912 Epoch: 470 Test Loss: 1.618422901935783\n",
      "Epoch: 480 Training Loss: 1.4161439551833994 Epoch: 480 Test Loss: 1.6133503462965109\n",
      "Epoch: 490 Training Loss: 1.4090001548553959 Epoch: 490 Test Loss: 1.6104204097600556\n",
      "Epoch: 500 Training Loss: 1.4020595428052915 Epoch: 500 Test Loss: 1.602974672539701\n",
      "Epoch: 510 Training Loss: 1.3951560541667873 Epoch: 510 Test Loss: 1.5983367987133419\n",
      "Epoch: 520 Training Loss: 1.3883965651986496 Epoch: 520 Test Loss: 1.5938666396197982\n",
      "Epoch: 530 Training Loss: 1.3816749580713168 Epoch: 530 Test Loss: 1.5908006838993856\n",
      "Epoch: 540 Training Loss: 1.3753425270046076 Epoch: 540 Test Loss: 1.5871444665005217\n",
      "Epoch: 550 Training Loss: 1.369203515194846 Epoch: 550 Test Loss: 1.5814370942969747\n",
      "Epoch: 560 Training Loss: 1.3631760519073475 Epoch: 560 Test Loss: 1.5799658774776864\n",
      "Epoch: 570 Training Loss: 1.35722575596148 Epoch: 570 Test Loss: 1.5770263417178625\n",
      "Epoch: 580 Training Loss: 1.35155442895488 Epoch: 580 Test Loss: 1.5760070128665817\n",
      "Epoch: 590 Training Loss: 1.3461330383307857 Epoch: 590 Test Loss: 1.573934486206097\n",
      "Epoch: 600 Training Loss: 1.3409750688472601 Epoch: 600 Test Loss: 1.5702398719137591\n",
      "Epoch: 610 Training Loss: 1.335924516144807 Epoch: 610 Test Loss: 1.5683203894749043\n",
      "Epoch: 620 Training Loss: 1.330957294436254 Epoch: 620 Test Loss: 1.5651236669757738\n",
      "Epoch: 630 Training Loss: 1.3262308994698693 Epoch: 630 Test Loss: 1.5641898255476991\n",
      "Epoch: 640 Training Loss: 1.3215979667531805 Epoch: 640 Test Loss: 1.5620330792859627\n",
      "Epoch: 650 Training Loss: 1.3169731333812087 Epoch: 650 Test Loss: 1.5592920482207024\n",
      "Epoch: 660 Training Loss: 1.312565363579641 Epoch: 660 Test Loss: 1.5580372667207605\n",
      "Epoch: 670 Training Loss: 1.3085818188898226 Epoch: 670 Test Loss: 1.5562507244696882\n",
      "Epoch: 680 Training Loss: 1.304684220076685 Epoch: 680 Test Loss: 1.55462555784166\n",
      "Epoch: 690 Training Loss: 1.3008915533399863 Epoch: 690 Test Loss: 1.5527453778833893\n",
      "Epoch: 700 Training Loss: 1.2970304185578345 Epoch: 700 Test Loss: 1.5529572852671845\n",
      "Epoch: 710 Training Loss: 1.2933429442538686 Epoch: 710 Test Loss: 1.5475636084386228\n",
      "Epoch: 720 Training Loss: 1.2897541902647573 Epoch: 720 Test Loss: 1.5459047539700939\n",
      "Epoch: 730 Training Loss: 1.2862714126072214 Epoch: 730 Test Loss: 1.5462340254433755\n",
      "Epoch: 740 Training Loss: 1.282922978797489 Epoch: 740 Test Loss: 1.544646712356555\n",
      "Epoch: 750 Training Loss: 1.2797046976652153 Epoch: 750 Test Loss: 1.543903587573896\n",
      "Epoch: 760 Training Loss: 1.2765910259939723 Epoch: 760 Test Loss: 1.5427737770951664\n",
      "Epoch: 770 Training Loss: 1.2735247363383129 Epoch: 770 Test Loss: 1.5425439390865887\n",
      "Epoch: 780 Training Loss: 1.2705847392950547 Epoch: 780 Test Loss: 1.54159470267764\n",
      "Epoch: 790 Training Loss: 1.2676936354658703 Epoch: 790 Test Loss: 1.540914244523009\n",
      "Epoch: 800 Training Loss: 1.264781858780935 Epoch: 800 Test Loss: 1.5398536208995015\n",
      "Epoch: 810 Training Loss: 1.2619817975858858 Epoch: 810 Test Loss: 1.5390154094702677\n",
      "Epoch: 820 Training Loss: 1.259199471748399 Epoch: 820 Test Loss: 1.537265452938449\n",
      "Epoch: 830 Training Loss: 1.2565401048307638 Epoch: 830 Test Loss: 1.5354801785093313\n",
      "Epoch: 840 Training Loss: 1.2539380304476497 Epoch: 840 Test Loss: 1.5341802987712994\n",
      "Epoch: 850 Training Loss: 1.251378900703279 Epoch: 850 Test Loss: 1.532570346006723\n",
      "Epoch: 860 Training Loss: 1.2489374576638026 Epoch: 860 Test Loss: 1.5313117610409182\n",
      "Epoch: 870 Training Loss: 1.2465814426183783 Epoch: 870 Test Loss: 1.5334201489509678\n",
      "Epoch: 880 Training Loss: 1.2441250602215428 Epoch: 880 Test Loss: 1.5312893024805843\n",
      "Epoch: 890 Training Loss: 1.241705594649248 Epoch: 890 Test Loss: 1.528806544859163\n",
      "Epoch: 900 Training Loss: 1.239383757027356 Epoch: 900 Test Loss: 1.5270037020401066\n",
      "Epoch: 910 Training Loss: 1.2369717314191637 Epoch: 910 Test Loss: 1.5271377289324215\n",
      "Epoch: 920 Training Loss: 1.234643826363633 Epoch: 920 Test Loss: 1.5281648458328503\n",
      "Epoch: 930 Training Loss: 1.2324103514040223 Epoch: 930 Test Loss: 1.5255491480236467\n",
      "Epoch: 940 Training Loss: 1.2302920841045917 Epoch: 940 Test Loss: 1.5254935449750782\n",
      "Epoch: 950 Training Loss: 1.2280296473372116 Epoch: 950 Test Loss: 1.5236318389938566\n",
      "Epoch: 960 Training Loss: 1.2257491016575865 Epoch: 960 Test Loss: 1.521925169525907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 970 Training Loss: 1.22353313848872 Epoch: 970 Test Loss: 1.5213191506317376\n",
      "Epoch: 980 Training Loss: 1.2213589200556756 Epoch: 980 Test Loss: 1.5203608583516866\n",
      "Epoch: 990 Training Loss: 1.2192197365239583 Epoch: 990 Test Loss: 1.5204505114756\n",
      "Epoch: 1000 Training Loss: 1.217099759928857 Epoch: 1000 Test Loss: 1.519369059348557\n",
      "Epoch: 1010 Training Loss: 1.2150532122985975 Epoch: 1010 Test Loss: 1.5205979410571462\n",
      "Epoch: 1020 Training Loss: 1.2130350802938812 Epoch: 1020 Test Loss: 1.518468452967428\n",
      "Epoch: 1030 Training Loss: 1.211065793393923 Epoch: 1030 Test Loss: 1.518130216181755\n",
      "Epoch: 1040 Training Loss: 1.2091057208428118 Epoch: 1040 Test Loss: 1.5170655174162513\n",
      "Epoch: 1050 Training Loss: 1.2072280991681277 Epoch: 1050 Test Loss: 1.5182819020227194\n",
      "Epoch: 1060 Training Loss: 1.2053102184864741 Epoch: 1060 Test Loss: 1.5166390764460416\n",
      "Epoch: 1070 Training Loss: 1.2034773611146328 Epoch: 1070 Test Loss: 1.5161452597948302\n",
      "Epoch: 1080 Training Loss: 1.2016471327753708 Epoch: 1080 Test Loss: 1.5181693375449172\n",
      "Epoch: 1090 Training Loss: 1.1998841216393663 Epoch: 1090 Test Loss: 1.5165148298945172\n",
      "Epoch: 1100 Training Loss: 1.198140688164701 Epoch: 1100 Test Loss: 1.5137036158282138\n",
      "Epoch: 1110 Training Loss: 1.1963796183810638 Epoch: 1110 Test Loss: 1.514269698330822\n",
      "Epoch: 1120 Training Loss: 1.1946105822608164 Epoch: 1120 Test Loss: 1.5133398414860322\n",
      "Epoch: 1130 Training Loss: 1.1929200435632679 Epoch: 1130 Test Loss: 1.5151645995131564\n",
      "Epoch: 1140 Training Loss: 1.1912441895267039 Epoch: 1140 Test Loss: 1.51153736090182\n",
      "Epoch: 1150 Training Loss: 1.1885361614516923 Epoch: 1150 Test Loss: 1.5116252934102237\n",
      "Epoch: 1160 Training Loss: 1.186228411369284 Epoch: 1160 Test Loss: 1.5077110742436537\n",
      "Epoch: 1170 Training Loss: 1.1842785012035253 Epoch: 1170 Test Loss: 1.5079258795062014\n",
      "Epoch: 1180 Training Loss: 1.1824081666751187 Epoch: 1180 Test Loss: 1.5066647588964879\n",
      "Epoch: 1190 Training Loss: 1.1806390824455562 Epoch: 1190 Test Loss: 1.506735123014953\n",
      "Epoch: 1200 Training Loss: 1.178820932374377 Epoch: 1200 Test Loss: 1.505734992610409\n",
      "Epoch: 1210 Training Loss: 1.1769186136412209 Epoch: 1210 Test Loss: 1.5079683515416715\n",
      "Epoch: 1220 Training Loss: 1.1750962469300597 Epoch: 1220 Test Loss: 1.5082139467659672\n",
      "Epoch: 1230 Training Loss: 1.173340106936254 Epoch: 1230 Test Loss: 1.5081259236988525\n",
      "Epoch: 1240 Training Loss: 1.1716828174354497 Epoch: 1240 Test Loss: 1.5081645922684594\n",
      "Epoch: 1250 Training Loss: 1.170028872947034 Epoch: 1250 Test Loss: 1.5085031007302654\n",
      "Epoch: 1260 Training Loss: 1.1684349009649937 Epoch: 1260 Test Loss: 1.50825487930335\n",
      "Epoch: 1270 Training Loss: 1.1669084659716586 Epoch: 1270 Test Loss: 1.508847586066999\n",
      "Epoch: 1280 Training Loss: 1.1653805282697127 Epoch: 1280 Test Loss: 1.5090178364437232\n",
      "Epoch: 1290 Training Loss: 1.1639206001597455 Epoch: 1290 Test Loss: 1.5106248913295468\n",
      "Epoch: 1300 Training Loss: 1.162450042721072 Epoch: 1300 Test Loss: 1.5100690419612843\n",
      "Epoch: 1310 Training Loss: 1.1610574478427105 Epoch: 1310 Test Loss: 1.5087827460299061\n",
      "Epoch: 1320 Training Loss: 1.1596070906565143 Epoch: 1320 Test Loss: 1.5095387301495304\n",
      "Epoch: 1330 Training Loss: 1.1581975895987886 Epoch: 1330 Test Loss: 1.5097586066998956\n",
      "Epoch: 1340 Training Loss: 1.1568616703237293 Epoch: 1340 Test Loss: 1.5102223578590472\n",
      "Epoch: 1350 Training Loss: 1.1555968254870248 Epoch: 1350 Test Loss: 1.5079195403964298\n",
      "Epoch: 1360 Training Loss: 1.1542966033238669 Epoch: 1360 Test Loss: 1.5084852606641939\n",
      "Epoch: 1370 Training Loss: 1.1530466384153675 Epoch: 1370 Test Loss: 1.509113285325142\n",
      "Epoch: 1380 Training Loss: 1.1517452389889664 Epoch: 1380 Test Loss: 1.5093890366002087\n",
      "Epoch: 1390 Training Loss: 1.1505146564745856 Epoch: 1390 Test Loss: 1.5099344717167034\n",
      "Epoch: 1400 Training Loss: 1.1492441007791672 Epoch: 1400 Test Loss: 1.510017242378579\n",
      "Epoch: 1410 Training Loss: 1.1479540947705966 Epoch: 1410 Test Loss: 1.5138388199837718\n",
      "Epoch: 1420 Training Loss: 1.14675111285336 Epoch: 1420 Test Loss: 1.509786679900313\n",
      "Epoch: 1430 Training Loss: 1.1455567623132679 Epoch: 1430 Test Loss: 1.5111139989277849\n",
      "Epoch: 1440 Training Loss: 1.1444356426408369 Epoch: 1440 Test Loss: 1.5119930523356904\n",
      "Epoch: 1450 Training Loss: 1.1433577223035238 Epoch: 1450 Test Loss: 1.5107192535064333\n",
      "Epoch: 1460 Training Loss: 1.1422354932868828 Epoch: 1460 Test Loss: 1.5112487502897878\n",
      "Epoch: 1470 Training Loss: 1.1411205797161166 Epoch: 1470 Test Loss: 1.5110248891561378\n",
      "Epoch: 1480 Training Loss: 1.140037084358156 Epoch: 1480 Test Loss: 1.511495522777327\n",
      "Epoch: 1490 Training Loss: 1.1389641504098686 Epoch: 1490 Test Loss: 1.5116100795467717\n",
      "Epoch: 1500 Training Loss: 1.1378742055737074 Epoch: 1500 Test Loss: 1.511926310565666\n",
      "Epoch: 1510 Training Loss: 1.13682447729512 Epoch: 1510 Test Loss: 1.5109248217804567\n",
      "Epoch: 1520 Training Loss: 1.1357162311094529 Epoch: 1520 Test Loss: 1.5134137373942274\n",
      "Epoch: 1530 Training Loss: 1.1345913419306755 Epoch: 1530 Test Loss: 1.510667544482439\n",
      "Epoch: 1540 Training Loss: 1.1335893069632403 Epoch: 1540 Test Loss: 1.5125911020632896\n",
      "Epoch: 1550 Training Loss: 1.1325689932859773 Epoch: 1550 Test Loss: 1.5135107257737337\n",
      "Epoch: 1560 Training Loss: 1.1316284420234075 Epoch: 1560 Test Loss: 1.5112463052045901\n",
      "Epoch: 1570 Training Loss: 1.1307004784216703 Epoch: 1570 Test Loss: 1.5139249413179552\n",
      "Epoch: 1580 Training Loss: 1.1297165721761984 Epoch: 1580 Test Loss: 1.5139149798597427\n",
      "Epoch: 1590 Training Loss: 1.1288504941336066 Epoch: 1590 Test Loss: 1.5148460139677755\n",
      "Epoch: 1600 Training Loss: 1.1279384264156138 Epoch: 1600 Test Loss: 1.5127896973165642\n",
      "Epoch: 1610 Training Loss: 1.1270399509941535 Epoch: 1610 Test Loss: 1.5143173322128203\n",
      "Epoch: 1620 Training Loss: 1.1262010320749898 Epoch: 1620 Test Loss: 1.5131186065550017\n",
      "Epoch: 1630 Training Loss: 1.125351189511309 Epoch: 1630 Test Loss: 1.5139210472933813\n",
      "Epoch: 1640 Training Loss: 1.1245074681505014 Epoch: 1640 Test Loss: 1.5129127666048452\n",
      "Epoch: 1650 Training Loss: 1.1235424971655124 Epoch: 1650 Test Loss: 1.5144192107627217\n",
      "Epoch: 1660 Training Loss: 1.122672380770401 Epoch: 1660 Test Loss: 1.5151947555639271\n",
      "Epoch: 1670 Training Loss: 1.1217472895777791 Epoch: 1670 Test Loss: 1.5160578706386925\n",
      "Epoch: 1680 Training Loss: 1.120824581211241 Epoch: 1680 Test Loss: 1.5168688238959083\n",
      "Epoch: 1690 Training Loss: 1.119974860335828 Epoch: 1690 Test Loss: 1.5149019792511882\n",
      "Epoch: 1700 Training Loss: 1.1191082389409701 Epoch: 1700 Test Loss: 1.515567585777211\n",
      "Epoch: 1710 Training Loss: 1.1182883543308797 Epoch: 1710 Test Loss: 1.5144082531586878\n",
      "Epoch: 1720 Training Loss: 1.1174852344021675 Epoch: 1720 Test Loss: 1.5160836798713342\n",
      "Epoch: 1730 Training Loss: 1.1166961737133418 Epoch: 1730 Test Loss: 1.5179699272632432\n",
      "Epoch: 1740 Training Loss: 1.1158041534300018 Epoch: 1740 Test Loss: 1.5204713399791352\n",
      "Epoch: 1750 Training Loss: 1.1150400897753783 Epoch: 1750 Test Loss: 1.5174044786716123\n",
      "Epoch: 1760 Training Loss: 1.1141982729774618 Epoch: 1760 Test Loss: 1.5167499203083343\n",
      "Epoch: 1770 Training Loss: 1.113374304735496 Epoch: 1770 Test Loss: 1.5168360416425177\n",
      "Epoch: 1780 Training Loss: 1.112520590786919 Epoch: 1780 Test Loss: 1.517992476382288\n",
      "Epoch: 1790 Training Loss: 1.1117349571793136 Epoch: 1790 Test Loss: 1.5163700265155906\n",
      "Epoch: 1800 Training Loss: 1.1109637025970427 Epoch: 1800 Test Loss: 1.5176154804682973\n",
      "Epoch: 1810 Training Loss: 1.110186142864517 Epoch: 1810 Test Loss: 1.5167118856497044\n",
      "Epoch: 1820 Training Loss: 1.1094634022817174 Epoch: 1820 Test Loss: 1.516456600643329\n",
      "Epoch: 1830 Training Loss: 1.1086779328117755 Epoch: 1830 Test Loss: 1.5182253028283297\n",
      "Epoch: 1840 Training Loss: 1.1080001291593615 Epoch: 1840 Test Loss: 1.517268731163788\n",
      "Epoch: 1850 Training Loss: 1.1072268766255289 Epoch: 1850 Test Loss: 1.5176438253448477\n",
      "Epoch: 1860 Training Loss: 1.106444494641641 Epoch: 1860 Test Loss: 1.5170383498029443\n",
      "Epoch: 1870 Training Loss: 1.1056709591118363 Epoch: 1870 Test Loss: 1.517556526747421\n",
      "Epoch: 1880 Training Loss: 1.1049712006319188 Epoch: 1880 Test Loss: 1.5172923669873652\n",
      "Epoch: 1890 Training Loss: 1.1042505740290296 Epoch: 1890 Test Loss: 1.518538273733627\n",
      "Epoch: 1900 Training Loss: 1.1035703790606526 Epoch: 1900 Test Loss: 1.5162882519995364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1910 Training Loss: 1.1028623146489582 Epoch: 1910 Test Loss: 1.5176559602121247\n",
      "Epoch: 1920 Training Loss: 1.1021582376505086 Epoch: 1920 Test Loss: 1.5193456952011128\n",
      "Epoch: 1930 Training Loss: 1.1014865835005652 Epoch: 1930 Test Loss: 1.5184717130810248\n",
      "Epoch: 1940 Training Loss: 1.1008598719409268 Epoch: 1940 Test Loss: 1.519146828271705\n",
      "Epoch: 1950 Training Loss: 1.1001427544880897 Epoch: 1950 Test Loss: 1.5176408369073837\n",
      "Epoch: 1960 Training Loss: 1.0994521226282674 Epoch: 1960 Test Loss: 1.5179365110988756\n",
      "Epoch: 1970 Training Loss: 1.0988051259173597 Epoch: 1970 Test Loss: 1.5200765039990727\n",
      "Epoch: 1980 Training Loss: 1.0980714050908484 Epoch: 1980 Test Loss: 1.519608315463081\n",
      "Epoch: 1990 Training Loss: 1.0974040920991146 Epoch: 1990 Test Loss: 1.5195293482670684\n",
      "Epoch: 2000 Training Loss: 1.0966497210565305 Epoch: 2000 Test Loss: 1.5186289230033616\n",
      "Epoch: 2010 Training Loss: 1.095943281041715 Epoch: 2010 Test Loss: 1.5215232699663845\n",
      "Epoch: 2020 Training Loss: 1.0951709199451938 Epoch: 2020 Test Loss: 1.5220282253390518\n",
      "Epoch: 2030 Training Loss: 1.0945030437914758 Epoch: 2030 Test Loss: 1.5196479801785094\n",
      "Epoch: 2040 Training Loss: 1.0938229790012461 Epoch: 2040 Test Loss: 1.5191453793323286\n",
      "Epoch: 2050 Training Loss: 1.0931248845376433 Epoch: 2050 Test Loss: 1.520496877535644\n",
      "Epoch: 2060 Training Loss: 1.0924698479111727 Epoch: 2060 Test Loss: 1.5218922061550946\n",
      "Epoch: 2070 Training Loss: 1.0918581860773227 Epoch: 2070 Test Loss: 1.5197014098180133\n",
      "Epoch: 2080 Training Loss: 1.091203240009563 Epoch: 2080 Test Loss: 1.521124268285615\n",
      "Epoch: 2090 Training Loss: 1.0905437037471384 Epoch: 2090 Test Loss: 1.5201455097368726\n",
      "Epoch: 2100 Training Loss: 1.0897782081328966 Epoch: 2100 Test Loss: 1.5216080329199027\n",
      "Epoch: 2110 Training Loss: 1.0891221895104033 Epoch: 2110 Test Loss: 1.522159535470036\n",
      "Epoch: 2120 Training Loss: 1.0884445556555726 Epoch: 2120 Test Loss: 1.5229857931494146\n",
      "Epoch: 2130 Training Loss: 1.0878318750362235 Epoch: 2130 Test Loss: 1.5219950808508171\n",
      "Epoch: 2140 Training Loss: 1.087173861292128 Epoch: 2140 Test Loss: 1.5255219804103397\n",
      "Epoch: 2150 Training Loss: 1.0866349067833907 Epoch: 2150 Test Loss: 1.52499203083343\n",
      "Epoch: 2160 Training Loss: 1.085930014756542 Epoch: 2160 Test Loss: 1.5238738118697113\n",
      "Epoch: 2170 Training Loss: 1.0852525874887706 Epoch: 2170 Test Loss: 1.5245930291526602\n",
      "Epoch: 2180 Training Loss: 1.0846415822055755 Epoch: 2180 Test Loss: 1.5252169786716123\n",
      "Epoch: 2190 Training Loss: 1.0840451936643314 Epoch: 2190 Test Loss: 1.5260573635099108\n",
      "Epoch: 2200 Training Loss: 1.0835049996332373 Epoch: 2200 Test Loss: 1.524565499304509\n",
      "Epoch: 2210 Training Loss: 1.0828823462358366 Epoch: 2210 Test Loss: 1.526951177987713\n",
      "Epoch: 2220 Training Loss: 1.0822974077116176 Epoch: 2220 Test Loss: 1.5254093253738263\n",
      "Epoch: 2230 Training Loss: 1.0816866033555625 Epoch: 2230 Test Loss: 1.5273012779645299\n",
      "Epoch: 2240 Training Loss: 1.0811150477108569 Epoch: 2240 Test Loss: 1.5274963414280747\n",
      "Epoch: 2250 Training Loss: 1.0805229890079837 Epoch: 2250 Test Loss: 1.5296275501333023\n",
      "Epoch: 2260 Training Loss: 1.0799780547943594 Epoch: 2260 Test Loss: 1.5295400704184536\n",
      "Epoch: 2270 Training Loss: 1.0793759440745625 Epoch: 2270 Test Loss: 1.5287820940071868\n",
      "Epoch: 2280 Training Loss: 1.0788402892988582 Epoch: 2280 Test Loss: 1.5317717992929176\n",
      "Epoch: 2290 Training Loss: 1.0782778659341674 Epoch: 2290 Test Loss: 1.5298726020053321\n",
      "Epoch: 2300 Training Loss: 1.0776958818878957 Epoch: 2300 Test Loss: 1.528610032456242\n",
      "Epoch: 2310 Training Loss: 1.077144554795265 Epoch: 2310 Test Loss: 1.528829184536919\n",
      "Epoch: 2320 Training Loss: 1.0765650469637476 Epoch: 2320 Test Loss: 1.5306912527529848\n",
      "Epoch: 2330 Training Loss: 1.0759879304481932 Epoch: 2330 Test Loss: 1.5315564506781036\n",
      "Epoch: 2340 Training Loss: 1.0754523520814014 Epoch: 2340 Test Loss: 1.5362662281210153\n",
      "Epoch: 2350 Training Loss: 1.0749545904663411 Epoch: 2350 Test Loss: 1.5320981728874463\n",
      "Epoch: 2360 Training Loss: 1.0744366682182394 Epoch: 2360 Test Loss: 1.5324978990379043\n",
      "Epoch: 2370 Training Loss: 1.0738846987247523 Epoch: 2370 Test Loss: 1.5346896009620958\n",
      "Epoch: 2380 Training Loss: 1.0734498244745783 Epoch: 2380 Test Loss: 1.533012996986206\n",
      "Epoch: 2390 Training Loss: 1.072922696367509 Epoch: 2390 Test Loss: 1.5335335284571694\n",
      "Epoch: 2400 Training Loss: 1.0724492073169627 Epoch: 2400 Test Loss: 1.5335617827750088\n",
      "Epoch: 2410 Training Loss: 1.0720128331881376 Epoch: 2410 Test Loss: 1.535217196012519\n",
      "Epoch: 2420 Training Loss: 1.0714899585105266 Epoch: 2420 Test Loss: 1.5355845021444303\n",
      "Epoch: 2430 Training Loss: 1.0709488305927248 Epoch: 2430 Test Loss: 1.5342282948881418\n",
      "Epoch: 2440 Training Loss: 1.0704129013109278 Epoch: 2440 Test Loss: 1.5343735510606236\n",
      "Epoch: 2450 Training Loss: 1.07000621345956 Epoch: 2450 Test Loss: 1.5339122449866698\n",
      "Epoch: 2460 Training Loss: 1.0695597788239322 Epoch: 2460 Test Loss: 1.535037346412426\n",
      "Epoch: 2470 Training Loss: 1.0689953490178001 Epoch: 2470 Test Loss: 1.5381241306363742\n",
      "Epoch: 2480 Training Loss: 1.0686108622004318 Epoch: 2480 Test Loss: 1.5363392184421003\n",
      "Epoch: 2490 Training Loss: 1.0680395980415773 Epoch: 2490 Test Loss: 1.5343807957575055\n",
      "Epoch: 2500 Training Loss: 1.06761847739564 Epoch: 2500 Test Loss: 1.5355423017850933\n",
      "Epoch: 2510 Training Loss: 1.0671997622154645 Epoch: 2510 Test Loss: 1.5353273154051235\n",
      "Epoch: 2520 Training Loss: 1.0667557330455981 Epoch: 2520 Test Loss: 1.5382587008809552\n",
      "Epoch: 2530 Training Loss: 1.0663403374081735 Epoch: 2530 Test Loss: 1.5375019922916424\n",
      "Epoch: 2540 Training Loss: 1.0658495827733787 Epoch: 2540 Test Loss: 1.5363937347861365\n",
      "Epoch: 2550 Training Loss: 1.0653724232650763 Epoch: 2550 Test Loss: 1.5355442940767359\n",
      "Epoch: 2560 Training Loss: 1.0649761949448315 Epoch: 2560 Test Loss: 1.5362062782543178\n",
      "Epoch: 2570 Training Loss: 1.0645226090009925 Epoch: 2570 Test Loss: 1.535294533151733\n",
      "Epoch: 2580 Training Loss: 1.0640695833891778 Epoch: 2580 Test Loss: 1.5386553480352383\n",
      "Epoch: 2590 Training Loss: 1.0636695374532716 Epoch: 2590 Test Loss: 1.5372929827866002\n",
      "Epoch: 2600 Training Loss: 1.0632334576402573 Epoch: 2600 Test Loss: 1.5385104540976005\n",
      "Epoch: 2610 Training Loss: 1.0627805933361467 Epoch: 2610 Test Loss: 1.5389138025964995\n",
      "Epoch: 2620 Training Loss: 1.0623490952279182 Epoch: 2620 Test Loss: 1.5373478613654805\n",
      "Epoch: 2630 Training Loss: 1.061869824569665 Epoch: 2630 Test Loss: 1.53962432624319\n",
      "Epoch: 2640 Training Loss: 1.0614892430967096 Epoch: 2640 Test Loss: 1.540767358293729\n",
      "Epoch: 2650 Training Loss: 1.0609937199797872 Epoch: 2650 Test Loss: 1.5385077373362699\n",
      "Epoch: 2660 Training Loss: 1.0606088341380984 Epoch: 2660 Test Loss: 1.540597832386693\n",
      "Epoch: 2670 Training Loss: 1.0601637918426525 Epoch: 2670 Test Loss: 1.540797786020633\n",
      "Epoch: 2680 Training Loss: 1.0596767897344457 Epoch: 2680 Test Loss: 1.5420666946794945\n",
      "Epoch: 2690 Training Loss: 1.0593087846024836 Epoch: 2690 Test Loss: 1.5438032485220818\n",
      "Epoch: 2700 Training Loss: 1.0588575475252116 Epoch: 2700 Test Loss: 1.541633280688536\n",
      "Epoch: 2710 Training Loss: 1.0583828189523081 Epoch: 2710 Test Loss: 1.5421590645647385\n",
      "Epoch: 2720 Training Loss: 1.0580010488962703 Epoch: 2720 Test Loss: 1.5415914425640431\n",
      "Epoch: 2730 Training Loss: 1.0575354469434624 Epoch: 2730 Test Loss: 1.5454023342413354\n",
      "Epoch: 2740 Training Loss: 1.0571543956971934 Epoch: 2740 Test Loss: 1.5445329706155095\n",
      "Epoch: 2750 Training Loss: 1.0567600974094775 Epoch: 2750 Test Loss: 1.544054458386461\n",
      "Epoch: 2760 Training Loss: 1.0562983753314448 Epoch: 2760 Test Loss: 1.54329521415324\n",
      "Epoch: 2770 Training Loss: 1.0558063189151792 Epoch: 2770 Test Loss: 1.5450152863104207\n",
      "Epoch: 2780 Training Loss: 1.0554416078563305 Epoch: 2780 Test Loss: 1.545508831285499\n",
      "Epoch: 2790 Training Loss: 1.0550211522509274 Epoch: 2790 Test Loss: 1.5448260186043816\n",
      "Epoch: 2800 Training Loss: 1.0545370338716746 Epoch: 2800 Test Loss: 1.5472002868899966\n",
      "Epoch: 2810 Training Loss: 1.0541424752276647 Epoch: 2810 Test Loss: 1.5448993711603107\n",
      "Epoch: 2820 Training Loss: 1.0536548590181987 Epoch: 2820 Test Loss: 1.5464348846644256\n",
      "Epoch: 2830 Training Loss: 1.0532548328920106 Epoch: 2830 Test Loss: 1.5473201866233917\n",
      "Epoch: 2840 Training Loss: 1.0528153769370507 Epoch: 2840 Test Loss: 1.5443101961863916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2850 Training Loss: 1.0523681668924598 Epoch: 2850 Test Loss: 1.5452188622928016\n",
      "Epoch: 2860 Training Loss: 1.0519567388585618 Epoch: 2860 Test Loss: 1.5462932508403848\n",
      "Epoch: 2870 Training Loss: 1.0515333146254129 Epoch: 2870 Test Loss: 1.5490270372087631\n",
      "Epoch: 2880 Training Loss: 1.0511676696798569 Epoch: 2880 Test Loss: 1.5475284716587459\n",
      "Epoch: 2890 Training Loss: 1.0507403740618118 Epoch: 2890 Test Loss: 1.5489623782890924\n",
      "Epoch: 2900 Training Loss: 1.0503588190827127 Epoch: 2900 Test Loss: 1.5471397936710327\n",
      "Epoch: 2910 Training Loss: 1.0499345203920105 Epoch: 2910 Test Loss: 1.5495580734902052\n",
      "Epoch: 2920 Training Loss: 1.049599458821143 Epoch: 2920 Test Loss: 1.5495310869943202\n",
      "Epoch: 2930 Training Loss: 1.0491234086570504 Epoch: 2930 Test Loss: 1.5487673148255476\n",
      "Epoch: 2940 Training Loss: 1.0487595465861177 Epoch: 2940 Test Loss: 1.5496142198910399\n",
      "Epoch: 2950 Training Loss: 1.0483650728408993 Epoch: 2950 Test Loss: 1.5480459241335343\n",
      "Epoch: 2960 Training Loss: 1.0479311126677147 Epoch: 2960 Test Loss: 1.5483952996406631\n",
      "Epoch: 2970 Training Loss: 1.0475273962740523 Epoch: 2970 Test Loss: 1.5529926031644836\n",
      "Epoch: 2980 Training Loss: 1.047134060172641 Epoch: 2980 Test Loss: 1.5500546974614582\n",
      "Epoch: 2990 Training Loss: 1.0468129899226266 Epoch: 2990 Test Loss: 1.5516862031992582\n"
     ]
    }
   ],
   "source": [
    "nb_of_epochs=3000\n",
    "batch_size=int(x_variable.shape[0]/40  )\n",
    "# Train the network #\n",
    "my_net.train()\n",
    "for t in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = my_net(x_variable.narrow(0,b,batch_size))                 # input x and predict based on x\n",
    "        loss = loss_func(out, y_variable.narrow(0,b,batch_size))     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "        sum_loss+=loss.data[0]\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #print(t,loss.data[0])\n",
    "        optimizer.step()        # apply gradients\n",
    "    if t%10==0:\n",
    "        my_net.eval()\n",
    "        test_loss=loss_func(my_net(x_variable_test),y_variable_test).data[0]\n",
    "        my_net.train()\n",
    "        print(\"Epoch:\",t,\"Training Loss:\",sum_loss/x_variable.size(0),\"Epoch:\",t,\"Test Loss:\",test_loss/x_variable_test.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable,x_variable_test,my_net=x_variable.cpu(),x_variable_test.cpu(),my_net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (bn_input): BatchNorm1d(25, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc0): Linear(in_features=25, out_features=50, bias=True)\n",
       "  (bn0): BatchNorm1d(50, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (predict): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc3): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (bn3): BatchNorm1d(50, eps=1e-05, momentum=0.5, affine=True)\n",
       "  (fc4): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (bn4): BatchNorm1d(50, eps=1e-05, momentum=0.5, affine=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=my_net(x_variable_test)\n",
    "prediction=prediction.data.numpy()\n",
    "prediction=np.round(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=prediction[190:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contour=list(x_variable_test[190].data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contour.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contour=np.array(contour).reshape(12,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 45 extra points\n",
      "13 -0.2341525918338448 0.12279024013939 0\n",
      "14 -0.157746198688242 -0.3658814677342144 0\n",
      "15 -0.6723824973005741 0.03000363747791006 0\n",
      "16 0.07532353930559398 -0.5621544592872253 0\n",
      "17 0.1983512219090162 0.2579315553591214 0\n",
      "18 -0.2481189864884868 -0.6387766449783331 0\n",
      "19 -0.3172098607834909 0.499402553306591 0\n",
      "20 0.465722769799161 0.247761634896949 0\n",
      "21 -0.409351036251544 -0.1261382322579114 0\n",
      "22 0.06359720866612732 0.0335524450811403 0\n",
      "23 0.1656954377679808 -0.245124624680587 0\n",
      "24 0.6658192618063579 0.1886436231883205 0\n",
      "25 -0.4727426815159586 -0.4269761864510691 0\n",
      "26 -0.509820441873186 0.2804965152077329 0\n",
      "27 -0.07662465073972889 0.3779068597798318 0\n",
      "28 -0.09676279387369076 -0.1320323823056426 0\n",
      "29 0.006589908917604723 0.1938099664685354 0\n",
      "30 -0.750987186780133 0.2184824521038016 0\n",
      "31 -0.4643607584199431 0.05304510597107721 0\n",
      "32 -0.2957539864983069 0.317334793789271 0\n",
      "33 -0.471367436742233 -0.6602457556218724 0\n",
      "34 -0.5117079290338172 0.6356681102772324 0\n",
      "35 0.1319112371290609 0.4812439159242291 0\n",
      "36 0.3116323819474804 0.007428500709734345 0\n",
      "37 -0.3359200346903555 0.6988422770515946 0\n",
      "38 -0.5301024389345864 0.4814155000611627 0\n",
      "39 0.8275624830323797 0.2982293291145369 0\n",
      "40 0.8238158644015489 0.4788719609827019 0\n",
      "41 0.2841141714797975 -0.5752568367775571 0\n",
      "42 0.2114110098379298 -0.7152734248202797 0\n",
      "43 0.630543957884404 0.3954651306179458 0\n",
      "44 0.9975420742716025 0.357037715377221 0\n",
      "45 -0.6362763722844166 -0.2364944255545622 0\n",
      "46 1.010969424581615 0.5750796607172921 0\n",
      "47 1.06739709232019 0.4578745323005273 0\n",
      "48 0.364197196005064 -0.3467565357384206 0\n",
      "49 0.3611788374924377 -0.1790956779978262 0\n",
      "50 0.4252297259501949 -0.4779844657419476 0\n",
      "51 -0.7804872620901946 -0.1830277860696944 0\n",
      "52 0.8677753471175856 0.559019484341323 0\n",
      "53 0.6289744236220478 0.5801898255825397 0\n",
      "54 1.09069475580692 0.4439120927994861 0\n",
      "55 1.007341675865132 0.6019364900827585 0\n",
      "56 1.021760330285976 0.3020658361222374 0\n",
      "57 0.9031038752483196 0.2258719265008451 0\n",
      "found 7 extra points\n",
      "13 -0.1657238672820053 0.2229426464154607 0\n",
      "14 -0.1594081553678047 -0.3735206854738751 0\n",
      "15 -0.5840187450946898 -0.0332207147614891 0\n",
      "16 0.2452853730646893 -0.5839914561559757 0\n",
      "17 -0.3200061136158183 -0.7044596256067356 0\n",
      "18 0.1875558982137591 0.3214397840201855 0\n",
      "19 -0.391564886784181 0.5687193535268307 0\n",
      "found 2 extra points\n",
      "13 -0.3427007412710399 0.001466589393439114 0\n",
      "14 -0.07456469359911151 -0.5564693229066001 0\n",
      "found 1 extra points\n",
      "13 -0.243970621842891 -0.2644676063209772 0\n",
      "found 1 extra points\n",
      "13 -0.243970621842891 -0.2644676063209772 0\n",
      "found 1 extra points\n",
      "13 -0.243970621842891 -0.2644676063209772 0\n",
      "found 1 extra points\n",
      "13 -0.243970621842891 -0.2644676063209772 0\n",
      "found 1 extra points\n",
      "13 -0.243970621842891 -0.2644676063209772 0\n",
      "found 1 extra points\n",
      "13 -0.243970621842891 -0.2644676063209772 0\n",
      "found 1 extra points\n",
      "13 -0.243970621842891 -0.2644676063209772 0\n"
     ]
    }
   ],
   "source": [
    "real_number_of_points=[]\n",
    "for i in np.linspace(.1,1,10):\n",
    "    number_of_points,_=get_extrapoints_target_length(contour,i)\n",
    "    real_number_of_points.append(number_of_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x245745b3b70>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24574011eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Neural_net_prediction=plt.plot(np.linspace(.1,1,10),prediction,'-o',label='Prediction')\n",
    "Real_value=plt.plot(np.linspace(.1,1,10),real_number_of_points,'-o',label='Real Value')\n",
    "plt.xlabel('Target edge length')\n",
    "plt.ylabel('Number of points')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor=torch.Tensor([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_variable=Variable(a_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=lambda x: x+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_function=y(a_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of variables does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-2f9fb9a08f8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of variables does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "my_function.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
