{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.tensor as Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from Triangulation import *\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 14, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons=load_dataset('14_polygons.pkl')\n",
    "polygons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(1,2*14))\n",
    "\n",
    "polygons_reshaped=np.array(polygons_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 14, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygons_reshaped=polygons_reshaped.reshape(polygons_reshaped.shape[0],1,2*14)\n",
    "polygons.shape\n",
    "#polygons_reshaped=polygons_reshaped.reshape(polygons_reshaped.shape[0],2*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(polygons_reshaped).type(torch.FloatTensor)\n",
    "x_variable,y_variable=Variable(x_tensor).cuda(),Variable(x_tensor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This auto encoder uses linear connections (Results may be the same as using PCA)\n",
    "class simple_autoencoder(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(simple_autoencoder,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "        nn.Linear(2*12,12),\n",
    "        nn.BatchNorm1d(12,momentum=0.5),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(12,6),\n",
    "        nn.BatchNorm1d(6,momentum=0.5),\n",
    "        nn.ReLU(True),      \n",
    "        nn.Linear(6,2),\n",
    "        nn.BatchNorm1d(2,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        nn.Linear(2,6),\n",
    "        nn.BatchNorm1d(6,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(6,12),\n",
    "        nn.BatchNorm1d(12,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True)    ,\n",
    "        nn.Linear(12,2*12)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Convolutional autoencoder\n",
    "class conv_autoencoder(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "        nn.Conv1d(1,30,kernel_size=2,stride=2),# d 24->12 stride=2 , filters =3\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv1d(30,600,kernel_size=2,stride=1),# d 12->6 stride=2, filters=6\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv1d(600,2,kernel_size=6,stride=1),#d  6->1 stride=1 ,filters=2   \n",
    "        )\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        nn.ReLU(True),\n",
    "\n",
    "        nn.ConvTranspose1d(2,600,kernel_size=6,stride=1),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose1d(600,300,kernel_size=2,stride=1),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose1d(300,1,kernel_size=2,stride=2),\n",
    "\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=conv_autoencoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(\n",
    "model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.223128472222223\n",
      "9.313846961805556\n",
      "4.292263953993055\n",
      "2.196617708333333\n",
      "1.581539724392361\n",
      "1.3538589518229167\n",
      "1.201212109375\n",
      "1.1357349609375\n",
      "1.1109207139756945\n",
      "1.0974444227430555\n",
      "1.0884735894097222\n",
      "1.0801198350694445\n",
      "1.0711683485243055\n",
      "1.0608554036458333\n",
      "1.0486826171875\n",
      "1.034157693142361\n",
      "1.0165019422743056\n",
      "0.9950139539930556\n",
      "0.9683070421006944\n",
      "0.9348644748263889\n",
      "0.8935029730902778\n",
      "0.8443825466579861\n",
      "0.7890141167534722\n",
      "0.7295626410590278\n",
      "0.6697966254340277\n",
      "0.6143817057291666\n",
      "0.5683676486545138\n",
      "0.5338056911892362\n",
      "0.5078324435763889\n",
      "0.4856871148003472\n",
      "0.46539354926215276\n",
      "0.4462226969401042\n",
      "0.42777779405381944\n",
      "0.4100181640625\n",
      "0.3930921875\n",
      "0.37705679253472224\n",
      "0.36182964409722224\n",
      "0.3474094807942708\n",
      "0.33390733235677084\n",
      "0.3214554416232639\n",
      "0.3098956271701389\n",
      "0.2992146891276042\n",
      "0.2894330023871528\n",
      "0.28046488172743056\n",
      "0.2722395290798611\n",
      "0.26472058376736113\n",
      "0.2578618869357639\n",
      "0.2515937201605903\n",
      "0.2459047417534722\n",
      "0.24063578830295138\n",
      "0.23585027398003472\n",
      "0.23124908311631945\n",
      "0.2271281467013889\n",
      "0.2237432630750868\n",
      "0.21999094916449652\n",
      "0.2168085435655382\n",
      "0.21390027940538195\n",
      "0.2111904269748264\n",
      "0.20863811442057292\n",
      "0.20630635986328125\n",
      "0.20438761393229166\n",
      "0.20208553738064236\n",
      "0.20007177327473957\n",
      "0.1982906290690104\n",
      "0.19669408637152777\n",
      "0.1952077392578125\n",
      "0.19360982259114584\n",
      "0.19223994411892362\n",
      "0.19126375054253472\n",
      "0.18962799886067708\n",
      "0.18852322455512152\n",
      "0.18727627766927082\n",
      "0.18626185167100695\n",
      "0.1858944566514757\n",
      "0.18436195203993055\n",
      "0.18346004774305555\n",
      "0.18251260986328124\n",
      "0.1817670640733507\n",
      "0.18099852023654514\n",
      "0.18041588948567708\n",
      "0.17994354248046876\n",
      "0.1789774183485243\n",
      "0.17814772135416668\n",
      "0.17757068549262153\n",
      "0.1769578138563368\n",
      "0.1764402845594618\n",
      "0.17634305962456598\n",
      "0.1755908175998264\n",
      "0.17494938151041667\n",
      "0.17464080403645835\n",
      "0.17420810004340279\n",
      "0.1736171888563368\n",
      "0.17338739691840277\n",
      "0.1735765150282118\n",
      "0.17253169623480902\n",
      "0.1721885538736979\n",
      "0.17175489366319444\n",
      "0.17143619249131944\n",
      "0.17145867784288193\n",
      "0.1713057820638021\n",
      "0.17056566704644097\n",
      "0.17033004828559029\n",
      "0.17014804009331597\n",
      "0.16976375461154514\n",
      "0.16966148546006946\n",
      "0.16975704481336806\n",
      "0.16899538167317707\n",
      "0.16874430202907986\n",
      "0.16884790581597223\n",
      "0.16864215223524306\n",
      "0.16808062744140626\n",
      "0.16794867350260417\n",
      "0.16826977132161458\n",
      "0.16766001654730903\n",
      "0.1673326673719618\n",
      "0.16733173828125\n",
      "0.16732197672526042\n",
      "0.16681041666666666\n",
      "0.16666058485243054\n",
      "0.16702616373697918\n",
      "0.16670384657118056\n",
      "0.16620240342881945\n",
      "0.1661414048936632\n",
      "0.16598235541449652\n",
      "0.16576002468532985\n",
      "0.1658724405924479\n",
      "0.16596017523871529\n",
      "0.16531907280815972\n",
      "0.16520492350260416\n",
      "0.16545698649088542\n",
      "0.1652564480251736\n",
      "0.1647817613389757\n",
      "0.16475111219618055\n",
      "0.1651290730794271\n",
      "0.16462491319444444\n",
      "0.16432724338107638\n",
      "0.16438448079427084\n",
      "0.16462093777126735\n",
      "0.16405924886067708\n",
      "0.16389776882595486\n",
      "0.16410379231770833\n",
      "0.16431517740885418\n",
      "0.16360065239800348\n",
      "0.16352679172092013\n",
      "0.16372968071831598\n",
      "0.16354036593967014\n",
      "0.16321436767578126\n",
      "0.1632649658203125\n",
      "0.16373802897135417\n",
      "0.16307401123046875\n",
      "0.1628605997721354\n",
      "0.16297835693359375\n",
      "0.16301929253472222\n",
      "0.16264061008029515\n",
      "0.1625859144422743\n",
      "0.16296688096788195\n",
      "0.16270525580512152\n",
      "0.1622582546657986\n",
      "0.16230166965060763\n",
      "0.16262374267578125\n",
      "0.1621839653862847\n",
      "0.1619532511393229\n",
      "0.16213286810980904\n",
      "0.16250958930121528\n",
      "0.16178563096788195\n",
      "0.1616923055013021\n",
      "0.16192706705729168\n",
      "0.1619031236436632\n",
      "0.1614877482096354\n",
      "0.16149408908420138\n",
      "0.1619554443359375\n",
      "0.16159595675998265\n",
      "0.16120952690972223\n",
      "0.16130240614149305\n",
      "0.16158029242621527\n",
      "0.16114989149305556\n",
      "0.16098045111762152\n",
      "0.16122791069878473\n",
      "0.16147794325086806\n",
      "0.16080829399956598\n",
      "0.16076356201171876\n",
      "0.1610765638563368\n",
      "0.16098727349175349\n",
      "0.16055968424479167\n",
      "0.16060846354166666\n",
      "0.16106692030164932\n",
      "0.160652099609375\n",
      "0.16033011338975695\n",
      "0.1604575453016493\n",
      "0.16079621175130207\n",
      "0.1603058553059896\n",
      "0.16013012966579862\n",
      "0.16037312554253472\n",
      "0.16074114312065974\n",
      "0.1600131863064236\n",
      "0.15994937744140625\n",
      "0.16020723605685763\n",
      "0.16015442979600694\n",
      "0.15981119384765624\n",
      "0.1598505140516493\n",
      "0.16025668674045138\n",
      "0.15993833414713543\n",
      "0.1595746622721354\n",
      "0.15967542995876735\n",
      "0.1600886528862847\n",
      "0.15965435791015625\n",
      "0.15939488525390624\n",
      "0.15954931911892362\n",
      "0.1599472452799479\n",
      "0.1594044216579861\n",
      "0.15921825222439237\n",
      "0.15940864935980903\n",
      "0.15976465928819444\n",
      "0.15918568250868056\n",
      "0.15904737141927083\n",
      "0.15926524386935764\n",
      "0.1596057644314236\n",
      "0.15899573838975695\n",
      "0.1588798583984375\n",
      "0.15910963134765624\n",
      "0.15944466281467015\n",
      "0.15882959798177082\n",
      "0.15872049967447915\n",
      "0.1589581787109375\n",
      "0.1592798312717014\n",
      "0.15866817762586805\n",
      "0.15856500379774305\n",
      "0.15880812310112846\n",
      "0.1591316623263889\n",
      "0.15851309136284722\n",
      "0.15841210394965277\n",
      "0.15864669596354167\n",
      "0.15899302300347223\n",
      "0.15838001437717014\n",
      "0.15826105414496527\n",
      "0.15846783311631946\n",
      "0.1589014390733507\n",
      "0.15827253689236112\n",
      "0.15811709662543402\n",
      "0.15828519151475695\n",
      "0.15871185709635416\n",
      "0.15818053792317707\n",
      "0.1579718777126736\n",
      "0.1581183580186632\n",
      "0.1586495591905382\n",
      "0.15810080023871528\n",
      "0.15783481987847223\n",
      "0.1579468017578125\n",
      "0.1584025390625\n",
      "0.1579978569878472\n",
      "0.15768275010850694\n",
      "0.15776576063368056\n",
      "0.15839875081380209\n",
      "0.1580121120876736\n",
      "0.15756145968967014\n",
      "0.1576103285047743\n",
      "0.15795559760199654\n",
      "0.15775104573567708\n",
      "0.15741130235460069\n",
      "0.1574747097439236\n",
      "0.15825965033637152\n",
      "0.15782678765190972\n",
      "0.1573268079969618\n",
      "0.15736208767361112\n",
      "0.15745641954210068\n",
      "0.15728073187934027\n",
      "0.15734337158203124\n",
      "0.15784500325520834\n",
      "0.1574562757703993\n",
      "0.15702562934027778\n",
      "0.15704781087239583\n",
      "0.1577487019856771\n",
      "0.15784033067491318\n",
      "0.15701448974609375\n",
      "0.15699646809895834\n",
      "0.1570805365668403\n",
      "0.1569150160047743\n",
      "0.15702063937717015\n",
      "0.15775281168619792\n",
      "0.15717936197916665\n",
      "0.15679310574001737\n",
      "0.15679355740017362\n",
      "0.15716708170572916\n",
      "0.15711278211805554\n",
      "0.1566557874891493\n",
      "0.15664775390625\n",
      "0.1573159912109375\n",
      "0.15725973442925348\n",
      "0.15748216417100694\n",
      "0.15722134060329862\n",
      "0.1565950954861111\n",
      "0.1565386501736111\n",
      "0.1565602037217882\n",
      "0.15652362060546876\n",
      "0.1567405490451389\n",
      "0.15723328586154514\n",
      "0.15649439290364583\n",
      "0.15635957980685763\n",
      "0.15686267361111111\n",
      "0.1570349880642361\n",
      "0.15648047960069444\n",
      "0.15657429470486112\n",
      "0.1566343017578125\n",
      "0.15651218804253472\n",
      "0.15619541829427083\n",
      "0.1561223334418403\n",
      "0.15670116916232638\n",
      "0.15779180636935763\n",
      "0.15631328667534722\n",
      "0.15621868082682291\n",
      "0.15609253743489584\n",
      "0.15619702826605902\n",
      "0.15633751763237846\n",
      "0.15599539388020833\n",
      "0.15588269992404513\n",
      "0.1562023206922743\n",
      "0.15719192979600694\n",
      "0.1560822035047743\n",
      "0.156001611328125\n",
      "0.1561153347439236\n",
      "0.1560499009874132\n",
      "0.15614065755208334\n",
      "0.15581222059461805\n",
      "0.15633115098741318\n",
      "0.15635641682942708\n",
      "0.15595606553819444\n",
      "0.1559574951171875\n",
      "0.15584275580512152\n",
      "0.15585168728298612\n",
      "0.1561525634765625\n",
      "0.15591543511284722\n",
      "0.1557054402669271\n",
      "0.15603616400824652\n",
      "0.15559947916666667\n",
      "0.15619273952907986\n",
      "0.1566849826388889\n",
      "0.1555934760199653\n",
      "0.1554656711154514\n",
      "0.15549825032552084\n",
      "0.15551175672743056\n",
      "0.15584903293185765\n",
      "0.15546419270833334\n",
      "0.15571673990885418\n",
      "0.1571729736328125\n",
      "0.15560125868055555\n",
      "0.15536844482421874\n",
      "0.15526041124131945\n",
      "0.15531753200954862\n",
      "0.15551083713107638\n",
      "0.15545219184027778\n",
      "0.15514785834418401\n",
      "0.15514409857855901\n",
      "0.15568154161241318\n",
      "0.15636975775824652\n",
      "0.15635721164279515\n",
      "0.1553587185329861\n",
      "0.15508170030381943\n",
      "0.15506864420572916\n",
      "0.1549450005425347\n",
      "0.15505293782552082\n",
      "0.15578377278645833\n",
      "0.1561303480360243\n",
      "0.15512579888237849\n",
      "0.1549946818033854\n",
      "0.15492174479166668\n",
      "0.15493577745225695\n",
      "0.1552235080295139\n",
      "0.1553196492513021\n",
      "0.15517609456380207\n",
      "0.15557218153211805\n",
      "0.15502854546440972\n",
      "0.15481150987413195\n",
      "0.15468943413628472\n",
      "0.15503459337022568\n",
      "0.15671617567274307\n",
      "0.15527011990017361\n",
      "0.15491887749565972\n",
      "0.1546281060112847\n",
      "0.15465252685546876\n",
      "0.15459306098090278\n",
      "0.15490768364800347\n",
      "0.1566582546657986\n",
      "0.15501051703559027\n",
      "0.15465306667751735\n",
      "0.15458768446180557\n",
      "0.15449766303168402\n",
      "0.15462656928168403\n",
      "0.15503021782769097\n",
      "0.15478176676432293\n",
      "0.15436741536458334\n",
      "0.15441863471137152\n",
      "0.15476056586371528\n",
      "0.15534466281467013\n",
      "0.15586888427734374\n",
      "0.15464916042751736\n",
      "0.15438695882161457\n",
      "0.15440571017795138\n",
      "0.1543299112955729\n",
      "0.15436995442708334\n",
      "0.1551317830403646\n",
      "0.15493946533203126\n",
      "0.15427603759765626\n",
      "0.15436541748046875\n",
      "0.15448669704861112\n",
      "0.1543322252061632\n",
      "0.15482664794921874\n",
      "0.15505844319661458\n",
      "0.1542474636501736\n",
      "0.1545749254014757\n",
      "0.15473067084418402\n",
      "0.1540602308485243\n",
      "0.15405171983506943\n",
      "0.15436539306640626\n",
      "0.15521147732204862\n",
      "0.15530218641493054\n",
      "0.1543754380967882\n",
      "0.15406839192708333\n",
      "0.15399719916449653\n",
      "0.15401615668402777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15440066731770832\n",
      "0.15423601752387153\n",
      "0.15381780327690972\n",
      "0.153856005859375\n",
      "0.15450140245225694\n",
      "0.15551841769748265\n",
      "0.1545827338324653\n",
      "0.15409902615017362\n",
      "0.1538066677517361\n",
      "0.1537931667751736\n",
      "0.15371251627604166\n",
      "0.1537408908420139\n",
      "0.15411428493923612\n",
      "0.1565966064453125\n",
      "0.15436429985894098\n",
      "0.15386983235677085\n",
      "0.15375528157552085\n",
      "0.1536454833984375\n",
      "0.15369807807074654\n",
      "0.15377009548611112\n",
      "0.15394164089626736\n",
      "0.15375877685546874\n",
      "0.15355574273003472\n",
      "0.15367447781032986\n",
      "0.15438561333550346\n",
      "0.15374271375868057\n",
      "0.15365973036024305\n",
      "0.15450499810112847\n",
      "0.15445706380208332\n",
      "0.1537018568250868\n",
      "0.15361238199869792\n",
      "0.1533932142469618\n",
      "0.15340719807942707\n",
      "0.15396871473524307\n",
      "0.15397977973090277\n",
      "0.15339864230685765\n",
      "0.15368605143229166\n",
      "0.15579612765842013\n",
      "0.15397393120659722\n",
      "0.15358758002387152\n",
      "0.1533062974717882\n",
      "0.1532930636935764\n",
      "0.15322611219618054\n",
      "0.1532454074435764\n",
      "0.1534664021809896\n",
      "0.15375491265190971\n",
      "0.1532073988172743\n",
      "0.1531378634982639\n",
      "0.1535632080078125\n",
      "0.15642501220703126\n",
      "0.15396531168619793\n",
      "0.15342736952039931\n",
      "0.15324780815972222\n",
      "0.15308387179904515\n",
      "0.15305760362413195\n",
      "0.15303639865451388\n",
      "0.15325533989800347\n",
      "0.15378265787760417\n",
      "0.15309107123480903\n",
      "0.1530304673936632\n",
      "0.15313752305772568\n",
      "0.15338507215711805\n",
      "0.15310675320095485\n",
      "0.15304812689887154\n",
      "0.15501440836588543\n",
      "0.1541469007703993\n",
      "0.1533303683810764\n",
      "0.15288886040581598\n",
      "0.15288181694878472\n",
      "0.15291642252604168\n",
      "0.15354015299479168\n",
      "0.15339268256293404\n",
      "0.15280227864583334\n",
      "0.1527697211371528\n",
      "0.15294501410590278\n",
      "0.1539897216796875\n",
      "0.1530889892578125\n",
      "0.1528401353624132\n",
      "0.15294014078776041\n",
      "0.15270848388671876\n",
      "0.15264649386935764\n",
      "0.1530504136827257\n",
      "0.1546574503580729\n",
      "0.15369081624348957\n",
      "0.1531346693250868\n",
      "0.15266607259114584\n",
      "0.15258583170572917\n",
      "0.15261425374348958\n",
      "0.15303370225694443\n",
      "0.15349300265842014\n",
      "0.15255783013237847\n",
      "0.1525801784939236\n",
      "0.15249086371527779\n",
      "0.1529247300889757\n",
      "0.1539816880967882\n",
      "0.15269281005859375\n",
      "0.15265642632378473\n",
      "0.15254544813368057\n",
      "0.1523511488172743\n",
      "0.15241032443576388\n",
      "0.15313419731987848\n",
      "0.15344059244791666\n",
      "0.15347313368055557\n",
      "0.15261134168836807\n",
      "0.15244830864800346\n",
      "0.15232070719401042\n",
      "0.15225493706597223\n",
      "0.1522632093641493\n",
      "0.15245035129123263\n",
      "0.15405829671223958\n",
      "0.15494546983506943\n",
      "0.15296552734375\n",
      "0.15242855495876736\n",
      "0.1522280775282118\n",
      "0.15217953559027778\n",
      "0.15212015109592014\n",
      "0.15211949734157987\n",
      "0.15211734619140624\n",
      "0.15211218668619791\n",
      "0.15224105902777776\n",
      "0.15266390923394096\n",
      "0.1524569390190972\n",
      "0.15260572102864584\n",
      "0.1532093722873264\n",
      "0.1523590128580729\n",
      "0.1520706732855903\n",
      "0.1520301527235243\n",
      "0.15254114176432293\n",
      "0.15284581027560765\n",
      "0.15211958957248264\n",
      "0.15204605170355903\n",
      "0.1522111111111111\n",
      "0.15300308837890625\n",
      "0.15322034912109375\n",
      "0.15233042670355904\n",
      "0.15197288547092014\n",
      "0.15203367784288194\n",
      "0.1520866265190972\n",
      "0.15242431640625\n",
      "0.15207305501302085\n",
      "0.15177262505425348\n",
      "0.15179949408637153\n",
      "0.15225124918619792\n",
      "0.15273211398654513\n",
      "0.15222271457248263\n",
      "0.1543465847439236\n",
      "0.15222527398003471\n",
      "0.15195428738064237\n",
      "0.1517693874782986\n",
      "0.15171040852864584\n",
      "0.1516435994466146\n",
      "0.1516692396375868\n",
      "0.15177173800998264\n",
      "0.15256187744140626\n",
      "0.1523277818467882\n",
      "0.1517278578016493\n",
      "0.15202233208550348\n",
      "0.1519271497938368\n",
      "0.15177381049262154\n",
      "0.15180120578342013\n",
      "0.15198358018663194\n",
      "0.15193114420572917\n",
      "0.15211302897135418\n",
      "0.15256314832899306\n",
      "0.15174781765407985\n",
      "0.15167333306206598\n",
      "0.15181458468967013\n",
      "0.15254400363498263\n",
      "0.1515426011827257\n",
      "0.1515746297200521\n",
      "0.15186352403428818\n",
      "0.15178256022135417\n",
      "0.15172049153645834\n",
      "0.15177387017144098\n",
      "0.1517508083767361\n",
      "0.15149558512369793\n",
      "0.15203797336154515\n",
      "0.15319590386284723\n",
      "0.1516893256293403\n",
      "0.15156482340494792\n",
      "0.15133822699652777\n",
      "0.1513018351236979\n",
      "0.15131427680121529\n",
      "0.15277013617621527\n",
      "0.15246567654079862\n",
      "0.15161776936848959\n",
      "0.15124342447916667\n",
      "0.15122342393663193\n",
      "0.15123853624131944\n",
      "0.15135261908637151\n",
      "0.15176004774305554\n",
      "0.1516082261827257\n",
      "0.15111173502604167\n",
      "0.15121135932074653\n",
      "0.1518379408094618\n",
      "0.1517190226236979\n",
      "0.1518306627061632\n",
      "0.15141500922309029\n",
      "0.15120713297526042\n",
      "0.15163075764973957\n",
      "0.1516579874674479\n",
      "0.1510153835720486\n",
      "0.1512929904513889\n",
      "0.15181365966796875\n",
      "0.15110953369140626\n",
      "0.1516935533311632\n",
      "0.15232318657769098\n",
      "0.15117609185112849\n",
      "0.15112209879557292\n",
      "0.1508884033203125\n",
      "0.15091635064019096\n",
      "0.15154144422743054\n",
      "0.15243005777994792\n",
      "0.1511628662109375\n",
      "0.15119449869791668\n",
      "0.15080571831597223\n",
      "0.15082469889322916\n",
      "0.15086067843967013\n",
      "0.1514221394856771\n",
      "0.15323652750651043\n",
      "0.1512721218532986\n",
      "0.15099993489583333\n",
      "0.15079122721354166\n",
      "0.15078443060980903\n",
      "0.15077476535373263\n",
      "0.15097529703776041\n",
      "0.15129512396918401\n",
      "0.15083926323784722\n",
      "0.1506438001844618\n",
      "0.15096063639322915\n",
      "0.15195906982421875\n",
      "0.15118722466362847\n",
      "0.1508616495768229\n",
      "0.15073765190972221\n",
      "0.1509252210828993\n",
      "0.1517801554361979\n",
      "0.1507373291015625\n",
      "0.15080442572699654\n",
      "0.1509230428059896\n",
      "0.1506308797200521\n",
      "0.15072853325737848\n",
      "0.1514958753797743\n",
      "0.1509679931640625\n",
      "0.15057107204861112\n",
      "0.15062198079427083\n",
      "0.15192823486328125\n",
      "0.15128275146484374\n",
      "0.15068167046440972\n",
      "0.15054500325520834\n",
      "0.15040060221354168\n",
      "0.1504109375\n",
      "0.1504524671766493\n",
      "0.15266895616319445\n",
      "0.15197309163411457\n",
      "0.15092304416232638\n",
      "0.15042312825520834\n",
      "0.15036785617404513\n",
      "0.15031405436197917\n",
      "0.15030398491753472\n",
      "0.1504206787109375\n",
      "0.15093237982855903\n",
      "0.1505372829861111\n",
      "0.150242138671875\n",
      "0.15031012234157987\n",
      "0.15071561143663195\n",
      "0.15065630560980903\n",
      "0.15017102728949652\n",
      "0.1508164808485243\n",
      "0.15334066026475696\n",
      "0.15092333034939237\n",
      "0.15030608995225694\n",
      "0.1502590603298611\n",
      "0.15020174018012153\n",
      "0.1501993177625868\n",
      "0.15040513644748263\n",
      "0.1507477294921875\n",
      "0.15023939751519097\n",
      "0.15002872992621527\n",
      "0.15002356906467013\n",
      "0.15036305745442707\n",
      "0.15244451361762154\n",
      "0.15042484266493056\n",
      "0.15024931640625\n",
      "0.15001726888020833\n",
      "0.1500210435655382\n",
      "0.1499472913953993\n",
      "0.15001927761501735\n",
      "0.15044329833984374\n",
      "0.15122329779730903\n",
      "0.1500976345486111\n",
      "0.15023514946831598\n",
      "0.1500029513888889\n",
      "0.14984684516059027\n",
      "0.15006448703342015\n",
      "0.15143664957682293\n",
      "0.15060158148871527\n",
      "0.15003088785807292\n",
      "0.14998004964192707\n",
      "0.14984782579210068\n",
      "0.14994201931423612\n",
      "0.15040338406032985\n",
      "0.15111880696614582\n",
      "0.14997197808159723\n",
      "0.1501050048828125\n",
      "0.14975735812717014\n",
      "0.14967020941840278\n",
      "0.14988553059895834\n",
      "0.15120264756944443\n",
      "0.15040017768012154\n",
      "0.14993179253472222\n",
      "0.1497348890516493\n",
      "0.1498319118923611\n",
      "0.1504008544921875\n",
      "0.15007537299262153\n",
      "0.14964183349609375\n",
      "0.15014483642578125\n",
      "0.15008855794270834\n",
      "0.14963306342230903\n",
      "0.14955866970486112\n",
      "0.14997337103949654\n",
      "0.15037467854817707\n",
      "0.14955033501519097\n",
      "0.1512436998155382\n",
      "0.1520937486436632\n",
      "0.1501062255859375\n",
      "0.14958252631293403\n",
      "0.14946219889322918\n",
      "0.1494557413736979\n",
      "0.14943229844835068\n",
      "0.14956508246527778\n",
      "0.1500475056966146\n",
      "0.14979349772135417\n",
      "0.14935249565972222\n",
      "0.14933740776909724\n",
      "0.14955619167751735\n",
      "0.15125301106770833\n",
      "0.14972201877170138\n",
      "0.14956947835286458\n",
      "0.14930828179253472\n",
      "0.14928449164496527\n",
      "0.1493835679796007\n",
      "0.1494246351453993\n",
      "0.14989718424479168\n",
      "0.15016442328559026\n",
      "0.14943727484809027\n",
      "0.14947266438802084\n",
      "0.15023425971137153\n",
      "0.14940467529296875\n",
      "0.14919472249348958\n",
      "0.1494001261393229\n",
      "0.1493535847981771\n",
      "0.14960210639105903\n",
      "0.15140935872395833\n",
      "0.14973429497612847\n",
      "0.14948703884548611\n",
      "0.14912190212673612\n",
      "0.14916611735026042\n",
      "0.14932738986545138\n",
      "0.14940011664496528\n",
      "0.14926035970052084\n",
      "0.14915818413628473\n",
      "0.1493389187282986\n",
      "0.14978309461805556\n",
      "0.14963080240885418\n",
      "0.14993148600260417\n",
      "0.1492328599717882\n",
      "0.14903494873046874\n",
      "0.14899654812282986\n",
      "0.1498158230251736\n",
      "0.149911328125\n",
      "0.14906358371310763\n",
      "0.14912419840494792\n",
      "0.1493256618923611\n",
      "0.1496708251953125\n",
      "0.14896446804470487\n",
      "0.1492323961046007\n",
      "0.14931310085720487\n",
      "0.14892091335720486\n",
      "0.14916785481770833\n",
      "0.14989139268663196\n",
      "0.14892076551649305\n",
      "0.14892186279296876\n",
      "0.1507048312717014\n",
      "0.14958648003472222\n",
      "0.1491353271484375\n",
      "0.14878297661675347\n",
      "0.1487557603624132\n",
      "0.1487141398111979\n",
      "0.14868086208767362\n",
      "0.14891129692925348\n",
      "0.1501700398763021\n",
      "0.14986829291449652\n",
      "0.1490840576171875\n",
      "0.1487904106987847\n",
      "0.14862481553819445\n",
      "0.14876373291015624\n",
      "0.14889522705078126\n",
      "0.14877445068359374\n",
      "0.14878150092230902\n",
      "0.14904244384765625\n",
      "0.14896293131510416\n",
      "0.1489332261827257\n",
      "0.15019312608506943\n",
      "0.1490218248155382\n",
      "0.14878610297309028\n",
      "0.14856774088541666\n",
      "0.14873230116102432\n",
      "0.14887090115017362\n",
      "0.14880673149956597\n",
      "0.14871949462890624\n",
      "0.14854165852864584\n",
      "0.1489878445095486\n",
      "0.1504726847330729\n",
      "0.14894868842230902\n",
      "0.14875149739583332\n",
      "0.1483806844075521\n",
      "0.14845092366536458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1483380832248264\n",
      "0.14834889458550346\n",
      "0.14865523003472222\n",
      "0.14973692626953125\n",
      "0.1498288316514757\n",
      "0.14884946153428819\n",
      "0.14833304307725695\n",
      "0.14834703097873264\n",
      "0.14832024875217015\n",
      "0.14828079020182292\n",
      "0.14846131320529513\n",
      "0.14894559054904513\n",
      "0.14924624701605904\n",
      "0.1482925265842014\n",
      "0.14856526828342015\n",
      "0.14860011664496528\n",
      "0.1482910142686632\n",
      "0.1482309054904514\n",
      "0.14867420247395832\n",
      "0.14880628119574651\n",
      "0.14818130967881946\n",
      "0.1500734090169271\n",
      "0.14936087782118054\n",
      "0.14856810980902777\n",
      "0.14809216851128473\n",
      "0.1481520751953125\n",
      "0.14831082763671874\n",
      "0.1485931369357639\n",
      "0.14822705756293403\n",
      "0.14799681939019096\n",
      "0.1481580539279514\n",
      "0.1492727511935764\n",
      "0.14834458414713542\n",
      "0.14809944254557292\n",
      "0.148120361328125\n",
      "0.14801306559244792\n",
      "0.14811317681206598\n",
      "0.1486267049153646\n",
      "0.14851433648003473\n",
      "0.1481808132595486\n",
      "0.14798702256944443\n",
      "0.14830396999782985\n",
      "0.14888217366536458\n",
      "0.14881244167751737\n",
      "0.14805503743489584\n",
      "0.14806319986979166\n",
      "0.14778859592013888\n",
      "0.14802910563151042\n",
      "0.14834969753689237\n",
      "0.14784266764322918\n",
      "0.14825548773871527\n",
      "0.15033149142795138\n",
      "0.14822188313802084\n",
      "0.14800343153211806\n",
      "0.1477749254014757\n",
      "0.1477915296766493\n",
      "0.14786982964409723\n",
      "0.14811995442708334\n",
      "0.14785924207899306\n",
      "0.14764820692274305\n",
      "0.14781264241536457\n",
      "0.14821633978949653\n",
      "0.14830644802517362\n",
      "0.1484614990234375\n",
      "0.14770017903645832\n",
      "0.14814736870659723\n",
      "0.14763768174913194\n",
      "0.14762499457465278\n",
      "0.1482019517686632\n",
      "0.1479408406575521\n",
      "0.14771414388020834\n",
      "0.1499576605902778\n",
      "0.14803494059244793\n",
      "0.14776824544270833\n",
      "0.1474679456922743\n",
      "0.14754993625217014\n",
      "0.1475224093967014\n",
      "0.1477249715169271\n",
      "0.1479571573893229\n",
      "0.147462548828125\n",
      "0.1474883748372396\n",
      "0.14854573432074653\n",
      "0.14804566243489584\n",
      "0.14745899115668404\n",
      "0.1478698255750868\n",
      "0.14794420166015626\n",
      "0.14738274197048612\n",
      "0.14751546766493057\n",
      "0.14804600423177083\n",
      "0.1477384562174479\n",
      "0.14739246283637153\n",
      "0.14745426025390626\n",
      "0.14833177761501737\n",
      "0.14772542046440973\n",
      "0.14768289116753472\n",
      "0.14741233452690972\n",
      "0.14747414143880208\n",
      "0.14858829752604166\n",
      "0.14741156819661458\n",
      "0.14740962592230902\n",
      "0.14752478841145833\n",
      "0.1472393310546875\n",
      "0.1472425306532118\n",
      "0.14796247829861112\n",
      "0.14755225151909723\n",
      "0.14710917290581596\n",
      "0.14741373969184027\n",
      "0.1501530002170139\n",
      "0.14807657877604166\n",
      "0.14743872205946182\n",
      "0.14718533393012154\n",
      "0.14708666178385416\n",
      "0.14702696804470486\n",
      "0.14706075032552082\n",
      "0.14703387179904515\n",
      "0.14715993109809028\n",
      "0.14737977159288193\n",
      "0.1472628919813368\n",
      "0.14702085639105902\n",
      "0.14711902669270832\n",
      "0.14779134114583334\n",
      "0.14880089111328124\n",
      "0.14780391710069443\n",
      "0.14723430718315972\n",
      "0.14700812038845487\n",
      "0.14690565049913196\n",
      "0.14701536593967013\n",
      "0.14724328206380208\n",
      "0.14718391520182292\n",
      "0.14698484700520834\n",
      "0.14702681477864582\n",
      "0.14732828776041668\n",
      "0.14746594780815972\n",
      "0.1475162122938368\n",
      "0.14687816569010417\n",
      "0.14728302001953125\n",
      "0.14708964165581598\n",
      "0.14720184326171876\n",
      "0.14756203206380208\n",
      "0.14681507161458332\n",
      "0.14699977620442709\n",
      "0.1471853054470486\n",
      "0.1470745347764757\n",
      "0.14745445556640624\n",
      "0.14695388319227431\n",
      "0.1469010728624132\n",
      "0.1471742160373264\n",
      "0.1469699232313368\n",
      "0.14738465033637152\n",
      "0.14686099446614584\n",
      "0.1467990953233507\n",
      "0.14713462999131943\n",
      "0.14692224663628473\n",
      "0.1474224175347222\n",
      "0.14698710123697917\n",
      "0.14673082817925348\n",
      "0.1468645060221354\n",
      "0.1469971652560764\n",
      "0.1468629136827257\n",
      "0.14670087212456598\n",
      "0.14674589979383681\n",
      "0.1470934353298611\n",
      "0.14718310275607638\n",
      "0.14796762017144097\n",
      "0.1467326687282986\n",
      "0.14675739203559027\n",
      "0.14640246039496527\n",
      "0.14655355360243055\n",
      "0.14706829562717014\n",
      "0.14669028998480904\n",
      "0.14638831787109374\n",
      "0.14663327907986112\n",
      "0.1470009765625\n",
      "0.14656666531032986\n",
      "0.14822924126519096\n",
      "0.14718500027126735\n",
      "0.14655459662543402\n",
      "0.14644851888020832\n",
      "0.14642987196180557\n",
      "0.1464806382921007\n",
      "0.14662900119357639\n",
      "0.14657603895399304\n",
      "0.14640301242404513\n",
      "0.14646706949869792\n",
      "0.14671690402560764\n",
      "0.14658228352864583\n",
      "0.14695090738932293\n",
      "0.1470813720703125\n",
      "0.1465284898546007\n",
      "0.1465802734375\n",
      "0.14645122205946182\n",
      "0.14623602023654514\n",
      "0.1464500230577257\n",
      "0.14676185574001735\n",
      "0.14657058783637153\n",
      "0.14665315619574654\n",
      "0.1463175564236111\n",
      "0.14657016872829862\n",
      "0.14639172498914932\n",
      "0.14606681043836806\n",
      "0.1472065728081597\n",
      "0.14750157470703126\n",
      "0.14638413628472222\n",
      "0.1463756876627604\n",
      "0.14601268039279514\n",
      "0.14616545003255207\n",
      "0.1462912855360243\n",
      "0.1467673109266493\n",
      "0.1460801283094618\n",
      "0.146378759765625\n",
      "0.1464561767578125\n",
      "0.14596817762586806\n",
      "0.14616564127604167\n",
      "0.1466164808485243\n",
      "0.14598353678385417\n",
      "0.14667994791666666\n",
      "0.14818235541449654\n",
      "0.14644627414279515\n",
      "0.14610951063368055\n",
      "0.14594737277560763\n",
      "0.14594070366753473\n",
      "0.14596039767795138\n",
      "0.1461016886393229\n",
      "0.14611918402777777\n",
      "0.1460422146267361\n",
      "0.14595189344618056\n",
      "0.14603876274956598\n",
      "0.1462360120985243\n",
      "0.1467476806640625\n",
      "0.14649975043402777\n",
      "0.14613167588975695\n",
      "0.14619045681423612\n",
      "0.14580662977430556\n",
      "0.1458684543185764\n",
      "0.14621759982638888\n",
      "0.14612650417751735\n",
      "0.1459849609375\n",
      "0.1460766140407986\n",
      "0.14599179823133682\n",
      "0.14615206163194444\n",
      "0.1467495130750868\n",
      "0.14571661376953124\n",
      "0.14631119791666666\n",
      "0.14617187364366319\n",
      "0.1455852023654514\n",
      "0.14582157389322917\n",
      "0.1457887681749132\n",
      "0.14653472493489583\n",
      "0.14663241373697916\n",
      "0.14586322835286458\n",
      "0.14590993245442707\n",
      "0.14558175184461805\n",
      "0.14570041368272568\n",
      "0.14606096462673612\n",
      "0.1456786607530382\n",
      "0.14559399820963542\n",
      "0.14633933919270833\n",
      "0.14639921332465278\n",
      "0.1455361287434896\n",
      "0.14615854899088543\n",
      "0.14587806396484376\n",
      "0.14543665228949654\n",
      "0.14588726399739582\n",
      "0.1459232218424479\n",
      "0.14578762478298612\n",
      "0.14560385335286458\n",
      "0.1457989773220486\n",
      "0.14604046902126736\n",
      "0.14555691596137152\n",
      "0.1454666978624132\n",
      "0.14698611246744792\n",
      "0.14592070719401043\n",
      "0.1456325697157118\n",
      "0.14550170220269099\n",
      "0.14530633409288193\n",
      "0.14551853569878473\n",
      "0.1454858696831597\n",
      "0.14544909125434027\n",
      "0.14611657307942708\n",
      "0.1467419881184896\n",
      "0.14555547688802084\n",
      "0.14585069308810764\n",
      "0.1452076904296875\n",
      "0.1453902045355903\n",
      "0.14563004692925346\n",
      "0.1457933132595486\n",
      "0.1452710435655382\n",
      "0.14527090386284722\n",
      "0.14546417371961806\n",
      "0.14577339409722223\n",
      "0.14682745225694444\n",
      "0.14542019721137153\n",
      "0.14547530517578125\n",
      "0.14519380696614584\n",
      "0.1452376478407118\n",
      "0.14550035400390626\n",
      "0.14532904052734374\n",
      "0.14517062445746529\n",
      "0.14543729248046874\n",
      "0.145557373046875\n",
      "0.14545467664930556\n",
      "0.14701961127387153\n",
      "0.14550473768446182\n",
      "0.14530328776041668\n",
      "0.14514493543836807\n",
      "0.1451736572265625\n",
      "0.1453134711371528\n",
      "0.14521745062934027\n",
      "0.14515741102430554\n",
      "0.14534282497829862\n",
      "0.14528230387369792\n",
      "0.14506544596354168\n",
      "0.14526260308159722\n",
      "0.14584649386935764\n",
      "0.14578756239149304\n",
      "0.14500169677734376\n",
      "0.14559552815755208\n",
      "0.14496685384114583\n",
      "0.14504242621527777\n",
      "0.14542112630208334\n",
      "0.1450446017795139\n",
      "0.1450380086263021\n",
      "0.14564651285807292\n",
      "0.1453437228732639\n",
      "0.14480167236328126\n",
      "0.1459746378580729\n",
      "0.1455804728190104\n",
      "0.14497097032335068\n",
      "0.14517210422092014\n",
      "0.14501884223090278\n",
      "0.14503694932725694\n",
      "0.1449178453233507\n",
      "0.14531496446397568\n",
      "0.1453769517686632\n",
      "0.14472696533203125\n",
      "0.14519067925347223\n",
      "0.14505314127604166\n",
      "0.14478922390407986\n",
      "0.1461537339952257\n",
      "0.14508967013888888\n",
      "0.14495403781467014\n",
      "0.14496443820529514\n",
      "0.14458876139322915\n",
      "0.1448300550672743\n",
      "0.14503446044921875\n",
      "0.14493042534722222\n",
      "0.14566229248046875\n",
      "0.14482187228732638\n",
      "0.14548035617404514\n",
      "0.14500267198350694\n",
      "0.14456175944010416\n",
      "0.14484895968967013\n",
      "0.14483257649739584\n",
      "0.14504354790581597\n",
      "0.1449564453125\n",
      "0.14462881673177083\n",
      "0.1453102091471354\n",
      "0.14718104248046876\n",
      "0.14528814968532985\n",
      "0.14457703450520834\n",
      "0.14459671902126736\n",
      "0.1444567640516493\n",
      "0.14444593505859374\n",
      "0.14466794840494793\n",
      "0.14490155571831598\n",
      "0.14473316107855902\n",
      "0.14439271375868057\n",
      "0.1444110568576389\n",
      "0.14504181315104167\n",
      "0.14596936577690972\n",
      "0.1446342312282986\n",
      "0.14452720269097222\n",
      "0.14434628634982638\n",
      "0.1444354505750868\n",
      "0.144534716796875\n",
      "0.14450048285590278\n",
      "0.14451999240451388\n",
      "0.1446544908311632\n",
      "0.1444956814236111\n",
      "0.14480547960069445\n",
      "0.1469693115234375\n",
      "0.14486587592230904\n",
      "0.14456056450737848\n",
      "0.14433514404296874\n",
      "0.1443788343641493\n",
      "0.14432056477864583\n",
      "0.14435950792100694\n",
      "0.14452760416666666\n",
      "0.14442142333984376\n",
      "0.1443080050998264\n",
      "0.14449212239583334\n",
      "0.1445344211154514\n",
      "0.1442875054253472\n",
      "0.1445712931315104\n",
      "0.14532046983506944\n",
      "0.1443141370985243\n",
      "0.14457620307074653\n",
      "0.14480917154947917\n",
      "0.14408304307725694\n",
      "0.144445654296875\n",
      "0.1443431654188368\n",
      "0.1443115953233507\n",
      "0.1448084974500868\n",
      "0.14430509168836805\n",
      "0.14421677110460068\n",
      "0.14543465711805556\n",
      "0.14411219618055557\n",
      "0.14456791042751735\n",
      "0.14432523735894098\n",
      "0.14403888481987848\n",
      "0.14414419759114583\n",
      "0.14437330186631944\n",
      "0.14549415690104167\n",
      "0.14411674533420138\n",
      "0.14429110378689236\n",
      "0.14403605278862847\n",
      "0.14397698160807293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14434790581597223\n",
      "0.14412007242838543\n",
      "0.1441356947157118\n",
      "0.1452529812282986\n",
      "0.14413956163194444\n",
      "0.14434375813802083\n",
      "0.14429354383680557\n",
      "0.143794677734375\n",
      "0.1440913343641493\n",
      "0.1441277560763889\n",
      "0.14386133219401043\n",
      "0.14406889241536458\n",
      "0.1443243394639757\n",
      "0.14615358479817708\n",
      "0.1453507826063368\n",
      "0.1440939954969618\n",
      "0.1440057169596354\n",
      "0.1438563232421875\n",
      "0.14376785753038193\n",
      "0.14387792154947918\n",
      "0.14395865478515624\n",
      "0.14393970269097223\n",
      "0.14391302625868055\n",
      "0.14398889024522568\n",
      "0.14396961669921876\n",
      "0.1438806599934896\n",
      "0.14392960883246528\n",
      "0.1440272745768229\n",
      "0.1439891845703125\n",
      "0.1439384290907118\n",
      "0.1439482896592882\n",
      "0.1438879150390625\n",
      "0.14392064480251737\n",
      "0.14536018337673612\n",
      "0.14427331949869793\n",
      "0.14421867811414932\n",
      "0.14352498101128472\n",
      "0.14369457058376736\n",
      "0.14381644694010418\n",
      "0.14387208387586806\n",
      "0.14369143473307291\n",
      "0.14372163899739585\n",
      "0.14410066596137153\n",
      "0.14399756401909722\n",
      "0.1435217298719618\n",
      "0.1439431165907118\n",
      "0.14384719780815972\n",
      "0.14343661295572915\n",
      "0.14443932427300346\n",
      "0.14531436767578124\n",
      "0.14388781060112849\n",
      "0.1436788777669271\n",
      "0.1434325154622396\n",
      "0.14356147189670138\n",
      "0.14335735270182293\n",
      "0.1435838121202257\n",
      "0.1436049574110243\n",
      "0.14355965440538193\n",
      "0.1440929931640625\n",
      "0.14353685709635416\n",
      "0.14336180962456596\n",
      "0.14503085530598958\n",
      "0.1449689710828993\n",
      "0.14398676079644096\n",
      "0.14334716118706597\n",
      "0.14340098876953125\n",
      "0.14324148627387154\n",
      "0.14333859320746528\n",
      "0.14338461235894098\n",
      "0.14338389892578124\n",
      "0.1434909437391493\n",
      "0.14347916124131943\n",
      "0.1434451429578993\n",
      "0.14379683702256946\n",
      "0.14423143581814235\n",
      "0.14327330457899307\n",
      "0.14401202392578125\n",
      "0.14341399739583333\n",
      "0.14324405246310765\n",
      "0.14354711371527779\n",
      "0.1432805948893229\n",
      "0.14360882975260417\n",
      "0.14398778347439237\n",
      "0.14311732855902778\n",
      "0.1438774685329861\n",
      "0.14352828369140624\n",
      "0.14323692220052084\n",
      "0.14383989935980904\n",
      "0.14313531358506945\n",
      "0.14316349012586804\n",
      "0.14334295789930557\n",
      "0.1430868611653646\n",
      "0.14336489664713542\n",
      "0.1458647447374132\n",
      "0.1439710394965278\n",
      "0.14340614420572917\n",
      "0.14311851806640624\n",
      "0.14308195122612846\n",
      "0.14295570068359376\n",
      "0.14308228759765626\n",
      "0.1430662326388889\n",
      "0.14308609890407986\n",
      "0.14320758463541666\n",
      "0.1431518364800347\n",
      "0.14307273084852432\n",
      "0.1432387424045139\n",
      "0.1432618855794271\n",
      "0.14326363254123264\n",
      "0.1437234836154514\n",
      "0.14301419270833332\n",
      "0.14371963840060764\n",
      "0.14340804850260416\n",
      "0.1429781032986111\n",
      "0.14319702826605904\n",
      "0.1428630072699653\n",
      "0.14330590006510416\n",
      "0.14386908501519097\n",
      "0.1427942437065972\n",
      "0.1431258056640625\n",
      "0.14292722981770833\n",
      "0.1431779771592882\n",
      "0.14289541422526042\n",
      "0.1429259996202257\n",
      "0.1447852728949653\n",
      "0.1430503892686632\n",
      "0.1431087646484375\n",
      "0.14264879014756945\n",
      "0.1428285169813368\n",
      "0.14282787950303819\n",
      "0.1427146931966146\n",
      "0.14295848931206598\n",
      "0.1429046346028646\n",
      "0.14272925618489582\n",
      "0.1430964057074653\n",
      "0.14494014485677084\n",
      "0.14339455159505207\n",
      "0.1430900377061632\n",
      "0.14262385932074653\n",
      "0.14273692898220486\n",
      "0.1425833509657118\n",
      "0.1427039564344618\n",
      "0.14280520833333332\n",
      "0.14268467610677082\n",
      "0.14276192220052084\n",
      "0.14288665771484374\n",
      "0.14270941297743056\n",
      "0.14272353651258682\n",
      "0.1430857896592882\n",
      "0.1432338636610243\n",
      "0.14272978922526042\n",
      "0.14292573784722223\n",
      "0.14332906358506944\n",
      "0.1424450656467014\n",
      "0.1431132310655382\n",
      "0.14264205593532986\n",
      "0.14246766493055554\n",
      "0.14263743625217015\n",
      "0.1424598347981771\n",
      "0.14269031711154515\n",
      "0.14478455810546875\n",
      "0.14339726155598959\n",
      "0.14287132568359376\n",
      "0.14239256863064237\n",
      "0.14246150580512154\n",
      "0.14233160942925346\n",
      "0.14246905381944444\n",
      "0.14247943793402779\n",
      "0.14244973822699653\n",
      "0.1425560329861111\n",
      "0.14252253960503472\n",
      "0.14242891438802083\n",
      "0.14256532389322918\n",
      "0.1427664537217882\n",
      "0.14338695475260416\n",
      "0.14248637830946181\n",
      "0.14289200032552082\n",
      "0.1424170681423611\n",
      "0.14227855631510417\n",
      "0.1425146240234375\n",
      "0.14237736545138888\n",
      "0.14275170762803818\n",
      "0.14269637044270833\n",
      "0.14216538899739584\n",
      "0.14299597710503473\n",
      "0.14238843722873265\n",
      "0.14243429633246527\n",
      "0.14332376166449654\n",
      "0.14209071316189237\n",
      "0.14244250081380208\n",
      "0.14215204535590278\n",
      "0.14241964382595487\n",
      "0.14278724636501736\n",
      "0.14210335557725695\n",
      "0.14239327121310763\n",
      "0.14287770182291668\n",
      "0.14201329345703126\n",
      "0.14287711859809027\n",
      "0.14286684705946182\n",
      "0.14211273871527777\n",
      "0.14229557291666667\n",
      "0.14204620090060763\n",
      "0.14236711018880208\n",
      "0.14217481011284722\n",
      "0.14206126302083333\n",
      "0.14236127794053818\n",
      "0.1424203803168403\n",
      "0.1432832044813368\n",
      "0.14230157335069443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-7d1a95d68704>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0msum_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \"\"\"\n\u001b[0;32m   1281\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[1;32m-> 1282\u001b[1;33m                            input, target, size_average, reduce)\n\u001b[0m\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[1;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=model.cuda()\n",
    "nb_of_epochs=4000\n",
    "batch_size=int(x_variable.size(0)/10)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = model(x_variable.narrow(0,b,batch_size))                \n",
    "        loss = criterion(out, y_variable.narrow(0,b,batch_size))     \n",
    "        sum_loss+=loss.data[0]\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "    print(sum_loss/x_variable.size(0))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(1, 30, kernel_size=(2,), stride=(2,))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv1d(30, 600, kernel_size=(2,), stride=(1,))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Conv1d(600, 2, kernel_size=(6,), stride=(1,))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ReLU(inplace)\n",
       "    (1): ConvTranspose1d(2, 600, kernel_size=(6,), stride=(1,))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose1d(600, 300, kernel_size=(2,), stride=(1,))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): ConvTranspose1d(300, 1, kernel_size=(2,), stride=(2,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable=model.cpu(),x_variable.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0  ,.,.) = \n",
       "  5.8568e-01 -4.2664e-02  9.4762e-01  ...  -7.7057e-01  1.0163e+00 -4.0515e-01\n",
       "\n",
       "(  1  ,.,.) = \n",
       "  6.2823e-01 -2.8880e-02  4.5809e-01  ...  -3.8123e-01  1.2614e+00 -2.9751e-01\n",
       "\n",
       "(  2  ,.,.) = \n",
       "  9.9070e-01 -1.7096e-01  8.4111e-01  ...  -4.5242e-01  7.3963e-01 -4.1780e-01\n",
       " ...  \n",
       "\n",
       "(89997,.,.) = \n",
       "  5.3168e-01 -4.4960e-02  4.1257e-01  ...  -7.3768e-01  8.9729e-01 -3.5814e-01\n",
       "\n",
       "(89998,.,.) = \n",
       "  1.0826e+00  1.7485e-01  4.3488e-01  ...  -4.4951e-01  8.8360e-01 -6.9928e-01\n",
       "\n",
       "(89999,.,.) = \n",
       "  5.8561e-01  1.1619e-01  1.2359e+00  ...  -1.0258e+00  3.3638e-01 -1.9004e-01\n",
       "[torch.FloatTensor of size 90000x1x28]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 28 into shape (7,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-08d1f3ade552>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 28 into shape (7,2)"
     ]
    }
   ],
   "source": [
    "polygon_prediction=model(x_variable)\n",
    "polygon_prediction=polygon_prediction[8].data.cpu()\n",
    "polygon_prediction=polygon_prediction.numpy()\n",
    "polygon_prediction=polygon_prediction.reshape(14,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(polygon_prediction)\n",
    "plot_contour(polygons[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv1d in module torch.nn.modules.conv:\n",
      "\n",
      "class Conv1d(_ConvNd)\n",
      " |  Applies a 1D convolution over an input signal composed of several input\n",
      " |  planes.\n",
      " |  \n",
      " |  In the simplest case, the output value of the layer with input size\n",
      " |  :math:`(N, C_{in}, L)` and output :math:`(N, C_{out}, L_{out})` can be\n",
      " |  precisely described as:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      \\begin{array}{ll}\n",
      " |      out(N_i, C_{out_j})  = bias(C_{out_j})\n",
      " |                     + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \\star input(N_i, k)\n",
      " |      \\end{array}\n",
      " |  \n",
      " |  where :math:`\\star` is the valid `cross-correlation`_ operator,\n",
      " |  :math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
      " |  :math:`L` is a length of signal sequence.\n",
      " |  \n",
      " |  | :attr:`stride` controls the stride for the cross-correlation, a single\n",
      " |    number or a one-element tuple.\n",
      " |  | :attr:`padding` controls the amount of implicit zero-paddings on both\n",
      " |  |  sides for :attr:`padding` number of points.\n",
      " |  | :attr:`dilation` controls the spacing between the kernel points; also\n",
      " |    known as the à trous algorithm. It is harder to describe, but this `link`_\n",
      " |    has a nice visualization of what :attr:`dilation` does.\n",
      " |  | :attr:`groups` controls the connections between inputs and outputs.\n",
      " |    `in_channels` and `out_channels` must both be divisible by `groups`.\n",
      " |  |       At groups=1, all inputs are convolved to all outputs.\n",
      " |  |       At groups=2, the operation becomes equivalent to having two conv\n",
      " |               layers side by side, each seeing half the input channels,\n",
      " |               and producing half the output channels, and both subsequently\n",
      " |               concatenated.\n",
      " |          At groups=`in_channels`, each input channel is convolved with its\n",
      " |               own set of filters (of size `out_channels // in_channels`).\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |       Depending of the size of your kernel, several (of the last)\n",
      " |       columns of the input might be lost, because it is a valid\n",
      " |       `cross-correlation`_, and not a full `cross-correlation`_.\n",
      " |       It is up to the user to add proper padding.\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |       The configuration when `groups == in_channels` and `out_channels = K * in_channels`\n",
      " |       where `K` is a positive integer is termed in literature as depthwise convolution.\n",
      " |  \n",
      " |       In other words, for an input of size :math:`(N, C_{in}, L_{in})`, if you want a\n",
      " |       depthwise convolution with a depthwise multiplier `K`,\n",
      " |       then you use the constructor arguments\n",
      " |       :math:`(in\\_channels=C_{in}, out\\_channels=C_{in} * K, ..., groups=C_{in})`\n",
      " |  \n",
      " |  Args:\n",
      " |      in_channels (int): Number of channels in the input image\n",
      " |      out_channels (int): Number of channels produced by the convolution\n",
      " |      kernel_size (int or tuple): Size of the convolving kernel\n",
      " |      stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      " |      padding (int or tuple, optional): Zero-padding added to both sides of\n",
      " |          the input. Default: 0\n",
      " |      dilation (int or tuple, optional): Spacing between kernel\n",
      " |          elements. Default: 1\n",
      " |      groups (int, optional): Number of blocked connections from input\n",
      " |          channels to output channels. Default: 1\n",
      " |      bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C_{in}, L_{in})`\n",
      " |      - Output: :math:`(N, C_{out}, L_{out})` where\n",
      " |        :math:`L_{out} = floor((L_{in}  + 2 * padding - dilation * (kernel\\_size - 1) - 1) / stride + 1)`\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight (Tensor): the learnable weights of the module of shape\n",
      " |          (out_channels, in_channels, kernel_size)\n",
      " |      bias (Tensor):   the learnable bias of the module of shape\n",
      " |          (out_channels)\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Conv1d(16, 33, 3, stride=2)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 16, 50))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  .. _cross-correlation:\n",
      " |      https://en.wikipedia.org/wiki/Cross-correlation\n",
      " |  \n",
      " |  .. _link:\n",
      " |      https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Conv1d\n",
      " |      _ConvNd\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _ConvNd:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97967017,  0.00463816],\n",
       "       [ 0.84873545,  0.50282168],\n",
       "       [ 0.38454553,  0.68619001],\n",
       "       [-0.00177162,  0.84484351],\n",
       "       [-0.40613338,  0.69851059],\n",
       "       [-0.9370811 ,  0.56263232],\n",
       "       [-0.90546513,  0.00325594],\n",
       "       [-0.76080012, -0.44539672],\n",
       "       [-0.39749897, -0.67931378],\n",
       "       [ 0.00169786, -0.92938364],\n",
       "       [ 0.49300653, -0.82771182],\n",
       "       [ 0.70441866, -0.41366461]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNorm1d in module torch.nn.modules.batchnorm:\n",
      "\n",
      "class BatchNorm1d(_BatchNorm)\n",
      " |  Applies Batch Normalization over a 2d or 3d input that is seen as a\n",
      " |  mini-batch.\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated per-dimension over\n",
      " |  the mini-batches and gamma and beta are learnable parameter vectors\n",
      " |  of size C (where C is the input size).\n",
      " |  \n",
      " |  During training, this layer keeps a running estimate of its computed mean\n",
      " |  and variance. The running sum is kept with a default momentum of 0.1.\n",
      " |  \n",
      " |  During evaluation, this running mean/variance is used for normalization.\n",
      " |  \n",
      " |  Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      " |  on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
      " |  \n",
      " |  Args:\n",
      " |      num_features: num_features from an expected input of size\n",
      " |          `batch_size x num_features [x width]`\n",
      " |      eps: a value added to the denominator for numerical stability.\n",
      " |          Default: 1e-5\n",
      " |      momentum: the value used for the running_mean and running_var\n",
      " |          computation. Default: 0.1\n",
      " |      affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      " |          affine parameters. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
      " |      - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100)\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100, affine=False)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 100))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNorm1d\n",
      " |      _BatchNorm\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _BatchNorm:\n",
      " |  \n",
      " |  __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNorm1d in module torch.nn.modules.batchnorm:\n",
      "\n",
      "class BatchNorm1d(_BatchNorm)\n",
      " |  Applies Batch Normalization over a 2d or 3d input that is seen as a\n",
      " |  mini-batch.\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated per-dimension over\n",
      " |  the mini-batches and gamma and beta are learnable parameter vectors\n",
      " |  of size C (where C is the input size).\n",
      " |  \n",
      " |  During training, this layer keeps a running estimate of its computed mean\n",
      " |  and variance. The running sum is kept with a default momentum of 0.1.\n",
      " |  \n",
      " |  During evaluation, this running mean/variance is used for normalization.\n",
      " |  \n",
      " |  Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      " |  on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
      " |  \n",
      " |  Args:\n",
      " |      num_features: num_features from an expected input of size\n",
      " |          `batch_size x num_features [x width]`\n",
      " |      eps: a value added to the denominator for numerical stability.\n",
      " |          Default: 1e-5\n",
      " |      momentum: the value used for the running_mean and running_var\n",
      " |          computation. Default: 0.1\n",
      " |      affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      " |          affine parameters. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
      " |      - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100)\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100, affine=False)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 100))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNorm1d\n",
      " |      _BatchNorm\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _BatchNorm:\n",
      " |  \n",
      " |  __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Variable(torch.FloatTensor([[[0,0,3]]]))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 24])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(in_channels=1,out_channels=2,kernel_size=2)\n",
    "x_variable=x_variable.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.2492 -1.6630\n",
       "  0.1763  0.8156\n",
       "[torch.FloatTensor of size 1x2x2]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 24])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(in_channels=1,out_channels=1,kernel_size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (CUDAFloatTensor) and weight type (CPUFloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c33b0b8d459e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 168\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0m_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (CUDAFloatTensor) and weight type (CPUFloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "m(x_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "func=nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ba008a11b8a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "func(m(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable=x_variable.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(24, 14)\n",
    "        self.fc21 = nn.Linear(14, 2)\n",
    "        self.fc22 = nn.Linear(14, 2)\n",
    "        self.fc3 = nn.Linear(2, 14)\n",
    "        self.fc4 = nn.Linear(14, 24)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "reconstruction_function = nn.BCELoss()\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08427379811604818\n",
      "0.05906171976725261\n",
      "0.042195732879638674\n",
      "0.02583740832010905\n",
      "0.012065117899576823\n",
      "0.003920155477523803\n",
      "0.0017312102317810058\n",
      "0.0017457127412160238\n",
      "0.0014478217919667562\n",
      "0.0011840258439381917\n",
      "0.0010786115248998006\n",
      "0.0009954208215077719\n",
      "0.0009241549412409465\n",
      "0.0008680975914001465\n",
      "0.0008176822662353516\n",
      "0.0007710820436477661\n",
      "0.0007279782851537069\n",
      "0.000687526289621989\n",
      "0.0006492488662401835\n",
      "0.0006128788749376932\n",
      "0.00057838667233785\n",
      "0.0005447821577390034\n",
      "0.0005132324814796448\n",
      "0.0004823383053143819\n",
      "0.00045259085496266685\n",
      "0.0004236271778742472\n",
      "0.0003961742361386617\n",
      "0.0003692155043284098\n",
      "0.00034255663951237995\n",
      "0.0003169219692548116\n",
      "0.00029175037145614624\n",
      "0.00026713016231854757\n",
      "0.00024306388894716898\n",
      "0.00021923057238260906\n",
      "0.00019581817785898844\n",
      "0.00017181203365325927\n",
      "0.00014876575271288554\n",
      "0.0001262182076772054\n",
      "0.00010323645273844401\n",
      "8.076347708702088e-05\n",
      "5.763450264930725e-05\n",
      "3.502514958381653e-05\n",
      "1.3118181626001994e-05\n",
      "-9.948378801345826e-06\n",
      "-3.226481874783834e-05\n",
      "-5.5160810550053915e-05\n",
      "-7.773837248484293e-05\n",
      "-0.00010087252259254456\n",
      "-0.0001245050589243571\n",
      "-0.00014702658255894978\n",
      "-0.00016969714959462484\n",
      "-0.00019413987596829732\n",
      "-0.00021716567675272624\n",
      "-0.00024070487419764201\n",
      "-0.0002648278892040253\n",
      "-0.00028889471093813577\n",
      "-0.0003140231390794118\n",
      "-0.000338344810406367\n",
      "-0.00036256824334462485\n",
      "-0.00038730589548746744\n",
      "-0.00041285643180211383\n",
      "-0.00043880128463109333\n",
      "-0.0004643892248471578\n",
      "-0.0004898603439331055\n",
      "-0.0005171652634938558\n",
      "-0.0005414639512697855\n",
      "-0.0005675115744272868\n",
      "-0.00059354194800059\n",
      "-0.00061952512661616\n",
      "-0.000646462603410085\n",
      "-0.000672138520081838\n",
      "-0.0006985012372334798\n",
      "-0.0007241184949874878\n",
      "-0.0007483807245890299\n",
      "-0.0007721872329711914\n",
      "-0.0007963706572850545\n",
      "-0.0008200628836949667\n",
      "-0.0008421722094217936\n",
      "-0.0008645760297775268\n",
      "-0.0008869690497716267\n",
      "-0.000906771961847941\n",
      "-0.0009258384466171264\n",
      "-0.0009429646094640096\n",
      "-0.0009616099754969279\n",
      "-0.000977546739578247\n",
      "-0.0009956653197606405\n",
      "-0.0010086373805999755\n",
      "-0.0010235898812611898\n",
      "-0.0010366857290267945\n",
      "-0.001048962680498759\n",
      "-0.0010609579881032307\n",
      "-0.0010727603594462077\n",
      "-0.0010831118106842041\n",
      "-0.001092466974258423\n",
      "-0.0011020208994547526\n",
      "-0.0011104041735331218\n",
      "-0.0011187591552734375\n",
      "-0.0011256612300872802\n",
      "-0.0011327897389729817\n",
      "-0.001139420970280965\n",
      "-0.0011452138423919678\n",
      "-0.001151212207476298\n",
      "-0.001156295649210612\n",
      "-0.0011609936793645222\n",
      "-0.0011651495377222698\n",
      "-0.0011696418444315593\n",
      "-0.0011731675068537395\n",
      "-0.0011766764958699545\n",
      "-0.0011799752871195475\n",
      "-0.0011829618215560913\n",
      "-0.001185554536183675\n",
      "-0.0011882928450902303\n",
      "-0.0011904172897338867\n",
      "-0.0011926185846328735\n",
      "-0.0011944268067677817\n",
      "-0.0011962531248728434\n",
      "-0.0011979233741760255\n",
      "-0.0011995075702667236\n",
      "-0.001200924770037333\n",
      "-0.001202187689145406\n",
      "-0.0012035684585571288\n",
      "-0.0012047712961832683\n",
      "-0.0012058857361475627\n",
      "-0.001207015617688497\n",
      "-0.0012079926490783691\n",
      "-0.0012090163866678874\n",
      "-0.001209925373395284\n",
      "-0.0012107817729314168\n",
      "-0.0012115832726160686\n",
      "-0.0012124318599700927\n",
      "-0.001213224705060323\n",
      "-0.001214000129699707\n",
      "-0.0012147220055262248\n",
      "-0.0012153810660044352\n",
      "-0.0012160804669062296\n",
      "-0.0012167289733886718\n",
      "-0.0012173409700393678\n",
      "-0.0012179548978805542\n",
      "-0.0012185983022054037\n",
      "-0.0012191402594248454\n",
      "-0.0012196829319000243\n",
      "-0.001220228640238444\n",
      "-0.0012207616647084555\n",
      "-0.0012212678988774617\n",
      "-0.0012217735131581624\n",
      "-0.00122223223845164\n",
      "-0.0012227161566416424\n",
      "-0.001223235861460368\n",
      "-0.0012236465613047283\n",
      "-0.0012240599632263184\n",
      "-0.0012245021184285481\n",
      "-0.0012249342838923137\n",
      "-0.0012252930800120037\n",
      "-0.0012257636626561482\n",
      "-0.0012260949373245238\n",
      "-0.0012264157613118489\n",
      "-0.0012268179416656494\n",
      "-0.0012272163073221844\n",
      "-0.0012275152683258056\n",
      "-0.0012278796275456746\n",
      "-0.0012282392342885336\n",
      "-0.0012285046100616456\n",
      "-0.0012288379748662313\n",
      "-0.001229147736231486\n",
      "-0.0012294867277145385\n",
      "-0.0012297534227371217\n",
      "-0.0012300935427347818\n",
      "-0.001230324912071228\n",
      "-0.0012306315342585246\n",
      "-0.001230921181042989\n",
      "-0.00123118683497111\n",
      "-0.0012314383506774901\n",
      "-0.001231650455792745\n",
      "-0.0012319421370824179\n",
      "-0.0012321635643641154\n",
      "-0.0012324432293574015\n",
      "-0.0012326678276062012\n",
      "-0.0012329065720240275\n",
      "-0.001233125376701355\n",
      "-0.001233336369196574\n",
      "-0.0012335424900054932\n",
      "-0.0012337837616602579\n",
      "-0.0012339744329452514\n",
      "-0.0012341950575510661\n",
      "-0.0012344226996103922\n",
      "-0.0012346192121505738\n",
      "-0.0012348027467727661\n",
      "-0.0012350183327992758\n",
      "-0.0012351822137832641\n",
      "-0.0012353712956110637\n",
      "-0.0012355668306350707\n",
      "-0.001235733962059021\n",
      "-0.0012359388113021852\n",
      "-0.0012360803842544556\n",
      "-0.0012362808704376221\n",
      "-0.001236451546351115\n",
      "-0.001236619488398234\n",
      "-0.0012367979288101197\n",
      "-0.001236928701400757\n",
      "-0.0012371576070785522\n",
      "-0.0012372647762298585\n",
      "-0.0012374217907587686\n",
      "-0.001237554955482483\n",
      "-0.0012377196232477823\n",
      "-0.0012378894885381063\n",
      "-0.0012380483547846477\n",
      "-0.0012381714741388956\n",
      "-0.0012383075078328451\n",
      "-0.001238483476638794\n",
      "-0.0012385749816894531\n",
      "-0.0012387377580006917\n",
      "-0.0012388960043589275\n",
      "-0.0012389691511789957\n",
      "-0.0012391486962636312\n",
      "-0.0012392761945724487\n",
      "-0.0012393794298171997\n",
      "-0.0012395119190216064\n",
      "-0.0012396649916966756\n",
      "-0.0012397494713465372\n",
      "-0.0012398880958557129\n",
      "-0.0012399969021479288\n",
      "-0.0012401222387949625\n",
      "-0.0012402298291524251\n",
      "-0.0012403820276260377\n",
      "-0.0012404666503270467\n",
      "-0.0012405905405680339\n",
      "-0.0012407252073287963\n",
      "-0.0012408049583435058\n",
      "-0.00124096941947937\n",
      "-0.0012410502036412556\n",
      "-0.0012411648750305176\n",
      "-0.0012412681579589844\n",
      "-0.00124139297803243\n",
      "-0.0012414655367533366\n",
      "-0.0012415517171223958\n",
      "-0.001241709089279175\n",
      "-0.0012417890469233195\n",
      "-0.0012419016440709432\n",
      "-0.0012420185327529907\n",
      "-0.001242108416557312\n",
      "-0.0012422093868255616\n",
      "-0.0012423048575719198\n",
      "-0.0012423940499623617\n",
      "-0.0012425136168797812\n",
      "-0.0012426137129465738\n",
      "-0.0012427076657613118\n",
      "-0.0012428005456924438\n",
      "-0.0012429101228713989\n",
      "-0.001242996350924174\n",
      "-0.0012430423418680826\n",
      "-0.0012431825717290242\n",
      "-0.0012432677666346231\n",
      "-0.0012433658043543498\n",
      "-0.0012434782981872558\n",
      "-0.0012435287157694498\n",
      "-0.0012436463673909505\n",
      "-0.0012437087138493855\n",
      "-0.001243823226292928\n",
      "-0.0012438987255096435\n",
      "-0.0012439783652623494\n",
      "-0.0012440734386444093\n",
      "-0.0012441547632217407\n",
      "-0.0012442294200261433\n",
      "-0.0012443506797154744\n",
      "-0.0012444143851598104\n",
      "-0.0012444582303365072\n",
      "-0.0012445868810017904\n",
      "-0.0012446454048156738\n",
      "-0.0012447481632232667\n",
      "-0.0012448442379633586\n",
      "-0.0012448943694432577\n",
      "-0.001245007586479187\n",
      "-0.001245080065727234\n",
      "-0.001245167318979899\n",
      "-0.001245264450709025\n",
      "-0.0012453317721684773\n",
      "-0.0012453942696253458\n",
      "-0.0012454755783081054\n",
      "-0.0012455838521321615\n",
      "-0.0012456401427586874\n",
      "-0.0012457578976949055\n",
      "-0.0012457786242167155\n",
      "-0.0012458621501922607\n",
      "-0.0012459725220998127\n",
      "-0.0012460209210713705\n",
      "-0.0012460956811904907\n",
      "-0.0012461942911148071\n",
      "-0.001246280352274577\n",
      "-0.0012463392575581868\n",
      "-0.001246423554420471\n",
      "-0.0012464807192484539\n",
      "-0.0012465386788050334\n",
      "-0.0012466282367706299\n",
      "-0.0012466727733612061\n",
      "-0.0012467590729395548\n",
      "-0.0012468356529871623\n",
      "-0.0012469284057617187\n",
      "-0.0012470382610956828\n",
      "-0.0012470735788345337\n",
      "-0.001247126038869222\n",
      "-0.0012472362279891968\n",
      "-0.0012472904920578004\n",
      "-0.0012473863124847413\n",
      "-0.0012474538803100585\n",
      "-0.0012475524425506592\n",
      "-0.0012475937604904176\n",
      "-0.001247694993019104\n",
      "-0.0012477683067321776\n",
      "-0.0012478516578674317\n",
      "-0.0012478532552719116\n",
      "-0.0012479936997095743\n",
      "-0.001248007567723592\n",
      "-0.0012481063604354858\n",
      "-0.0012481571594874063\n",
      "-0.0012482614596684774\n",
      "-0.0012483050028483072\n",
      "-0.0012484041929244996\n",
      "-0.0012484810829162599\n",
      "-0.001248557432492574\n",
      "-0.0012486231883366903\n",
      "-0.0012486682891845703\n",
      "-0.0012487359523773193\n",
      "-0.0012488667090733846\n",
      "-0.001248843820889791\n",
      "-0.001248946777979533\n",
      "-0.001248987913131714\n",
      "-0.001249081301689148\n",
      "-0.0012491668462753297\n",
      "-0.0012492084741592406\n",
      "-0.0012492985010147096\n",
      "-0.0012493875900904338\n",
      "-0.0012494199434916178\n",
      "-0.0012495031674702962\n",
      "-0.001249535576502482\n",
      "-0.0012496412595113119\n",
      "-0.0012497000376383463\n",
      "-0.0012497437477111816\n",
      "-0.0012498330911000569\n",
      "-0.0012498919248580933\n",
      "-0.0012499701182047526\n",
      "-0.0012500519752502441\n",
      "-0.0012501142183939616\n",
      "-0.0012501633723576865\n",
      "-0.0012502117872238159\n",
      "-0.0012502467393875122\n",
      "-0.0012503542025883991\n",
      "-0.0012504365046819052\n",
      "-0.0012504802942276002\n",
      "-0.0012505703767140706\n",
      "-0.0012506165186564127\n",
      "-0.0012506740729014078\n",
      "-0.0012507869164148967\n",
      "-0.0012508083661397298\n",
      "-0.0012509148041407268\n",
      "-0.001250914478302002\n",
      "-0.0012509743928909303\n",
      "-0.0012510811726252238\n",
      "-0.001251150870323181\n",
      "-0.0012511801958084107\n",
      "-0.0012512809673945109\n",
      "-0.0012513704856236775\n",
      "-0.0012514143864313762\n",
      "-0.0012514701684316\n",
      "-0.0012515354951222737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012515852053960165\n",
      "-0.0012516491492589315\n",
      "-0.0012517378568649292\n",
      "-0.0012517849524815877\n",
      "-0.0012518394072850545\n",
      "-0.001251940401395162\n",
      "-0.0012519787073135375\n",
      "-0.0012520352840423583\n",
      "-0.0012521003246307374\n",
      "-0.0012521643002827963\n",
      "-0.0012522292455037435\n",
      "-0.001252248771985372\n",
      "-0.001252344552675883\n",
      "-0.001252402949333191\n",
      "-0.0012524743239084879\n",
      "-0.00125256343682607\n",
      "-0.0012526012182235717\n",
      "-0.001252672553062439\n",
      "-0.001252714236577352\n",
      "-0.0012528040409088134\n",
      "-0.0012528634309768676\n",
      "-0.001252859934171041\n",
      "-0.0012529809554417928\n",
      "-0.0012529974301656087\n",
      "-0.0012530914862950643\n",
      "-0.0012531592766443888\n",
      "-0.0012532113472620645\n",
      "-0.0012532693703969319\n",
      "-0.0012533439874649048\n",
      "-0.0012533462444941203\n",
      "-0.0012534990946451822\n",
      "-0.0012535181522369385\n",
      "-0.0012535861571629841\n",
      "-0.0012536409457524617\n",
      "-0.0012537391185760498\n",
      "-0.0012537651062011718\n",
      "-0.0012538124561309815\n",
      "-0.001253877862294515\n",
      "-0.0012539216041564942\n",
      "-0.0012539740800857544\n",
      "-0.0012540358622868855\n",
      "-0.0012541396220525107\n",
      "-0.001254183602333069\n",
      "-0.001254201873143514\n",
      "-0.0012543163061141968\n",
      "-0.001254315416018168\n",
      "-0.001254396645228068\n",
      "-0.0012544441382090251\n",
      "-0.001254478136698405\n",
      "-0.001254572566350301\n",
      "-0.001254589581489563\n",
      "-0.0012546985546747842\n",
      "-0.0012547329902648926\n",
      "-0.0012548084576924643\n",
      "-0.0012548912366231282\n",
      "-0.001254892404874166\n",
      "-0.0012549559195836385\n",
      "-0.0012550140857696534\n",
      "-0.001255047877629598\n",
      "-0.0012550638119379679\n",
      "-0.0012551714340845743\n",
      "-0.0012552200078964233\n",
      "-0.0012552645444869996\n",
      "-0.0012552829424540201\n",
      "-0.0012553702036539713\n",
      "-0.0012554354747136435\n",
      "-0.0012555009206136067\n",
      "-0.0012555327812830608\n",
      "-0.0012556129455566407\n",
      "-0.0012556322733561199\n",
      "-0.0012556998729705811\n",
      "-0.0012557320435841878\n",
      "-0.0012558189551035563\n",
      "-0.0012558171033859253\n",
      "-0.0012558948357899984\n",
      "-0.001255942718187968\n",
      "-0.0012559601306915283\n",
      "-0.0012560522556304932\n",
      "-0.0012561034679412842\n",
      "-0.0012561666250228882\n",
      "-0.0012562058369318643\n",
      "-0.001256257685025533\n",
      "-0.001256278657913208\n",
      "-0.0012563753445943196\n",
      "-0.0012564125219980875\n",
      "-0.0012564541737238566\n",
      "-0.0012565122524897257\n",
      "-0.001256526811917623\n",
      "-0.0012566028356552125\n",
      "-0.0012566625038782754\n",
      "-0.001256672724088033\n",
      "-0.0012567298730214436\n",
      "-0.0012567548990249633\n",
      "-0.0012568252642949423\n",
      "-0.001256902281443278\n",
      "-0.0012569178183873494\n",
      "-0.0012569779793421428\n",
      "-0.001257033658027649\n",
      "-0.001257071844736735\n",
      "-0.0012570900599161783\n",
      "-0.0012571307023366291\n",
      "-0.0012571793874104817\n",
      "-0.001257226045926412\n",
      "-0.0012572858174641926\n",
      "-0.001257304294904073\n",
      "-0.001257357954978943\n",
      "-0.0012573967933654786\n",
      "-0.0012574463764826456\n",
      "-0.0012574628591537475\n",
      "-0.0012575455904006959\n",
      "-0.0012575699249903361\n",
      "-0.0012576123873392741\n",
      "-0.0012576103528340657\n",
      "-0.0012576875925064087\n",
      "-0.0012577220678329467\n",
      "-0.0012577943483988445\n",
      "-0.0012578356345494589\n",
      "-0.0012578316211700438\n",
      "-0.0012579135338465373\n",
      "-0.0012579407930374145\n",
      "-0.0012579456726710002\n",
      "-0.0012580350399017334\n",
      "-0.0012580711841583252\n",
      "-0.0012580925226211548\n",
      "-0.0012580955743789672\n",
      "-0.001258169945081075\n",
      "-0.0012581488291422526\n",
      "-0.0012582358439763388\n",
      "-0.0012582788546880086\n",
      "-0.0012582778056462606\n",
      "-0.0012583336114883423\n",
      "-0.0012583622137705484\n",
      "-0.0012584272066752116\n",
      "-0.0012584436178207398\n",
      "-0.0012584814548492432\n",
      "-0.0012585025548934937\n",
      "-0.0012585318883260091\n",
      "-0.0012585981289545696\n",
      "-0.0012585924863815308\n",
      "-0.0012586486180623372\n",
      "-0.0012586474180221558\n",
      "-0.0012586661577224732\n",
      "-0.001258730991681417\n",
      "-0.0012587418794631957\n",
      "-0.0012587705135345458\n",
      "-0.0012587881326675415\n",
      "-0.0012588635842005413\n",
      "-0.00125879062016805\n",
      "-0.0012588617960611979\n",
      "-0.0012589748620986938\n",
      "-0.001258957266807556\n",
      "-0.0012589823484420776\n",
      "-0.0012588920434315999\n",
      "-0.0012589467763900756\n",
      "-0.0012590702613194784\n",
      "-0.0012591113567352295\n",
      "-0.0012591260671615601\n",
      "-0.001259171716372172\n",
      "-0.00125920250415802\n",
      "-0.0012592170397440591\n",
      "-0.0012591983954111734\n",
      "-0.0012591799656550089\n",
      "-0.001259282914797465\n",
      "-0.0012593206882476806\n",
      "-0.001259374221165975\n",
      "-0.0012593659083048503\n",
      "-0.0012593790928522746\n",
      "-0.001258986496925354\n",
      "-0.001259317390124003\n",
      "-0.0012594175418217976\n",
      "-0.0012594813585281373\n",
      "-0.0012594972848892212\n",
      "-0.001259526522954305\n",
      "-0.0012595270077387492\n",
      "-0.0012595791578292846\n",
      "-0.0012596012512842813\n",
      "-0.0012596403042475382\n",
      "-0.0012596331119537354\n",
      "-0.0012596508105595908\n",
      "-0.001259625236193339\n",
      "-0.001259367283185323\n",
      "-0.001259652098019918\n",
      "-0.0012597532033920289\n",
      "-0.001259753934542338\n",
      "-0.0012597647905349732\n",
      "-0.001259799599647522\n",
      "-0.0012598477125167846\n",
      "-0.0012598567962646484\n",
      "-0.0012598876555760701\n",
      "-0.001259888219833374\n",
      "-0.0012599019368489583\n",
      "-0.001259747854868571\n",
      "-0.0012592697302500407\n",
      "-0.001259768009185791\n",
      "-0.0012599406719207764\n",
      "-0.0012599160432815552\n",
      "-0.001260000745455424\n",
      "-0.0012600125551223754\n",
      "-0.0012600631793340047\n",
      "-0.0012600241502126058\n",
      "-0.0012600638389587402\n",
      "-0.00126007186571757\n",
      "-0.001260101858774821\n",
      "-0.0012601098537445069\n",
      "-0.001260142421722412\n",
      "-0.0012601338227589925\n",
      "-0.0012601529439290366\n",
      "-0.0012601710637410483\n",
      "-0.0012601814190546672\n",
      "-0.001260197138786316\n",
      "-0.0012601953903834024\n",
      "-0.0012601422389348347\n",
      "-0.0012589783906936645\n",
      "-0.0012598117192586264\n",
      "-0.0012601711670557657\n",
      "-0.0012602538108825684\n",
      "-0.0012602849006652832\n",
      "-0.001260312255223592\n",
      "-0.0012603152831395467\n",
      "-0.0012603230635325113\n",
      "-0.0012603785117467244\n",
      "-0.001260386315981547\n",
      "-0.0012603911797205607\n",
      "-0.0012603941122690838\n",
      "-0.0012604053417841594\n",
      "-0.0012603979508082072\n",
      "-0.0012604052384694417\n",
      "-0.001260458485285441\n",
      "-0.0012604389508565268\n",
      "-0.0012604524374008178\n",
      "-0.0012604853709538778\n",
      "-0.0012604415814081828\n",
      "-0.0012604843457539876\n",
      "-0.001260497283935547\n",
      "-0.0012605131228764852\n",
      "-0.001260502862930298\n",
      "-0.0012605276981989542\n",
      "-0.001259797469774882\n",
      "-0.0012600415229797363\n",
      "-0.0012604388316472372\n",
      "-0.0012604888916015625\n",
      "-0.0012605205059051513\n",
      "-0.0012605817159016927\n",
      "-0.0012605940421422323\n",
      "-0.001260616914431254\n",
      "-0.0012606146971384684\n",
      "-0.0012606562852859498\n",
      "-0.0012606306870778401\n",
      "-0.0012606499354044596\n",
      "-0.0012606429020563762\n",
      "-0.0012606703122456867\n",
      "-0.001260668436686198\n",
      "-0.0012606672604878744\n",
      "-0.0012606884161631266\n",
      "-0.0012605853796005249\n",
      "-0.0012597787857055664\n",
      "-0.0012604778130849202\n",
      "-0.001260589114824931\n",
      "-0.0012606703678766887\n",
      "-0.0012607181628545125\n",
      "-0.001260724727312724\n",
      "-0.001260731037457784\n",
      "-0.0012607531944910685\n",
      "-0.0012607517639795938\n",
      "-0.0012607391675313313\n",
      "-0.0012607406775156657\n",
      "-0.0012608215729395548\n",
      "-0.0012607711553573608\n",
      "-0.001260785969098409\n",
      "-0.0012608130931854248\n",
      "-0.0012607850392659505\n",
      "-0.0012599170684814453\n",
      "-0.0012602992137273152\n",
      "-0.0012606984694798788\n",
      "-0.0012607420206069947\n",
      "-0.0012608150164286295\n",
      "-0.0012608271916707357\n",
      "-0.001260811177889506\n",
      "-0.0012608157475789388\n",
      "-0.0012608530282974242\n",
      "-0.001260880208015442\n",
      "-0.001260848037401835\n",
      "-0.0012608764251073202\n",
      "-0.0012608848651250203\n",
      "-0.0012608940442403157\n",
      "-0.0012608816623687743\n",
      "-0.001260902778307597\n",
      "-0.0012608998934427898\n",
      "-0.0012605419317881267\n",
      "-0.0012601253430048625\n",
      "-0.0012606746594111126\n",
      "-0.0012608273347218832\n",
      "-0.00126089559396108\n",
      "-0.001260927907625834\n",
      "-0.0012609430074691772\n",
      "-0.001260946544011434\n",
      "-0.0012609424670537313\n",
      "-0.001260936427116394\n",
      "-0.001260931412378947\n",
      "-0.00126095023949941\n",
      "-0.0012609535853068034\n",
      "-0.00126096826394399\n",
      "-0.0012609445730845134\n",
      "-0.00126045028368632\n",
      "-0.0012603166739145915\n",
      "-0.0012607593139012654\n",
      "-0.0012609209378560385\n",
      "-0.0012609650532404582\n",
      "-0.0012609640836715697\n",
      "-0.0012609668811162313\n",
      "-0.0012610003391901653\n",
      "-0.0012609975735346475\n",
      "-0.0012609878778457643\n",
      "-0.0012609899759292603\n",
      "-0.0012609922488530476\n",
      "-0.001261006212234497\n",
      "-0.0012609516302744548\n",
      "-0.0012603862524032594\n",
      "-0.0012606349388758341\n",
      "-0.0012609005769093832\n",
      "-0.001260973048210144\n",
      "-0.0012610177993774413\n",
      "-0.0012610350131988526\n",
      "-0.0012610208749771119\n",
      "-0.0012610317468643188\n",
      "-0.0012610722621281942\n",
      "-0.0012610635995864868\n",
      "-0.0012610604524612428\n",
      "-0.0012610374450683593\n",
      "-0.0012604126532872518\n",
      "-0.0012605784257253012\n",
      "-0.00126090513865153\n",
      "-0.0012610188086827597\n",
      "-0.0012610487778981527\n",
      "-0.0012611175855000813\n",
      "-0.0012610928694407144\n",
      "-0.0012610666116078695\n",
      "-0.0012610939979553224\n",
      "-0.0012610979000727337\n",
      "-0.0012610749085744221\n",
      "-0.0012610795974731445\n",
      "-0.0012609601736068726\n",
      "-0.0012603428920110067\n",
      "-0.0012609427611033122\n",
      "-0.0012610164006551107\n",
      "-0.0012610655625661215\n",
      "-0.0012610996882120769\n",
      "-0.001261098829905192\n",
      "-0.0012611077547073364\n",
      "-0.0012610937595367432\n",
      "-0.0012611122131347657\n",
      "-0.0012610753854115804\n",
      "-0.0012607493003209433\n",
      "-0.0012606952587763467\n",
      "-0.0012609874804814657\n",
      "-0.001261077086130778\n",
      "-0.001261129101117452\n",
      "-0.0012611283938090006\n",
      "-0.0012611280838648478\n",
      "-0.0012611411094665527\n",
      "-0.001261091717084249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001260905392964681\n",
      "-0.001260707139968872\n",
      "-0.0012610512653986614\n",
      "-0.0012611152410507201\n",
      "-0.0012611063559850057\n",
      "-0.0012611244678497314\n",
      "-0.0012611426671346028\n",
      "-0.0012611341794331868\n",
      "-0.0012611191511154175\n",
      "-0.0012605311632156373\n",
      "-0.0012606348911921184\n",
      "-0.0012609764337539672\n",
      "-0.0012611244837443035\n",
      "-0.0012611381928126018\n",
      "-0.0012611489216486614\n",
      "-0.0012611704508463542\n",
      "-0.0012611680269241333\n",
      "-0.001261158299446106\n",
      "-0.0012611669858296711\n",
      "-0.0012611825704574584\n",
      "-0.001261122194925944\n",
      "-0.0012606677214304605\n",
      "-0.0012608527898788453\n",
      "-0.0012610427618026734\n",
      "-0.001261119786898295\n",
      "-0.0012611679633458455\n",
      "-0.0012611983140309651\n",
      "-0.001261160675684611\n",
      "-0.0012611855824788412\n",
      "-0.0012611587603886922\n",
      "-0.0012609032074610393\n",
      "-0.0012607102553049724\n",
      "-0.0012610742966334026\n",
      "-0.0012611283699671427\n",
      "-0.0012611643075942994\n",
      "-0.0012612041234970092\n",
      "-0.001261186401049296\n",
      "-0.0012611685037612916\n",
      "-0.0012610745747884114\n",
      "-0.0012607531706492106\n",
      "-0.001261076283454895\n",
      "-0.0012611297448476156\n",
      "-0.0012611671288808187\n",
      "-0.0012611610968907673\n",
      "-0.0012611966848373414\n",
      "-0.001261195429166158\n",
      "-0.0012611932277679443\n",
      "-0.001260905933380127\n",
      "-0.0012602036476135254\n",
      "-0.001260919745763143\n",
      "-0.0012610986550649008\n",
      "-0.0012611577192942302\n",
      "-0.001261181942621867\n",
      "-0.0012612120469411214\n",
      "-0.0012612243175506592\n",
      "-0.001261231811841329\n",
      "-0.0012612200578053792\n",
      "-0.0012612329800923666\n",
      "-0.0012612041473388672\n",
      "-0.001261214820543925\n",
      "-0.0012611705859502157\n",
      "-0.001260771075884501\n",
      "-0.0012608955224355063\n",
      "-0.0012610978603363038\n",
      "-0.0012611739238103231\n",
      "-0.001261228624979655\n",
      "-0.001261218293507894\n",
      "-0.0012612144947052003\n",
      "-0.001261209193865458\n",
      "-0.0012610265811284383\n",
      "-0.0012607395728429158\n",
      "-0.0012611093362172444\n",
      "-0.0012611615657806396\n",
      "-0.0012611799478530884\n",
      "-0.0012612118005752564\n",
      "-0.001261230476697286\n",
      "-0.0012612367312113445\n",
      "-0.001261228632926941\n",
      "-0.0012606244723002116\n",
      "-0.0012603849649429322\n",
      "-0.0012609748840332032\n",
      "-0.0012611801862716676\n",
      "-0.0012611860116322835\n",
      "-0.0012612212260564167\n",
      "-0.0012611578543980917\n",
      "-0.0012612250169118245\n",
      "-0.0012612532059351604\n",
      "-0.001261239473025004\n",
      "-0.0012612417777379354\n",
      "-0.0012612505515416463\n",
      "-0.0012612548033396403\n",
      "-0.001261236834526062\n",
      "-0.0012610223134358724\n",
      "-0.0012606352885564168\n",
      "-0.001261126923561096\n",
      "-0.0012612087488174438\n",
      "-0.0012612027327219646\n",
      "-0.0012612300872802734\n",
      "-0.0012612597624460857\n",
      "-0.0012612420161565146\n",
      "-0.0012611250003178914\n",
      "-0.0012607310771942139\n",
      "-0.0012611062208811442\n",
      "-0.0012611478646596273\n",
      "-0.0012611738681793214\n",
      "-0.0012612463394800822\n",
      "-0.001261258856455485\n",
      "-0.001261245568593343\n",
      "-0.0012612429300944011\n",
      "-0.0012608940124511719\n",
      "-0.001260236692428589\n",
      "-0.0012609666347503663\n",
      "-0.001261172890663147\n",
      "-0.0012612473090489705\n",
      "-0.001261256504058838\n",
      "-0.0012612640301386516\n",
      "-0.0012612483819325765\n",
      "-0.0012612582127253214\n",
      "-0.0012612501780192057\n",
      "-0.0012612771034240724\n",
      "-0.0012612648089726766\n",
      "-0.001261215392748515\n",
      "-0.0012609100341796876\n",
      "-0.0012607671817143758\n",
      "-0.001261134910583496\n",
      "-0.001261203908920288\n",
      "-0.0012612422943115234\n",
      "-0.001261263863245646\n",
      "-0.0012612873474756876\n",
      "-0.0012612237930297852\n",
      "-0.0012610668420791626\n",
      "-0.0012607242822647094\n",
      "-0.0012611512263615925\n",
      "-0.0012612082004547118\n",
      "-0.001261219064394633\n",
      "-0.0012612413009007771\n",
      "-0.0012612819592158\n",
      "-0.001261265778541565\n",
      "-0.0012612579266230266\n",
      "-0.0012610921223958334\n",
      "-0.0012599446773529054\n",
      "-0.0012609074751536052\n",
      "-0.0012611197789510092\n",
      "-0.0012612379630406697\n",
      "-0.0012612628777821859\n",
      "-0.0012612922112147013\n",
      "-0.0012612716754277547\n",
      "-0.0012612568219502766\n",
      "-0.0012612574497858683\n",
      "-0.001261140775680542\n",
      "-0.0012612417856852213\n",
      "-0.0012612837235132853\n",
      "-0.0012612668673197428\n",
      "-0.0012612711747487387\n",
      "-0.001261154596010844\n",
      "-0.0012607714970906576\n",
      "-0.0012611602306365967\n",
      "-0.0012612012227376302\n",
      "-0.0012612302700678508\n",
      "-0.0012612238168716431\n",
      "-0.0012612738370895386\n",
      "-0.0012612778902053833\n",
      "-0.0012612862586975097\n",
      "-0.0012612675031026205\n",
      "-0.001260923926035563\n",
      "-0.0012597490549087524\n",
      "-0.001260753877957662\n",
      "-0.0012611506621042887\n",
      "-0.0012612340211868285\n",
      "-0.0012612648646036784\n",
      "-0.0012612751642862956\n",
      "-0.001261281156539917\n",
      "-0.0012612836678822835\n",
      "-0.0012612813949584962\n",
      "-0.0012612773180007936\n",
      "-0.001261269481976827\n",
      "-0.0012612799485524496\n",
      "-0.0012612846851348877\n",
      "-0.0012612174113591512\n",
      "-0.0012612331787745159\n",
      "-0.0012612590710322062\n",
      "-0.0012612205505371094\n",
      "-0.0012609171628952026\n",
      "-0.001260977554321289\n",
      "-0.0012611730893452961\n",
      "-0.0012612786769866944\n",
      "-0.001261210521062215\n",
      "-0.0012612728118896485\n",
      "-0.0012612844387690225\n",
      "-0.0012612783114115398\n",
      "-0.00126130424340566\n",
      "-0.0012612082004547118\n",
      "-0.0012596346696217855\n",
      "-0.0012605950435002646\n",
      "-0.0012610313653945923\n",
      "-0.0012612189610799155\n",
      "-0.0012612601280212403\n",
      "-0.0012612870772679646\n",
      "-0.0012612842241923013\n",
      "-0.0012612939834594726\n",
      "-0.0012612842877705891\n",
      "-0.0012612876812616984\n",
      "-0.0012612765073776245\n",
      "-0.0012612845420837402\n",
      "-0.0012612498919169109\n",
      "-0.0012612009207407634\n",
      "-0.0012612637599309286\n",
      "-0.0012612845897674561\n",
      "-0.00126129523118337\n",
      "-0.0012612544059753417\n",
      "-0.0012611009041468303\n",
      "-0.0012607525428136189\n",
      "-0.0012611966768900553\n",
      "-0.0012612391153971354\n",
      "-0.0012612419684727987\n",
      "-0.0012612720012664795\n",
      "-0.0012612892707188925\n",
      "-0.001261283016204834\n",
      "-0.0012612943172454834\n",
      "-0.0012610191106796264\n",
      "-0.0012602014223734538\n",
      "-0.0012609954913457234\n",
      "-0.0012611764748891195\n",
      "-0.0012612573464711508\n",
      "-0.0012613002300262452\n",
      "-0.0012612864176432292\n",
      "-0.001261312206586202\n",
      "-0.0012613047281901041\n",
      "-0.0012612964073816936\n",
      "-0.0012612971782684326\n",
      "-0.0012612892866134644\n",
      "-0.0012612485726674397\n",
      "-0.0012608140389124551\n",
      "-0.00126088019212087\n",
      "-0.0012611518144607543\n",
      "-0.0012612321376800537\n",
      "-0.001261292028427124\n",
      "-0.0012612884680430095\n",
      "-0.0012612955729166667\n",
      "-0.0012612404425938924\n",
      "-0.0012610470453898112\n",
      "-0.0012609878857930501\n",
      "-0.0012612714131673176\n",
      "-0.0012612204472223917\n",
      "-0.0012612009604771933\n",
      "-0.0012612359603246053\n",
      "-0.0012610722700754802\n",
      "-0.0012608089923858642\n",
      "-0.0012612149000167847\n",
      "-0.0012612346728642782\n",
      "-0.0012612337827682496\n",
      "-0.0012612539211908976\n",
      "-0.0012612951119740803\n",
      "-0.0012613061825434368\n",
      "-0.0012612926721572875\n",
      "-0.0012610231955846151\n",
      "-0.0012600234349568686\n",
      "-0.0012609269618988038\n",
      "-0.0012611570994059244\n",
      "-0.0012612685521443685\n",
      "-0.001261284875869751\n",
      "-0.0012612913290659586\n",
      "-0.0012612863381703694\n",
      "-0.0012612939993540445\n",
      "-0.0012612962007522582\n",
      "-0.001261310577392578\n",
      "-0.0012612907965977988\n",
      "-0.0012612934350967407\n",
      "-0.0012613085985183716\n",
      "-0.001261134696006775\n",
      "-0.0012605045874913533\n",
      "-0.001261104933420817\n",
      "-0.001261220105489095\n",
      "-0.001261259158452352\n",
      "-0.001261288102467855\n",
      "-0.0012612824122111002\n",
      "-0.0012612801631291708\n",
      "-0.0012612934271494548\n",
      "-0.0012613108158111572\n",
      "-0.0012612390279769898\n",
      "-0.0012603742043177286\n",
      "-0.0012608360608418782\n",
      "-0.0012611307144165039\n",
      "-0.0012612338701883951\n",
      "-0.0012612810055414837\n",
      "-0.0012612778584162395\n",
      "-0.0012613075097401938\n",
      "-0.00126129363377889\n",
      "-0.0012613176822662353\n",
      "-0.0012613049745559693\n",
      "-0.0012612936019897462\n",
      "-0.0012611925363540648\n",
      "-0.0012605053742726644\n",
      "-0.0012610619544982911\n",
      "-0.0012611964384714762\n",
      "-0.001261246116956075\n",
      "-0.0012612800280253093\n",
      "-0.0012613049189249674\n",
      "-0.001261310601234436\n",
      "-0.001261297845840454\n",
      "-0.0012613072633743286\n",
      "-0.0012612919092178344\n",
      "-0.0012610241333643596\n",
      "-0.0012600997845331828\n",
      "-0.0012609840949376423\n",
      "-0.001261163862546285\n",
      "-0.0012612597544987996\n",
      "-0.0012613030274709065\n",
      "-0.0012613053083419799\n",
      "-0.0012612895091374716\n",
      "-0.0012613194306691487\n",
      "-0.0012613187551498413\n",
      "-0.0012613072315851848\n",
      "-0.0012612923383712768\n",
      "-0.0012612931887308757\n",
      "-0.0012612502892812092\n",
      "-0.0012607303619384766\n",
      "-0.0012609317620595297\n",
      "-0.001261146887143453\n",
      "-0.001261224635442098\n",
      "-0.0012612957795461019\n",
      "-0.001261289930343628\n",
      "-0.0012612735827763875\n",
      "-0.0012612937053044636\n",
      "-0.0012612701813379922\n",
      "-0.0012610979795455932\n",
      "-0.0012606767098108926\n",
      "-0.0012611787716547648\n",
      "-0.0012612277746200562\n",
      "-0.0012612746953964233\n",
      "-0.001261291734377543\n",
      "-0.0012612890323003133\n",
      "-0.0012613102595011394\n",
      "-0.0012611918767293294\n",
      "-0.0012607764800389607\n",
      "-0.0012611133654912314\n",
      "-0.001261188824971517\n",
      "-0.001261255407333374\n",
      "-0.0012612874587376913\n",
      "-0.001261306921641032\n",
      "-0.0012612836281458537\n",
      "-0.001261309544245402\n",
      "-0.001261140775680542\n",
      "-0.0012602448066075644\n",
      "-0.0012610181252161662\n",
      "-0.0012612033923467\n",
      "-0.0012612660725911458\n",
      "-0.0012612876892089844\n",
      "-0.0012613088607788085\n",
      "-0.0012613112370173137\n",
      "-0.0012613070964813233\n",
      "-0.0012613170782725016\n",
      "-0.0012612974246342978\n",
      "-0.0012611634333928427\n",
      "-0.0012610983530680338\n",
      "-0.00126078675587972\n",
      "-0.0012612258911132812\n",
      "-0.0012612353801727295\n",
      "-0.0012612585544586183\n",
      "-0.0012612788597742718\n",
      "-0.0012613097667694092\n",
      "-0.0012612845500310263\n",
      "-0.0012612536907196045\n",
      "-0.0012607269207636515\n",
      "-0.0012608058214187623\n",
      "-0.0012611128966013591\n",
      "-0.0012612457434336343\n",
      "-0.0012612795193990072\n",
      "-0.0012613061904907226\n",
      "-0.001261296033859253\n",
      "-0.001261315123240153\n",
      "-0.0012613049666086833\n",
      "-0.001261256202061971\n",
      "-0.0012610207955042522\n",
      "-0.001260685642560323\n",
      "-0.001261139678955078\n",
      "-0.001261234990755717\n",
      "-0.001261286155382792\n",
      "-0.0012612888892491658\n",
      "-0.0012612949927647908\n",
      "-0.0012612617333730063\n",
      "-0.0012612658818562826\n",
      "-0.0012611179987589518\n",
      "-0.0012608616352081299\n",
      "-0.0012612433751424154\n",
      "-0.001261206881205241\n",
      "-0.0012613155762354533\n",
      "-0.0012612513542175293\n",
      "-0.0012610725879669189\n",
      "-0.0012610313415527344\n",
      "-0.0012612770080566405\n",
      "-0.0012611806392669677\n",
      "-0.00126123472849528\n",
      "-0.0012612587531407673\n",
      "-0.001261049214998881\n",
      "-0.001260746971766154\n",
      "-0.001261193577448527\n",
      "-0.0012612579266230266\n",
      "-0.0012612594525019329\n",
      "-0.0012612969716389974\n",
      "-0.0012613173166910808\n",
      "-0.0012612961848576863\n",
      "-0.0012611980199813842\n",
      "-0.0012606989224751791\n",
      "-0.0012610321124394736\n",
      "-0.001261177388827006\n",
      "-0.0012612486441930136\n",
      "-0.0012613024552663168\n",
      "-0.0012613094727198283\n",
      "-0.0012612988392512003\n",
      "-0.0012612933715184529\n",
      "-0.0012612525145212809\n",
      "-0.0012609758297602335\n",
      "-0.0012607122898101806\n",
      "-0.001261153523127238\n",
      "-0.001261239473025004\n",
      "-0.0012612931569417317\n",
      "-0.0012613051335016885\n",
      "-0.0012613001585006715\n",
      "-0.0012613049745559693\n",
      "-0.0012612571080525717\n",
      "-0.0012610086520512898\n",
      "-0.0012608530044555665\n",
      "-0.0012612088521321615\n",
      "-0.001261258053779602\n",
      "-0.001261242945988973\n",
      "-0.0012613049904505412\n",
      "-0.0012613117218017579\n",
      "-0.001261288809776306\n",
      "-0.001261160628000895\n",
      "-0.001260427991549174\n",
      "-0.0012610633293787638\n",
      "-0.0012612023750940959\n",
      "-0.001261274472872416\n",
      "-0.0012612964073816936\n",
      "-0.00126130424340566\n",
      "-0.0012613094727198283\n",
      "-0.00126130797068278\n",
      "-0.0012613150199254355\n",
      "-0.0012612746715545654\n",
      "-0.0012611118872960408\n",
      "-0.0012604259411493938\n",
      "-0.0012609208742777507\n",
      "-0.00126116357644399\n",
      "-0.001261269172032674\n",
      "-0.0012613003412882487\n",
      "-0.0012613135655721028\n",
      "-0.0012613134860992431\n",
      "-0.0012613139629364014\n",
      "-0.0012613242626190186\n",
      "-0.0012612972815831502\n",
      "-0.0012613007307052612\n",
      "-0.0012612399260203044\n",
      "-0.0012603587071100871\n",
      "-0.0012609107573827109\n",
      "-0.0012611413876215617\n",
      "-0.0012612532059351604\n",
      "-0.0012613205273946126\n",
      "-0.0012613096555074057\n",
      "-0.0012613134463628133\n",
      "-0.0012613147735595703\n",
      "-0.0012613167683283488\n",
      "-0.0012613056262334187\n",
      "-0.0012613067388534546\n",
      "-0.0012612306435902914\n",
      "-0.0012605105400085449\n",
      "-0.001260953195889791\n",
      "-0.0012611510594685872\n",
      "-0.0012612532695134482\n",
      "-0.0012612834850947061\n",
      "-0.001261304235458374\n",
      "-0.001261326543490092\n",
      "-0.001261307724316915\n",
      "-0.0012613239765167236\n",
      "-0.0012612788359324138\n",
      "-0.0012610958814620972\n",
      "-0.001260692803064982\n",
      "-0.0012611295859018962\n",
      "-0.0012612438360850016\n",
      "-0.0012612519184748332\n",
      "-0.00126129359404246\n",
      "-0.001261311904589335\n",
      "-0.0012612848043441773\n",
      "-0.001261254628499349\n",
      "-0.001261121988296509\n",
      "-0.001260902460416158\n",
      "-0.001261176331837972\n",
      "-0.0012612382332483926\n",
      "-0.0012612906297047933\n",
      "-0.0012612807432810466\n",
      "-0.0012611976464589436\n",
      "-0.0012610270420710247\n",
      "-0.0012611961603164674\n",
      "-0.0012612819592158\n",
      "-0.001261280878384908\n",
      "-0.0012613015810648601\n",
      "-0.0012612141132354736\n",
      "-0.001260486658414205\n",
      "-0.001260889188448588\n",
      "-0.00126113068262736\n",
      "-0.0012612504482269287\n",
      "-0.0012613002061843872\n",
      "-0.001261303146680196\n",
      "-0.0012613098541895548\n",
      "-0.0012613163153330485\n",
      "-0.0012613251447677611\n",
      "-0.001261298934618632\n",
      "-0.0012613144636154175\n",
      "-0.0012610298951466878\n",
      "-0.001260343352953593\n",
      "-0.0012610681136449179\n",
      "-0.0012612035274505615\n",
      "-0.001261265468597412\n",
      "-0.0012613027254740397\n",
      "-0.0012613142331441244\n",
      "-0.0012613121906916301\n",
      "-0.001261328069368998\n",
      "-0.0012613021691640217\n",
      "-0.0012613130966822306\n",
      "-0.0012613076607386271\n",
      "-0.0012610968510309854\n",
      "-0.0012603516896565756\n",
      "-0.001261023211479187\n",
      "-0.0012611856937408447\n",
      "-0.0012612879037857056\n",
      "-0.0012613041877746582\n",
      "-0.0012613017002741495\n",
      "-0.001261320439974467\n",
      "-0.0012613175789515177\n",
      "-0.0012613072554270427\n",
      "-0.0012611688693364462\n",
      "-0.0012612839778264363\n",
      "-0.0012613194227218628\n",
      "-0.0012613155364990234\n",
      "-0.0012610472361246744\n",
      "-0.0012598597208658853\n",
      "-0.0012608057737350463\n",
      "-0.0012611905813217164\n",
      "-0.001261256750424703\n",
      "-0.001261305562655131\n",
      "-0.0012613140106201171\n",
      "-0.0012613384405771892\n",
      "-0.0012613116900126139\n",
      "-0.0012613167842229207\n",
      "-0.0012613202333450317\n",
      "-0.0012613149642944335\n",
      "-0.0012613125085830688\n",
      "-0.0012611541350682576\n",
      "-0.001261224643389384\n",
      "-0.001261299220720927\n",
      "-0.0012613141695658366\n",
      "-0.0012613038937250773\n",
      "-0.0012611579100290935\n",
      "-0.0012605497121810914\n",
      "-0.0012611344973246256\n",
      "-0.0012612157265345255\n",
      "-0.0012612728595733642\n",
      "-0.0012612915436426798\n",
      "-0.001261322577794393\n",
      "-0.0012613209247589112\n",
      "-0.0012613274892171223\n",
      "-0.001261281704902649\n",
      "-0.0012609256744384766\n",
      "-0.0012607424815495808\n",
      "-0.00126114292939504\n",
      "-0.0012612545172373455\n",
      "-0.001261309274037679\n",
      "-0.0012612934112548827\n",
      "-0.0012613027890523276\n",
      "-0.0012613187948862712\n",
      "-0.0012613141854604085\n",
      "-0.001261324707667033\n",
      "-0.0012612472852071125\n",
      "-0.0012601177295049032\n",
      "-0.0012607689539591471\n",
      "-0.0012611530065536499\n",
      "-0.0012612577676773071\n",
      "-0.0012612850745519003\n",
      "-0.0012613203207651775\n",
      "-0.0012613236824671427\n",
      "-0.001261324111620585\n",
      "-0.0012613128980000813\n",
      "-0.0012613089640935262\n",
      "-0.0012613080104192098\n",
      "-0.0012613308668136597\n",
      "-0.0012613195260365805\n",
      "-0.0012613070646921793\n",
      "-0.0012609697103500366\n",
      "-0.0012604618390401204\n",
      "-0.0012610523223876952\n",
      "-0.0012612228949864706\n",
      "-0.0012612744092941285\n",
      "-0.0012613001982371011\n",
      "-0.0012613124450047812\n",
      "-0.0012613075892130535\n",
      "-0.0012613363822301228\n",
      "-0.0012613117138544718\n",
      "-0.0012612648010253906\n",
      "-0.001260601003964742\n",
      "-0.0012609325965245565\n",
      "-0.001261161494255066\n",
      "-0.001261251425743103\n",
      "-0.0012612895568211873\n",
      "-0.0012613061825434368\n",
      "-0.0012613240798314412\n",
      "-0.001261317245165507\n",
      "-0.0012613198518753052\n",
      "-0.0012613004207611084\n",
      "-0.001261191479365031\n",
      "-0.0012605953613917032\n",
      "-0.0012611047108968098\n",
      "-0.001261194880803426\n",
      "-0.0012612604061762493\n",
      "-0.001261292282740275\n",
      "-0.0012613072395324706\n",
      "-0.0012613036870956422\n",
      "-0.0012613224267959595\n",
      "-0.0012613234202067058\n",
      "-0.001261303742726644\n",
      "-0.0012606701850891113\n",
      "-0.0012604848543802897\n",
      "-0.0012610361496607464\n",
      "-0.0012612399657567343\n",
      "-0.0012613130966822306\n",
      "-0.0012613105456034343\n",
      "-0.0012613235553105672\n",
      "-0.0012613214333852133\n",
      "-0.0012613193909327188\n",
      "-0.0012613189935684205\n",
      "-0.00126131591796875\n",
      "-0.0012613245407740274\n",
      "-0.0012613103389739991\n",
      "-0.0012610633293787638\n",
      "-0.001260424820582072\n",
      "-0.0012610718329747518\n",
      "-0.0012612241347630818\n",
      "-0.001261289389928182\n",
      "-0.0012612976948420207\n",
      "-0.001261313756306966\n",
      "-0.001261328689257304\n",
      "-0.001261312429110209\n",
      "-0.001261312182744344\n",
      "-0.001261266859372457\n",
      "-0.0012607808192571005\n",
      "-0.0012608747641245524\n",
      "-0.0012611323833465577\n",
      "-0.001261227798461914\n",
      "-0.001261294968922933\n",
      "-0.0012613274812698365\n",
      "-0.0012613178571065267\n",
      "-0.0012613232453664144\n",
      "-0.001261306095123291\n",
      "-0.0012613123337427774\n",
      "-0.0012613004366556803\n",
      "-0.001260629399617513\n",
      "-0.0012603314399719239\n",
      "-0.0012609828233718873\n",
      "-0.0012612381219863891\n",
      "-0.0012612847407658895\n",
      "-0.0012613166729609172\n",
      "-0.0012613147815068563\n",
      "-0.0012613118569056194\n",
      "-0.0012613174041112263\n",
      "-0.0012613186836242676\n",
      "-0.0012613135655721028\n",
      "-0.0012613282124201456\n",
      "-0.0012613271633783976\n",
      "-0.0012613024075826009\n",
      "-0.0012611268043518067\n",
      "-0.001260945963859558\n",
      "-0.0012605343023935954\n",
      "-0.001261108946800232\n",
      "-0.0012612199465433756\n",
      "-0.0012612834294637044\n",
      "-0.0012613105217615763\n",
      "-0.001261314829190572\n",
      "-0.0012613065083821615\n",
      "-0.0012613277037938435\n",
      "-0.0012613141536712647\n",
      "-0.00126132173538208\n",
      "-0.0012613072951634726\n",
      "-0.001261054507891337\n",
      "-0.0012603455384572347\n",
      "-0.0012610232512156169\n",
      "-0.0012611921946207682\n",
      "-0.001261274290084839\n",
      "-0.001261308757464091\n",
      "-0.0012613217989603678\n",
      "-0.0012613126595815024\n",
      "-0.0012613219261169433\n",
      "-0.0012613216559092203\n",
      "-0.0012611476500829061\n",
      "-0.0012611111958821615\n",
      "-0.001261187481880188\n",
      "-0.0012612242698669433\n",
      "-0.0012612168312072754\n",
      "-0.001261054555575053\n",
      "-0.0012611299912134806\n",
      "-0.0012612915198008218\n",
      "-0.0012612413962682088\n",
      "-0.0012613034884134929\n",
      "-0.001261314622561137\n",
      "-0.0012613269488016764\n",
      "-0.0012612991412480673\n",
      "-0.0012605851491292318\n",
      "-0.0012597938219706217\n",
      "-0.0012609767119089762\n",
      "-0.001261159054438273\n",
      "-0.0012612927675247191\n",
      "-0.0012613027175267537\n",
      "-0.0012613019863764445\n",
      "-0.0012613126913706462\n",
      "-0.0012613189776738486\n",
      "-0.0012613102038701374\n",
      "-0.0012613120714823406\n",
      "-0.001261311944325765\n",
      "-0.0012613227208455404\n",
      "-0.0012613189776738486\n",
      "-0.001261307470003764\n",
      "-0.0012612914562225342\n",
      "-0.0012611386934916178\n",
      "-0.0012611879587173462\n",
      "-0.0012612753629684449\n",
      "-0.0012613298018773398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012613094886144002\n",
      "-0.001261136031150818\n",
      "-0.001260197377204895\n",
      "-0.001261032287279765\n",
      "-0.0012612074851989746\n",
      "-0.0012612823804219564\n",
      "-0.001261323595046997\n",
      "-0.001261323062578837\n",
      "-0.0012613223393758137\n",
      "-0.001261320988337199\n",
      "-0.0012613139073053997\n",
      "-0.001261317459742228\n",
      "-0.0012613213936487834\n",
      "-0.001261318604151408\n",
      "-0.0012611668904622395\n",
      "-0.0012603310664494832\n",
      "-0.0012610600471496581\n",
      "-0.0012612011671066284\n",
      "-0.0012612761815388998\n",
      "-0.001261309774716695\n",
      "-0.0012613171497980753\n",
      "-0.0012613224903742473\n",
      "-0.0012613208293914796\n",
      "-0.0012613160928090414\n",
      "-0.0012613259792327882\n",
      "-0.0012613160451253255\n",
      "-0.001261288046836853\n",
      "-0.0012606056769688923\n",
      "-0.0012605525175730388\n",
      "-0.0012609931071599324\n",
      "-0.0012612492481867473\n",
      "-0.0012612878640492757\n",
      "-0.0012613258679707844\n",
      "-0.0012613147894541422\n",
      "-0.0012613333145777385\n",
      "-0.0012613056818644205\n",
      "-0.0012613064527511597\n",
      "-0.0012612211624781291\n",
      "-0.0012611501534779866\n",
      "-0.0012612662076950073\n",
      "-0.0012613113641738892\n",
      "-0.0012612822612126668\n",
      "-0.0012610477606455484\n",
      "-0.001260719648996989\n",
      "-0.0012611631155014038\n",
      "-0.0012612680435180664\n",
      "-0.0012612969636917114\n",
      "-0.0012612933476765951\n",
      "-0.0012613109827041627\n",
      "-0.0012613287369410196\n",
      "-0.001261315703392029\n",
      "-0.0012613230069478353\n",
      "-0.0012613065083821615\n",
      "-0.0012605586846669516\n",
      "-0.0012601587851842244\n",
      "-0.0012609846353530884\n",
      "-0.0012612524112065634\n",
      "-0.0012612888018290202\n",
      "-0.0012613135178883871\n",
      "-0.001261314304669698\n",
      "-0.001261326837539673\n",
      "-0.0012613242308298746\n",
      "-0.0012613279660542806\n",
      "-0.0012613240242004395\n",
      "-0.0012613124450047812\n",
      "-0.0012613311131795246\n",
      "-0.0012613215684890748\n",
      "-0.0012613146384557088\n",
      "-0.0012612814823786418\n",
      "-0.001260862668355306\n",
      "-0.0012606427987416585\n",
      "-0.0012611396710077922\n",
      "-0.001261248477300008\n",
      "-0.0012612990776697795\n",
      "-0.001261303424835205\n",
      "-0.0012613008260726928\n",
      "-0.0012613205194473268\n",
      "-0.0012613165616989135\n",
      "-0.0012613035758336384\n",
      "-0.0012612516244252523\n",
      "-0.0012605331818262735\n",
      "-0.0012608976602554321\n",
      "-0.0012611579577128092\n",
      "-0.0012612775325775147\n",
      "-0.0012612826426823935\n",
      "-0.0012613208373387654\n",
      "-0.0012613107919692992\n",
      "-0.001261318031946818\n",
      "-0.0012613172690073649\n",
      "-0.0012613110383351645\n",
      "-0.0012613185405731201\n",
      "-0.001261187473932902\n",
      "-0.0012602380037307738\n",
      "-0.001260925817489624\n",
      "-0.0012611451069513956\n",
      "-0.001261270546913147\n",
      "-0.0012613039414087932\n",
      "-0.0012613181193669638\n",
      "-0.0012613094091415405\n",
      "-0.0012613242149353027\n",
      "-0.001261328371365865\n",
      "-0.0012613155364990234\n",
      "-0.0012613207181294758\n",
      "-0.0012613031148910523\n",
      "-0.0012608503977457682\n",
      "-0.00126042320728302\n",
      "-0.0012610204855600994\n",
      "-0.0012612346967061361\n",
      "-0.0012613086064656575\n",
      "-0.0012613205194473268\n",
      "-0.0012613160371780395\n",
      "-0.0012613116184870402\n",
      "-0.0012613101959228516\n",
      "-0.00126132017771403\n",
      "-0.0012613171895345051\n",
      "-0.0012613217274347942\n",
      "-0.0012613236427307128\n",
      "-0.001261298664410909\n",
      "-0.0012607014258702596\n",
      "-0.0012606825908025106\n",
      "-0.0012611010471979777\n",
      "-0.001261241348584493\n",
      "-0.0012612961689631144\n",
      "-0.0012613274017969768\n",
      "-0.0012613258520762125\n",
      "-0.0012613293806711832\n",
      "-0.0012613181749979655\n",
      "-0.0012613286972045898\n",
      "-0.0012613228480021158\n",
      "-0.0012613163630167644\n",
      "-0.0012610193729400635\n",
      "-0.0012604684114456176\n",
      "-0.0012610794067382813\n",
      "-0.0012611337502797444\n",
      "-0.00126118799050649\n",
      "-0.001261295715967814\n",
      "-0.0012613170544306437\n",
      "-0.0012613102197647095\n",
      "-0.0012613136768341064\n",
      "-0.001261311411857605\n",
      "-0.0012613179683685302\n",
      "-0.001261313048998515\n",
      "-0.0012612993637720743\n",
      "-0.0012609915018081666\n",
      "-0.0012603865067164104\n",
      "-0.0012610537846883137\n",
      "-0.0012612284898757934\n",
      "-0.0012612974882125854\n",
      "-0.0012613224585851033\n",
      "-0.001261319367090861\n",
      "-0.0012613078673680623\n",
      "-0.0012613266309102376\n",
      "-0.0012613102436065673\n",
      "-0.001261316204071045\n",
      "-0.001261310601234436\n",
      "-0.001261169187227885\n",
      "-0.001260427729288737\n",
      "-0.0012610815684000651\n",
      "-0.0012612101554870605\n",
      "-0.0012612780253092447\n",
      "-0.001261294412612915\n",
      "-0.0012612100839614869\n",
      "-0.0012612967491149902\n",
      "-0.0012613163471221923\n",
      "-0.0012613286336263022\n",
      "-0.0012613220453262328\n",
      "-0.0012613063017527263\n",
      "-0.0012611374934514365\n",
      "-0.0012604192972183227\n",
      "-0.0012610913515090942\n",
      "-0.0012612274090449014\n",
      "-0.0012612846771876017\n",
      "-0.001261299443244934\n",
      "-0.0012613302310307821\n",
      "-0.0012613165299097697\n",
      "-0.0012613234678904215\n",
      "-0.0012613152503967286\n",
      "-0.0012611974080403647\n",
      "-0.0012612650235493978\n",
      "-0.0012610107342402141\n",
      "-0.0012605871041615805\n",
      "-0.0012611184040705362\n",
      "-0.0012612312237421672\n",
      "-0.001261295485496521\n",
      "-0.0012613060633341472\n",
      "-0.0012613167444864909\n",
      "-0.001261322577794393\n",
      "-0.001261322291692098\n",
      "-0.001261322061220805\n",
      "-0.0012613002061843872\n",
      "-0.0012610427141189575\n",
      "-0.0012606189489364624\n",
      "-0.001260958774884542\n",
      "-0.0012611772378285726\n",
      "-0.0012612614472707112\n",
      "-0.0012613041798273722\n",
      "-0.001261314058303833\n",
      "-0.001261325240135193\n",
      "-0.0012613171895345051\n",
      "-0.0012613274653752644\n",
      "-0.0012613128105799357\n",
      "-0.0012612090349197389\n",
      "-0.0012604731718699137\n",
      "-0.001261086654663086\n",
      "-0.001261209718386332\n",
      "-0.001261290733019511\n",
      "-0.0012612958908081054\n",
      "-0.0012613186597824097\n",
      "-0.0012613093455632527\n",
      "-0.0012613245169321697\n",
      "-0.0012613213300704956\n",
      "-0.001261321512858073\n",
      "-0.0012611127217610678\n",
      "-0.001260686190923055\n",
      "-0.0012609644810358683\n",
      "-0.0012611718654632568\n",
      "-0.0012612638394037883\n",
      "-0.0012612866004308066\n",
      "-0.00126131382783254\n",
      "-0.0012613140662511189\n",
      "-0.001261313549677531\n",
      "-0.0012613267103830973\n",
      "-0.001261325510342916\n",
      "-0.0012612712065378825\n",
      "-0.0012603759209314982\n",
      "-0.0012608282883961996\n",
      "-0.0012611480077107746\n",
      "-0.001261266311009725\n",
      "-0.001261307430267334\n",
      "-0.0012613080104192098\n",
      "-0.0012613208373387654\n",
      "-0.0012613194624582927\n",
      "-0.0012613124926884969\n",
      "-0.0012613146861394247\n",
      "-0.0012612353642781576\n",
      "-0.0012611596743265787\n",
      "-0.0012611745198567708\n",
      "-0.0012611419757207235\n",
      "-0.0012612006425857544\n",
      "-0.0012613009373346965\n",
      "-0.0012613140662511189\n",
      "-0.001261311904589335\n",
      "-0.001260773229598999\n",
      "-0.001260677194595337\n",
      "-0.0012611032644907632\n",
      "-0.0012612410306930542\n",
      "-0.0012613051176071166\n",
      "-0.0012613186120986938\n",
      "-0.0012613208691279094\n",
      "-0.0012613118648529052\n",
      "-0.001261325494448344\n",
      "-0.0012613194863001507\n",
      "-0.0012612277030944823\n",
      "-0.0012610463380813598\n",
      "-0.0012608951250712077\n",
      "-0.0012612476189931233\n",
      "-0.0012612418333689372\n",
      "-0.0012612995942433674\n",
      "-0.0012612658818562826\n",
      "-0.0012612840652465821\n",
      "-0.0012612091064453125\n",
      "-0.001260865553220113\n",
      "-0.0012611574093500772\n",
      "-0.0012612202088038127\n",
      "-0.001261282245318095\n",
      "-0.0012612980763117472\n",
      "-0.001261303440729777\n",
      "-0.0012613125483194986\n",
      "-0.0012613180001576742\n",
      "-0.0012613105297088624\n",
      "-0.001260691181818644\n",
      "-0.001260084311167399\n",
      "-0.0012609798431396483\n",
      "-0.001261217490832011\n",
      "-0.00126129633585612\n",
      "-0.001261302145322164\n",
      "-0.0012613286097844442\n",
      "-0.0012613288720448811\n",
      "-0.0012613231579462687\n",
      "-0.0012613124450047812\n",
      "-0.0012613244930903117\n",
      "-0.001261308471361796\n",
      "-0.0012613096555074057\n",
      "-0.0012613204717636108\n",
      "-0.0012613158384958904\n",
      "-0.001261042332649231\n",
      "-0.0012603919744491577\n",
      "-0.001261084508895874\n",
      "-0.0012612234592437744\n",
      "-0.0012612928787867229\n",
      "-0.0012613136450449626\n",
      "-0.001261314098040263\n",
      "-0.001261316434542338\n",
      "-0.0012613199472427368\n",
      "-0.0012613196929295858\n",
      "-0.0012613241036732992\n",
      "-0.0012613213459650675\n",
      "-0.001261264983812968\n",
      "-0.0012603724479675292\n",
      "-0.0012608068227767943\n",
      "-0.0012611430724461872\n",
      "-0.0012612595081329346\n",
      "-0.001261315647761027\n",
      "-0.0012613185087839763\n",
      "-0.0012613229751586915\n",
      "-0.0012613259156545003\n",
      "-0.0012613233963648478\n",
      "-0.0012613375981648763\n",
      "-0.0012613288084665935\n",
      "-0.0012613158941268921\n",
      "-0.001261292028427124\n",
      "-0.001260880390803019\n",
      "-0.0012604475895563762\n",
      "-0.0012610855738321941\n",
      "-0.0012612175941467286\n",
      "-0.001261280099550883\n",
      "-0.001261308757464091\n",
      "-0.0012613307237625121\n",
      "-0.0012613263289133708\n",
      "-0.0012613250732421875\n",
      "-0.0012613189697265625\n",
      "-0.0012613250732421875\n",
      "-0.0012613193194071452\n",
      "-0.0012609809637069702\n",
      "-0.0012604694684346517\n",
      "-0.0012610615650812784\n",
      "-0.001261223578453064\n",
      "-0.001261294643084208\n",
      "-0.001261307175954183\n",
      "-0.0012613288323084513\n",
      "-0.0012613277355829876\n",
      "-0.001261326018969218\n",
      "-0.0012613250811894735\n",
      "-0.0012613206386566161\n",
      "-0.001261261264483134\n",
      "-0.0012610183795293173\n",
      "-0.0012607313632965087\n",
      "-0.0012611984650293985\n",
      "-0.0012612552404403688\n",
      "-0.001261267606417338\n",
      "-0.0012613043944040934\n",
      "-0.00126131960550944\n",
      "-0.0012613177061080933\n",
      "-0.0012612633069356283\n",
      "-0.0012609492301940918\n",
      "-0.00126101819674174\n",
      "-0.0012612201134363811\n",
      "-0.0012612893978754679\n",
      "-0.0012612892866134644\n",
      "-0.0012613170703252156\n",
      "-0.0012613144874572754\n",
      "-0.0012613166968027752\n",
      "-0.00126113387743632\n",
      "-0.0012603108485539755\n",
      "-0.0012610334157943726\n",
      "-0.0012612132708231607\n",
      "-0.0012612720648447673\n",
      "-0.00126131858030955\n",
      "-0.0012613138437271118\n",
      "-0.0012613248507181804\n",
      "-0.0012613163550694783\n",
      "-0.0012613179683685302\n",
      "-0.0012613264799118042\n",
      "-0.0012612237771352133\n",
      "-0.0012611975034077963\n",
      "-0.0012609471321105958\n",
      "-0.0012607680082321166\n",
      "-0.0012611506223678588\n",
      "-0.0012612475395202636\n",
      "-0.0012613101800282797\n",
      "-0.001261327846844991\n",
      "-0.0012613024950027466\n",
      "-0.0012613230228424072\n",
      "-0.0012613245328267416\n",
      "-0.0012613139788309733\n",
      "-0.0012613035202026367\n",
      "-0.00126113760471344\n",
      "-0.001260076403617859\n",
      "-0.001260976012547811\n",
      "-0.0012611772060394286\n",
      "-0.0012612787326176961\n",
      "-0.0012613166968027752\n",
      "-0.0012613120396931965\n",
      "-0.0012613256057103475\n",
      "-0.0012613325595855712\n",
      "-0.0012613256533940632\n",
      "-0.0012612737735112507\n",
      "-0.0012611587047576904\n",
      "-0.0012612820307413737\n",
      "-0.001261318604151408\n",
      "-0.0012613292296727498\n",
      "-0.001261278780301412\n",
      "-0.0012606328010559083\n",
      "-0.0012608513991038005\n",
      "-0.0012611617644627889\n",
      "-0.0012612725814183552\n",
      "-0.0012613047202428183\n",
      "-0.0012613095124562582\n",
      "-0.0012613033215204874\n",
      "-0.001261319088935852\n",
      "-0.0012613109747568766\n",
      "-0.0012613333622614542\n",
      "-0.001261318842569987\n",
      "-0.0012612606287002563\n",
      "-0.0012609840393066406\n",
      "-0.0012604994455973307\n",
      "-0.0012610611120859783\n",
      "-0.001261198353767395\n",
      "-0.001261281927426656\n",
      "-0.0012613043705622356\n",
      "-0.0012613222519556682\n",
      "-0.0012613161245981852\n",
      "-0.0012613169193267822\n",
      "-0.0012613189140955607\n",
      "-0.001261307152112325\n",
      "-0.0012611098766326903\n",
      "-0.0012606607834498088\n",
      "-0.0012611566861470541\n",
      "-0.001261271071434021\n",
      "-0.0012613053321838378\n",
      "-0.001261301565170288\n",
      "-0.0012613139231999716\n",
      "-0.00126131960550944\n",
      "-0.0012613224426905314\n",
      "-0.0012613327026367188\n",
      "-0.0012612992604573569\n",
      "-0.0012610427141189575\n",
      "-0.0012600605885187785\n",
      "-0.0012609843015670777\n",
      "-0.0012611672639846801\n",
      "-0.0012613068024317424\n",
      "-0.0012613121191660563\n",
      "-0.0012613253434499104\n",
      "-0.0012613197962443035\n",
      "-0.0012613301992416381\n",
      "-0.001261306643486023\n",
      "-0.0012613288561503092\n",
      "-0.0012612683534622192\n",
      "-0.0012611140012741089\n",
      "-0.0012612680673599243\n",
      "-0.0012612945079803466\n",
      "-0.0012612770477930704\n",
      "-0.001260984245936076\n",
      "-0.0012608930508295694\n",
      "-0.0012611815929412843\n",
      "-0.0012612704277038575\n",
      "-0.0012613088369369508\n",
      "-0.001261298402150472\n",
      "-0.0012613055070241292\n",
      "-0.0012612963438034057\n",
      "-0.001261281696955363\n",
      "-0.0012611589113871256\n",
      "-0.0012607879320780436\n",
      "-0.0012612138907114666\n",
      "-0.0012612480322519938\n",
      "-0.001261250670750936\n",
      "-0.0012613199869791667\n",
      "-0.0012613014936447143\n",
      "-0.0012612818876902262\n",
      "-0.001261251926422119\n",
      "-0.0012613118886947632\n",
      "-0.0012612187623977662\n",
      "-0.0012598524490992228\n",
      "-0.001260927693049113\n",
      "-0.0012611213127772013\n",
      "-0.001261266811688741\n",
      "-0.0012613012313842775\n",
      "-0.0012613173961639405\n",
      "-0.0012613141775131226\n",
      "-0.0012613073348999024\n",
      "-0.0012611392339070637\n",
      "-0.0012612693786621095\n",
      "-0.0012613120317459107\n",
      "-0.0012613256295522055\n",
      "-0.0012613164663314818\n",
      "-0.0012613170385360718\n",
      "-0.0012613181511561076\n",
      "-0.0012613173564275106\n",
      "-0.001261317777633667\n",
      "-0.0012611519575119019\n",
      "-0.001260430653889974\n",
      "-0.0012611087878545125\n",
      "-0.00126122518380483\n",
      "-0.0012612821420033772\n",
      "-0.001261326797803243\n",
      "-0.0012613123099009195\n",
      "-0.001261315671602885\n",
      "-0.0012613241275151572\n",
      "-0.0012613259553909302\n",
      "-0.0012612037261327108\n",
      "-0.0012612312078475953\n",
      "-0.0012610285679499308\n",
      "-0.0012607488791147867\n",
      "-0.0012611691236495972\n",
      "-0.0012612513224283853\n",
      "-0.001261288324991862\n",
      "-0.0012613046725591024\n",
      "-0.001261306651433309\n",
      "-0.00126131911277771\n",
      "-0.0012613114992777506\n",
      "-0.001261316974957784\n",
      "-0.0012612508694330852\n",
      "-0.001261207906405131\n",
      "-0.0012598519881566366\n",
      "-0.0012608952522277833\n",
      "-0.001261107858022054\n",
      "-0.001261271603902181\n",
      "-0.0012612900336583456\n",
      "-0.0012613253275553385\n",
      "-0.001261324111620585\n",
      "-0.001261312953631083\n",
      "-0.0012613263765970865\n",
      "-0.0012613178809483846\n",
      "-0.0012613146702448526\n",
      "-0.0012611078262329102\n",
      "-0.0012612515528996786\n",
      "-0.001261292854944865\n",
      "-0.0012613166411717732\n",
      "-0.0012613192319869996\n",
      "-0.0012613307555516561\n",
      "-0.0012613167842229207\n",
      "-0.0012609967867533366\n",
      "-0.0012604114055633545\n",
      "-0.0012610390583674112\n",
      "-0.0012612247625986735\n",
      "-0.001261297067006429\n",
      "-0.001261322538057963\n",
      "-0.0012613137245178223\n",
      "-0.0012613296826680502\n",
      "-0.0012613232135772704\n",
      "-0.001261322585741679\n",
      "-0.0012613245884577433\n",
      "-0.0012613227446873982\n",
      "-0.001261323340733846\n",
      "-0.0012612099011739095\n",
      "-0.001260173535346985\n",
      "-0.001260967493057251\n",
      "-0.0012611905256907146\n",
      "-0.001261276650428772\n",
      "-0.0012613053798675538\n",
      "-0.0012613141695658366\n",
      "-0.0012613243103027343\n",
      "-0.0012613311211268107\n",
      "-0.0012613202571868896\n",
      "-0.001261332138379415\n",
      "-0.0012613158782323202\n",
      "-0.0012613057613372802\n",
      "-0.0012611214955647787\n",
      "-0.0012605546871821086\n",
      "-0.001261124086380005\n",
      "-0.0012612323681513468\n",
      "-0.0012612752596537272\n",
      "-0.001261311928431193\n",
      "-0.0012613198677698771\n",
      "-0.001261326535542806\n",
      "-0.001261290939648946\n",
      "-0.0012611109813054403\n",
      "-0.001261258625984192\n",
      "-0.0012612522125244141\n",
      "-0.001260964258511861\n",
      "-0.0012608616511027018\n",
      "-0.0012611580530802408\n",
      "-0.001261254088083903\n",
      "-0.0012612995862960816\n",
      "-0.0012613197485605875\n",
      "-0.0012613211552302043\n",
      "-0.0012613274017969768\n",
      "-0.0012613187074661254\n",
      "-0.0012613179922103882\n",
      "-0.001261246124903361\n",
      "-0.0012602990945180257\n",
      "-0.0012608663320541381\n",
      "-0.0012611821015675862\n",
      "-0.0012612806638081868\n",
      "-0.0012612990379333496\n",
      "-0.001261320416132609\n",
      "-0.001261315933863322\n",
      "-0.0012612592697143555\n",
      "-0.0012612043460210165\n",
      "-0.0012612848043441773\n",
      "-0.001261318850517273\n",
      "-0.0012613273779551188\n",
      "-0.0012613130331039428\n",
      "-0.0012613214174906414\n",
      "-0.0012612310806910196\n",
      "-0.001260208519299825\n",
      "-0.0012609622160593668\n",
      "-0.0012611934820810954\n",
      "-0.00126127184232076\n",
      "-0.001261309774716695\n",
      "-0.0012613139947255452\n",
      "-0.0012613128264745076\n",
      "-0.0012612069368362427\n",
      "-0.0012612794478734334\n",
      "-0.001261292862892151\n",
      "-0.0012613228877385457\n",
      "-0.0012613207578659057\n",
      "-0.0012613201061884563\n",
      "-0.0012613171418507894\n",
      "-0.001261136786142985\n",
      "-0.0012603354454040526\n",
      "-0.0012610680341720581\n",
      "-0.0012612069606781006\n",
      "-0.0012612912019093832\n",
      "-0.0012613102277119954\n",
      "-0.0012613210519154866\n",
      "-0.0012613138596216837\n",
      "-0.0012613194942474365\n",
      "-0.001261286958058675\n",
      "-0.0012610959609349569\n",
      "-0.0012612680753072102\n",
      "-0.0012613070885340372\n",
      "-0.0012613166093826294\n",
      "-0.0012611137946446738\n",
      "-0.001260172669092814\n",
      "-0.0012610016187032064\n",
      "-0.0012611860513687134\n",
      "-0.001261289127667745\n",
      "-0.0012613103389739991\n",
      "-0.0012613154331843057\n",
      "-0.0012613245248794555\n",
      "-0.0012613183895746867\n",
      "-0.001261322538057963\n",
      "-0.0012613230148951213\n",
      "-0.0012613141298294067\n",
      "-0.0012613237619400024\n",
      "-0.0012613195498784383\n",
      "-0.0012612904230753581\n",
      "-0.0012609667539596559\n",
      "-0.0012605164448420206\n",
      "-0.0012611026207605999\n",
      "-0.0012612165609995525\n",
      "-0.0012612882614135741\n",
      "-0.0012613166491190592\n",
      "-0.0012613128900527955\n",
      "-0.0012613369941711426\n",
      "-0.0012613269646962483\n",
      "-0.001261323873202006\n",
      "-0.0012613093455632527\n",
      "-0.0012611148993174235\n",
      "-0.001260709540049235\n",
      "-0.0012611777544021606\n",
      "-0.0012612585941950481\n",
      "-0.0012612912893295288\n",
      "-0.0012613009770711264\n",
      "-0.001261309846242269\n",
      "-0.0012613204876581827\n",
      "-0.0012612377405166627\n",
      "-0.0012612287521362306\n",
      "-0.0012611921310424804\n",
      "-0.001260516635576884\n",
      "-0.0012610333840052286\n",
      "-0.0012612014055252076\n",
      "-0.001261272676785787\n",
      "-0.0012612930456797281\n",
      "-0.0012613133192062378\n",
      "-0.0012613187789916993\n",
      "-0.0012613127708435059\n",
      "-0.001261326789855957\n",
      "-0.0012613195260365805\n",
      "-0.0012613186995188395\n",
      "-0.001261311904589335\n",
      "-0.0012606818596522012\n",
      "-0.001260166629155477\n",
      "-0.0012609370470046998\n",
      "-0.0012612250089645387\n",
      "-0.00126130690574646\n",
      "-0.0012613182385762534\n",
      "-0.00126133660475413\n",
      "-0.0012613291263580322\n",
      "-0.0012613137086232504\n",
      "-0.0012613203605016073\n",
      "-0.0012613351742426554\n",
      "-0.0012613217274347942\n",
      "-0.0012613185962041219\n",
      "-0.0012613318045934041\n",
      "-0.0012613163391749064\n",
      "-0.0012612136205037434\n",
      "-0.001260425329208374\n",
      "-0.0012610550721486409\n",
      "-0.0012612215518951417\n",
      "-0.0012612871408462525\n",
      "-0.0012613149325052897\n",
      "-0.0012613160848617554\n",
      "-0.0012613207419713338\n",
      "-0.001261321187019348\n",
      "-0.0012613274415334066\n",
      "-0.001261315147082011\n",
      "-0.0012613319238026937\n",
      "-0.0012612369696299234\n",
      "-0.0012610158761342367\n",
      "-0.001260273281733195\n",
      "-0.0012610235134760538\n",
      "-0.0012612040042877196\n",
      "-0.0012612806638081868\n",
      "-0.0012613147735595703\n",
      "-0.0012613269011179605\n",
      "-0.0012613308906555176\n",
      "-0.001261330238978068\n",
      "-0.0012613319635391236\n",
      "-0.0012613143603006998\n",
      "-0.0012613292535146078\n",
      "-0.001261319915453593\n",
      "-0.001261281951268514\n",
      "-0.0012605326016743978\n",
      "-0.0012607932726542155\n",
      "-0.0012611358006795247\n",
      "-0.0012612687269846598\n",
      "-0.001261312468846639\n",
      "-0.0012613278150558473\n",
      "-0.0012613184928894042\n",
      "-0.0012613262335459392\n",
      "-0.0012613136450449626\n",
      "-0.0012612911542256673\n",
      "-0.0012611858129501342\n",
      "-0.0012612828652064005\n",
      "-0.0012612870931625365\n",
      "-0.0012610347191492717\n",
      "-0.0012607343594233196\n",
      "-0.0012611489375432333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012612590471903482\n",
      "-0.0012612995306650798\n",
      "-0.0012613062461217244\n",
      "-0.0012613162438074748\n",
      "-0.0012613253672917684\n",
      "-0.0012613165775934856\n",
      "-0.001261293363571167\n",
      "-0.0012612084070841472\n",
      "-0.0012612086137135823\n",
      "-0.0012601007143656412\n",
      "-0.0012609589417775472\n",
      "-0.0012611823558807372\n",
      "-0.001261256488164266\n",
      "-0.001261318532625834\n",
      "-0.0012613117297490437\n",
      "-0.001261312182744344\n",
      "-0.001261317213376363\n",
      "-0.0012613161166508992\n",
      "-0.0012613185405731201\n",
      "-0.0012613207896550497\n",
      "-0.001261246697107951\n",
      "-0.0012611839294433594\n",
      "-0.001261250376701355\n",
      "-0.001261194602648417\n",
      "-0.0012608324845631918\n",
      "-0.0012612077713012695\n",
      "-0.001261226757367452\n",
      "-0.0012612683216730754\n",
      "-0.001261312953631083\n",
      "-0.0012613171418507894\n",
      "-0.0012613040844599405\n",
      "-0.0012613099654515585\n",
      "-0.0012611323753992717\n",
      "-0.0012605317831039429\n",
      "-0.0012610722541809081\n",
      "-0.0012612220684687296\n",
      "-0.0012612616856892904\n",
      "-0.0012613168795903523\n",
      "-0.0012613245248794555\n",
      "-0.0012613173007965089\n",
      "-0.00126131645043691\n",
      "-0.0012611375013987223\n",
      "-0.0012612433513005574\n",
      "-0.0012612956126530966\n",
      "-0.0012612944046656292\n",
      "-0.00126072781085968\n",
      "-0.0012605957825978598\n",
      "-0.0012610843181610108\n",
      "-0.0012612571318944296\n",
      "-0.0012613118092219034\n",
      "-0.0012613233009974162\n",
      "-0.0012613232056299846\n",
      "-0.0012613216241200765\n",
      "-0.001261320185661316\n",
      "-0.0012613263765970865\n",
      "-0.0012613115628560385\n",
      "-0.0012613205273946126\n",
      "-0.0012613082726796467\n",
      "-0.0012613085587819418\n",
      "-0.0012607745409011842\n",
      "-0.001260496457417806\n",
      "-0.0012610654910405476\n",
      "-0.0012612334966659545\n",
      "-0.0012612914085388183\n",
      "-0.0012613308509190878\n",
      "-0.0012613130569458007\n",
      "-0.0012613181034723917\n",
      "-0.0012613310098648072\n",
      "-0.0012613248348236083\n",
      "-0.0012613083283106487\n",
      "-0.001261319661140442\n",
      "-0.001261251449584961\n",
      "-0.001260342526435852\n",
      "-0.0012609083414077758\n",
      "-0.0012611881812413534\n",
      "-0.001261276666323344\n",
      "-0.0012613008658091227\n",
      "-0.0012613192637761434\n",
      "-0.001261322514216105\n",
      "-0.001261321234703064\n",
      "-0.001261322585741679\n",
      "-0.0012613185485204062\n",
      "-0.0012612908522288005\n",
      "-0.0012611258506774901\n",
      "-0.0012612468083699545\n",
      "-0.0012612314224243165\n",
      "-0.0012607917547225952\n",
      "-0.0012610263268152872\n",
      "-0.001261183269818624\n",
      "-0.0012612556934356689\n",
      "-0.0012612903118133544\n",
      "-0.0012613119602203368\n",
      "-0.0012613180716832479\n",
      "-0.0012613188982009888\n",
      "-0.0012613112370173137\n",
      "-0.0012611982981363932\n",
      "-0.0012607763687769572\n",
      "-0.0012611878792444864\n",
      "-0.0012612379948298137\n",
      "-0.001261259365081787\n",
      "-0.0012612797737121583\n",
      "-0.0012612688461939494\n",
      "-0.0012613230228424072\n",
      "-0.0012613011121749879\n",
      "-0.0012610595305760702\n",
      "-0.0012608254591623942\n",
      "-0.0012611899932225546\n",
      "-0.0012612715005874634\n",
      "-0.001261285408337911\n",
      "-0.0012613153457641602\n",
      "-0.0012613285144170126\n",
      "-0.0012613268852233886\n",
      "-0.001261293609937032\n",
      "-0.0012607287804285686\n",
      "-0.0012607499361038208\n",
      "-0.0012611169576644897\n",
      "-0.001261252776781718\n",
      "-0.0012613112608591715\n",
      "-0.0012613210916519165\n",
      "-0.0012613103628158569\n",
      "-0.0012612311363220214\n",
      "-0.0012612308422724405\n",
      "-0.0012612940152486164\n",
      "-0.001261306357383728\n",
      "-0.0012613162120183308\n",
      "-0.0012613099416097005\n",
      "-0.0012609434048334758\n",
      "-0.0012606208006540934\n",
      "-0.0012611046234766643\n",
      "-0.001261242397626241\n",
      "-0.0012612990140914917\n",
      "-0.0012613245089848836\n",
      "-0.0012613224744796752\n",
      "-0.0012613227605819703\n",
      "-0.0012613116900126139\n",
      "-0.0012611117045084636\n",
      "-0.0012612640937169394\n",
      "-0.0012612885475158692\n",
      "-0.0012611476182937621\n",
      "-0.001260694177945455\n",
      "-0.0012611708958943685\n",
      "-0.0012612507899602255\n",
      "-0.0012612916707992554\n",
      "-0.0012612996339797973\n",
      "-0.0012613214413324991\n",
      "-0.001261316188176473\n",
      "-0.001261319359143575\n",
      "-0.0012613310972849527\n",
      "-0.0012612969239552815\n",
      "-0.001260703984896342\n",
      "-0.001260619044303894\n",
      "-0.0012610807418823242\n",
      "-0.0012612454652786256\n",
      "-0.0012612976789474488\n",
      "-0.0012613144159317018\n",
      "-0.0012613269170125326\n",
      "-0.0012613241910934448\n",
      "-0.0012612032016118368\n",
      "-0.0012612282355626424\n",
      "-0.0012612867434819539\n",
      "-0.001261312429110209\n",
      "-0.0012613209247589112\n",
      "-0.0012613147815068563\n",
      "-0.0012610032796859742\n",
      "-0.0012605849981307984\n",
      "-0.001261099123954773\n",
      "-0.0012612324237823486\n",
      "-0.001261284605662028\n",
      "-0.001261319859822591\n",
      "-0.0012613233725229898\n",
      "-0.0012613211154937744\n",
      "-0.00126131542523702\n",
      "-0.0012612741947174073\n",
      "-0.001261150042215983\n",
      "-0.0012612756888071696\n",
      "-0.0012613011757532755\n",
      "-0.0012612680753072102\n",
      "-0.0012602308750152588\n",
      "-0.001260794480641683\n",
      "-0.0012611775000890095\n",
      "-0.0012612469514211018\n",
      "-0.0012612873395284017\n",
      "-0.0012613194147745768\n",
      "-0.001261323078473409\n",
      "-0.0012613276561101278\n",
      "-0.0012613191445668538\n",
      "-0.00126131960550944\n",
      "-0.0012613263209660848\n",
      "-0.0012613294045130412\n",
      "-0.0012613211949666341\n",
      "-0.0012613161484400432\n",
      "-0.001261058235168457\n",
      "-0.0012609360933303834\n",
      "-0.001260775327682495\n",
      "-0.001261154596010844\n",
      "-0.0012612654209136964\n",
      "-0.0012613048235575357\n",
      "-0.0012612993955612183\n",
      "-0.0012613060315450032\n",
      "-0.0012613179922103882\n",
      "-0.001261328689257304\n",
      "-0.00126132976214091\n",
      "-0.0012611831347147623\n",
      "-0.0012604498465855916\n",
      "-0.0012610786914825439\n",
      "-0.0012612311283747356\n",
      "-0.0012612895806630453\n",
      "-0.0012613051652908326\n",
      "-0.001261321751276652\n",
      "-0.0012613194227218628\n",
      "-0.0012613166650136311\n",
      "-0.0012613241036732992\n",
      "-0.0012613143682479858\n",
      "-0.0012612618525822956\n",
      "-0.0012610556205113728\n",
      "-0.0012606847922007243\n",
      "-0.0012610279719034831\n",
      "-0.0012611833413441976\n",
      "-0.0012612641016642252\n",
      "-0.0012613083680470785\n",
      "-0.0012613188664118448\n",
      "-0.0012613166014353433\n",
      "-0.0012613269249598185\n",
      "-0.0012613269249598185\n",
      "-0.0012613125801086427\n",
      "-0.0012610872745513915\n",
      "-0.0012605822960535685\n",
      "-0.001261129053433736\n",
      "-0.0012612391312917073\n",
      "-0.0012612863699595132\n",
      "-0.001261316680908203\n",
      "-0.001261310124397278\n",
      "-0.0012613242467244465\n",
      "-0.001261322053273519\n",
      "-0.0012611806074778239\n",
      "-0.0012612275441487631\n",
      "-0.0012611923297246297\n",
      "-0.0012608256498972575\n",
      "-0.001261118737856547\n",
      "-0.0012612051248550415\n",
      "-0.0012612601359685263\n",
      "-0.0012613038698832194\n",
      "-0.0012613027413686116\n",
      "-0.0012613250255584718\n",
      "-0.0012612910588582357\n",
      "-0.0012611543575922648\n",
      "-0.001260863169034322\n",
      "-0.0012612442096074422\n",
      "-0.0012612524271011353\n",
      "-0.0012612661520640055\n",
      "-0.001261318333943685\n",
      "-0.0012611812591552735\n",
      "-0.0012611590147018432\n",
      "-0.0012612443288167318\n",
      "-0.0012611353556315104\n",
      "-0.0012607727289199828\n",
      "-0.0012612035195032755\n",
      "-0.0012612682580947876\n",
      "-0.0012612807989120483\n",
      "-0.001261296518643697\n",
      "-0.0012613151868184408\n",
      "-0.0012613173961639405\n",
      "-0.001261300261815389\n",
      "-0.0012612212419509888\n",
      "-0.0012607719739278157\n",
      "-0.0012611397822697957\n",
      "-0.0012612159729003907\n",
      "-0.0012612719456354777\n",
      "-0.0012612934668858847\n",
      "-0.0012613140185674032\n",
      "-0.0012611725648244221\n",
      "-0.001261059856414795\n",
      "-0.0012612685283025105\n",
      "-0.001261299967765808\n",
      "-0.0012611498991648355\n",
      "-0.0012603269338607788\n",
      "-0.0012610660552978515\n",
      "-0.0012612013339996337\n",
      "-0.0012612846374511718\n",
      "-0.0012613112290700277\n",
      "-0.0012613109191258749\n",
      "-0.0012613182544708253\n",
      "-0.0012613167603810628\n",
      "-0.0012613250255584718\n",
      "-0.0012613140185674032\n",
      "-0.0012613253196080525\n",
      "-0.0012613359928131103\n",
      "-0.0012613179365793864\n",
      "-0.0012611238876978556\n",
      "-0.00126034251054128\n",
      "-0.0012610695997873943\n",
      "-0.0012612103541692098\n",
      "-0.001261288849512736\n",
      "-0.001261312174797058\n",
      "-0.0012613237063090006\n",
      "-0.001261292807261149\n",
      "-0.001261162535349528\n",
      "-0.0012612801551818847\n",
      "-0.001261315099398295\n",
      "-0.0012613279581069946\n",
      "-0.0012613207181294758\n",
      "-0.0012613195260365805\n",
      "-0.0012612461566925048\n",
      "-0.001260382620493571\n",
      "-0.0012609841505686443\n",
      "-0.0012612085501352945\n",
      "-0.0012612850427627565\n",
      "-0.001261303965250651\n",
      "-0.001261314312616984\n",
      "-0.0012613258123397827\n",
      "-0.001261330811182658\n",
      "-0.0012613109827041627\n",
      "-0.001261056089401245\n",
      "-0.0012612287918726604\n",
      "-0.0012612717469533284\n",
      "-0.0012613283793131511\n",
      "-0.001261304744084676\n",
      "-0.0012607796907424928\n",
      "-0.0012605584541956585\n",
      "-0.001261068844795227\n",
      "-0.00126125545501709\n",
      "-0.0012613032658894856\n",
      "-0.0012613247315088909\n",
      "-0.0012613339026769001\n",
      "-0.00126131591796875\n",
      "-0.0012613265673319498\n",
      "-0.0012613228797912599\n",
      "-0.0012613246997197469\n",
      "-0.00126132333278656\n",
      "-0.0012613216956456502\n",
      "-0.001261327330271403\n",
      "-0.0012611937046051026\n",
      "-0.0012603550513585408\n",
      "-0.0012610440095265707\n",
      "-0.0012611769199371338\n",
      "-0.0012612802267074584\n",
      "-0.00126129732131958\n",
      "-0.001261312182744344\n",
      "-0.001261323857307434\n",
      "-0.0012613239844640096\n",
      "-0.001261321512858073\n",
      "-0.0012613075176874796\n",
      "-0.0012611191908518473\n",
      "-0.001261263664563497\n",
      "-0.0012612933079401653\n",
      "-0.0012610363006591797\n",
      "-0.0012605324188868204\n",
      "-0.0012610952774683634\n",
      "-0.0012612241983413696\n",
      "-0.0012612910906473795\n",
      "-0.0012613183736801148\n",
      "-0.0012613179047902426\n",
      "-0.0012613208452860515\n",
      "-0.0012613266070683796\n",
      "-0.0012613162120183308\n",
      "-0.0012613195260365805\n",
      "-0.001261320964495341\n",
      "-0.0012612600406010945\n",
      "-0.0012605461359024048\n",
      "-0.0012608790715535481\n",
      "-0.0012611552079518635\n",
      "-0.0012612603823343913\n",
      "-0.0012613122542699177\n",
      "-0.0012613177061080933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00126132230758667\n",
      "-0.0012613242467244465\n",
      "-0.0012612505197525025\n",
      "-0.0012612372716267904\n",
      "-0.0012612959782282512\n",
      "-0.0012613145430882772\n",
      "-0.0012612889528274536\n",
      "-0.001260586134592692\n",
      "-0.001260830775896708\n",
      "-0.0012611480156580607\n",
      "-0.001261268162727356\n",
      "-0.0012613110701243083\n",
      "-0.0012613127390543619\n",
      "-0.001261324644088745\n",
      "-0.0012613219102223714\n",
      "-0.0012612838347752888\n",
      "-0.0012611372788747153\n",
      "-0.0012612641016642252\n",
      "-0.001261299188931783\n",
      "-0.0012613286018371582\n",
      "-0.0012613211790720622\n",
      "-0.0012612303733825683\n",
      "-0.0012598822673161824\n",
      "-0.0012609342575073243\n",
      "-0.0012611198027928671\n",
      "-0.0012612773100535075\n",
      "-0.0012613057295481364\n",
      "-0.0012613277355829876\n",
      "-0.0012613182226816812\n",
      "-0.0012613306999206544\n",
      "-0.0012613273779551188\n",
      "-0.001261326003074646\n",
      "-0.00126131698290507\n",
      "-0.0012613252639770507\n",
      "-0.0012613234122594197\n",
      "-0.0012611978689829508\n",
      "-0.0012610707521438598\n",
      "-0.0012612746556599935\n",
      "-0.001261302391688029\n",
      "-0.0012613036553064982\n",
      "-0.0012609695990880331\n",
      "-0.001260546056429545\n",
      "-0.001261091430981954\n",
      "-0.0012612316528956096\n",
      "-0.0012613053083419799\n",
      "-0.0012613072792689005\n",
      "-0.0012613183577855427\n",
      "-0.00126132603486379\n",
      "-0.0012613178491592406\n",
      "-0.0012613174041112263\n",
      "-0.0012613216718037922\n",
      "-0.0012613260984420777\n",
      "-0.001261327616373698\n",
      "-0.0012612935622533163\n",
      "-0.001260449242591858\n",
      "-0.0012607096751530965\n",
      "-0.001261144717534383\n",
      "-0.001261275601387024\n",
      "-0.0012613022009531657\n",
      "-0.0012613028764724731\n",
      "-0.0012612446705500284\n",
      "-0.0012613056341807048\n",
      "-0.0012613078594207764\n",
      "-0.0012613233884175617\n",
      "-0.0012613226413726807\n",
      "-0.0012613191445668538\n",
      "-0.0012613274733225504\n",
      "-0.0012612876335779827\n",
      "-0.0012611631472905476\n",
      "-0.0012609431266784668\n",
      "-0.0012607179959615072\n",
      "-0.0012611285050710041\n",
      "-0.0012612515211105346\n",
      "-0.0012612929026285808\n",
      "-0.0012613087177276612\n",
      "-0.0012613291025161744\n",
      "-0.0012613313754399618\n",
      "-0.0012613208691279094\n",
      "-0.0012613166173299154\n",
      "-0.0012613215366999307\n",
      "-0.0012612324953079224\n",
      "-0.001260459081331889\n",
      "-0.0012610317468643188\n",
      "-0.0012612099726994831\n",
      "-0.0012612822373708088\n",
      "-0.0012613103151321412\n",
      "-0.0012613189856211344\n",
      "-0.0012613276402155557\n",
      "-0.00126131755510966\n",
      "-0.0012613187948862712\n",
      "-0.001261322331428528\n",
      "-0.0012612649520238241\n",
      "-0.0012602938652038573\n",
      "-0.0012608721097310384\n",
      "-0.0012611725966135661\n",
      "-0.001261280393600464\n",
      "-0.0012613090753555297\n",
      "-0.0012613220453262328\n",
      "-0.0012613184531529746\n",
      "-0.0012613232930501301\n",
      "-0.0012613258361816406\n",
      "-0.0012613146384557088\n",
      "-0.0012613257010777792\n",
      "-0.0012613180001576742\n",
      "-0.0012613226493199666\n",
      "-0.0012612699588139852\n",
      "-0.0012605364799499511\n",
      "-0.0012608356714248657\n",
      "-0.0012611547549565634\n",
      "-0.0012612656911214193\n",
      "-0.0012612933238347372\n",
      "-0.001261321465174357\n",
      "-0.001261327838897705\n",
      "-0.0012613261143366496\n",
      "-0.0012613219022750855\n",
      "-0.001261316967010498\n",
      "-0.0012612872918446858\n",
      "-0.0012609829584757488\n",
      "-0.0012608035882314046\n",
      "-0.0012611628691355386\n",
      "-0.0012612602551778157\n",
      "-0.0012612979968388875\n",
      "-0.0012613196849822997\n",
      "-0.0012613184611002604\n",
      "-0.001261316196123759\n",
      "-0.0012613234996795655\n",
      "-0.0012613273859024049\n",
      "-0.0012612994273503621\n",
      "-0.00126084094842275\n",
      "-0.0012606935103734334\n",
      "-0.0012611239592234293\n",
      "-0.0012612525860468547\n",
      "-0.0012612972736358642\n",
      "-0.0012613128980000813\n",
      "-0.0012613105376561482\n",
      "-0.001261319955190023\n",
      "-0.0012613180716832479\n",
      "-0.0012613116979599\n",
      "-0.0012612298091252644\n",
      "-0.0012605069160461426\n",
      "-0.0012610738118489584\n",
      "-0.001261218778292338\n",
      "-0.0012612806797027587\n",
      "-0.0012613145510355632\n",
      "-0.001261323094367981\n",
      "-0.0012613225698471068\n",
      "-0.0012613220850626627\n",
      "-0.0012613187789916993\n",
      "-0.0012613166411717732\n",
      "-0.0012613195816675823\n",
      "-0.0012611565033594768\n",
      "-0.001260509244600932\n",
      "-0.001261082394917806\n",
      "-0.0012612162113189698\n",
      "-0.0012612876097361247\n",
      "-0.0012612878243128459\n",
      "-0.0012613187313079833\n",
      "-0.001261322792371114\n",
      "-0.001261328673362732\n",
      "-0.0012613226970036825\n",
      "-0.0012613113164901732\n",
      "-0.0012608078002929687\n",
      "-0.0012605495691299438\n",
      "-0.001261052640279134\n",
      "-0.0012612566391626994\n",
      "-0.001261308725674947\n",
      "-0.0012613212744394938\n",
      "-0.0012613188187281291\n",
      "-0.0012613248030344645\n",
      "-0.0012613215843836467\n",
      "-0.0012613229990005492\n",
      "-0.0012613255023956299\n",
      "-0.0012613178253173829\n",
      "-0.00126114501953125\n",
      "-0.0012612134377161662\n",
      "-0.0012609503110249838\n",
      "-0.0012609118064244589\n",
      "-0.0012611706256866455\n",
      "-0.0012612759669621786\n",
      "-0.0012613057454427083\n",
      "-0.0012612968683242797\n",
      "-0.001261310029029846\n",
      "-0.001261318556467692\n",
      "-0.0012613167603810628\n",
      "-0.0012613247235616048\n",
      "-0.001261221448580424\n",
      "-0.001260170062383016\n",
      "-0.001261018657684326\n",
      "-0.001261192242304484\n",
      "-0.0012612647612889607\n",
      "-0.0012613110303878784\n",
      "-0.0012613133827845254\n",
      "-0.0012612745841344198\n",
      "-0.0012612942854563396\n",
      "-0.0012613214095433553\n",
      "-0.001261315647761027\n",
      "-0.0012612624009450276\n",
      "-0.0012613142331441244\n",
      "-0.0012613195975621542\n",
      "-0.0012613159577051797\n",
      "-0.0012613187630971274\n",
      "-0.0012610278685887654\n",
      "-0.0012604083061218262\n",
      "-0.0012610192696253459\n",
      "-0.0012611954689025879\n",
      "-0.0012613016287485758\n",
      "-0.0012613206148147584\n",
      "-0.0012613109827041627\n",
      "-0.0012613090117772421\n",
      "-0.0012613183895746867\n",
      "-0.0012613214333852133\n",
      "-0.0012613316535949708\n",
      "-0.0012613227685292561\n",
      "-0.0012611557324727375\n",
      "-0.0012611713488896688\n",
      "-0.001261238145828247\n",
      "-0.0012607092539469401\n",
      "-0.0012610009670257567\n",
      "-0.0012611775557200113\n",
      "-0.0012612696329752605\n",
      "-0.0012612946192423504\n",
      "-0.0012613134860992431\n",
      "-0.0012613189458847045\n",
      "-0.0012613224585851033\n",
      "-0.0012613247235616048\n",
      "-0.0012613221883773804\n",
      "-0.0012613208373387654\n",
      "-0.0012612699111302693\n",
      "-0.0012601412852605183\n",
      "-0.0012608714421590169\n",
      "-0.0012611751317977906\n",
      "-0.001261258832613627\n",
      "-0.0012613023360570272\n",
      "-0.0012613217035929362\n",
      "-0.001261321528752645\n",
      "-0.0012613107840220134\n",
      "-0.0012610898017883301\n",
      "-0.0012612648566563923\n",
      "-0.0012612948099772136\n",
      "-0.0012613199710845948\n",
      "-0.0012613197247187296\n",
      "-0.0012613200823465983\n",
      "-0.0012613165140151978\n",
      "-0.001261323634783427\n",
      "-0.0012612417380015056\n",
      "-0.0012603222211201987\n",
      "-0.0012610054095586142\n",
      "-0.0012612021366755167\n",
      "-0.0012612767060597738\n",
      "-0.0012613033215204874\n",
      "-0.0012613205989201865\n",
      "-0.0012613218863805134\n",
      "-0.0012613139629364014\n",
      "-0.0012613238970438638\n",
      "-0.0012613220691680908\n",
      "-0.0012613287130991617\n",
      "-0.0012612160285313924\n",
      "-0.0012610835790634156\n",
      "-0.0012612721999486287\n",
      "-0.0012612122933069864\n",
      "-0.001260202137629191\n",
      "-0.0012609968821207683\n",
      "-0.001261192234357198\n",
      "-0.0012612661838531493\n",
      "-0.0012613115072250367\n",
      "-0.0012613184531529746\n",
      "-0.0012613289038340251\n",
      "-0.0012613178412119548\n",
      "-0.0012613260825475056\n",
      "-0.0012613200743993123\n",
      "-0.001261327846844991\n",
      "-0.0012613284508387248\n",
      "-0.0012613231658935547\n",
      "-0.0012613250414530437\n",
      "-0.0012612985054651896\n",
      "-0.0012606231689453126\n",
      "-0.0012606419483820598\n",
      "-0.0012611123164494832\n",
      "-0.0012612760066986084\n",
      "-0.0012613042990366617\n",
      "-0.0012613108396530152\n",
      "-0.0012611345370610555\n",
      "-0.0012612301508585612\n",
      "-0.0012612977027893066\n",
      "-0.0012613193909327188\n",
      "-0.001261319335301717\n",
      "-0.0012613264242808024\n",
      "-0.0012613179922103882\n",
      "-0.0012613261540730795\n",
      "-0.001261325208346049\n",
      "-0.0012611422379811604\n",
      "-0.0012602970123291016\n",
      "-0.0012610602617263793\n",
      "-0.0012611968835194907\n",
      "-0.0012612801790237427\n",
      "-0.0012613217751185099\n",
      "-0.0012613250176111857\n",
      "-0.0012613244454065958\n",
      "-0.0012613287051518758\n",
      "-0.0012612423261006674\n",
      "-0.0012611452182133991\n",
      "-0.0012612661441167195\n",
      "-0.0012613117456436156\n",
      "-0.0012613213221232095\n",
      "-0.0012613260825475056\n",
      "-0.0012612641493479411\n",
      "-0.0012602474133173625\n",
      "-0.001260910701751709\n",
      "-0.0012611912806828817\n",
      "-0.0012612622340520223\n",
      "-0.0012613007545471192\n",
      "-0.0012613162755966186\n",
      "-0.0012613192081451416\n",
      "-0.001261324143409729\n",
      "-0.0012613229195276897\n",
      "-0.0012613278786341349\n",
      "-0.001261324183146159\n",
      "-0.00126132706006368\n",
      "-0.0012613252480824788\n",
      "-0.001261181386311849\n",
      "-0.0012611112197240195\n",
      "-0.0012611363887786865\n",
      "-0.0012604300737380982\n",
      "-0.0012610969146092733\n",
      "-0.0012612228552500407\n",
      "-0.001261281402905782\n",
      "-0.0012613063097000122\n",
      "-0.0012613129138946534\n",
      "-0.0012613211234410605\n",
      "-0.0012613234043121339\n",
      "-0.0012613207499186198\n",
      "-0.0012613268534342448\n",
      "-0.001261323587099711\n",
      "-0.0012613012234369914\n",
      "-0.0012609707355499267\n",
      "-0.0012606601635615031\n",
      "-0.0012611189603805543\n",
      "-0.001261241046587626\n",
      "-0.0012612937053044636\n",
      "-0.0012613192399342854\n",
      "-0.0012613222201665242\n",
      "-0.0012613217910130818\n",
      "-0.0012613207181294758\n",
      "-0.0012612517436345419\n",
      "-0.001261122218767802\n",
      "-0.0012612561146418254\n",
      "-0.0012613090912501018\n",
      "-0.0012610976775487264\n",
      "-0.0012603481769561768\n",
      "-0.0012610579967498778\n",
      "-0.0012612018744150798\n",
      "-0.001261296812693278\n",
      "-0.0012613332827885945\n",
      "-0.0012613207260767619\n",
      "-0.0012613219579060873\n",
      "-0.00126131858030955\n",
      "-0.001261321465174357\n",
      "-0.0012613169511159262\n",
      "-0.001261314900716146\n",
      "-0.0012613160689671835\n",
      "-0.0012613199869791667\n",
      "-0.0012612659136454264\n",
      "-0.0012604459285736084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012609307924906412\n",
      "-0.001261154596010844\n",
      "-0.0012612628936767578\n",
      "-0.0012613080422083536\n",
      "-0.0012613234599431356\n",
      "-0.0012613284349441529\n",
      "-0.0012613274892171223\n",
      "-0.0012613071839014688\n",
      "-0.0012612229665120442\n",
      "-0.0012612976710001628\n",
      "-0.0012613118171691895\n",
      "-0.0012613263448079427\n",
      "-0.0012612101475397745\n",
      "-0.0012603353182474772\n",
      "-0.0012610674937566122\n",
      "-0.0012612205823262532\n",
      "-0.001261284327507019\n",
      "-0.0012613110383351645\n",
      "-0.0012613258679707844\n",
      "-0.0012613110939661662\n",
      "-0.001261186973253886\n",
      "-0.0012612725496292114\n",
      "-0.0012613144000371296\n",
      "-0.0012613158305486043\n",
      "-0.001261317483584086\n",
      "-0.0012613219340642293\n",
      "-0.0012612482865651448\n",
      "-0.0012604036569595336\n",
      "-0.0012610119422276815\n",
      "-0.0012612060546875\n",
      "-0.001261290661493937\n",
      "-0.0012613076368967692\n",
      "-0.0012613235553105672\n",
      "-0.0012613148848215738\n",
      "-0.001261224627494812\n",
      "-0.0012611441214879354\n",
      "-0.0012612680276234945\n",
      "-0.0012613101720809936\n",
      "-0.001261316688855489\n",
      "-0.0012613155682881672\n",
      "-0.0012612932205200195\n",
      "-0.0012605411211649576\n",
      "-0.0012606868346532185\n",
      "-0.0012611336628595988\n",
      "-0.0012612643241882325\n",
      "-0.0012612983226776124\n",
      "-0.0012613170305887857\n",
      "-0.0012613134066263834\n",
      "-0.0012613140424092611\n",
      "-0.001261325740814209\n",
      "-0.0012613235632578533\n",
      "-0.0012613243420918783\n",
      "-0.001261323587099711\n",
      "-0.0012613234281539916\n",
      "-0.0012613239447275798\n",
      "-0.001261094363530477\n",
      "-0.0012609726985295614\n",
      "-0.0012604288736979166\n",
      "-0.001261095094680786\n",
      "-0.0012612236897150675\n",
      "-0.0012612848917643229\n",
      "-0.0012613181591033936\n",
      "-0.0012613192081451416\n",
      "-0.0012613231658935547\n",
      "-0.0012613286336263022\n",
      "-0.0012613179365793864\n",
      "-0.0012613207181294758\n",
      "-0.0012613222996393839\n",
      "-0.0012613017559051515\n",
      "-0.0012609503587086995\n",
      "-0.0012606547117233276\n",
      "-0.0012611125310262043\n",
      "-0.0012612427473068237\n",
      "-0.0012613038221995037\n",
      "-0.0012613190094629924\n",
      "-0.0012613154331843057\n",
      "-0.0012613197485605875\n",
      "-0.0012613186915715535\n",
      "-0.001261326313018799\n",
      "-0.0012613306681315104\n",
      "-0.0012613087177276612\n",
      "-0.0012610562562942506\n",
      "-0.0012607819477717082\n",
      "-0.0012608163913091024\n",
      "-0.0012611557801564535\n",
      "-0.0012612521648406982\n",
      "-0.0012612958113352457\n",
      "-0.0012613229831059773\n",
      "-0.0012613197565078736\n",
      "-0.0012613136132558188\n",
      "-0.0012613208055496216\n",
      "-0.0012613222281138102\n",
      "-0.0012613211711247762\n",
      "-0.0012612475633621216\n",
      "-0.0012604502121607462\n",
      "-0.0012610188166300455\n",
      "-0.0012612077156702677\n",
      "-0.0012612786451975506\n",
      "-0.0012613150676091512\n",
      "-0.0012613194942474365\n",
      "-0.001261318850517273\n",
      "-0.001261329444249471\n",
      "-0.001261245044072469\n",
      "-0.0012611813068389892\n",
      "-0.0012612805604934692\n",
      "-0.0012613163948059082\n",
      "-0.0012613140185674032\n",
      "-0.001261084032058716\n",
      "-0.00126040248076121\n",
      "-0.0012610658407211303\n",
      "-0.0012612089236577351\n",
      "-0.0012612919807434082\n",
      "-0.0012613215525945029\n",
      "-0.0012613155047098796\n",
      "-0.0012613213221232095\n",
      "-0.001261317801475525\n",
      "-0.0012613136688868205\n",
      "-0.0012613220850626627\n",
      "-0.001261313279469808\n",
      "-0.0012613184611002604\n",
      "-0.0012611021518707276\n",
      "-0.0012610256512959799\n",
      "-0.0012606111685434978\n",
      "-0.0012611437638600667\n",
      "-0.0012612448692321777\n",
      "-0.0012612865686416625\n",
      "-0.0012613219817479452\n",
      "-0.001261315353711446\n",
      "-0.0012613207896550497\n",
      "-0.0012613181511561076\n",
      "-0.0012613178968429565\n",
      "-0.001261318055788676\n",
      "-0.0012611991246541342\n",
      "-0.001260635232925415\n",
      "-0.0012611438671747844\n",
      "-0.0012612386067708333\n",
      "-0.0012612825949986776\n",
      "-0.001261306627591451\n",
      "-0.001261318278312683\n",
      "-0.0012613243420918783\n",
      "-0.0012613215843836467\n",
      "-0.0012612287918726604\n",
      "-0.0012611064751942953\n",
      "-0.0012611703236897786\n",
      "-0.001260690975189209\n",
      "-0.0012611398696899415\n",
      "-0.0012612314144770304\n",
      "-0.0012612752278645834\n",
      "-0.001261292290687561\n",
      "-0.0012613167762756347\n",
      "-0.0012613179683685302\n",
      "-0.0012613195896148681\n",
      "-0.0012613192081451416\n",
      "-0.0012612693945566814\n",
      "-0.0012608301083246868\n",
      "-0.0012609780311584472\n",
      "-0.001261181624730428\n",
      "-0.0012612664063771566\n",
      "-0.0012612987200419108\n",
      "-0.0012613206466039022\n",
      "-0.0012613197167714437\n",
      "-0.0012613166650136311\n",
      "-0.0012613261540730795\n",
      "-0.0012613089323043824\n",
      "-0.00126105268796285\n",
      "-0.0012608177185058594\n",
      "-0.0012608197291692099\n",
      "-0.001261142635345459\n",
      "-0.0012612597306569417\n",
      "-0.0012612966458002727\n",
      "-0.0012613125244776407\n",
      "-0.0012613245010375976\n",
      "-0.0012613141298294067\n",
      "-0.0012613336086273192\n",
      "-0.0012613199472427368\n",
      "-0.0012612945238749185\n",
      "-0.0012610402981440227\n",
      "-0.0012606979131698608\n",
      "-0.0012611339886983235\n",
      "-0.001261245369911194\n",
      "-0.001261298712094625\n",
      "-0.001261314598719279\n",
      "-0.0012613248348236083\n",
      "-0.0012613251606623333\n",
      "-0.001261313525835673\n",
      "-0.001261167844136556\n",
      "-0.001261277969678243\n",
      "-0.0012613056659698486\n",
      "-0.0012611489057540893\n",
      "-0.0012602968613306682\n",
      "-0.0012610541264216105\n",
      "-0.0012612058480580648\n",
      "-0.001261286481221517\n",
      "-0.0012613120079040527\n",
      "-0.0012613216241200765\n",
      "-0.0012613183418909708\n",
      "-0.0012613210519154866\n",
      "-0.001261323618888855\n",
      "-0.0012613301992416381\n",
      "-0.0012613161325454713\n",
      "-0.00126121776898702\n",
      "-0.0012611959139506022\n",
      "-0.0012612770795822144\n",
      "-0.0012611709594726563\n",
      "-0.0012603837887446086\n",
      "-0.0012610944112141926\n",
      "-0.0012612293243408203\n",
      "-0.0012612843672434489\n",
      "-0.001261303424835205\n",
      "-0.0012613229354222616\n",
      "-0.0012613232056299846\n",
      "-0.0012613227287928263\n",
      "-0.001261313001314799\n",
      "-0.0012613286972045898\n",
      "-0.0012613230069478353\n",
      "-0.001261319080988566\n",
      "-0.0012613159815470377\n",
      "-0.0012611045281092326\n",
      "-0.0012602369546890258\n",
      "-0.0012608607133229573\n",
      "-0.0012611478408177694\n",
      "-0.001261286703745524\n",
      "-0.001261320169766744\n",
      "-0.0012613176981608072\n",
      "-0.0012613199234008789\n",
      "-0.001261327862739563\n",
      "-0.0012613173087437947\n",
      "-0.0012613193114598593\n",
      "-0.0012613189776738486\n",
      "-0.0012613282203674317\n",
      "-0.0012613220453262328\n",
      "-0.0012612034797668456\n",
      "-0.0012611680348714193\n",
      "-0.0012611766815185546\n",
      "-0.0012604245901107788\n",
      "-0.0012610684951146443\n",
      "-0.0012612239519755046\n",
      "-0.0012612775087356568\n",
      "-0.001261314304669698\n",
      "-0.0012613279183705647\n",
      "-0.0012613306204477947\n",
      "-0.0012613358577092488\n",
      "-0.0012613209486007691\n",
      "-0.0012613285382588703\n",
      "-0.0012613206704457601\n",
      "-0.0012613174517949423\n",
      "-0.0012611764272054036\n",
      "-0.001260463237762451\n",
      "-0.0012610960960388184\n",
      "-0.001261225660641988\n",
      "-0.0012612770318984985\n",
      "-0.0012612695614496866\n",
      "-0.0012612903833389283\n",
      "-0.0012613186597824097\n",
      "-0.0012613098700841269\n",
      "-0.001261326257387797\n",
      "-0.001261326559384664\n",
      "-0.0012613152186075846\n",
      "-0.0012612886826197307\n",
      "-0.0012609792470932007\n",
      "-0.0012605377833048502\n",
      "-0.0012611210982004801\n",
      "-0.0012612361192703247\n",
      "-0.001261291790008545\n",
      "-0.0012613122781117757\n",
      "-0.001261322553952535\n",
      "-0.0012613202651341757\n",
      "-0.0012613237380981446\n",
      "-0.001261321481068929\n",
      "-0.0012613207976023355\n",
      "-0.0012613192319869996\n",
      "-0.0012611461639404297\n",
      "-0.0012603321234385173\n",
      "-0.001261073366800944\n",
      "-0.0012612118323644002\n",
      "-0.0012612897793451946\n",
      "-0.0012613197088241577\n",
      "-0.001261247706413269\n",
      "-0.001261211895942688\n",
      "-0.0012612914164861044\n",
      "-0.001261320940653483\n",
      "-0.0012613259394963583\n",
      "-0.0012613221565882366\n",
      "-0.001261324707667033\n",
      "-0.0012613243420918783\n",
      "-0.0012613149722417196\n",
      "-0.0012608768781026205\n",
      "-0.0012606311003367106\n",
      "-0.0012610900322596232\n",
      "-0.001261250368754069\n",
      "-0.0012613019704818726\n",
      "-0.0012613235155741374\n",
      "-0.0012613016287485758\n",
      "-0.001261097224553426\n",
      "-0.0012612611691157024\n",
      "-0.0012612950960795084\n",
      "-0.0012613292217254638\n",
      "-0.0012613242944081624\n",
      "-0.0012613203128178914\n",
      "-0.00126131378809611\n",
      "-0.0012612669626871744\n",
      "-0.0012601999521255492\n",
      "-0.0012608985344568888\n",
      "-0.0012611814498901368\n",
      "-0.0012612512111663818\n",
      "-0.0012613130569458007\n",
      "-0.0012613183180491129\n",
      "-0.0012613288402557373\n",
      "-0.0012613271713256837\n",
      "-0.0012613226811091106\n",
      "-0.001261327290534973\n",
      "-0.0012613226175308228\n",
      "-0.001261313796043396\n",
      "-0.0012611087242762249\n",
      "-0.0012612664937973023\n",
      "-0.001261294388771057\n",
      "-0.0012613090674082439\n",
      "-0.0012611058712005616\n",
      "-0.0012604994058609009\n",
      "-0.0012610960642496746\n",
      "-0.0012612273375193278\n",
      "-0.00126128888130188\n",
      "-0.001261315631866455\n",
      "-0.0012613199392954508\n",
      "-0.0012613242387771607\n",
      "-0.0012613176822662353\n",
      "-0.0012613229274749756\n",
      "-0.0012613176345825194\n",
      "-0.001261320169766744\n",
      "-0.001261316482226054\n",
      "-0.001261189874013265\n",
      "-0.0012603137254714966\n",
      "-0.001260963773727417\n",
      "-0.001261187752087911\n",
      "-0.0012612744092941285\n",
      "-0.0012613178412119548\n",
      "-0.0012613175630569458\n",
      "-0.0012613145192464192\n",
      "-0.0012613275210062664\n",
      "-0.0012613257487614949\n",
      "-0.00126133021513621\n",
      "-0.0012612564007441203\n",
      "-0.001261213239034017\n",
      "-0.0012612906217575072\n",
      "-0.0012613032023111978\n",
      "-0.0012610125462214153\n",
      "-0.0012605468273162842\n",
      "-0.0012610899845759073\n",
      "-0.0012612325191497802\n",
      "-0.0012612947225570678\n",
      "-0.0012613205989201865\n",
      "-0.0012613251765569052\n",
      "-0.0012613222440083821\n",
      "-0.001261325216293335\n",
      "-0.0012613249778747559\n",
      "-0.0012613275766372681\n",
      "-0.0012612377325693766\n",
      "-0.001261176856358846\n",
      "-0.0012610538323720296\n",
      "-0.0012607619603474936\n",
      "-0.0012611763159434\n",
      "-0.0012612608909606934\n",
      "-0.0012613046566645305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012613115549087524\n",
      "-0.0012613178888956705\n",
      "-0.0012613197882970174\n",
      "-0.0012613301436106364\n",
      "-0.0012613109668095906\n",
      "-0.0012611867348353068\n",
      "-0.0012606117645899454\n",
      "-0.0012611302216847737\n",
      "-0.0012612404664357503\n",
      "-0.0012612911144892374\n",
      "-0.001261311133702596\n",
      "-0.0012612740993499757\n",
      "-0.0012611730972925822\n",
      "-0.0012612815539042154\n",
      "-0.001261314328511556\n",
      "-0.0012613221486409505\n",
      "-0.0012613202889760334\n",
      "-0.001261206038792928\n",
      "-0.0012603111743927002\n",
      "-0.0012610543966293335\n",
      "-0.0012612123886744182\n",
      "-0.00126127982934316\n",
      "-0.0012613084236780803\n",
      "-0.0012613203525543213\n",
      "-0.0012613226334253947\n",
      "-0.0012613213698069255\n",
      "-0.0012613263765970865\n",
      "-0.0012612011273701986\n",
      "-0.0012612414280573528\n",
      "-0.0012613024870554606\n",
      "-0.001261321512858073\n",
      "-0.0012613263686498007\n",
      "-0.0012611411809921264\n",
      "-0.001260328205426534\n",
      "-0.0012610622485478718\n",
      "-0.0012612097342809042\n",
      "-0.0012612936894098917\n",
      "-0.0012613146543502807\n",
      "-0.0012613255500793458\n",
      "-0.0012613184611002604\n",
      "-0.0012613203128178914\n",
      "-0.001261324683825175\n",
      "-0.001261290454864502\n",
      "-0.0012611475547154745\n",
      "-0.001261277993520101\n",
      "-0.0012613098859786988\n",
      "-0.0012613232771555582\n",
      "-0.0012612225532531737\n",
      "-0.00126033562819163\n",
      "-0.0012610477685928345\n",
      "-0.0012612157742182414\n",
      "-0.0012612772305806478\n",
      "-0.0012613097508748373\n",
      "-0.001261320424079895\n",
      "-0.0012613250176111857\n",
      "-0.0012613142172495525\n",
      "-0.0012613191843032837\n",
      "-0.0012613221168518067\n",
      "-0.0012613197326660156\n",
      "-0.0012613228638966877\n",
      "-0.001261320988337199\n",
      "-0.0012610072294871012\n",
      "-0.0012607116142908732\n",
      "-0.0012608066082000732\n",
      "-0.0012611462195714314\n",
      "-0.001261266819636027\n",
      "-0.0012613047202428183\n",
      "-0.0012613158226013183\n",
      "-0.001261322832107544\n",
      "-0.0012613246520360312\n",
      "-0.001261327870686849\n",
      "-0.0012613244533538819\n",
      "-0.0012613253911336263\n",
      "-0.0012613219976425171\n",
      "-0.0012612924655278524\n",
      "-0.0012602818489074706\n",
      "-0.0012606499036153158\n",
      "-0.0012611653884251912\n",
      "-0.0012612408876419067\n",
      "-0.0012613101641337078\n",
      "-0.0012613231182098388\n",
      "-0.0012613187074661254\n",
      "-0.0012613210757573446\n",
      "-0.0012613144954045613\n",
      "-0.0012613258441289267\n",
      "-0.0012613130728403726\n",
      "-0.0012613259236017864\n",
      "-0.00126132386525472\n",
      "-0.0012613281806310018\n",
      "-0.0012610925912857056\n",
      "-0.0012611789226531982\n",
      "-0.0012612481594085694\n",
      "-0.001261134632428487\n",
      "-0.001260836927096049\n",
      "-0.0012612290382385254\n",
      "-0.0012612605174382528\n",
      "-0.0012612776835759481\n",
      "-0.0012613078673680623\n",
      "-0.0012613084475199383\n",
      "-0.0012613244533538819\n",
      "-0.0012613232930501301\n",
      "-0.0012612860679626465\n",
      "-0.0012605552752812704\n",
      "-0.0012608527501424154\n",
      "-0.0012611589749654134\n",
      "-0.0012612774848937988\n",
      "-0.0012613095919291179\n",
      "-0.001261322514216105\n",
      "-0.0012613189935684205\n",
      "-0.0012613251765569052\n",
      "-0.0012613176107406617\n",
      "-0.0012613217989603678\n",
      "-0.0012613210519154866\n",
      "-0.0012613147417704265\n",
      "-0.0012611090103785197\n",
      "-0.0012612390200297037\n",
      "-0.001261008644104004\n",
      "-0.001260491140683492\n",
      "-0.001261082649230957\n",
      "-0.0012612279256184896\n",
      "-0.001261297313372294\n",
      "-0.0012613168001174926\n",
      "-0.001261321004231771\n",
      "-0.0012613207817077636\n",
      "-0.0012613283554712932\n",
      "-0.0012613203525543213\n",
      "-0.0012613207976023355\n",
      "-0.0012613209009170532\n",
      "-0.0012613183577855427\n",
      "-0.001261247722307841\n",
      "-0.0012605173190434775\n",
      "-0.001261029815673828\n",
      "-0.0012612104495366414\n",
      "-0.0012612765789031981\n",
      "-0.0012613076368967692\n",
      "-0.0012613190015157063\n",
      "-0.001261248747507731\n",
      "-0.0012611456314722696\n",
      "-0.0012612709045410155\n",
      "-0.0012613140106201171\n",
      "-0.001261324119567871\n",
      "-0.0012613200505574545\n",
      "-0.0012612550258636476\n",
      "-0.0012605900128682455\n",
      "-0.00126101553440094\n",
      "-0.0012612033367156983\n",
      "-0.0012612825155258179\n",
      "-0.0012613115708033243\n",
      "-0.0012613117774327596\n",
      "-0.0012613194386164347\n",
      "-0.0012613213141759237\n",
      "-0.0012613232533137003\n",
      "-0.0012613294124603272\n",
      "-0.0012613191207249959\n",
      "-0.0012613201459248861\n",
      "-0.0012613079388936362\n",
      "-0.001260584020614624\n",
      "-0.0012601842482884726\n",
      "-0.0012610503991444906\n",
      "-0.0012612093925476074\n",
      "-0.0012613014936447143\n",
      "-0.0012613154411315918\n",
      "-0.001261323626836141\n",
      "-0.0012613204638163248\n",
      "-0.0012613196690877278\n",
      "-0.0012613240242004395\n",
      "-0.001261328673362732\n",
      "-0.0012613230307896932\n",
      "-0.001261324954032898\n",
      "-0.0012613298813501995\n",
      "-0.0012613165616989135\n",
      "-0.0012610743522644043\n",
      "-0.001261271095275879\n",
      "-0.0012612841208775839\n",
      "-0.0012612137556076049\n",
      "-0.0012606857299804688\n",
      "-0.0012611307859420775\n",
      "-0.0012612337509791056\n",
      "-0.0012612856944402059\n",
      "-0.0012613040129343669\n",
      "-0.0012613086064656575\n",
      "-0.0012613269805908202\n",
      "-0.001261316442489624\n",
      "-0.0012613210439682008\n",
      "-0.0012613253196080525\n",
      "-0.0012612792094548544\n",
      "-0.0012604904174804688\n",
      "-0.001260892669359843\n",
      "-0.0012611817836761475\n",
      "-0.001261283278465271\n",
      "-0.0012613123496373493\n",
      "-0.0012613205512364706\n",
      "-0.0012612137953440347\n",
      "-0.0012612213055292764\n",
      "-0.0012612982114156087\n",
      "-0.0012613155841827393\n",
      "-0.0012613194624582927\n",
      "-0.001261317213376363\n",
      "-0.0012613283077875773\n",
      "-0.0012613288482030234\n",
      "-0.0012611936887105305\n",
      "-0.0012603456656138101\n",
      "-0.0012610815048217773\n",
      "-0.0012612231334050497\n",
      "-0.001261280083656311\n",
      "-0.0012613083680470785\n",
      "-0.0012613196849822997\n",
      "-0.0012613305966059367\n",
      "-0.0012612812995910644\n",
      "-0.0012611531496047974\n",
      "-0.0012612762928009033\n",
      "-0.0012613068103790282\n",
      "-0.0012613163073857626\n",
      "-0.0012613242149353027\n",
      "-0.0012612760305404664\n",
      "-0.00126064403851827\n",
      "-0.0012609466632207236\n",
      "-0.0012611757357915243\n",
      "-0.0012612749973932903\n",
      "-0.0012613114595413208\n",
      "-0.0012613151629765828\n",
      "-0.001261326766014099\n",
      "-0.001261316752433777\n",
      "-0.001261321218808492\n",
      "-0.0012613178173700968\n",
      "-0.0012613043705622356\n",
      "-0.0012611025015513103\n",
      "-0.0012612601200739542\n",
      "-0.00126109885374705\n",
      "-0.0012603156566619873\n",
      "-0.0012610490163167318\n",
      "-0.0012612010637919108\n",
      "-0.0012612966140111287\n",
      "-0.0012613152424494425\n",
      "-0.0012613243818283082\n",
      "-0.001261320439974467\n",
      "-0.0012613250970840454\n",
      "-0.0012613191922505697\n",
      "-0.0012613242705663045\n",
      "-0.0012613229036331176\n",
      "-0.0012613165299097697\n",
      "-0.0012613228638966877\n",
      "-0.0012613175392150878\n",
      "-0.0012611344416936239\n",
      "-0.0012603599230448404\n",
      "-0.0012610051552454631\n",
      "-0.0012611894766489664\n",
      "-0.001261278820037842\n",
      "-0.001261322585741679\n",
      "-0.0012613168160120645\n",
      "-0.0012613142410914102\n",
      "-0.0012613162676493328\n",
      "-0.001261320153872172\n",
      "-0.0012612229665120442\n",
      "-0.0012612165609995525\n",
      "-0.0012612879594167074\n",
      "-0.0012613154172897338\n",
      "-0.0012613182703653972\n",
      "-0.0012610785722732543\n",
      "-0.0012604916334152223\n",
      "-0.0012610872824986776\n",
      "-0.0012612241586049398\n",
      "-0.0012612949053446452\n",
      "-0.001261316164334615\n",
      "-0.0012613253275553385\n",
      "-0.0012613208134969077\n",
      "-0.0012613250176111857\n",
      "-0.001261323841412862\n",
      "-0.0012613207260767619\n",
      "-0.00126125172773997\n",
      "-0.001261180289586385\n",
      "-0.00126126123269399\n",
      "-0.0012610114812850951\n",
      "-0.0012606281518936157\n",
      "-0.0012611112435658772\n",
      "-0.0012612489541371664\n",
      "-0.0012612948815027873\n",
      "-0.0012613190571467083\n",
      "-0.0012613269726435344\n",
      "-0.0012613203684488932\n",
      "-0.0012613247553507486\n",
      "-0.0012613235791524252\n",
      "-0.0012613240003585815\n",
      "-0.0012613200028737386\n",
      "-0.0012613207658131917\n",
      "-0.0012612020015716552\n",
      "-0.0012601011594136557\n",
      "-0.0012609984318415323\n",
      "-0.0012611258506774901\n",
      "-0.0012611323833465577\n",
      "-0.0012612692912419637\n",
      "-0.0012613114356994628\n",
      "-0.0012613183895746867\n",
      "-0.0012613252480824788\n",
      "-0.0012613292773564658\n",
      "-0.0012613173166910808\n",
      "-0.001261322029431661\n",
      "-0.0012613218943277995\n",
      "-0.0012613210995992025\n",
      "-0.0012613258361816406\n",
      "-0.0012613239526748658\n",
      "-0.0012613228638966877\n",
      "-0.0012612576087315877\n",
      "-0.001260405135154724\n",
      "-0.0012608046293258667\n",
      "-0.0012611548821131388\n",
      "-0.0012612714926401774\n",
      "-0.0012613193988800049\n",
      "-0.001261327568689982\n",
      "-0.0012613269329071045\n",
      "-0.0012613242149353027\n",
      "-0.001261320161819458\n",
      "-0.001261321989695231\n",
      "-0.0012613276878992716\n",
      "-0.0012613189776738486\n",
      "-0.0012612754265467327\n",
      "-0.0012605955839157105\n",
      "-0.0012609666347503663\n",
      "-0.0012611916303634644\n",
      "-0.001261283818880717\n",
      "-0.001261312222480774\n",
      "-0.0012613253037134806\n",
      "-0.0012613221009572346\n",
      "-0.0012613269011179605\n",
      "-0.0012613256216049194\n",
      "-0.0012613187948862712\n",
      "-0.0012613168319066366\n",
      "-0.0012612082878748576\n",
      "-0.0012611006339391072\n",
      "-0.001260807474454244\n",
      "-0.0012608107089996338\n",
      "-0.0012611411492029826\n",
      "-0.0012612678289413452\n",
      "-0.0012612968921661377\n",
      "-0.0012613157192866008\n",
      "-0.0012613202889760334\n",
      "-0.0012613205989201865\n",
      "-0.0012613187313079833\n",
      "-0.0012613263289133708\n",
      "-0.0012613229513168335\n",
      "-0.001261322808265686\n",
      "-0.0012613168319066366\n",
      "-0.0012609913190205892\n",
      "-0.0012601388772328695\n",
      "-0.0012609148025512695\n",
      "-0.0012612526655197145\n",
      "-0.001261275299390157\n",
      "-0.0012613158941268921\n",
      "-0.0012613192001978556\n",
      "-0.001261323912938436\n",
      "-0.0012613144238789876\n",
      "-0.001261199164390564\n",
      "-0.0012612857898076375\n",
      "-0.0012613099813461304\n",
      "-0.0012613208214441935\n",
      "-0.0012613230307896932\n",
      "-0.0012613200585047404\n",
      "-0.0012613238175710041\n",
      "-0.0012613228956858318\n",
      "-0.001261320455869039\n",
      "-0.001261293339729309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001260442344347636\n",
      "-0.001260753830273946\n",
      "-0.0012611632347106934\n",
      "-0.001261281402905782\n",
      "-0.0012612974484761555\n",
      "-0.001261326281229655\n",
      "-0.0012613248348236083\n",
      "-0.0012613245884577433\n",
      "-0.0012612071752548218\n",
      "-0.001261242135365804\n",
      "-0.0012613020817438762\n",
      "-0.0012613224267959595\n",
      "-0.0012613162358601888\n",
      "-0.001261323356628418\n",
      "-0.0012613229433695475\n",
      "-0.0012613219817479452\n",
      "-0.0012612706661224366\n",
      "-0.0012598430554072062\n",
      "-0.0012608863512674968\n",
      "-0.001261100713411967\n",
      "-0.0012612737735112507\n",
      "-0.0012613013505935668\n",
      "-0.0012613118251164753\n",
      "-0.0012613148848215738\n",
      "-0.0012613292773564658\n",
      "-0.0012612456639607748\n",
      "-0.0012612209558486938\n",
      "-0.0012612945079803466\n",
      "-0.0012613229115804037\n",
      "-0.0012613253752390544\n",
      "-0.0012613206307093303\n",
      "-0.0012613213141759237\n",
      "-0.0012613224744796752\n",
      "-0.0012613206386566161\n",
      "-0.001261322037378947\n",
      "-0.001261323912938436\n",
      "-0.0012612871328989664\n",
      "-0.0012605288902918497\n",
      "-0.0012606920878092448\n",
      "-0.0012611257155736287\n",
      "-0.0012612743616104126\n",
      "-0.0012612953344980876\n",
      "-0.001261312222480774\n",
      "-0.0012613214731216431\n",
      "-0.0012613221883773804\n",
      "-0.0012613300005594888\n",
      "-0.001261310052871704\n",
      "-0.0012611862421035766\n",
      "-0.001261276650428772\n",
      "-0.0012613161404927571\n",
      "-0.001261322585741679\n",
      "-0.0012613185962041219\n",
      "-0.0012611345688501993\n",
      "-0.0012603933890660605\n",
      "-0.0012610805749893189\n",
      "-0.0012612162510553997\n",
      "-0.0012612829367319742\n",
      "-0.001261319931348165\n",
      "-0.0012613257090250652\n",
      "-0.0012613232851028443\n",
      "-0.0012613152901331584\n",
      "-0.0012611818631490072\n",
      "-0.0012612460851669312\n",
      "-0.0012612996260325114\n",
      "-0.0012613159418106078\n",
      "-0.0012613191445668538\n",
      "-0.0012612894455591838\n",
      "-0.0012607750097910562\n",
      "-0.0012608795881271363\n",
      "-0.0012611666520436606\n",
      "-0.0012612655798594156\n",
      "-0.0012613126834233601\n",
      "-0.0012613194465637208\n",
      "-0.001261322291692098\n",
      "-0.0012613246997197469\n",
      "-0.001261328935623169\n",
      "-0.0012613241751988729\n",
      "-0.0012612958510716755\n",
      "-0.0012611141284306843\n",
      "-0.0012610350052515665\n",
      "-0.0012606101751327515\n",
      "-0.0012611313978830973\n",
      "-0.0012612483501434327\n",
      "-0.0012612926403681437\n",
      "-0.0012613234519958496\n",
      "-0.0012613259394963583\n",
      "-0.0012613268772761028\n",
      "-0.0012613254229227701\n",
      "-0.001261326519648234\n",
      "-0.001261322808265686\n",
      "-0.0012613279740015665\n",
      "-0.0012613086700439453\n",
      "-0.0012606703281402589\n",
      "-0.0012605348507563274\n",
      "-0.0012611003081003825\n",
      "-0.0012612650712331135\n",
      "-0.0012612952868143717\n",
      "-0.0012613166014353433\n",
      "-0.001261319335301717\n",
      "-0.0012612444400787353\n",
      "-0.0012612236420313518\n",
      "-0.0012612846851348877\n",
      "-0.0012613218545913696\n",
      "-0.0012613223393758137\n",
      "-0.0012613260746002198\n",
      "-0.0012613220453262328\n",
      "-0.001261325510342916\n",
      "-0.00126131378809611\n",
      "-0.0012612666447957357\n",
      "-0.0012601910750071209\n",
      "-0.001260896857579549\n",
      "-0.0012611805518468221\n",
      "-0.001261243971188863\n",
      "-0.00126131645043691\n",
      "-0.0012613202412923177\n",
      "-0.0012612828969955445\n",
      "-0.0012612746000289917\n",
      "-0.0012613192796707153\n",
      "-0.001261313549677531\n",
      "-0.0012613224824269613\n",
      "-0.0012613220771153769\n",
      "-0.0012613200187683105\n",
      "-0.0012612565358479818\n",
      "-0.0012612913370132447\n",
      "-0.0012613075971603393\n",
      "-0.0012612666447957357\n",
      "-0.0012604151328404745\n",
      "-0.0012609641710917155\n",
      "-0.0012612016598383586\n",
      "-0.0012612819115320841\n",
      "-0.0012613046169281006\n",
      "-0.0012613202730814615\n",
      "-0.0012612760861714682\n",
      "-0.0012612305402755737\n",
      "-0.0012612863938013712\n",
      "-0.0012613149404525758\n",
      "-0.001261325494448344\n",
      "-0.0012613260904947917\n",
      "-0.0012613213618596394\n",
      "-0.0012613195896148681\n",
      "-0.0012612928708394368\n",
      "-0.001260324764251709\n",
      "-0.0012607168594996135\n",
      "-0.0012611710468928019\n",
      "-0.0012612450838088989\n",
      "-0.0012613070011138917\n",
      "-0.0012612635691960652\n",
      "-0.0012612590948740642\n",
      "-0.0012613069693247477\n",
      "-0.001261319406827291\n",
      "-0.0012613245010375976\n",
      "-0.0012613192081451416\n",
      "-0.0012613171180089315\n",
      "-0.0012613287210464477\n",
      "-0.0012613238334655762\n",
      "-0.001261273725827535\n",
      "-0.0012611945708592733\n",
      "-0.0012612120628356933\n",
      "-0.0012607607920964558\n",
      "-0.0012611452182133991\n",
      "-0.0012612404266993205\n",
      "-0.0012612721602121989\n",
      "-0.0012612976789474488\n",
      "-0.0012613145351409911\n",
      "-0.0012613128900527955\n",
      "-0.0012613278071085612\n",
      "-0.0012613276799519856\n",
      "-0.0012612526098887125\n",
      "-0.0012606632312138876\n",
      "-0.0012610536257425945\n",
      "-0.0012612044731775919\n",
      "-0.0012612788915634156\n",
      "-0.0012612951358159383\n",
      "-0.0012613187154134114\n",
      "-0.0012613173405329387\n",
      "-0.001261249876022339\n",
      "-0.0012611898104349772\n",
      "-0.0012612611929575602\n",
      "-0.0012612619717915852\n",
      "-0.0012610718568166097\n",
      "-0.0012609694321950276\n",
      "-0.0012612276156743368\n",
      "-0.001261291495958964\n",
      "-0.0012612828731536866\n",
      "-0.0012613180239995322\n",
      "-0.0012613112052281697\n",
      "-0.0012613166332244873\n",
      "-0.0012612475633621216\n",
      "-0.0012607515573501587\n",
      "-0.001261107850074768\n",
      "-0.001261211371421814\n",
      "-0.0012612782955169677\n",
      "-0.0012612991333007812\n",
      "-0.0012613123019536336\n",
      "-0.0012611993312835693\n",
      "-0.0012612718264261881\n",
      "-0.0012613041162490846\n",
      "-0.0012613148689270019\n",
      "-0.0012612237056096394\n",
      "-0.001260564136505127\n",
      "-0.001261093513170878\n",
      "-0.0012612189372380575\n",
      "-0.0012612868626912434\n",
      "-0.0012613149483998616\n",
      "-0.001261323062578837\n",
      "-0.0012613195021947226\n",
      "-0.001261317459742228\n",
      "-0.001261323634783427\n",
      "-0.0012613176743189493\n",
      "-0.0012610823472340902\n",
      "-0.0012612067858378093\n",
      "-0.0012608342011769613\n",
      "-0.0012609096686045328\n",
      "-0.0012611694653828938\n",
      "-0.0012612602551778157\n",
      "-0.0012613009452819824\n",
      "-0.0012613186995188395\n",
      "-0.0012613212505976359\n",
      "-0.0012613193909327188\n",
      "-0.001261321210861206\n",
      "-0.0012613231261571249\n",
      "-0.0012612899700800578\n",
      "-0.0012609596967697143\n",
      "-0.0012609078168869018\n",
      "-0.0012611738602320353\n",
      "-0.0012612588167190551\n",
      "-0.0012612974882125854\n",
      "-0.0012613226493199666\n",
      "-0.0012613175948460898\n",
      "-0.0012613213698069255\n",
      "-0.0012613210916519165\n",
      "-0.0012613133986790976\n",
      "-0.0012612091779708862\n",
      "-0.0012610668182373047\n",
      "-0.0012604545831680298\n",
      "-0.001261074701944987\n",
      "-0.001261229109764099\n",
      "-0.0012613034327824911\n",
      "-0.0012613272984822591\n",
      "-0.0012613267421722413\n",
      "-0.0012613237460454305\n",
      "-0.0012613213777542115\n",
      "-0.0012613190253575643\n",
      "-0.0012613271236419678\n",
      "-0.0012613198359807333\n",
      "-0.0012613282521565755\n",
      "-0.001261316180229187\n",
      "-0.0012607956329981486\n",
      "-0.0012605051040649414\n",
      "-0.0012610612074534099\n",
      "-0.0012612421035766601\n",
      "-0.0012613086382548015\n",
      "-0.0012613272269566855\n",
      "-0.0012613241990407308\n",
      "-0.001261323618888855\n",
      "-0.0012613306681315104\n",
      "-0.001261320734024048\n",
      "-0.001261328069368998\n",
      "-0.001261324691772461\n",
      "-0.0012613204956054688\n",
      "-0.0012613322099049886\n",
      "-0.001261324151357015\n",
      "-0.0012612999439239503\n",
      "-0.0012597585995992025\n",
      "-0.001260661021868388\n",
      "-0.0012610248406728108\n",
      "-0.0012612358490626018\n",
      "-0.001261306071281433\n",
      "-0.0012613099813461304\n",
      "-0.001261299459139506\n",
      "-0.0012611888408660889\n",
      "-0.0012612768491109213\n",
      "-0.0012613195021947226\n",
      "-0.0012613299051920572\n",
      "-0.0012613187154134114\n",
      "-0.0012613259553909302\n",
      "-0.0012613194624582927\n",
      "-0.001261326789855957\n",
      "-0.001261322816212972\n",
      "-0.0012613271156946817\n",
      "-0.001261328673362732\n",
      "-0.0012613030513127644\n",
      "-0.0012611523787180582\n",
      "-0.0012612625042597453\n",
      "-0.0012613068342208862\n",
      "-0.001261312429110209\n",
      "-0.0012611104647318521\n",
      "-0.0012605823596318563\n",
      "-0.0012611243724822997\n",
      "-0.0012612450838088989\n",
      "-0.0012612924098968506\n",
      "-0.0012613177935282389\n",
      "-0.001261319859822591\n",
      "-0.0012613242626190186\n",
      "-0.001261319406827291\n",
      "-0.0012613319873809815\n",
      "-0.0012613162914911905\n",
      "-0.001261321465174357\n",
      "-0.0012612359762191772\n",
      "-0.0012604523181915283\n",
      "-0.001260988958676656\n",
      "-0.0012611857573191325\n",
      "-0.001261285146077474\n",
      "-0.001261314312616984\n",
      "-0.0012613208134969077\n",
      "-0.001261320980389913\n",
      "-0.0012613280296325685\n",
      "-0.0012613271554311116\n",
      "-0.0012613219658533731\n",
      "-0.0012613230148951213\n",
      "-0.0012613247791926066\n",
      "-0.0012613178968429565\n",
      "-0.0012606844981511434\n",
      "-0.001260339339574178\n",
      "-0.0012610865831375123\n",
      "-0.0012612430413564046\n",
      "-0.001261294158299764\n",
      "-0.001261300746599833\n",
      "-0.0012612277348836264\n",
      "-0.0012613036473592123\n",
      "-0.0012613149881362915\n",
      "-0.0012613211393356324\n",
      "-0.0012613253037134806\n",
      "-0.0012613205115000407\n",
      "-0.0012613197724024455\n",
      "-0.0012613200108210247\n",
      "-0.0012613179604212444\n",
      "-0.0012612199465433756\n",
      "-0.0012612725178400676\n",
      "-0.001261242914199829\n",
      "-0.0012609443346659342\n",
      "-0.0012610602140426636\n",
      "-0.001261200475692749\n",
      "-0.0012612784465154013\n",
      "-0.0012613147258758546\n",
      "-0.001261307692527771\n",
      "-0.0012613253593444823\n",
      "-0.0012613224347432454\n",
      "-0.0012613253196080525\n",
      "-0.0012612136363983155\n",
      "-0.001260444688796997\n",
      "-0.0012609249114990235\n",
      "-0.0012611775239308675\n",
      "-0.0012612792332967122\n",
      "-0.0012613088528315227\n",
      "-0.0012613195021947226\n",
      "-0.001261325184504191\n",
      "-0.001261327886581421\n",
      "-0.0012613223155339558\n",
      "-0.0012613196929295858\n",
      "-0.0012613282283147175\n",
      "-0.0012613236904144287\n",
      "-0.0012613112529118856\n",
      "-0.0012610198259353637\n",
      "-0.0012603672822316487\n",
      "-0.0012610026915868123\n",
      "-0.0012612266381581624\n",
      "-0.0012613053878148396\n",
      "-0.0012613091786702474\n",
      "-0.001261331836382548\n",
      "-0.0012612896919250488\n",
      "-0.001261166254679362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012612836440404256\n",
      "-0.0012613191445668538\n",
      "-0.0012613165855407714\n",
      "-0.0012613198757171632\n",
      "-0.001261322331428528\n",
      "-0.0012613214095433553\n",
      "-0.0012613236506779989\n",
      "-0.0012613121668497722\n",
      "-0.001260781208674113\n",
      "-0.001260355814297994\n",
      "-0.0012610487143198648\n",
      "-0.0012612608353296916\n",
      "-0.001261295509338379\n",
      "-0.0012613194783528646\n",
      "-0.0012613212664922078\n",
      "-0.0012612931648890177\n",
      "-0.001261217427253723\n",
      "-0.0012612971544265748\n"
     ]
    }
   ],
   "source": [
    "nb_of_epochs=4000\n",
    "batch_size=int(x_variable.size(0)/10)\n",
    "\n",
    "for epoch in range(nb_of_epochs):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        recon_batch,mu,logvar = model(x_variable.narrow(0,b,batch_size))                \n",
    "        loss = loss_function(recon_batch, y_variable.narrow(0,b,batch_size),mu,logvar)     \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "        train_loss+=loss.data[0]\n",
    "    print(train_loss/x_variable.size(0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VAE(\n",
       "   (fc1): Linear(in_features=24, out_features=14, bias=True)\n",
       "   (fc21): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc22): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc3): Linear(in_features=2, out_features=14, bias=True)\n",
       "   (fc4): Linear(in_features=14, out_features=24, bias=True)\n",
       " ), Variable containing:\n",
       " (  0  ,.,.) = \n",
       "   7.2590e-01  1.7160e-01  8.4208e-01  ...  -8.6179e-01  6.2929e-01 -2.9497e-01\n",
       " \n",
       " (  1  ,.,.) = \n",
       "   1.1056e+00  8.5052e-02  1.0595e+00  ...  -8.8101e-01  5.4964e-01 -4.5626e-01\n",
       " \n",
       " (  2  ,.,.) = \n",
       "   1.3043e+00 -1.6374e-01  7.1454e-01  ...  -1.0654e+00  3.5652e-01 -3.6640e-01\n",
       "  ...  \n",
       " \n",
       " (59997,.,.) = \n",
       "   1.0052e+00  1.3537e-01  7.4466e-01  ...  -8.4981e-01  6.0202e-01 -5.4357e-01\n",
       " \n",
       " (59998,.,.) = \n",
       "   7.9475e-01 -1.6893e-01  4.9571e-01  ...  -4.6446e-01  1.2147e+00 -5.7865e-01\n",
       " \n",
       " (59999,.,.) = \n",
       "   1.3487e+00  1.5864e-02  7.7996e-01  ...  -4.4117e-01  5.4940e-01 -2.1820e-01\n",
       " [torch.cuda.FloatTensor of size 60000x1x24 (GPU 0)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable=model.cuda(),x_variable.cuda()\n",
    "model.eval()\n",
    "model,x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1fa97f21dbca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-14edde0d2c3a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparametrize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-14edde0d2c3a>\u001b[0m in \u001b[0;36mreparametrize\u001b[1;34m(self, mu, logvar)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'other'"
     ]
    }
   ],
   "source": [
    "polygon_prediction=model(x_variable)\n",
    "polygon_prediction=polygon_prediction[14].data.cpu()\n",
    "polygon_prediction=polygon_prediction.numpy()\n",
    "polygon_prediction=polygon_prediction.reshape(12,2)\n",
    "\n",
    "plot_contour(polygon_prediction)\n",
    "plot_contour(polygons[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VAE(\n",
       "   (fc1): Linear(in_features=24, out_features=14, bias=True)\n",
       "   (fc21): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc22): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc3): Linear(in_features=2, out_features=14, bias=True)\n",
       "   (fc4): Linear(in_features=14, out_features=24, bias=True)\n",
       " ), Variable containing:\n",
       " (  0  ,.,.) = \n",
       "   7.2590e-01  1.7160e-01  8.4208e-01  ...  -8.6179e-01  6.2929e-01 -2.9497e-01\n",
       " \n",
       " (  1  ,.,.) = \n",
       "   1.1056e+00  8.5052e-02  1.0595e+00  ...  -8.8101e-01  5.4964e-01 -4.5626e-01\n",
       " \n",
       " (  2  ,.,.) = \n",
       "   1.3043e+00 -1.6374e-01  7.1454e-01  ...  -1.0654e+00  3.5652e-01 -3.6640e-01\n",
       "  ...  \n",
       " \n",
       " (59997,.,.) = \n",
       "   1.0052e+00  1.3537e-01  7.4466e-01  ...  -8.4981e-01  6.0202e-01 -5.4357e-01\n",
       " \n",
       " (59998,.,.) = \n",
       "   7.9475e-01 -1.6893e-01  4.9571e-01  ...  -4.6446e-01  1.2147e+00 -5.7865e-01\n",
       " \n",
       " (59999,.,.) = \n",
       "   1.3487e+00  1.5864e-02  7.7996e-01  ...  -4.4117e-01  5.4940e-01 -2.1820e-01\n",
       " [torch.FloatTensor of size 60000x1x24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
