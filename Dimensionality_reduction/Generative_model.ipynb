{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.tensor as Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from Triangulation import *\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons=load_dataset('7_polygons.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(1,2*7))\n",
    "\n",
    "polygons_reshaped=np.array(polygons_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped=polygons_reshaped.reshape(polygons_reshaped.shape[0],1,2*7)\n",
    "\n",
    "#polygons_reshaped=polygons_reshaped.reshape(polygons_reshaped.shape[0],2*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(polygons_reshaped).type(torch.FloatTensor)\n",
    "x_variable,y_variable=Variable(x_tensor).cuda(),Variable(x_tensor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This auto encoder uses linear connections (Results may be the same as using PCA)\n",
    "class simple_autoencoder(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(simple_autoencoder,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "        nn.Linear(2*12,12),\n",
    "        nn.BatchNorm1d(12,momentum=0.5),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(12,6),\n",
    "        nn.BatchNorm1d(6,momentum=0.5),\n",
    "        nn.ReLU(True),      \n",
    "        nn.Linear(6,2),\n",
    "        nn.BatchNorm1d(2,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        nn.Linear(2,6),\n",
    "        nn.BatchNorm1d(6,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(6,12),\n",
    "        nn.BatchNorm1d(12,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True)    ,\n",
    "        nn.Linear(12,2*12)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Convolutional autoencoder\n",
    "class conv_autoencoder(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "        nn.Conv1d(1,30,kernel_size=2,stride=2),# d 24->12 stride=2 , filters =3\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv1d(30,600,kernel_size=2,stride=1),# d 12->6 stride=2, filters=6\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv1d(600,2,kernel_size=6,stride=1),#d  6->1 stride=1 ,filters=2   \n",
    "        )\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        nn.ReLU(True),\n",
    "\n",
    "        nn.ConvTranspose1d(2,600,kernel_size=6,stride=1),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose1d(600,300,kernel_size=2,stride=1),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose1d(300,1,kernel_size=2,stride=2),\n",
    "\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=conv_autoencoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(\n",
    "model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.876580517578125\n",
      "4.49729599609375\n",
      "2.1850667602539064\n",
      "0.8868205749511718\n",
      "0.7426623779296875\n",
      "0.70820361328125\n",
      "0.6839828186035156\n",
      "0.6793693542480469\n",
      "0.6767006103515625\n",
      "0.6756413818359375\n",
      "0.6748087646484375\n",
      "0.6740256286621094\n",
      "0.6731517578125\n",
      "0.6720369750976563\n",
      "0.6706502685546875\n",
      "0.6688459838867188\n",
      "0.6664459350585937\n",
      "0.6632261413574219\n",
      "0.6589274475097656\n",
      "0.6532849548339844\n",
      "0.645982958984375\n",
      "0.6369350952148437\n",
      "0.6266072692871094\n",
      "0.616315625\n",
      "0.6079470153808594\n",
      "0.6029228332519532\n",
      "0.6008080932617188\n",
      "0.5998474487304688\n",
      "0.5990849060058594\n",
      "0.5983993225097656\n",
      "0.5977627990722656\n",
      "0.5971379028320313\n",
      "0.5965008850097656\n",
      "0.5958615112304687\n",
      "0.59521328125\n",
      "0.5945861206054688\n",
      "0.5939191528320312\n",
      "0.5932093078613281\n",
      "0.592517724609375\n",
      "0.591770263671875\n",
      "0.590993359375\n",
      "0.5901551147460937\n",
      "0.5892615539550782\n",
      "0.5883211181640625\n",
      "0.5873148315429687\n",
      "0.5862550598144531\n",
      "0.5851270751953125\n",
      "0.5839325805664063\n",
      "0.5826412658691407\n",
      "0.5812702392578125\n",
      "0.5797406555175781\n",
      "0.5781012573242188\n",
      "0.5763590515136718\n",
      "0.5744937866210937\n",
      "0.5725388305664062\n",
      "0.5705303466796875\n",
      "0.5684293518066407\n",
      "0.566271533203125\n",
      "0.5640803894042968\n",
      "0.5618551696777344\n",
      "0.5596139221191406\n",
      "0.5573577087402344\n",
      "0.5550443481445313\n",
      "0.5528442932128906\n",
      "0.5507153015136719\n",
      "0.5486722900390625\n",
      "0.546566943359375\n",
      "0.5446818359375\n",
      "0.5428996520996093\n",
      "0.5412598205566406\n",
      "0.5397032958984375\n",
      "0.5382796447753906\n",
      "0.5369847473144531\n",
      "0.5357777160644531\n",
      "0.5346681518554688\n",
      "0.5336381591796875\n",
      "0.53276865234375\n",
      "0.5319606994628906\n",
      "0.5312291625976563\n",
      "0.5305539611816407\n",
      "0.5299157287597657\n",
      "0.5293221618652344\n",
      "0.5287627319335938\n",
      "0.5282249755859375\n",
      "0.5277004028320312\n",
      "0.5272032043457031\n",
      "0.5267154541015625\n",
      "0.5262506286621094\n",
      "0.5258021423339844\n",
      "0.5253615783691407\n",
      "0.524932861328125\n",
      "0.5245172424316407\n",
      "0.5241085632324218\n",
      "0.5237056396484375\n",
      "0.523312841796875\n",
      "0.5229291015625\n",
      "0.5225509765625\n",
      "0.5221718139648438\n",
      "0.5217877197265625\n",
      "0.5214132690429687\n",
      "0.5210399597167968\n",
      "0.5206689270019531\n",
      "0.5203016784667969\n",
      "0.5199419738769532\n",
      "0.5195896911621094\n",
      "0.5192428466796875\n",
      "0.518910107421875\n",
      "0.5185788635253906\n",
      "0.5182540344238281\n",
      "0.5179403564453126\n",
      "0.5176338073730469\n",
      "0.5173342254638672\n",
      "0.5170385681152344\n",
      "0.5167439270019532\n",
      "0.5164513763427734\n",
      "0.5161515167236328\n",
      "0.5158673370361329\n",
      "0.515592855834961\n",
      "0.5153061431884766\n",
      "0.5150323028564453\n",
      "0.5147556243896484\n",
      "0.5144802001953125\n",
      "0.5142107635498047\n",
      "0.5139538665771485\n",
      "0.5136987518310547\n",
      "0.5134475280761719\n",
      "0.5132028778076172\n",
      "0.5129608947753906\n",
      "0.5127154418945312\n",
      "0.5124751647949218\n",
      "0.5122387664794922\n",
      "0.5120025970458985\n",
      "0.5117684020996094\n",
      "0.5115358642578125\n",
      "0.5113047637939453\n",
      "0.5110697387695312\n",
      "0.5108333435058594\n",
      "0.5106024383544921\n",
      "0.5103717559814454\n",
      "0.5101381072998047\n",
      "0.5099127868652343\n",
      "0.5096881683349609\n",
      "0.5094617462158203\n",
      "0.5092340789794921\n",
      "0.5090041259765625\n",
      "0.5087700897216797\n",
      "0.5085414184570313\n",
      "0.5083220001220703\n",
      "0.5081027069091797\n",
      "0.5078883026123047\n",
      "0.5076774963378906\n",
      "0.5074707092285157\n",
      "0.5072680297851563\n",
      "0.507066665649414\n",
      "0.506867807006836\n",
      "0.506671923828125\n",
      "0.506472006225586\n",
      "0.5062784149169922\n",
      "0.5060879089355469\n",
      "0.5059028411865234\n",
      "0.5057175079345703\n",
      "0.5055345153808594\n",
      "0.505353988647461\n",
      "0.5051725128173828\n",
      "0.504996435546875\n",
      "0.5048200744628907\n",
      "0.5046471466064453\n",
      "0.5044756530761719\n",
      "0.5043051086425782\n",
      "0.50413701171875\n",
      "0.5039682708740234\n",
      "0.5037993896484375\n",
      "0.503632485961914\n",
      "0.5034622772216797\n",
      "0.5032870056152344\n",
      "0.5031170745849609\n",
      "0.5029512512207032\n",
      "0.5027858154296875\n",
      "0.5026228149414063\n",
      "0.5024629272460938\n",
      "0.5023059753417969\n",
      "0.5021495147705078\n",
      "0.5019937896728516\n",
      "0.5018393920898437\n",
      "0.5016848297119141\n",
      "0.5015323516845703\n",
      "0.5013784698486328\n",
      "0.5012239807128906\n",
      "0.5010659271240234\n",
      "0.5009111206054687\n",
      "0.5007556762695312\n",
      "0.5006039947509766\n",
      "0.5004546844482421\n",
      "0.5003057495117188\n",
      "0.5001571533203125\n",
      "0.5000095031738281\n",
      "0.4998588623046875\n",
      "0.4997086212158203\n",
      "0.49956570434570313\n",
      "0.49942280883789064\n",
      "0.49928128051757814\n",
      "0.49913912658691406\n",
      "0.4989995941162109\n",
      "0.49885662536621095\n",
      "0.4987121520996094\n",
      "0.4985741058349609\n",
      "0.4984354797363281\n",
      "0.49829483947753905\n",
      "0.49815662841796876\n",
      "0.49802124633789063\n",
      "0.49788645935058595\n",
      "0.4977538452148437\n",
      "0.4976208282470703\n",
      "0.4974887756347656\n",
      "0.4973574951171875\n",
      "0.49722046508789064\n",
      "0.49708380737304686\n",
      "0.496944384765625\n",
      "0.4968073181152344\n",
      "0.4966699859619141\n",
      "0.4965350158691406\n",
      "0.49640022583007815\n",
      "0.49626788330078125\n",
      "0.496135791015625\n",
      "0.4960097900390625\n",
      "0.4958836334228516\n",
      "0.49576051635742185\n",
      "0.4956430419921875\n",
      "0.4955200714111328\n",
      "0.4953993530273437\n",
      "0.49527667541503906\n",
      "0.4951567504882812\n",
      "0.4950364044189453\n",
      "0.49491620178222656\n",
      "0.4947956512451172\n",
      "0.4946754821777344\n",
      "0.4945562713623047\n",
      "0.4944383056640625\n",
      "0.4943221343994141\n",
      "0.49420877075195313\n",
      "0.494091796875\n",
      "0.4939739929199219\n",
      "0.4938539337158203\n",
      "0.49373397827148435\n",
      "0.4936242401123047\n",
      "0.4935066253662109\n",
      "0.49339227905273436\n",
      "0.4932754089355469\n",
      "0.49316338500976564\n",
      "0.4930496856689453\n",
      "0.49293408813476564\n",
      "0.4928230712890625\n",
      "0.4927074249267578\n",
      "0.4926022247314453\n",
      "0.4924823333740234\n",
      "0.4923739990234375\n",
      "0.4922547149658203\n",
      "0.4921430755615234\n",
      "0.49203243408203123\n",
      "0.4919258331298828\n",
      "0.4918143127441406\n",
      "0.49170372619628905\n",
      "0.4916007934570312\n",
      "0.49148980102539064\n",
      "0.4913773162841797\n",
      "0.4912727600097656\n",
      "0.491162890625\n",
      "0.49105436401367186\n",
      "0.4909515350341797\n",
      "0.4908403839111328\n",
      "0.4907322814941406\n",
      "0.4906268463134766\n",
      "0.4905218719482422\n",
      "0.49041752014160156\n",
      "0.4903161102294922\n",
      "0.4902141326904297\n",
      "0.49010716247558594\n",
      "0.48999783935546876\n",
      "0.48989679260253904\n",
      "0.48979358825683594\n",
      "0.48968475341796874\n",
      "0.48958067321777343\n",
      "0.48947802429199216\n",
      "0.4893691375732422\n",
      "0.489267138671875\n",
      "0.48916963195800783\n",
      "0.4890690185546875\n",
      "0.48896462707519533\n",
      "0.48885970458984374\n",
      "0.48876236877441404\n",
      "0.48866356201171873\n",
      "0.4885563934326172\n",
      "0.48844950561523437\n",
      "0.4883524780273438\n",
      "0.4882546112060547\n",
      "0.4881525634765625\n",
      "0.4880486114501953\n",
      "0.4879514099121094\n",
      "0.4878567810058594\n",
      "0.48776510620117186\n",
      "0.48766621704101565\n",
      "0.48756347045898435\n",
      "0.48746941528320314\n",
      "0.48738126525878905\n",
      "0.48728583984375\n",
      "0.4871801025390625\n",
      "0.4870789520263672\n",
      "0.48698965759277346\n",
      "0.48690063171386716\n",
      "0.4868075103759766\n",
      "0.48671185913085935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-7d1a95d68704>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0msum_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     \"\"\"\n\u001b[0;32m   1281\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[1;32m-> 1282\u001b[1;33m                            input, target, size_average, reduce)\n\u001b[0m\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[1;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[0;32m   1246\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1248\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=model.cuda()\n",
    "nb_of_epochs=4000\n",
    "batch_size=int(x_variable.size(0)/10)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = model(x_variable.narrow(0,b,batch_size))                \n",
    "        loss = criterion(out, y_variable.narrow(0,b,batch_size))     \n",
    "        sum_loss+=loss.data[0]\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "    print(sum_loss/x_variable.size(0))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(1, 30, kernel_size=(2,), stride=(2,))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv1d(30, 600, kernel_size=(2,), stride=(1,))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Conv1d(600, 2, kernel_size=(6,), stride=(1,))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ReLU(inplace)\n",
       "    (1): ConvTranspose1d(2, 600, kernel_size=(6,), stride=(1,))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose1d(600, 300, kernel_size=(2,), stride=(1,))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): ConvTranspose1d(300, 1, kernel_size=(2,), stride=(2,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable=model.cpu(),x_variable.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0  ,.,.) = \n",
       "  1.2144e+00 -3.7611e-03  4.3527e-01  ...  -1.1191e+00  6.0773e-01 -6.7368e-01\n",
       "\n",
       "(  1  ,.,.) = \n",
       "  7.5948e-01 -2.7041e-01  2.7368e-01  ...  -8.5231e-01  3.8521e-01 -8.3622e-01\n",
       "\n",
       "(  2  ,.,.) = \n",
       "  8.4495e-01 -1.5438e-02  4.5442e-01  ...  -1.0577e+00  7.2514e-01 -5.8645e-01\n",
       " ...  \n",
       "\n",
       "(19997,.,.) = \n",
       "  7.8227e-01 -7.7490e-02  3.5144e-01  ...  -7.7372e-01  4.8899e-01 -6.0974e-01\n",
       "\n",
       "(19998,.,.) = \n",
       "  7.4771e-01 -1.1872e-01  2.9135e-01  ...  -8.5790e-01  5.4161e-01 -9.4875e-01\n",
       "\n",
       "(19999,.,.) = \n",
       "  9.2310e-01 -2.3932e-02  5.4168e-01  ...  -8.8124e-01  7.2238e-01 -1.0617e+00\n",
       "[torch.FloatTensor of size 20000x1x14]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_prediction=model(x_variable)\n",
    "polygon_prediction=polygon_prediction[89].data.cpu()\n",
    "polygon_prediction=polygon_prediction.numpy()\n",
    "polygon_prediction=polygon_prediction.reshape(7,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_contour(polygon_prediction)\n",
    "plot_contour(polygons[89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv1d in module torch.nn.modules.conv:\n",
      "\n",
      "class Conv1d(_ConvNd)\n",
      " |  Applies a 1D convolution over an input signal composed of several input\n",
      " |  planes.\n",
      " |  \n",
      " |  In the simplest case, the output value of the layer with input size\n",
      " |  :math:`(N, C_{in}, L)` and output :math:`(N, C_{out}, L_{out})` can be\n",
      " |  precisely described as:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      \\begin{array}{ll}\n",
      " |      out(N_i, C_{out_j})  = bias(C_{out_j})\n",
      " |                     + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \\star input(N_i, k)\n",
      " |      \\end{array}\n",
      " |  \n",
      " |  where :math:`\\star` is the valid `cross-correlation`_ operator,\n",
      " |  :math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
      " |  :math:`L` is a length of signal sequence.\n",
      " |  \n",
      " |  | :attr:`stride` controls the stride for the cross-correlation, a single\n",
      " |    number or a one-element tuple.\n",
      " |  | :attr:`padding` controls the amount of implicit zero-paddings on both\n",
      " |  |  sides for :attr:`padding` number of points.\n",
      " |  | :attr:`dilation` controls the spacing between the kernel points; also\n",
      " |    known as the à trous algorithm. It is harder to describe, but this `link`_\n",
      " |    has a nice visualization of what :attr:`dilation` does.\n",
      " |  | :attr:`groups` controls the connections between inputs and outputs.\n",
      " |    `in_channels` and `out_channels` must both be divisible by `groups`.\n",
      " |  |       At groups=1, all inputs are convolved to all outputs.\n",
      " |  |       At groups=2, the operation becomes equivalent to having two conv\n",
      " |               layers side by side, each seeing half the input channels,\n",
      " |               and producing half the output channels, and both subsequently\n",
      " |               concatenated.\n",
      " |          At groups=`in_channels`, each input channel is convolved with its\n",
      " |               own set of filters (of size `out_channels // in_channels`).\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |       Depending of the size of your kernel, several (of the last)\n",
      " |       columns of the input might be lost, because it is a valid\n",
      " |       `cross-correlation`_, and not a full `cross-correlation`_.\n",
      " |       It is up to the user to add proper padding.\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |       The configuration when `groups == in_channels` and `out_channels = K * in_channels`\n",
      " |       where `K` is a positive integer is termed in literature as depthwise convolution.\n",
      " |  \n",
      " |       In other words, for an input of size :math:`(N, C_{in}, L_{in})`, if you want a\n",
      " |       depthwise convolution with a depthwise multiplier `K`,\n",
      " |       then you use the constructor arguments\n",
      " |       :math:`(in\\_channels=C_{in}, out\\_channels=C_{in} * K, ..., groups=C_{in})`\n",
      " |  \n",
      " |  Args:\n",
      " |      in_channels (int): Number of channels in the input image\n",
      " |      out_channels (int): Number of channels produced by the convolution\n",
      " |      kernel_size (int or tuple): Size of the convolving kernel\n",
      " |      stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      " |      padding (int or tuple, optional): Zero-padding added to both sides of\n",
      " |          the input. Default: 0\n",
      " |      dilation (int or tuple, optional): Spacing between kernel\n",
      " |          elements. Default: 1\n",
      " |      groups (int, optional): Number of blocked connections from input\n",
      " |          channels to output channels. Default: 1\n",
      " |      bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C_{in}, L_{in})`\n",
      " |      - Output: :math:`(N, C_{out}, L_{out})` where\n",
      " |        :math:`L_{out} = floor((L_{in}  + 2 * padding - dilation * (kernel\\_size - 1) - 1) / stride + 1)`\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight (Tensor): the learnable weights of the module of shape\n",
      " |          (out_channels, in_channels, kernel_size)\n",
      " |      bias (Tensor):   the learnable bias of the module of shape\n",
      " |          (out_channels)\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Conv1d(16, 33, 3, stride=2)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 16, 50))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  .. _cross-correlation:\n",
      " |      https://en.wikipedia.org/wiki/Cross-correlation\n",
      " |  \n",
      " |  .. _link:\n",
      " |      https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Conv1d\n",
      " |      _ConvNd\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _ConvNd:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97967017,  0.00463816],\n",
       "       [ 0.84873545,  0.50282168],\n",
       "       [ 0.38454553,  0.68619001],\n",
       "       [-0.00177162,  0.84484351],\n",
       "       [-0.40613338,  0.69851059],\n",
       "       [-0.9370811 ,  0.56263232],\n",
       "       [-0.90546513,  0.00325594],\n",
       "       [-0.76080012, -0.44539672],\n",
       "       [-0.39749897, -0.67931378],\n",
       "       [ 0.00169786, -0.92938364],\n",
       "       [ 0.49300653, -0.82771182],\n",
       "       [ 0.70441866, -0.41366461]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNorm1d in module torch.nn.modules.batchnorm:\n",
      "\n",
      "class BatchNorm1d(_BatchNorm)\n",
      " |  Applies Batch Normalization over a 2d or 3d input that is seen as a\n",
      " |  mini-batch.\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated per-dimension over\n",
      " |  the mini-batches and gamma and beta are learnable parameter vectors\n",
      " |  of size C (where C is the input size).\n",
      " |  \n",
      " |  During training, this layer keeps a running estimate of its computed mean\n",
      " |  and variance. The running sum is kept with a default momentum of 0.1.\n",
      " |  \n",
      " |  During evaluation, this running mean/variance is used for normalization.\n",
      " |  \n",
      " |  Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      " |  on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
      " |  \n",
      " |  Args:\n",
      " |      num_features: num_features from an expected input of size\n",
      " |          `batch_size x num_features [x width]`\n",
      " |      eps: a value added to the denominator for numerical stability.\n",
      " |          Default: 1e-5\n",
      " |      momentum: the value used for the running_mean and running_var\n",
      " |          computation. Default: 0.1\n",
      " |      affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      " |          affine parameters. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
      " |      - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100)\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100, affine=False)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 100))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNorm1d\n",
      " |      _BatchNorm\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _BatchNorm:\n",
      " |  \n",
      " |  __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNorm1d in module torch.nn.modules.batchnorm:\n",
      "\n",
      "class BatchNorm1d(_BatchNorm)\n",
      " |  Applies Batch Normalization over a 2d or 3d input that is seen as a\n",
      " |  mini-batch.\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated per-dimension over\n",
      " |  the mini-batches and gamma and beta are learnable parameter vectors\n",
      " |  of size C (where C is the input size).\n",
      " |  \n",
      " |  During training, this layer keeps a running estimate of its computed mean\n",
      " |  and variance. The running sum is kept with a default momentum of 0.1.\n",
      " |  \n",
      " |  During evaluation, this running mean/variance is used for normalization.\n",
      " |  \n",
      " |  Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      " |  on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
      " |  \n",
      " |  Args:\n",
      " |      num_features: num_features from an expected input of size\n",
      " |          `batch_size x num_features [x width]`\n",
      " |      eps: a value added to the denominator for numerical stability.\n",
      " |          Default: 1e-5\n",
      " |      momentum: the value used for the running_mean and running_var\n",
      " |          computation. Default: 0.1\n",
      " |      affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      " |          affine parameters. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
      " |      - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100)\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100, affine=False)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 100))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNorm1d\n",
      " |      _BatchNorm\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _BatchNorm:\n",
      " |  \n",
      " |  __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Variable(torch.FloatTensor([[[0,0,3]]]))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 24])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(in_channels=1,out_channels=2,kernel_size=2)\n",
    "x_variable=x_variable.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.2492 -1.6630\n",
       "  0.1763  0.8156\n",
       "[torch.FloatTensor of size 1x2x2]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 24])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(in_channels=1,out_channels=1,kernel_size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (CUDAFloatTensor) and weight type (CPUFloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c33b0b8d459e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 168\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0m_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (CUDAFloatTensor) and weight type (CPUFloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "m(x_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "func=nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ba008a11b8a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "func(m(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable=x_variable.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(24, 14)\n",
    "        self.fc21 = nn.Linear(14, 2)\n",
    "        self.fc22 = nn.Linear(14, 2)\n",
    "        self.fc3 = nn.Linear(2, 14)\n",
    "        self.fc4 = nn.Linear(14, 24)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "reconstruction_function = nn.BCELoss()\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08427379811604818\n",
      "0.05906171976725261\n",
      "0.042195732879638674\n",
      "0.02583740832010905\n",
      "0.012065117899576823\n",
      "0.003920155477523803\n",
      "0.0017312102317810058\n",
      "0.0017457127412160238\n",
      "0.0014478217919667562\n",
      "0.0011840258439381917\n",
      "0.0010786115248998006\n",
      "0.0009954208215077719\n",
      "0.0009241549412409465\n",
      "0.0008680975914001465\n",
      "0.0008176822662353516\n",
      "0.0007710820436477661\n",
      "0.0007279782851537069\n",
      "0.000687526289621989\n",
      "0.0006492488662401835\n",
      "0.0006128788749376932\n",
      "0.00057838667233785\n",
      "0.0005447821577390034\n",
      "0.0005132324814796448\n",
      "0.0004823383053143819\n",
      "0.00045259085496266685\n",
      "0.0004236271778742472\n",
      "0.0003961742361386617\n",
      "0.0003692155043284098\n",
      "0.00034255663951237995\n",
      "0.0003169219692548116\n",
      "0.00029175037145614624\n",
      "0.00026713016231854757\n",
      "0.00024306388894716898\n",
      "0.00021923057238260906\n",
      "0.00019581817785898844\n",
      "0.00017181203365325927\n",
      "0.00014876575271288554\n",
      "0.0001262182076772054\n",
      "0.00010323645273844401\n",
      "8.076347708702088e-05\n",
      "5.763450264930725e-05\n",
      "3.502514958381653e-05\n",
      "1.3118181626001994e-05\n",
      "-9.948378801345826e-06\n",
      "-3.226481874783834e-05\n",
      "-5.5160810550053915e-05\n",
      "-7.773837248484293e-05\n",
      "-0.00010087252259254456\n",
      "-0.0001245050589243571\n",
      "-0.00014702658255894978\n",
      "-0.00016969714959462484\n",
      "-0.00019413987596829732\n",
      "-0.00021716567675272624\n",
      "-0.00024070487419764201\n",
      "-0.0002648278892040253\n",
      "-0.00028889471093813577\n",
      "-0.0003140231390794118\n",
      "-0.000338344810406367\n",
      "-0.00036256824334462485\n",
      "-0.00038730589548746744\n",
      "-0.00041285643180211383\n",
      "-0.00043880128463109333\n",
      "-0.0004643892248471578\n",
      "-0.0004898603439331055\n",
      "-0.0005171652634938558\n",
      "-0.0005414639512697855\n",
      "-0.0005675115744272868\n",
      "-0.00059354194800059\n",
      "-0.00061952512661616\n",
      "-0.000646462603410085\n",
      "-0.000672138520081838\n",
      "-0.0006985012372334798\n",
      "-0.0007241184949874878\n",
      "-0.0007483807245890299\n",
      "-0.0007721872329711914\n",
      "-0.0007963706572850545\n",
      "-0.0008200628836949667\n",
      "-0.0008421722094217936\n",
      "-0.0008645760297775268\n",
      "-0.0008869690497716267\n",
      "-0.000906771961847941\n",
      "-0.0009258384466171264\n",
      "-0.0009429646094640096\n",
      "-0.0009616099754969279\n",
      "-0.000977546739578247\n",
      "-0.0009956653197606405\n",
      "-0.0010086373805999755\n",
      "-0.0010235898812611898\n",
      "-0.0010366857290267945\n",
      "-0.001048962680498759\n",
      "-0.0010609579881032307\n",
      "-0.0010727603594462077\n",
      "-0.0010831118106842041\n",
      "-0.001092466974258423\n",
      "-0.0011020208994547526\n",
      "-0.0011104041735331218\n",
      "-0.0011187591552734375\n",
      "-0.0011256612300872802\n",
      "-0.0011327897389729817\n",
      "-0.001139420970280965\n",
      "-0.0011452138423919678\n",
      "-0.001151212207476298\n",
      "-0.001156295649210612\n",
      "-0.0011609936793645222\n",
      "-0.0011651495377222698\n",
      "-0.0011696418444315593\n",
      "-0.0011731675068537395\n",
      "-0.0011766764958699545\n",
      "-0.0011799752871195475\n",
      "-0.0011829618215560913\n",
      "-0.001185554536183675\n",
      "-0.0011882928450902303\n",
      "-0.0011904172897338867\n",
      "-0.0011926185846328735\n",
      "-0.0011944268067677817\n",
      "-0.0011962531248728434\n",
      "-0.0011979233741760255\n",
      "-0.0011995075702667236\n",
      "-0.001200924770037333\n",
      "-0.001202187689145406\n",
      "-0.0012035684585571288\n",
      "-0.0012047712961832683\n",
      "-0.0012058857361475627\n",
      "-0.001207015617688497\n",
      "-0.0012079926490783691\n",
      "-0.0012090163866678874\n",
      "-0.001209925373395284\n",
      "-0.0012107817729314168\n",
      "-0.0012115832726160686\n",
      "-0.0012124318599700927\n",
      "-0.001213224705060323\n",
      "-0.001214000129699707\n",
      "-0.0012147220055262248\n",
      "-0.0012153810660044352\n",
      "-0.0012160804669062296\n",
      "-0.0012167289733886718\n",
      "-0.0012173409700393678\n",
      "-0.0012179548978805542\n",
      "-0.0012185983022054037\n",
      "-0.0012191402594248454\n",
      "-0.0012196829319000243\n",
      "-0.001220228640238444\n",
      "-0.0012207616647084555\n",
      "-0.0012212678988774617\n",
      "-0.0012217735131581624\n",
      "-0.00122223223845164\n",
      "-0.0012227161566416424\n",
      "-0.001223235861460368\n",
      "-0.0012236465613047283\n",
      "-0.0012240599632263184\n",
      "-0.0012245021184285481\n",
      "-0.0012249342838923137\n",
      "-0.0012252930800120037\n",
      "-0.0012257636626561482\n",
      "-0.0012260949373245238\n",
      "-0.0012264157613118489\n",
      "-0.0012268179416656494\n",
      "-0.0012272163073221844\n",
      "-0.0012275152683258056\n",
      "-0.0012278796275456746\n",
      "-0.0012282392342885336\n",
      "-0.0012285046100616456\n",
      "-0.0012288379748662313\n",
      "-0.001229147736231486\n",
      "-0.0012294867277145385\n",
      "-0.0012297534227371217\n",
      "-0.0012300935427347818\n",
      "-0.001230324912071228\n",
      "-0.0012306315342585246\n",
      "-0.001230921181042989\n",
      "-0.00123118683497111\n",
      "-0.0012314383506774901\n",
      "-0.001231650455792745\n",
      "-0.0012319421370824179\n",
      "-0.0012321635643641154\n",
      "-0.0012324432293574015\n",
      "-0.0012326678276062012\n",
      "-0.0012329065720240275\n",
      "-0.001233125376701355\n",
      "-0.001233336369196574\n",
      "-0.0012335424900054932\n",
      "-0.0012337837616602579\n",
      "-0.0012339744329452514\n",
      "-0.0012341950575510661\n",
      "-0.0012344226996103922\n",
      "-0.0012346192121505738\n",
      "-0.0012348027467727661\n",
      "-0.0012350183327992758\n",
      "-0.0012351822137832641\n",
      "-0.0012353712956110637\n",
      "-0.0012355668306350707\n",
      "-0.001235733962059021\n",
      "-0.0012359388113021852\n",
      "-0.0012360803842544556\n",
      "-0.0012362808704376221\n",
      "-0.001236451546351115\n",
      "-0.001236619488398234\n",
      "-0.0012367979288101197\n",
      "-0.001236928701400757\n",
      "-0.0012371576070785522\n",
      "-0.0012372647762298585\n",
      "-0.0012374217907587686\n",
      "-0.001237554955482483\n",
      "-0.0012377196232477823\n",
      "-0.0012378894885381063\n",
      "-0.0012380483547846477\n",
      "-0.0012381714741388956\n",
      "-0.0012383075078328451\n",
      "-0.001238483476638794\n",
      "-0.0012385749816894531\n",
      "-0.0012387377580006917\n",
      "-0.0012388960043589275\n",
      "-0.0012389691511789957\n",
      "-0.0012391486962636312\n",
      "-0.0012392761945724487\n",
      "-0.0012393794298171997\n",
      "-0.0012395119190216064\n",
      "-0.0012396649916966756\n",
      "-0.0012397494713465372\n",
      "-0.0012398880958557129\n",
      "-0.0012399969021479288\n",
      "-0.0012401222387949625\n",
      "-0.0012402298291524251\n",
      "-0.0012403820276260377\n",
      "-0.0012404666503270467\n",
      "-0.0012405905405680339\n",
      "-0.0012407252073287963\n",
      "-0.0012408049583435058\n",
      "-0.00124096941947937\n",
      "-0.0012410502036412556\n",
      "-0.0012411648750305176\n",
      "-0.0012412681579589844\n",
      "-0.00124139297803243\n",
      "-0.0012414655367533366\n",
      "-0.0012415517171223958\n",
      "-0.001241709089279175\n",
      "-0.0012417890469233195\n",
      "-0.0012419016440709432\n",
      "-0.0012420185327529907\n",
      "-0.001242108416557312\n",
      "-0.0012422093868255616\n",
      "-0.0012423048575719198\n",
      "-0.0012423940499623617\n",
      "-0.0012425136168797812\n",
      "-0.0012426137129465738\n",
      "-0.0012427076657613118\n",
      "-0.0012428005456924438\n",
      "-0.0012429101228713989\n",
      "-0.001242996350924174\n",
      "-0.0012430423418680826\n",
      "-0.0012431825717290242\n",
      "-0.0012432677666346231\n",
      "-0.0012433658043543498\n",
      "-0.0012434782981872558\n",
      "-0.0012435287157694498\n",
      "-0.0012436463673909505\n",
      "-0.0012437087138493855\n",
      "-0.001243823226292928\n",
      "-0.0012438987255096435\n",
      "-0.0012439783652623494\n",
      "-0.0012440734386444093\n",
      "-0.0012441547632217407\n",
      "-0.0012442294200261433\n",
      "-0.0012443506797154744\n",
      "-0.0012444143851598104\n",
      "-0.0012444582303365072\n",
      "-0.0012445868810017904\n",
      "-0.0012446454048156738\n",
      "-0.0012447481632232667\n",
      "-0.0012448442379633586\n",
      "-0.0012448943694432577\n",
      "-0.001245007586479187\n",
      "-0.001245080065727234\n",
      "-0.001245167318979899\n",
      "-0.001245264450709025\n",
      "-0.0012453317721684773\n",
      "-0.0012453942696253458\n",
      "-0.0012454755783081054\n",
      "-0.0012455838521321615\n",
      "-0.0012456401427586874\n",
      "-0.0012457578976949055\n",
      "-0.0012457786242167155\n",
      "-0.0012458621501922607\n",
      "-0.0012459725220998127\n",
      "-0.0012460209210713705\n",
      "-0.0012460956811904907\n",
      "-0.0012461942911148071\n",
      "-0.001246280352274577\n",
      "-0.0012463392575581868\n",
      "-0.001246423554420471\n",
      "-0.0012464807192484539\n",
      "-0.0012465386788050334\n",
      "-0.0012466282367706299\n",
      "-0.0012466727733612061\n",
      "-0.0012467590729395548\n",
      "-0.0012468356529871623\n",
      "-0.0012469284057617187\n",
      "-0.0012470382610956828\n",
      "-0.0012470735788345337\n",
      "-0.001247126038869222\n",
      "-0.0012472362279891968\n",
      "-0.0012472904920578004\n",
      "-0.0012473863124847413\n",
      "-0.0012474538803100585\n",
      "-0.0012475524425506592\n",
      "-0.0012475937604904176\n",
      "-0.001247694993019104\n",
      "-0.0012477683067321776\n",
      "-0.0012478516578674317\n",
      "-0.0012478532552719116\n",
      "-0.0012479936997095743\n",
      "-0.001248007567723592\n",
      "-0.0012481063604354858\n",
      "-0.0012481571594874063\n",
      "-0.0012482614596684774\n",
      "-0.0012483050028483072\n",
      "-0.0012484041929244996\n",
      "-0.0012484810829162599\n",
      "-0.001248557432492574\n",
      "-0.0012486231883366903\n",
      "-0.0012486682891845703\n",
      "-0.0012487359523773193\n",
      "-0.0012488667090733846\n",
      "-0.001248843820889791\n",
      "-0.001248946777979533\n",
      "-0.001248987913131714\n",
      "-0.001249081301689148\n",
      "-0.0012491668462753297\n",
      "-0.0012492084741592406\n",
      "-0.0012492985010147096\n",
      "-0.0012493875900904338\n",
      "-0.0012494199434916178\n",
      "-0.0012495031674702962\n",
      "-0.001249535576502482\n",
      "-0.0012496412595113119\n",
      "-0.0012497000376383463\n",
      "-0.0012497437477111816\n",
      "-0.0012498330911000569\n",
      "-0.0012498919248580933\n",
      "-0.0012499701182047526\n",
      "-0.0012500519752502441\n",
      "-0.0012501142183939616\n",
      "-0.0012501633723576865\n",
      "-0.0012502117872238159\n",
      "-0.0012502467393875122\n",
      "-0.0012503542025883991\n",
      "-0.0012504365046819052\n",
      "-0.0012504802942276002\n",
      "-0.0012505703767140706\n",
      "-0.0012506165186564127\n",
      "-0.0012506740729014078\n",
      "-0.0012507869164148967\n",
      "-0.0012508083661397298\n",
      "-0.0012509148041407268\n",
      "-0.001250914478302002\n",
      "-0.0012509743928909303\n",
      "-0.0012510811726252238\n",
      "-0.001251150870323181\n",
      "-0.0012511801958084107\n",
      "-0.0012512809673945109\n",
      "-0.0012513704856236775\n",
      "-0.0012514143864313762\n",
      "-0.0012514701684316\n",
      "-0.0012515354951222737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012515852053960165\n",
      "-0.0012516491492589315\n",
      "-0.0012517378568649292\n",
      "-0.0012517849524815877\n",
      "-0.0012518394072850545\n",
      "-0.001251940401395162\n",
      "-0.0012519787073135375\n",
      "-0.0012520352840423583\n",
      "-0.0012521003246307374\n",
      "-0.0012521643002827963\n",
      "-0.0012522292455037435\n",
      "-0.001252248771985372\n",
      "-0.001252344552675883\n",
      "-0.001252402949333191\n",
      "-0.0012524743239084879\n",
      "-0.00125256343682607\n",
      "-0.0012526012182235717\n",
      "-0.001252672553062439\n",
      "-0.001252714236577352\n",
      "-0.0012528040409088134\n",
      "-0.0012528634309768676\n",
      "-0.001252859934171041\n",
      "-0.0012529809554417928\n",
      "-0.0012529974301656087\n",
      "-0.0012530914862950643\n",
      "-0.0012531592766443888\n",
      "-0.0012532113472620645\n",
      "-0.0012532693703969319\n",
      "-0.0012533439874649048\n",
      "-0.0012533462444941203\n",
      "-0.0012534990946451822\n",
      "-0.0012535181522369385\n",
      "-0.0012535861571629841\n",
      "-0.0012536409457524617\n",
      "-0.0012537391185760498\n",
      "-0.0012537651062011718\n",
      "-0.0012538124561309815\n",
      "-0.001253877862294515\n",
      "-0.0012539216041564942\n",
      "-0.0012539740800857544\n",
      "-0.0012540358622868855\n",
      "-0.0012541396220525107\n",
      "-0.001254183602333069\n",
      "-0.001254201873143514\n",
      "-0.0012543163061141968\n",
      "-0.001254315416018168\n",
      "-0.001254396645228068\n",
      "-0.0012544441382090251\n",
      "-0.001254478136698405\n",
      "-0.001254572566350301\n",
      "-0.001254589581489563\n",
      "-0.0012546985546747842\n",
      "-0.0012547329902648926\n",
      "-0.0012548084576924643\n",
      "-0.0012548912366231282\n",
      "-0.001254892404874166\n",
      "-0.0012549559195836385\n",
      "-0.0012550140857696534\n",
      "-0.001255047877629598\n",
      "-0.0012550638119379679\n",
      "-0.0012551714340845743\n",
      "-0.0012552200078964233\n",
      "-0.0012552645444869996\n",
      "-0.0012552829424540201\n",
      "-0.0012553702036539713\n",
      "-0.0012554354747136435\n",
      "-0.0012555009206136067\n",
      "-0.0012555327812830608\n",
      "-0.0012556129455566407\n",
      "-0.0012556322733561199\n",
      "-0.0012556998729705811\n",
      "-0.0012557320435841878\n",
      "-0.0012558189551035563\n",
      "-0.0012558171033859253\n",
      "-0.0012558948357899984\n",
      "-0.001255942718187968\n",
      "-0.0012559601306915283\n",
      "-0.0012560522556304932\n",
      "-0.0012561034679412842\n",
      "-0.0012561666250228882\n",
      "-0.0012562058369318643\n",
      "-0.001256257685025533\n",
      "-0.001256278657913208\n",
      "-0.0012563753445943196\n",
      "-0.0012564125219980875\n",
      "-0.0012564541737238566\n",
      "-0.0012565122524897257\n",
      "-0.001256526811917623\n",
      "-0.0012566028356552125\n",
      "-0.0012566625038782754\n",
      "-0.001256672724088033\n",
      "-0.0012567298730214436\n",
      "-0.0012567548990249633\n",
      "-0.0012568252642949423\n",
      "-0.001256902281443278\n",
      "-0.0012569178183873494\n",
      "-0.0012569779793421428\n",
      "-0.001257033658027649\n",
      "-0.001257071844736735\n",
      "-0.0012570900599161783\n",
      "-0.0012571307023366291\n",
      "-0.0012571793874104817\n",
      "-0.001257226045926412\n",
      "-0.0012572858174641926\n",
      "-0.001257304294904073\n",
      "-0.001257357954978943\n",
      "-0.0012573967933654786\n",
      "-0.0012574463764826456\n",
      "-0.0012574628591537475\n",
      "-0.0012575455904006959\n",
      "-0.0012575699249903361\n",
      "-0.0012576123873392741\n",
      "-0.0012576103528340657\n",
      "-0.0012576875925064087\n",
      "-0.0012577220678329467\n",
      "-0.0012577943483988445\n",
      "-0.0012578356345494589\n",
      "-0.0012578316211700438\n",
      "-0.0012579135338465373\n",
      "-0.0012579407930374145\n",
      "-0.0012579456726710002\n",
      "-0.0012580350399017334\n",
      "-0.0012580711841583252\n",
      "-0.0012580925226211548\n",
      "-0.0012580955743789672\n",
      "-0.001258169945081075\n",
      "-0.0012581488291422526\n",
      "-0.0012582358439763388\n",
      "-0.0012582788546880086\n",
      "-0.0012582778056462606\n",
      "-0.0012583336114883423\n",
      "-0.0012583622137705484\n",
      "-0.0012584272066752116\n",
      "-0.0012584436178207398\n",
      "-0.0012584814548492432\n",
      "-0.0012585025548934937\n",
      "-0.0012585318883260091\n",
      "-0.0012585981289545696\n",
      "-0.0012585924863815308\n",
      "-0.0012586486180623372\n",
      "-0.0012586474180221558\n",
      "-0.0012586661577224732\n",
      "-0.001258730991681417\n",
      "-0.0012587418794631957\n",
      "-0.0012587705135345458\n",
      "-0.0012587881326675415\n",
      "-0.0012588635842005413\n",
      "-0.00125879062016805\n",
      "-0.0012588617960611979\n",
      "-0.0012589748620986938\n",
      "-0.001258957266807556\n",
      "-0.0012589823484420776\n",
      "-0.0012588920434315999\n",
      "-0.0012589467763900756\n",
      "-0.0012590702613194784\n",
      "-0.0012591113567352295\n",
      "-0.0012591260671615601\n",
      "-0.001259171716372172\n",
      "-0.00125920250415802\n",
      "-0.0012592170397440591\n",
      "-0.0012591983954111734\n",
      "-0.0012591799656550089\n",
      "-0.001259282914797465\n",
      "-0.0012593206882476806\n",
      "-0.001259374221165975\n",
      "-0.0012593659083048503\n",
      "-0.0012593790928522746\n",
      "-0.001258986496925354\n",
      "-0.001259317390124003\n",
      "-0.0012594175418217976\n",
      "-0.0012594813585281373\n",
      "-0.0012594972848892212\n",
      "-0.001259526522954305\n",
      "-0.0012595270077387492\n",
      "-0.0012595791578292846\n",
      "-0.0012596012512842813\n",
      "-0.0012596403042475382\n",
      "-0.0012596331119537354\n",
      "-0.0012596508105595908\n",
      "-0.001259625236193339\n",
      "-0.001259367283185323\n",
      "-0.001259652098019918\n",
      "-0.0012597532033920289\n",
      "-0.001259753934542338\n",
      "-0.0012597647905349732\n",
      "-0.001259799599647522\n",
      "-0.0012598477125167846\n",
      "-0.0012598567962646484\n",
      "-0.0012598876555760701\n",
      "-0.001259888219833374\n",
      "-0.0012599019368489583\n",
      "-0.001259747854868571\n",
      "-0.0012592697302500407\n",
      "-0.001259768009185791\n",
      "-0.0012599406719207764\n",
      "-0.0012599160432815552\n",
      "-0.001260000745455424\n",
      "-0.0012600125551223754\n",
      "-0.0012600631793340047\n",
      "-0.0012600241502126058\n",
      "-0.0012600638389587402\n",
      "-0.00126007186571757\n",
      "-0.001260101858774821\n",
      "-0.0012601098537445069\n",
      "-0.001260142421722412\n",
      "-0.0012601338227589925\n",
      "-0.0012601529439290366\n",
      "-0.0012601710637410483\n",
      "-0.0012601814190546672\n",
      "-0.001260197138786316\n",
      "-0.0012601953903834024\n",
      "-0.0012601422389348347\n",
      "-0.0012589783906936645\n",
      "-0.0012598117192586264\n",
      "-0.0012601711670557657\n",
      "-0.0012602538108825684\n",
      "-0.0012602849006652832\n",
      "-0.001260312255223592\n",
      "-0.0012603152831395467\n",
      "-0.0012603230635325113\n",
      "-0.0012603785117467244\n",
      "-0.001260386315981547\n",
      "-0.0012603911797205607\n",
      "-0.0012603941122690838\n",
      "-0.0012604053417841594\n",
      "-0.0012603979508082072\n",
      "-0.0012604052384694417\n",
      "-0.001260458485285441\n",
      "-0.0012604389508565268\n",
      "-0.0012604524374008178\n",
      "-0.0012604853709538778\n",
      "-0.0012604415814081828\n",
      "-0.0012604843457539876\n",
      "-0.001260497283935547\n",
      "-0.0012605131228764852\n",
      "-0.001260502862930298\n",
      "-0.0012605276981989542\n",
      "-0.001259797469774882\n",
      "-0.0012600415229797363\n",
      "-0.0012604388316472372\n",
      "-0.0012604888916015625\n",
      "-0.0012605205059051513\n",
      "-0.0012605817159016927\n",
      "-0.0012605940421422323\n",
      "-0.001260616914431254\n",
      "-0.0012606146971384684\n",
      "-0.0012606562852859498\n",
      "-0.0012606306870778401\n",
      "-0.0012606499354044596\n",
      "-0.0012606429020563762\n",
      "-0.0012606703122456867\n",
      "-0.001260668436686198\n",
      "-0.0012606672604878744\n",
      "-0.0012606884161631266\n",
      "-0.0012605853796005249\n",
      "-0.0012597787857055664\n",
      "-0.0012604778130849202\n",
      "-0.001260589114824931\n",
      "-0.0012606703678766887\n",
      "-0.0012607181628545125\n",
      "-0.001260724727312724\n",
      "-0.001260731037457784\n",
      "-0.0012607531944910685\n",
      "-0.0012607517639795938\n",
      "-0.0012607391675313313\n",
      "-0.0012607406775156657\n",
      "-0.0012608215729395548\n",
      "-0.0012607711553573608\n",
      "-0.001260785969098409\n",
      "-0.0012608130931854248\n",
      "-0.0012607850392659505\n",
      "-0.0012599170684814453\n",
      "-0.0012602992137273152\n",
      "-0.0012606984694798788\n",
      "-0.0012607420206069947\n",
      "-0.0012608150164286295\n",
      "-0.0012608271916707357\n",
      "-0.001260811177889506\n",
      "-0.0012608157475789388\n",
      "-0.0012608530282974242\n",
      "-0.001260880208015442\n",
      "-0.001260848037401835\n",
      "-0.0012608764251073202\n",
      "-0.0012608848651250203\n",
      "-0.0012608940442403157\n",
      "-0.0012608816623687743\n",
      "-0.001260902778307597\n",
      "-0.0012608998934427898\n",
      "-0.0012605419317881267\n",
      "-0.0012601253430048625\n",
      "-0.0012606746594111126\n",
      "-0.0012608273347218832\n",
      "-0.00126089559396108\n",
      "-0.001260927907625834\n",
      "-0.0012609430074691772\n",
      "-0.001260946544011434\n",
      "-0.0012609424670537313\n",
      "-0.001260936427116394\n",
      "-0.001260931412378947\n",
      "-0.00126095023949941\n",
      "-0.0012609535853068034\n",
      "-0.00126096826394399\n",
      "-0.0012609445730845134\n",
      "-0.00126045028368632\n",
      "-0.0012603166739145915\n",
      "-0.0012607593139012654\n",
      "-0.0012609209378560385\n",
      "-0.0012609650532404582\n",
      "-0.0012609640836715697\n",
      "-0.0012609668811162313\n",
      "-0.0012610003391901653\n",
      "-0.0012609975735346475\n",
      "-0.0012609878778457643\n",
      "-0.0012609899759292603\n",
      "-0.0012609922488530476\n",
      "-0.001261006212234497\n",
      "-0.0012609516302744548\n",
      "-0.0012603862524032594\n",
      "-0.0012606349388758341\n",
      "-0.0012609005769093832\n",
      "-0.001260973048210144\n",
      "-0.0012610177993774413\n",
      "-0.0012610350131988526\n",
      "-0.0012610208749771119\n",
      "-0.0012610317468643188\n",
      "-0.0012610722621281942\n",
      "-0.0012610635995864868\n",
      "-0.0012610604524612428\n",
      "-0.0012610374450683593\n",
      "-0.0012604126532872518\n",
      "-0.0012605784257253012\n",
      "-0.00126090513865153\n",
      "-0.0012610188086827597\n",
      "-0.0012610487778981527\n",
      "-0.0012611175855000813\n",
      "-0.0012610928694407144\n",
      "-0.0012610666116078695\n",
      "-0.0012610939979553224\n",
      "-0.0012610979000727337\n",
      "-0.0012610749085744221\n",
      "-0.0012610795974731445\n",
      "-0.0012609601736068726\n",
      "-0.0012603428920110067\n",
      "-0.0012609427611033122\n",
      "-0.0012610164006551107\n",
      "-0.0012610655625661215\n",
      "-0.0012610996882120769\n",
      "-0.001261098829905192\n",
      "-0.0012611077547073364\n",
      "-0.0012610937595367432\n",
      "-0.0012611122131347657\n",
      "-0.0012610753854115804\n",
      "-0.0012607493003209433\n",
      "-0.0012606952587763467\n",
      "-0.0012609874804814657\n",
      "-0.001261077086130778\n",
      "-0.001261129101117452\n",
      "-0.0012611283938090006\n",
      "-0.0012611280838648478\n",
      "-0.0012611411094665527\n",
      "-0.001261091717084249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001260905392964681\n",
      "-0.001260707139968872\n",
      "-0.0012610512653986614\n",
      "-0.0012611152410507201\n",
      "-0.0012611063559850057\n",
      "-0.0012611244678497314\n",
      "-0.0012611426671346028\n",
      "-0.0012611341794331868\n",
      "-0.0012611191511154175\n",
      "-0.0012605311632156373\n",
      "-0.0012606348911921184\n",
      "-0.0012609764337539672\n",
      "-0.0012611244837443035\n",
      "-0.0012611381928126018\n",
      "-0.0012611489216486614\n",
      "-0.0012611704508463542\n",
      "-0.0012611680269241333\n",
      "-0.001261158299446106\n",
      "-0.0012611669858296711\n",
      "-0.0012611825704574584\n",
      "-0.001261122194925944\n",
      "-0.0012606677214304605\n",
      "-0.0012608527898788453\n",
      "-0.0012610427618026734\n",
      "-0.001261119786898295\n",
      "-0.0012611679633458455\n",
      "-0.0012611983140309651\n",
      "-0.001261160675684611\n",
      "-0.0012611855824788412\n",
      "-0.0012611587603886922\n",
      "-0.0012609032074610393\n",
      "-0.0012607102553049724\n",
      "-0.0012610742966334026\n",
      "-0.0012611283699671427\n",
      "-0.0012611643075942994\n",
      "-0.0012612041234970092\n",
      "-0.001261186401049296\n",
      "-0.0012611685037612916\n",
      "-0.0012610745747884114\n",
      "-0.0012607531706492106\n",
      "-0.001261076283454895\n",
      "-0.0012611297448476156\n",
      "-0.0012611671288808187\n",
      "-0.0012611610968907673\n",
      "-0.0012611966848373414\n",
      "-0.001261195429166158\n",
      "-0.0012611932277679443\n",
      "-0.001260905933380127\n",
      "-0.0012602036476135254\n",
      "-0.001260919745763143\n",
      "-0.0012610986550649008\n",
      "-0.0012611577192942302\n",
      "-0.001261181942621867\n",
      "-0.0012612120469411214\n",
      "-0.0012612243175506592\n",
      "-0.001261231811841329\n",
      "-0.0012612200578053792\n",
      "-0.0012612329800923666\n",
      "-0.0012612041473388672\n",
      "-0.001261214820543925\n",
      "-0.0012611705859502157\n",
      "-0.001260771075884501\n",
      "-0.0012608955224355063\n",
      "-0.0012610978603363038\n",
      "-0.0012611739238103231\n",
      "-0.001261228624979655\n",
      "-0.001261218293507894\n",
      "-0.0012612144947052003\n",
      "-0.001261209193865458\n",
      "-0.0012610265811284383\n",
      "-0.0012607395728429158\n",
      "-0.0012611093362172444\n",
      "-0.0012611615657806396\n",
      "-0.0012611799478530884\n",
      "-0.0012612118005752564\n",
      "-0.001261230476697286\n",
      "-0.0012612367312113445\n",
      "-0.001261228632926941\n",
      "-0.0012606244723002116\n",
      "-0.0012603849649429322\n",
      "-0.0012609748840332032\n",
      "-0.0012611801862716676\n",
      "-0.0012611860116322835\n",
      "-0.0012612212260564167\n",
      "-0.0012611578543980917\n",
      "-0.0012612250169118245\n",
      "-0.0012612532059351604\n",
      "-0.001261239473025004\n",
      "-0.0012612417777379354\n",
      "-0.0012612505515416463\n",
      "-0.0012612548033396403\n",
      "-0.001261236834526062\n",
      "-0.0012610223134358724\n",
      "-0.0012606352885564168\n",
      "-0.001261126923561096\n",
      "-0.0012612087488174438\n",
      "-0.0012612027327219646\n",
      "-0.0012612300872802734\n",
      "-0.0012612597624460857\n",
      "-0.0012612420161565146\n",
      "-0.0012611250003178914\n",
      "-0.0012607310771942139\n",
      "-0.0012611062208811442\n",
      "-0.0012611478646596273\n",
      "-0.0012611738681793214\n",
      "-0.0012612463394800822\n",
      "-0.001261258856455485\n",
      "-0.001261245568593343\n",
      "-0.0012612429300944011\n",
      "-0.0012608940124511719\n",
      "-0.001260236692428589\n",
      "-0.0012609666347503663\n",
      "-0.001261172890663147\n",
      "-0.0012612473090489705\n",
      "-0.001261256504058838\n",
      "-0.0012612640301386516\n",
      "-0.0012612483819325765\n",
      "-0.0012612582127253214\n",
      "-0.0012612501780192057\n",
      "-0.0012612771034240724\n",
      "-0.0012612648089726766\n",
      "-0.001261215392748515\n",
      "-0.0012609100341796876\n",
      "-0.0012607671817143758\n",
      "-0.001261134910583496\n",
      "-0.001261203908920288\n",
      "-0.0012612422943115234\n",
      "-0.001261263863245646\n",
      "-0.0012612873474756876\n",
      "-0.0012612237930297852\n",
      "-0.0012610668420791626\n",
      "-0.0012607242822647094\n",
      "-0.0012611512263615925\n",
      "-0.0012612082004547118\n",
      "-0.001261219064394633\n",
      "-0.0012612413009007771\n",
      "-0.0012612819592158\n",
      "-0.001261265778541565\n",
      "-0.0012612579266230266\n",
      "-0.0012610921223958334\n",
      "-0.0012599446773529054\n",
      "-0.0012609074751536052\n",
      "-0.0012611197789510092\n",
      "-0.0012612379630406697\n",
      "-0.0012612628777821859\n",
      "-0.0012612922112147013\n",
      "-0.0012612716754277547\n",
      "-0.0012612568219502766\n",
      "-0.0012612574497858683\n",
      "-0.001261140775680542\n",
      "-0.0012612417856852213\n",
      "-0.0012612837235132853\n",
      "-0.0012612668673197428\n",
      "-0.0012612711747487387\n",
      "-0.001261154596010844\n",
      "-0.0012607714970906576\n",
      "-0.0012611602306365967\n",
      "-0.0012612012227376302\n",
      "-0.0012612302700678508\n",
      "-0.0012612238168716431\n",
      "-0.0012612738370895386\n",
      "-0.0012612778902053833\n",
      "-0.0012612862586975097\n",
      "-0.0012612675031026205\n",
      "-0.001260923926035563\n",
      "-0.0012597490549087524\n",
      "-0.001260753877957662\n",
      "-0.0012611506621042887\n",
      "-0.0012612340211868285\n",
      "-0.0012612648646036784\n",
      "-0.0012612751642862956\n",
      "-0.001261281156539917\n",
      "-0.0012612836678822835\n",
      "-0.0012612813949584962\n",
      "-0.0012612773180007936\n",
      "-0.001261269481976827\n",
      "-0.0012612799485524496\n",
      "-0.0012612846851348877\n",
      "-0.0012612174113591512\n",
      "-0.0012612331787745159\n",
      "-0.0012612590710322062\n",
      "-0.0012612205505371094\n",
      "-0.0012609171628952026\n",
      "-0.001260977554321289\n",
      "-0.0012611730893452961\n",
      "-0.0012612786769866944\n",
      "-0.001261210521062215\n",
      "-0.0012612728118896485\n",
      "-0.0012612844387690225\n",
      "-0.0012612783114115398\n",
      "-0.00126130424340566\n",
      "-0.0012612082004547118\n",
      "-0.0012596346696217855\n",
      "-0.0012605950435002646\n",
      "-0.0012610313653945923\n",
      "-0.0012612189610799155\n",
      "-0.0012612601280212403\n",
      "-0.0012612870772679646\n",
      "-0.0012612842241923013\n",
      "-0.0012612939834594726\n",
      "-0.0012612842877705891\n",
      "-0.0012612876812616984\n",
      "-0.0012612765073776245\n",
      "-0.0012612845420837402\n",
      "-0.0012612498919169109\n",
      "-0.0012612009207407634\n",
      "-0.0012612637599309286\n",
      "-0.0012612845897674561\n",
      "-0.00126129523118337\n",
      "-0.0012612544059753417\n",
      "-0.0012611009041468303\n",
      "-0.0012607525428136189\n",
      "-0.0012611966768900553\n",
      "-0.0012612391153971354\n",
      "-0.0012612419684727987\n",
      "-0.0012612720012664795\n",
      "-0.0012612892707188925\n",
      "-0.001261283016204834\n",
      "-0.0012612943172454834\n",
      "-0.0012610191106796264\n",
      "-0.0012602014223734538\n",
      "-0.0012609954913457234\n",
      "-0.0012611764748891195\n",
      "-0.0012612573464711508\n",
      "-0.0012613002300262452\n",
      "-0.0012612864176432292\n",
      "-0.001261312206586202\n",
      "-0.0012613047281901041\n",
      "-0.0012612964073816936\n",
      "-0.0012612971782684326\n",
      "-0.0012612892866134644\n",
      "-0.0012612485726674397\n",
      "-0.0012608140389124551\n",
      "-0.00126088019212087\n",
      "-0.0012611518144607543\n",
      "-0.0012612321376800537\n",
      "-0.001261292028427124\n",
      "-0.0012612884680430095\n",
      "-0.0012612955729166667\n",
      "-0.0012612404425938924\n",
      "-0.0012610470453898112\n",
      "-0.0012609878857930501\n",
      "-0.0012612714131673176\n",
      "-0.0012612204472223917\n",
      "-0.0012612009604771933\n",
      "-0.0012612359603246053\n",
      "-0.0012610722700754802\n",
      "-0.0012608089923858642\n",
      "-0.0012612149000167847\n",
      "-0.0012612346728642782\n",
      "-0.0012612337827682496\n",
      "-0.0012612539211908976\n",
      "-0.0012612951119740803\n",
      "-0.0012613061825434368\n",
      "-0.0012612926721572875\n",
      "-0.0012610231955846151\n",
      "-0.0012600234349568686\n",
      "-0.0012609269618988038\n",
      "-0.0012611570994059244\n",
      "-0.0012612685521443685\n",
      "-0.001261284875869751\n",
      "-0.0012612913290659586\n",
      "-0.0012612863381703694\n",
      "-0.0012612939993540445\n",
      "-0.0012612962007522582\n",
      "-0.001261310577392578\n",
      "-0.0012612907965977988\n",
      "-0.0012612934350967407\n",
      "-0.0012613085985183716\n",
      "-0.001261134696006775\n",
      "-0.0012605045874913533\n",
      "-0.001261104933420817\n",
      "-0.001261220105489095\n",
      "-0.001261259158452352\n",
      "-0.001261288102467855\n",
      "-0.0012612824122111002\n",
      "-0.0012612801631291708\n",
      "-0.0012612934271494548\n",
      "-0.0012613108158111572\n",
      "-0.0012612390279769898\n",
      "-0.0012603742043177286\n",
      "-0.0012608360608418782\n",
      "-0.0012611307144165039\n",
      "-0.0012612338701883951\n",
      "-0.0012612810055414837\n",
      "-0.0012612778584162395\n",
      "-0.0012613075097401938\n",
      "-0.00126129363377889\n",
      "-0.0012613176822662353\n",
      "-0.0012613049745559693\n",
      "-0.0012612936019897462\n",
      "-0.0012611925363540648\n",
      "-0.0012605053742726644\n",
      "-0.0012610619544982911\n",
      "-0.0012611964384714762\n",
      "-0.001261246116956075\n",
      "-0.0012612800280253093\n",
      "-0.0012613049189249674\n",
      "-0.001261310601234436\n",
      "-0.001261297845840454\n",
      "-0.0012613072633743286\n",
      "-0.0012612919092178344\n",
      "-0.0012610241333643596\n",
      "-0.0012600997845331828\n",
      "-0.0012609840949376423\n",
      "-0.001261163862546285\n",
      "-0.0012612597544987996\n",
      "-0.0012613030274709065\n",
      "-0.0012613053083419799\n",
      "-0.0012612895091374716\n",
      "-0.0012613194306691487\n",
      "-0.0012613187551498413\n",
      "-0.0012613072315851848\n",
      "-0.0012612923383712768\n",
      "-0.0012612931887308757\n",
      "-0.0012612502892812092\n",
      "-0.0012607303619384766\n",
      "-0.0012609317620595297\n",
      "-0.001261146887143453\n",
      "-0.001261224635442098\n",
      "-0.0012612957795461019\n",
      "-0.001261289930343628\n",
      "-0.0012612735827763875\n",
      "-0.0012612937053044636\n",
      "-0.0012612701813379922\n",
      "-0.0012610979795455932\n",
      "-0.0012606767098108926\n",
      "-0.0012611787716547648\n",
      "-0.0012612277746200562\n",
      "-0.0012612746953964233\n",
      "-0.001261291734377543\n",
      "-0.0012612890323003133\n",
      "-0.0012613102595011394\n",
      "-0.0012611918767293294\n",
      "-0.0012607764800389607\n",
      "-0.0012611133654912314\n",
      "-0.001261188824971517\n",
      "-0.001261255407333374\n",
      "-0.0012612874587376913\n",
      "-0.001261306921641032\n",
      "-0.0012612836281458537\n",
      "-0.001261309544245402\n",
      "-0.001261140775680542\n",
      "-0.0012602448066075644\n",
      "-0.0012610181252161662\n",
      "-0.0012612033923467\n",
      "-0.0012612660725911458\n",
      "-0.0012612876892089844\n",
      "-0.0012613088607788085\n",
      "-0.0012613112370173137\n",
      "-0.0012613070964813233\n",
      "-0.0012613170782725016\n",
      "-0.0012612974246342978\n",
      "-0.0012611634333928427\n",
      "-0.0012610983530680338\n",
      "-0.00126078675587972\n",
      "-0.0012612258911132812\n",
      "-0.0012612353801727295\n",
      "-0.0012612585544586183\n",
      "-0.0012612788597742718\n",
      "-0.0012613097667694092\n",
      "-0.0012612845500310263\n",
      "-0.0012612536907196045\n",
      "-0.0012607269207636515\n",
      "-0.0012608058214187623\n",
      "-0.0012611128966013591\n",
      "-0.0012612457434336343\n",
      "-0.0012612795193990072\n",
      "-0.0012613061904907226\n",
      "-0.001261296033859253\n",
      "-0.001261315123240153\n",
      "-0.0012613049666086833\n",
      "-0.001261256202061971\n",
      "-0.0012610207955042522\n",
      "-0.001260685642560323\n",
      "-0.001261139678955078\n",
      "-0.001261234990755717\n",
      "-0.001261286155382792\n",
      "-0.0012612888892491658\n",
      "-0.0012612949927647908\n",
      "-0.0012612617333730063\n",
      "-0.0012612658818562826\n",
      "-0.0012611179987589518\n",
      "-0.0012608616352081299\n",
      "-0.0012612433751424154\n",
      "-0.001261206881205241\n",
      "-0.0012613155762354533\n",
      "-0.0012612513542175293\n",
      "-0.0012610725879669189\n",
      "-0.0012610313415527344\n",
      "-0.0012612770080566405\n",
      "-0.0012611806392669677\n",
      "-0.00126123472849528\n",
      "-0.0012612587531407673\n",
      "-0.001261049214998881\n",
      "-0.001260746971766154\n",
      "-0.001261193577448527\n",
      "-0.0012612579266230266\n",
      "-0.0012612594525019329\n",
      "-0.0012612969716389974\n",
      "-0.0012613173166910808\n",
      "-0.0012612961848576863\n",
      "-0.0012611980199813842\n",
      "-0.0012606989224751791\n",
      "-0.0012610321124394736\n",
      "-0.001261177388827006\n",
      "-0.0012612486441930136\n",
      "-0.0012613024552663168\n",
      "-0.0012613094727198283\n",
      "-0.0012612988392512003\n",
      "-0.0012612933715184529\n",
      "-0.0012612525145212809\n",
      "-0.0012609758297602335\n",
      "-0.0012607122898101806\n",
      "-0.001261153523127238\n",
      "-0.001261239473025004\n",
      "-0.0012612931569417317\n",
      "-0.0012613051335016885\n",
      "-0.0012613001585006715\n",
      "-0.0012613049745559693\n",
      "-0.0012612571080525717\n",
      "-0.0012610086520512898\n",
      "-0.0012608530044555665\n",
      "-0.0012612088521321615\n",
      "-0.001261258053779602\n",
      "-0.001261242945988973\n",
      "-0.0012613049904505412\n",
      "-0.0012613117218017579\n",
      "-0.001261288809776306\n",
      "-0.001261160628000895\n",
      "-0.001260427991549174\n",
      "-0.0012610633293787638\n",
      "-0.0012612023750940959\n",
      "-0.001261274472872416\n",
      "-0.0012612964073816936\n",
      "-0.00126130424340566\n",
      "-0.0012613094727198283\n",
      "-0.00126130797068278\n",
      "-0.0012613150199254355\n",
      "-0.0012612746715545654\n",
      "-0.0012611118872960408\n",
      "-0.0012604259411493938\n",
      "-0.0012609208742777507\n",
      "-0.00126116357644399\n",
      "-0.001261269172032674\n",
      "-0.0012613003412882487\n",
      "-0.0012613135655721028\n",
      "-0.0012613134860992431\n",
      "-0.0012613139629364014\n",
      "-0.0012613242626190186\n",
      "-0.0012612972815831502\n",
      "-0.0012613007307052612\n",
      "-0.0012612399260203044\n",
      "-0.0012603587071100871\n",
      "-0.0012609107573827109\n",
      "-0.0012611413876215617\n",
      "-0.0012612532059351604\n",
      "-0.0012613205273946126\n",
      "-0.0012613096555074057\n",
      "-0.0012613134463628133\n",
      "-0.0012613147735595703\n",
      "-0.0012613167683283488\n",
      "-0.0012613056262334187\n",
      "-0.0012613067388534546\n",
      "-0.0012612306435902914\n",
      "-0.0012605105400085449\n",
      "-0.001260953195889791\n",
      "-0.0012611510594685872\n",
      "-0.0012612532695134482\n",
      "-0.0012612834850947061\n",
      "-0.001261304235458374\n",
      "-0.001261326543490092\n",
      "-0.001261307724316915\n",
      "-0.0012613239765167236\n",
      "-0.0012612788359324138\n",
      "-0.0012610958814620972\n",
      "-0.001260692803064982\n",
      "-0.0012611295859018962\n",
      "-0.0012612438360850016\n",
      "-0.0012612519184748332\n",
      "-0.00126129359404246\n",
      "-0.001261311904589335\n",
      "-0.0012612848043441773\n",
      "-0.001261254628499349\n",
      "-0.001261121988296509\n",
      "-0.001260902460416158\n",
      "-0.001261176331837972\n",
      "-0.0012612382332483926\n",
      "-0.0012612906297047933\n",
      "-0.0012612807432810466\n",
      "-0.0012611976464589436\n",
      "-0.0012610270420710247\n",
      "-0.0012611961603164674\n",
      "-0.0012612819592158\n",
      "-0.001261280878384908\n",
      "-0.0012613015810648601\n",
      "-0.0012612141132354736\n",
      "-0.001260486658414205\n",
      "-0.001260889188448588\n",
      "-0.00126113068262736\n",
      "-0.0012612504482269287\n",
      "-0.0012613002061843872\n",
      "-0.001261303146680196\n",
      "-0.0012613098541895548\n",
      "-0.0012613163153330485\n",
      "-0.0012613251447677611\n",
      "-0.001261298934618632\n",
      "-0.0012613144636154175\n",
      "-0.0012610298951466878\n",
      "-0.001260343352953593\n",
      "-0.0012610681136449179\n",
      "-0.0012612035274505615\n",
      "-0.001261265468597412\n",
      "-0.0012613027254740397\n",
      "-0.0012613142331441244\n",
      "-0.0012613121906916301\n",
      "-0.001261328069368998\n",
      "-0.0012613021691640217\n",
      "-0.0012613130966822306\n",
      "-0.0012613076607386271\n",
      "-0.0012610968510309854\n",
      "-0.0012603516896565756\n",
      "-0.001261023211479187\n",
      "-0.0012611856937408447\n",
      "-0.0012612879037857056\n",
      "-0.0012613041877746582\n",
      "-0.0012613017002741495\n",
      "-0.001261320439974467\n",
      "-0.0012613175789515177\n",
      "-0.0012613072554270427\n",
      "-0.0012611688693364462\n",
      "-0.0012612839778264363\n",
      "-0.0012613194227218628\n",
      "-0.0012613155364990234\n",
      "-0.0012610472361246744\n",
      "-0.0012598597208658853\n",
      "-0.0012608057737350463\n",
      "-0.0012611905813217164\n",
      "-0.001261256750424703\n",
      "-0.001261305562655131\n",
      "-0.0012613140106201171\n",
      "-0.0012613384405771892\n",
      "-0.0012613116900126139\n",
      "-0.0012613167842229207\n",
      "-0.0012613202333450317\n",
      "-0.0012613149642944335\n",
      "-0.0012613125085830688\n",
      "-0.0012611541350682576\n",
      "-0.001261224643389384\n",
      "-0.001261299220720927\n",
      "-0.0012613141695658366\n",
      "-0.0012613038937250773\n",
      "-0.0012611579100290935\n",
      "-0.0012605497121810914\n",
      "-0.0012611344973246256\n",
      "-0.0012612157265345255\n",
      "-0.0012612728595733642\n",
      "-0.0012612915436426798\n",
      "-0.001261322577794393\n",
      "-0.0012613209247589112\n",
      "-0.0012613274892171223\n",
      "-0.001261281704902649\n",
      "-0.0012609256744384766\n",
      "-0.0012607424815495808\n",
      "-0.00126114292939504\n",
      "-0.0012612545172373455\n",
      "-0.001261309274037679\n",
      "-0.0012612934112548827\n",
      "-0.0012613027890523276\n",
      "-0.0012613187948862712\n",
      "-0.0012613141854604085\n",
      "-0.001261324707667033\n",
      "-0.0012612472852071125\n",
      "-0.0012601177295049032\n",
      "-0.0012607689539591471\n",
      "-0.0012611530065536499\n",
      "-0.0012612577676773071\n",
      "-0.0012612850745519003\n",
      "-0.0012613203207651775\n",
      "-0.0012613236824671427\n",
      "-0.001261324111620585\n",
      "-0.0012613128980000813\n",
      "-0.0012613089640935262\n",
      "-0.0012613080104192098\n",
      "-0.0012613308668136597\n",
      "-0.0012613195260365805\n",
      "-0.0012613070646921793\n",
      "-0.0012609697103500366\n",
      "-0.0012604618390401204\n",
      "-0.0012610523223876952\n",
      "-0.0012612228949864706\n",
      "-0.0012612744092941285\n",
      "-0.0012613001982371011\n",
      "-0.0012613124450047812\n",
      "-0.0012613075892130535\n",
      "-0.0012613363822301228\n",
      "-0.0012613117138544718\n",
      "-0.0012612648010253906\n",
      "-0.001260601003964742\n",
      "-0.0012609325965245565\n",
      "-0.001261161494255066\n",
      "-0.001261251425743103\n",
      "-0.0012612895568211873\n",
      "-0.0012613061825434368\n",
      "-0.0012613240798314412\n",
      "-0.001261317245165507\n",
      "-0.0012613198518753052\n",
      "-0.0012613004207611084\n",
      "-0.001261191479365031\n",
      "-0.0012605953613917032\n",
      "-0.0012611047108968098\n",
      "-0.001261194880803426\n",
      "-0.0012612604061762493\n",
      "-0.001261292282740275\n",
      "-0.0012613072395324706\n",
      "-0.0012613036870956422\n",
      "-0.0012613224267959595\n",
      "-0.0012613234202067058\n",
      "-0.001261303742726644\n",
      "-0.0012606701850891113\n",
      "-0.0012604848543802897\n",
      "-0.0012610361496607464\n",
      "-0.0012612399657567343\n",
      "-0.0012613130966822306\n",
      "-0.0012613105456034343\n",
      "-0.0012613235553105672\n",
      "-0.0012613214333852133\n",
      "-0.0012613193909327188\n",
      "-0.0012613189935684205\n",
      "-0.00126131591796875\n",
      "-0.0012613245407740274\n",
      "-0.0012613103389739991\n",
      "-0.0012610633293787638\n",
      "-0.001260424820582072\n",
      "-0.0012610718329747518\n",
      "-0.0012612241347630818\n",
      "-0.001261289389928182\n",
      "-0.0012612976948420207\n",
      "-0.001261313756306966\n",
      "-0.001261328689257304\n",
      "-0.001261312429110209\n",
      "-0.001261312182744344\n",
      "-0.001261266859372457\n",
      "-0.0012607808192571005\n",
      "-0.0012608747641245524\n",
      "-0.0012611323833465577\n",
      "-0.001261227798461914\n",
      "-0.001261294968922933\n",
      "-0.0012613274812698365\n",
      "-0.0012613178571065267\n",
      "-0.0012613232453664144\n",
      "-0.001261306095123291\n",
      "-0.0012613123337427774\n",
      "-0.0012613004366556803\n",
      "-0.001260629399617513\n",
      "-0.0012603314399719239\n",
      "-0.0012609828233718873\n",
      "-0.0012612381219863891\n",
      "-0.0012612847407658895\n",
      "-0.0012613166729609172\n",
      "-0.0012613147815068563\n",
      "-0.0012613118569056194\n",
      "-0.0012613174041112263\n",
      "-0.0012613186836242676\n",
      "-0.0012613135655721028\n",
      "-0.0012613282124201456\n",
      "-0.0012613271633783976\n",
      "-0.0012613024075826009\n",
      "-0.0012611268043518067\n",
      "-0.001260945963859558\n",
      "-0.0012605343023935954\n",
      "-0.001261108946800232\n",
      "-0.0012612199465433756\n",
      "-0.0012612834294637044\n",
      "-0.0012613105217615763\n",
      "-0.001261314829190572\n",
      "-0.0012613065083821615\n",
      "-0.0012613277037938435\n",
      "-0.0012613141536712647\n",
      "-0.00126132173538208\n",
      "-0.0012613072951634726\n",
      "-0.001261054507891337\n",
      "-0.0012603455384572347\n",
      "-0.0012610232512156169\n",
      "-0.0012611921946207682\n",
      "-0.001261274290084839\n",
      "-0.001261308757464091\n",
      "-0.0012613217989603678\n",
      "-0.0012613126595815024\n",
      "-0.0012613219261169433\n",
      "-0.0012613216559092203\n",
      "-0.0012611476500829061\n",
      "-0.0012611111958821615\n",
      "-0.001261187481880188\n",
      "-0.0012612242698669433\n",
      "-0.0012612168312072754\n",
      "-0.001261054555575053\n",
      "-0.0012611299912134806\n",
      "-0.0012612915198008218\n",
      "-0.0012612413962682088\n",
      "-0.0012613034884134929\n",
      "-0.001261314622561137\n",
      "-0.0012613269488016764\n",
      "-0.0012612991412480673\n",
      "-0.0012605851491292318\n",
      "-0.0012597938219706217\n",
      "-0.0012609767119089762\n",
      "-0.001261159054438273\n",
      "-0.0012612927675247191\n",
      "-0.0012613027175267537\n",
      "-0.0012613019863764445\n",
      "-0.0012613126913706462\n",
      "-0.0012613189776738486\n",
      "-0.0012613102038701374\n",
      "-0.0012613120714823406\n",
      "-0.001261311944325765\n",
      "-0.0012613227208455404\n",
      "-0.0012613189776738486\n",
      "-0.001261307470003764\n",
      "-0.0012612914562225342\n",
      "-0.0012611386934916178\n",
      "-0.0012611879587173462\n",
      "-0.0012612753629684449\n",
      "-0.0012613298018773398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012613094886144002\n",
      "-0.001261136031150818\n",
      "-0.001260197377204895\n",
      "-0.001261032287279765\n",
      "-0.0012612074851989746\n",
      "-0.0012612823804219564\n",
      "-0.001261323595046997\n",
      "-0.001261323062578837\n",
      "-0.0012613223393758137\n",
      "-0.001261320988337199\n",
      "-0.0012613139073053997\n",
      "-0.001261317459742228\n",
      "-0.0012613213936487834\n",
      "-0.001261318604151408\n",
      "-0.0012611668904622395\n",
      "-0.0012603310664494832\n",
      "-0.0012610600471496581\n",
      "-0.0012612011671066284\n",
      "-0.0012612761815388998\n",
      "-0.001261309774716695\n",
      "-0.0012613171497980753\n",
      "-0.0012613224903742473\n",
      "-0.0012613208293914796\n",
      "-0.0012613160928090414\n",
      "-0.0012613259792327882\n",
      "-0.0012613160451253255\n",
      "-0.001261288046836853\n",
      "-0.0012606056769688923\n",
      "-0.0012605525175730388\n",
      "-0.0012609931071599324\n",
      "-0.0012612492481867473\n",
      "-0.0012612878640492757\n",
      "-0.0012613258679707844\n",
      "-0.0012613147894541422\n",
      "-0.0012613333145777385\n",
      "-0.0012613056818644205\n",
      "-0.0012613064527511597\n",
      "-0.0012612211624781291\n",
      "-0.0012611501534779866\n",
      "-0.0012612662076950073\n",
      "-0.0012613113641738892\n",
      "-0.0012612822612126668\n",
      "-0.0012610477606455484\n",
      "-0.001260719648996989\n",
      "-0.0012611631155014038\n",
      "-0.0012612680435180664\n",
      "-0.0012612969636917114\n",
      "-0.0012612933476765951\n",
      "-0.0012613109827041627\n",
      "-0.0012613287369410196\n",
      "-0.001261315703392029\n",
      "-0.0012613230069478353\n",
      "-0.0012613065083821615\n",
      "-0.0012605586846669516\n",
      "-0.0012601587851842244\n",
      "-0.0012609846353530884\n",
      "-0.0012612524112065634\n",
      "-0.0012612888018290202\n",
      "-0.0012613135178883871\n",
      "-0.001261314304669698\n",
      "-0.001261326837539673\n",
      "-0.0012613242308298746\n",
      "-0.0012613279660542806\n",
      "-0.0012613240242004395\n",
      "-0.0012613124450047812\n",
      "-0.0012613311131795246\n",
      "-0.0012613215684890748\n",
      "-0.0012613146384557088\n",
      "-0.0012612814823786418\n",
      "-0.001260862668355306\n",
      "-0.0012606427987416585\n",
      "-0.0012611396710077922\n",
      "-0.001261248477300008\n",
      "-0.0012612990776697795\n",
      "-0.001261303424835205\n",
      "-0.0012613008260726928\n",
      "-0.0012613205194473268\n",
      "-0.0012613165616989135\n",
      "-0.0012613035758336384\n",
      "-0.0012612516244252523\n",
      "-0.0012605331818262735\n",
      "-0.0012608976602554321\n",
      "-0.0012611579577128092\n",
      "-0.0012612775325775147\n",
      "-0.0012612826426823935\n",
      "-0.0012613208373387654\n",
      "-0.0012613107919692992\n",
      "-0.001261318031946818\n",
      "-0.0012613172690073649\n",
      "-0.0012613110383351645\n",
      "-0.0012613185405731201\n",
      "-0.001261187473932902\n",
      "-0.0012602380037307738\n",
      "-0.001260925817489624\n",
      "-0.0012611451069513956\n",
      "-0.001261270546913147\n",
      "-0.0012613039414087932\n",
      "-0.0012613181193669638\n",
      "-0.0012613094091415405\n",
      "-0.0012613242149353027\n",
      "-0.001261328371365865\n",
      "-0.0012613155364990234\n",
      "-0.0012613207181294758\n",
      "-0.0012613031148910523\n",
      "-0.0012608503977457682\n",
      "-0.00126042320728302\n",
      "-0.0012610204855600994\n",
      "-0.0012612346967061361\n",
      "-0.0012613086064656575\n",
      "-0.0012613205194473268\n",
      "-0.0012613160371780395\n",
      "-0.0012613116184870402\n",
      "-0.0012613101959228516\n",
      "-0.00126132017771403\n",
      "-0.0012613171895345051\n",
      "-0.0012613217274347942\n",
      "-0.0012613236427307128\n",
      "-0.001261298664410909\n",
      "-0.0012607014258702596\n",
      "-0.0012606825908025106\n",
      "-0.0012611010471979777\n",
      "-0.001261241348584493\n",
      "-0.0012612961689631144\n",
      "-0.0012613274017969768\n",
      "-0.0012613258520762125\n",
      "-0.0012613293806711832\n",
      "-0.0012613181749979655\n",
      "-0.0012613286972045898\n",
      "-0.0012613228480021158\n",
      "-0.0012613163630167644\n",
      "-0.0012610193729400635\n",
      "-0.0012604684114456176\n",
      "-0.0012610794067382813\n",
      "-0.0012611337502797444\n",
      "-0.00126118799050649\n",
      "-0.001261295715967814\n",
      "-0.0012613170544306437\n",
      "-0.0012613102197647095\n",
      "-0.0012613136768341064\n",
      "-0.001261311411857605\n",
      "-0.0012613179683685302\n",
      "-0.001261313048998515\n",
      "-0.0012612993637720743\n",
      "-0.0012609915018081666\n",
      "-0.0012603865067164104\n",
      "-0.0012610537846883137\n",
      "-0.0012612284898757934\n",
      "-0.0012612974882125854\n",
      "-0.0012613224585851033\n",
      "-0.001261319367090861\n",
      "-0.0012613078673680623\n",
      "-0.0012613266309102376\n",
      "-0.0012613102436065673\n",
      "-0.001261316204071045\n",
      "-0.001261310601234436\n",
      "-0.001261169187227885\n",
      "-0.001260427729288737\n",
      "-0.0012610815684000651\n",
      "-0.0012612101554870605\n",
      "-0.0012612780253092447\n",
      "-0.001261294412612915\n",
      "-0.0012612100839614869\n",
      "-0.0012612967491149902\n",
      "-0.0012613163471221923\n",
      "-0.0012613286336263022\n",
      "-0.0012613220453262328\n",
      "-0.0012613063017527263\n",
      "-0.0012611374934514365\n",
      "-0.0012604192972183227\n",
      "-0.0012610913515090942\n",
      "-0.0012612274090449014\n",
      "-0.0012612846771876017\n",
      "-0.001261299443244934\n",
      "-0.0012613302310307821\n",
      "-0.0012613165299097697\n",
      "-0.0012613234678904215\n",
      "-0.0012613152503967286\n",
      "-0.0012611974080403647\n",
      "-0.0012612650235493978\n",
      "-0.0012610107342402141\n",
      "-0.0012605871041615805\n",
      "-0.0012611184040705362\n",
      "-0.0012612312237421672\n",
      "-0.001261295485496521\n",
      "-0.0012613060633341472\n",
      "-0.0012613167444864909\n",
      "-0.001261322577794393\n",
      "-0.001261322291692098\n",
      "-0.001261322061220805\n",
      "-0.0012613002061843872\n",
      "-0.0012610427141189575\n",
      "-0.0012606189489364624\n",
      "-0.001260958774884542\n",
      "-0.0012611772378285726\n",
      "-0.0012612614472707112\n",
      "-0.0012613041798273722\n",
      "-0.001261314058303833\n",
      "-0.001261325240135193\n",
      "-0.0012613171895345051\n",
      "-0.0012613274653752644\n",
      "-0.0012613128105799357\n",
      "-0.0012612090349197389\n",
      "-0.0012604731718699137\n",
      "-0.001261086654663086\n",
      "-0.001261209718386332\n",
      "-0.001261290733019511\n",
      "-0.0012612958908081054\n",
      "-0.0012613186597824097\n",
      "-0.0012613093455632527\n",
      "-0.0012613245169321697\n",
      "-0.0012613213300704956\n",
      "-0.001261321512858073\n",
      "-0.0012611127217610678\n",
      "-0.001260686190923055\n",
      "-0.0012609644810358683\n",
      "-0.0012611718654632568\n",
      "-0.0012612638394037883\n",
      "-0.0012612866004308066\n",
      "-0.00126131382783254\n",
      "-0.0012613140662511189\n",
      "-0.001261313549677531\n",
      "-0.0012613267103830973\n",
      "-0.001261325510342916\n",
      "-0.0012612712065378825\n",
      "-0.0012603759209314982\n",
      "-0.0012608282883961996\n",
      "-0.0012611480077107746\n",
      "-0.001261266311009725\n",
      "-0.001261307430267334\n",
      "-0.0012613080104192098\n",
      "-0.0012613208373387654\n",
      "-0.0012613194624582927\n",
      "-0.0012613124926884969\n",
      "-0.0012613146861394247\n",
      "-0.0012612353642781576\n",
      "-0.0012611596743265787\n",
      "-0.0012611745198567708\n",
      "-0.0012611419757207235\n",
      "-0.0012612006425857544\n",
      "-0.0012613009373346965\n",
      "-0.0012613140662511189\n",
      "-0.001261311904589335\n",
      "-0.001260773229598999\n",
      "-0.001260677194595337\n",
      "-0.0012611032644907632\n",
      "-0.0012612410306930542\n",
      "-0.0012613051176071166\n",
      "-0.0012613186120986938\n",
      "-0.0012613208691279094\n",
      "-0.0012613118648529052\n",
      "-0.001261325494448344\n",
      "-0.0012613194863001507\n",
      "-0.0012612277030944823\n",
      "-0.0012610463380813598\n",
      "-0.0012608951250712077\n",
      "-0.0012612476189931233\n",
      "-0.0012612418333689372\n",
      "-0.0012612995942433674\n",
      "-0.0012612658818562826\n",
      "-0.0012612840652465821\n",
      "-0.0012612091064453125\n",
      "-0.001260865553220113\n",
      "-0.0012611574093500772\n",
      "-0.0012612202088038127\n",
      "-0.001261282245318095\n",
      "-0.0012612980763117472\n",
      "-0.001261303440729777\n",
      "-0.0012613125483194986\n",
      "-0.0012613180001576742\n",
      "-0.0012613105297088624\n",
      "-0.001260691181818644\n",
      "-0.001260084311167399\n",
      "-0.0012609798431396483\n",
      "-0.001261217490832011\n",
      "-0.00126129633585612\n",
      "-0.001261302145322164\n",
      "-0.0012613286097844442\n",
      "-0.0012613288720448811\n",
      "-0.0012613231579462687\n",
      "-0.0012613124450047812\n",
      "-0.0012613244930903117\n",
      "-0.001261308471361796\n",
      "-0.0012613096555074057\n",
      "-0.0012613204717636108\n",
      "-0.0012613158384958904\n",
      "-0.001261042332649231\n",
      "-0.0012603919744491577\n",
      "-0.001261084508895874\n",
      "-0.0012612234592437744\n",
      "-0.0012612928787867229\n",
      "-0.0012613136450449626\n",
      "-0.001261314098040263\n",
      "-0.001261316434542338\n",
      "-0.0012613199472427368\n",
      "-0.0012613196929295858\n",
      "-0.0012613241036732992\n",
      "-0.0012613213459650675\n",
      "-0.001261264983812968\n",
      "-0.0012603724479675292\n",
      "-0.0012608068227767943\n",
      "-0.0012611430724461872\n",
      "-0.0012612595081329346\n",
      "-0.001261315647761027\n",
      "-0.0012613185087839763\n",
      "-0.0012613229751586915\n",
      "-0.0012613259156545003\n",
      "-0.0012613233963648478\n",
      "-0.0012613375981648763\n",
      "-0.0012613288084665935\n",
      "-0.0012613158941268921\n",
      "-0.001261292028427124\n",
      "-0.001260880390803019\n",
      "-0.0012604475895563762\n",
      "-0.0012610855738321941\n",
      "-0.0012612175941467286\n",
      "-0.001261280099550883\n",
      "-0.001261308757464091\n",
      "-0.0012613307237625121\n",
      "-0.0012613263289133708\n",
      "-0.0012613250732421875\n",
      "-0.0012613189697265625\n",
      "-0.0012613250732421875\n",
      "-0.0012613193194071452\n",
      "-0.0012609809637069702\n",
      "-0.0012604694684346517\n",
      "-0.0012610615650812784\n",
      "-0.001261223578453064\n",
      "-0.001261294643084208\n",
      "-0.001261307175954183\n",
      "-0.0012613288323084513\n",
      "-0.0012613277355829876\n",
      "-0.001261326018969218\n",
      "-0.0012613250811894735\n",
      "-0.0012613206386566161\n",
      "-0.001261261264483134\n",
      "-0.0012610183795293173\n",
      "-0.0012607313632965087\n",
      "-0.0012611984650293985\n",
      "-0.0012612552404403688\n",
      "-0.001261267606417338\n",
      "-0.0012613043944040934\n",
      "-0.00126131960550944\n",
      "-0.0012613177061080933\n",
      "-0.0012612633069356283\n",
      "-0.0012609492301940918\n",
      "-0.00126101819674174\n",
      "-0.0012612201134363811\n",
      "-0.0012612893978754679\n",
      "-0.0012612892866134644\n",
      "-0.0012613170703252156\n",
      "-0.0012613144874572754\n",
      "-0.0012613166968027752\n",
      "-0.00126113387743632\n",
      "-0.0012603108485539755\n",
      "-0.0012610334157943726\n",
      "-0.0012612132708231607\n",
      "-0.0012612720648447673\n",
      "-0.00126131858030955\n",
      "-0.0012613138437271118\n",
      "-0.0012613248507181804\n",
      "-0.0012613163550694783\n",
      "-0.0012613179683685302\n",
      "-0.0012613264799118042\n",
      "-0.0012612237771352133\n",
      "-0.0012611975034077963\n",
      "-0.0012609471321105958\n",
      "-0.0012607680082321166\n",
      "-0.0012611506223678588\n",
      "-0.0012612475395202636\n",
      "-0.0012613101800282797\n",
      "-0.001261327846844991\n",
      "-0.0012613024950027466\n",
      "-0.0012613230228424072\n",
      "-0.0012613245328267416\n",
      "-0.0012613139788309733\n",
      "-0.0012613035202026367\n",
      "-0.00126113760471344\n",
      "-0.001260076403617859\n",
      "-0.001260976012547811\n",
      "-0.0012611772060394286\n",
      "-0.0012612787326176961\n",
      "-0.0012613166968027752\n",
      "-0.0012613120396931965\n",
      "-0.0012613256057103475\n",
      "-0.0012613325595855712\n",
      "-0.0012613256533940632\n",
      "-0.0012612737735112507\n",
      "-0.0012611587047576904\n",
      "-0.0012612820307413737\n",
      "-0.001261318604151408\n",
      "-0.0012613292296727498\n",
      "-0.001261278780301412\n",
      "-0.0012606328010559083\n",
      "-0.0012608513991038005\n",
      "-0.0012611617644627889\n",
      "-0.0012612725814183552\n",
      "-0.0012613047202428183\n",
      "-0.0012613095124562582\n",
      "-0.0012613033215204874\n",
      "-0.001261319088935852\n",
      "-0.0012613109747568766\n",
      "-0.0012613333622614542\n",
      "-0.001261318842569987\n",
      "-0.0012612606287002563\n",
      "-0.0012609840393066406\n",
      "-0.0012604994455973307\n",
      "-0.0012610611120859783\n",
      "-0.001261198353767395\n",
      "-0.001261281927426656\n",
      "-0.0012613043705622356\n",
      "-0.0012613222519556682\n",
      "-0.0012613161245981852\n",
      "-0.0012613169193267822\n",
      "-0.0012613189140955607\n",
      "-0.001261307152112325\n",
      "-0.0012611098766326903\n",
      "-0.0012606607834498088\n",
      "-0.0012611566861470541\n",
      "-0.001261271071434021\n",
      "-0.0012613053321838378\n",
      "-0.001261301565170288\n",
      "-0.0012613139231999716\n",
      "-0.00126131960550944\n",
      "-0.0012613224426905314\n",
      "-0.0012613327026367188\n",
      "-0.0012612992604573569\n",
      "-0.0012610427141189575\n",
      "-0.0012600605885187785\n",
      "-0.0012609843015670777\n",
      "-0.0012611672639846801\n",
      "-0.0012613068024317424\n",
      "-0.0012613121191660563\n",
      "-0.0012613253434499104\n",
      "-0.0012613197962443035\n",
      "-0.0012613301992416381\n",
      "-0.001261306643486023\n",
      "-0.0012613288561503092\n",
      "-0.0012612683534622192\n",
      "-0.0012611140012741089\n",
      "-0.0012612680673599243\n",
      "-0.0012612945079803466\n",
      "-0.0012612770477930704\n",
      "-0.001260984245936076\n",
      "-0.0012608930508295694\n",
      "-0.0012611815929412843\n",
      "-0.0012612704277038575\n",
      "-0.0012613088369369508\n",
      "-0.001261298402150472\n",
      "-0.0012613055070241292\n",
      "-0.0012612963438034057\n",
      "-0.001261281696955363\n",
      "-0.0012611589113871256\n",
      "-0.0012607879320780436\n",
      "-0.0012612138907114666\n",
      "-0.0012612480322519938\n",
      "-0.001261250670750936\n",
      "-0.0012613199869791667\n",
      "-0.0012613014936447143\n",
      "-0.0012612818876902262\n",
      "-0.001261251926422119\n",
      "-0.0012613118886947632\n",
      "-0.0012612187623977662\n",
      "-0.0012598524490992228\n",
      "-0.001260927693049113\n",
      "-0.0012611213127772013\n",
      "-0.001261266811688741\n",
      "-0.0012613012313842775\n",
      "-0.0012613173961639405\n",
      "-0.0012613141775131226\n",
      "-0.0012613073348999024\n",
      "-0.0012611392339070637\n",
      "-0.0012612693786621095\n",
      "-0.0012613120317459107\n",
      "-0.0012613256295522055\n",
      "-0.0012613164663314818\n",
      "-0.0012613170385360718\n",
      "-0.0012613181511561076\n",
      "-0.0012613173564275106\n",
      "-0.001261317777633667\n",
      "-0.0012611519575119019\n",
      "-0.001260430653889974\n",
      "-0.0012611087878545125\n",
      "-0.00126122518380483\n",
      "-0.0012612821420033772\n",
      "-0.001261326797803243\n",
      "-0.0012613123099009195\n",
      "-0.001261315671602885\n",
      "-0.0012613241275151572\n",
      "-0.0012613259553909302\n",
      "-0.0012612037261327108\n",
      "-0.0012612312078475953\n",
      "-0.0012610285679499308\n",
      "-0.0012607488791147867\n",
      "-0.0012611691236495972\n",
      "-0.0012612513224283853\n",
      "-0.001261288324991862\n",
      "-0.0012613046725591024\n",
      "-0.001261306651433309\n",
      "-0.00126131911277771\n",
      "-0.0012613114992777506\n",
      "-0.001261316974957784\n",
      "-0.0012612508694330852\n",
      "-0.001261207906405131\n",
      "-0.0012598519881566366\n",
      "-0.0012608952522277833\n",
      "-0.001261107858022054\n",
      "-0.001261271603902181\n",
      "-0.0012612900336583456\n",
      "-0.0012613253275553385\n",
      "-0.001261324111620585\n",
      "-0.001261312953631083\n",
      "-0.0012613263765970865\n",
      "-0.0012613178809483846\n",
      "-0.0012613146702448526\n",
      "-0.0012611078262329102\n",
      "-0.0012612515528996786\n",
      "-0.001261292854944865\n",
      "-0.0012613166411717732\n",
      "-0.0012613192319869996\n",
      "-0.0012613307555516561\n",
      "-0.0012613167842229207\n",
      "-0.0012609967867533366\n",
      "-0.0012604114055633545\n",
      "-0.0012610390583674112\n",
      "-0.0012612247625986735\n",
      "-0.001261297067006429\n",
      "-0.001261322538057963\n",
      "-0.0012613137245178223\n",
      "-0.0012613296826680502\n",
      "-0.0012613232135772704\n",
      "-0.001261322585741679\n",
      "-0.0012613245884577433\n",
      "-0.0012613227446873982\n",
      "-0.001261323340733846\n",
      "-0.0012612099011739095\n",
      "-0.001260173535346985\n",
      "-0.001260967493057251\n",
      "-0.0012611905256907146\n",
      "-0.001261276650428772\n",
      "-0.0012613053798675538\n",
      "-0.0012613141695658366\n",
      "-0.0012613243103027343\n",
      "-0.0012613311211268107\n",
      "-0.0012613202571868896\n",
      "-0.001261332138379415\n",
      "-0.0012613158782323202\n",
      "-0.0012613057613372802\n",
      "-0.0012611214955647787\n",
      "-0.0012605546871821086\n",
      "-0.001261124086380005\n",
      "-0.0012612323681513468\n",
      "-0.0012612752596537272\n",
      "-0.001261311928431193\n",
      "-0.0012613198677698771\n",
      "-0.001261326535542806\n",
      "-0.001261290939648946\n",
      "-0.0012611109813054403\n",
      "-0.001261258625984192\n",
      "-0.0012612522125244141\n",
      "-0.001260964258511861\n",
      "-0.0012608616511027018\n",
      "-0.0012611580530802408\n",
      "-0.001261254088083903\n",
      "-0.0012612995862960816\n",
      "-0.0012613197485605875\n",
      "-0.0012613211552302043\n",
      "-0.0012613274017969768\n",
      "-0.0012613187074661254\n",
      "-0.0012613179922103882\n",
      "-0.001261246124903361\n",
      "-0.0012602990945180257\n",
      "-0.0012608663320541381\n",
      "-0.0012611821015675862\n",
      "-0.0012612806638081868\n",
      "-0.0012612990379333496\n",
      "-0.001261320416132609\n",
      "-0.001261315933863322\n",
      "-0.0012612592697143555\n",
      "-0.0012612043460210165\n",
      "-0.0012612848043441773\n",
      "-0.001261318850517273\n",
      "-0.0012613273779551188\n",
      "-0.0012613130331039428\n",
      "-0.0012613214174906414\n",
      "-0.0012612310806910196\n",
      "-0.001260208519299825\n",
      "-0.0012609622160593668\n",
      "-0.0012611934820810954\n",
      "-0.00126127184232076\n",
      "-0.001261309774716695\n",
      "-0.0012613139947255452\n",
      "-0.0012613128264745076\n",
      "-0.0012612069368362427\n",
      "-0.0012612794478734334\n",
      "-0.001261292862892151\n",
      "-0.0012613228877385457\n",
      "-0.0012613207578659057\n",
      "-0.0012613201061884563\n",
      "-0.0012613171418507894\n",
      "-0.001261136786142985\n",
      "-0.0012603354454040526\n",
      "-0.0012610680341720581\n",
      "-0.0012612069606781006\n",
      "-0.0012612912019093832\n",
      "-0.0012613102277119954\n",
      "-0.0012613210519154866\n",
      "-0.0012613138596216837\n",
      "-0.0012613194942474365\n",
      "-0.001261286958058675\n",
      "-0.0012610959609349569\n",
      "-0.0012612680753072102\n",
      "-0.0012613070885340372\n",
      "-0.0012613166093826294\n",
      "-0.0012611137946446738\n",
      "-0.001260172669092814\n",
      "-0.0012610016187032064\n",
      "-0.0012611860513687134\n",
      "-0.001261289127667745\n",
      "-0.0012613103389739991\n",
      "-0.0012613154331843057\n",
      "-0.0012613245248794555\n",
      "-0.0012613183895746867\n",
      "-0.001261322538057963\n",
      "-0.0012613230148951213\n",
      "-0.0012613141298294067\n",
      "-0.0012613237619400024\n",
      "-0.0012613195498784383\n",
      "-0.0012612904230753581\n",
      "-0.0012609667539596559\n",
      "-0.0012605164448420206\n",
      "-0.0012611026207605999\n",
      "-0.0012612165609995525\n",
      "-0.0012612882614135741\n",
      "-0.0012613166491190592\n",
      "-0.0012613128900527955\n",
      "-0.0012613369941711426\n",
      "-0.0012613269646962483\n",
      "-0.001261323873202006\n",
      "-0.0012613093455632527\n",
      "-0.0012611148993174235\n",
      "-0.001260709540049235\n",
      "-0.0012611777544021606\n",
      "-0.0012612585941950481\n",
      "-0.0012612912893295288\n",
      "-0.0012613009770711264\n",
      "-0.001261309846242269\n",
      "-0.0012613204876581827\n",
      "-0.0012612377405166627\n",
      "-0.0012612287521362306\n",
      "-0.0012611921310424804\n",
      "-0.001260516635576884\n",
      "-0.0012610333840052286\n",
      "-0.0012612014055252076\n",
      "-0.001261272676785787\n",
      "-0.0012612930456797281\n",
      "-0.0012613133192062378\n",
      "-0.0012613187789916993\n",
      "-0.0012613127708435059\n",
      "-0.001261326789855957\n",
      "-0.0012613195260365805\n",
      "-0.0012613186995188395\n",
      "-0.001261311904589335\n",
      "-0.0012606818596522012\n",
      "-0.001260166629155477\n",
      "-0.0012609370470046998\n",
      "-0.0012612250089645387\n",
      "-0.00126130690574646\n",
      "-0.0012613182385762534\n",
      "-0.00126133660475413\n",
      "-0.0012613291263580322\n",
      "-0.0012613137086232504\n",
      "-0.0012613203605016073\n",
      "-0.0012613351742426554\n",
      "-0.0012613217274347942\n",
      "-0.0012613185962041219\n",
      "-0.0012613318045934041\n",
      "-0.0012613163391749064\n",
      "-0.0012612136205037434\n",
      "-0.001260425329208374\n",
      "-0.0012610550721486409\n",
      "-0.0012612215518951417\n",
      "-0.0012612871408462525\n",
      "-0.0012613149325052897\n",
      "-0.0012613160848617554\n",
      "-0.0012613207419713338\n",
      "-0.001261321187019348\n",
      "-0.0012613274415334066\n",
      "-0.001261315147082011\n",
      "-0.0012613319238026937\n",
      "-0.0012612369696299234\n",
      "-0.0012610158761342367\n",
      "-0.001260273281733195\n",
      "-0.0012610235134760538\n",
      "-0.0012612040042877196\n",
      "-0.0012612806638081868\n",
      "-0.0012613147735595703\n",
      "-0.0012613269011179605\n",
      "-0.0012613308906555176\n",
      "-0.001261330238978068\n",
      "-0.0012613319635391236\n",
      "-0.0012613143603006998\n",
      "-0.0012613292535146078\n",
      "-0.001261319915453593\n",
      "-0.001261281951268514\n",
      "-0.0012605326016743978\n",
      "-0.0012607932726542155\n",
      "-0.0012611358006795247\n",
      "-0.0012612687269846598\n",
      "-0.001261312468846639\n",
      "-0.0012613278150558473\n",
      "-0.0012613184928894042\n",
      "-0.0012613262335459392\n",
      "-0.0012613136450449626\n",
      "-0.0012612911542256673\n",
      "-0.0012611858129501342\n",
      "-0.0012612828652064005\n",
      "-0.0012612870931625365\n",
      "-0.0012610347191492717\n",
      "-0.0012607343594233196\n",
      "-0.0012611489375432333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012612590471903482\n",
      "-0.0012612995306650798\n",
      "-0.0012613062461217244\n",
      "-0.0012613162438074748\n",
      "-0.0012613253672917684\n",
      "-0.0012613165775934856\n",
      "-0.001261293363571167\n",
      "-0.0012612084070841472\n",
      "-0.0012612086137135823\n",
      "-0.0012601007143656412\n",
      "-0.0012609589417775472\n",
      "-0.0012611823558807372\n",
      "-0.001261256488164266\n",
      "-0.001261318532625834\n",
      "-0.0012613117297490437\n",
      "-0.001261312182744344\n",
      "-0.001261317213376363\n",
      "-0.0012613161166508992\n",
      "-0.0012613185405731201\n",
      "-0.0012613207896550497\n",
      "-0.001261246697107951\n",
      "-0.0012611839294433594\n",
      "-0.001261250376701355\n",
      "-0.001261194602648417\n",
      "-0.0012608324845631918\n",
      "-0.0012612077713012695\n",
      "-0.001261226757367452\n",
      "-0.0012612683216730754\n",
      "-0.001261312953631083\n",
      "-0.0012613171418507894\n",
      "-0.0012613040844599405\n",
      "-0.0012613099654515585\n",
      "-0.0012611323753992717\n",
      "-0.0012605317831039429\n",
      "-0.0012610722541809081\n",
      "-0.0012612220684687296\n",
      "-0.0012612616856892904\n",
      "-0.0012613168795903523\n",
      "-0.0012613245248794555\n",
      "-0.0012613173007965089\n",
      "-0.00126131645043691\n",
      "-0.0012611375013987223\n",
      "-0.0012612433513005574\n",
      "-0.0012612956126530966\n",
      "-0.0012612944046656292\n",
      "-0.00126072781085968\n",
      "-0.0012605957825978598\n",
      "-0.0012610843181610108\n",
      "-0.0012612571318944296\n",
      "-0.0012613118092219034\n",
      "-0.0012613233009974162\n",
      "-0.0012613232056299846\n",
      "-0.0012613216241200765\n",
      "-0.001261320185661316\n",
      "-0.0012613263765970865\n",
      "-0.0012613115628560385\n",
      "-0.0012613205273946126\n",
      "-0.0012613082726796467\n",
      "-0.0012613085587819418\n",
      "-0.0012607745409011842\n",
      "-0.001260496457417806\n",
      "-0.0012610654910405476\n",
      "-0.0012612334966659545\n",
      "-0.0012612914085388183\n",
      "-0.0012613308509190878\n",
      "-0.0012613130569458007\n",
      "-0.0012613181034723917\n",
      "-0.0012613310098648072\n",
      "-0.0012613248348236083\n",
      "-0.0012613083283106487\n",
      "-0.001261319661140442\n",
      "-0.001261251449584961\n",
      "-0.001260342526435852\n",
      "-0.0012609083414077758\n",
      "-0.0012611881812413534\n",
      "-0.001261276666323344\n",
      "-0.0012613008658091227\n",
      "-0.0012613192637761434\n",
      "-0.001261322514216105\n",
      "-0.001261321234703064\n",
      "-0.001261322585741679\n",
      "-0.0012613185485204062\n",
      "-0.0012612908522288005\n",
      "-0.0012611258506774901\n",
      "-0.0012612468083699545\n",
      "-0.0012612314224243165\n",
      "-0.0012607917547225952\n",
      "-0.0012610263268152872\n",
      "-0.001261183269818624\n",
      "-0.0012612556934356689\n",
      "-0.0012612903118133544\n",
      "-0.0012613119602203368\n",
      "-0.0012613180716832479\n",
      "-0.0012613188982009888\n",
      "-0.0012613112370173137\n",
      "-0.0012611982981363932\n",
      "-0.0012607763687769572\n",
      "-0.0012611878792444864\n",
      "-0.0012612379948298137\n",
      "-0.001261259365081787\n",
      "-0.0012612797737121583\n",
      "-0.0012612688461939494\n",
      "-0.0012613230228424072\n",
      "-0.0012613011121749879\n",
      "-0.0012610595305760702\n",
      "-0.0012608254591623942\n",
      "-0.0012611899932225546\n",
      "-0.0012612715005874634\n",
      "-0.001261285408337911\n",
      "-0.0012613153457641602\n",
      "-0.0012613285144170126\n",
      "-0.0012613268852233886\n",
      "-0.001261293609937032\n",
      "-0.0012607287804285686\n",
      "-0.0012607499361038208\n",
      "-0.0012611169576644897\n",
      "-0.001261252776781718\n",
      "-0.0012613112608591715\n",
      "-0.0012613210916519165\n",
      "-0.0012613103628158569\n",
      "-0.0012612311363220214\n",
      "-0.0012612308422724405\n",
      "-0.0012612940152486164\n",
      "-0.001261306357383728\n",
      "-0.0012613162120183308\n",
      "-0.0012613099416097005\n",
      "-0.0012609434048334758\n",
      "-0.0012606208006540934\n",
      "-0.0012611046234766643\n",
      "-0.001261242397626241\n",
      "-0.0012612990140914917\n",
      "-0.0012613245089848836\n",
      "-0.0012613224744796752\n",
      "-0.0012613227605819703\n",
      "-0.0012613116900126139\n",
      "-0.0012611117045084636\n",
      "-0.0012612640937169394\n",
      "-0.0012612885475158692\n",
      "-0.0012611476182937621\n",
      "-0.001260694177945455\n",
      "-0.0012611708958943685\n",
      "-0.0012612507899602255\n",
      "-0.0012612916707992554\n",
      "-0.0012612996339797973\n",
      "-0.0012613214413324991\n",
      "-0.001261316188176473\n",
      "-0.001261319359143575\n",
      "-0.0012613310972849527\n",
      "-0.0012612969239552815\n",
      "-0.001260703984896342\n",
      "-0.001260619044303894\n",
      "-0.0012610807418823242\n",
      "-0.0012612454652786256\n",
      "-0.0012612976789474488\n",
      "-0.0012613144159317018\n",
      "-0.0012613269170125326\n",
      "-0.0012613241910934448\n",
      "-0.0012612032016118368\n",
      "-0.0012612282355626424\n",
      "-0.0012612867434819539\n",
      "-0.001261312429110209\n",
      "-0.0012613209247589112\n",
      "-0.0012613147815068563\n",
      "-0.0012610032796859742\n",
      "-0.0012605849981307984\n",
      "-0.001261099123954773\n",
      "-0.0012612324237823486\n",
      "-0.001261284605662028\n",
      "-0.001261319859822591\n",
      "-0.0012613233725229898\n",
      "-0.0012613211154937744\n",
      "-0.00126131542523702\n",
      "-0.0012612741947174073\n",
      "-0.001261150042215983\n",
      "-0.0012612756888071696\n",
      "-0.0012613011757532755\n",
      "-0.0012612680753072102\n",
      "-0.0012602308750152588\n",
      "-0.001260794480641683\n",
      "-0.0012611775000890095\n",
      "-0.0012612469514211018\n",
      "-0.0012612873395284017\n",
      "-0.0012613194147745768\n",
      "-0.001261323078473409\n",
      "-0.0012613276561101278\n",
      "-0.0012613191445668538\n",
      "-0.00126131960550944\n",
      "-0.0012613263209660848\n",
      "-0.0012613294045130412\n",
      "-0.0012613211949666341\n",
      "-0.0012613161484400432\n",
      "-0.001261058235168457\n",
      "-0.0012609360933303834\n",
      "-0.001260775327682495\n",
      "-0.001261154596010844\n",
      "-0.0012612654209136964\n",
      "-0.0012613048235575357\n",
      "-0.0012612993955612183\n",
      "-0.0012613060315450032\n",
      "-0.0012613179922103882\n",
      "-0.001261328689257304\n",
      "-0.00126132976214091\n",
      "-0.0012611831347147623\n",
      "-0.0012604498465855916\n",
      "-0.0012610786914825439\n",
      "-0.0012612311283747356\n",
      "-0.0012612895806630453\n",
      "-0.0012613051652908326\n",
      "-0.001261321751276652\n",
      "-0.0012613194227218628\n",
      "-0.0012613166650136311\n",
      "-0.0012613241036732992\n",
      "-0.0012613143682479858\n",
      "-0.0012612618525822956\n",
      "-0.0012610556205113728\n",
      "-0.0012606847922007243\n",
      "-0.0012610279719034831\n",
      "-0.0012611833413441976\n",
      "-0.0012612641016642252\n",
      "-0.0012613083680470785\n",
      "-0.0012613188664118448\n",
      "-0.0012613166014353433\n",
      "-0.0012613269249598185\n",
      "-0.0012613269249598185\n",
      "-0.0012613125801086427\n",
      "-0.0012610872745513915\n",
      "-0.0012605822960535685\n",
      "-0.001261129053433736\n",
      "-0.0012612391312917073\n",
      "-0.0012612863699595132\n",
      "-0.001261316680908203\n",
      "-0.001261310124397278\n",
      "-0.0012613242467244465\n",
      "-0.001261322053273519\n",
      "-0.0012611806074778239\n",
      "-0.0012612275441487631\n",
      "-0.0012611923297246297\n",
      "-0.0012608256498972575\n",
      "-0.001261118737856547\n",
      "-0.0012612051248550415\n",
      "-0.0012612601359685263\n",
      "-0.0012613038698832194\n",
      "-0.0012613027413686116\n",
      "-0.0012613250255584718\n",
      "-0.0012612910588582357\n",
      "-0.0012611543575922648\n",
      "-0.001260863169034322\n",
      "-0.0012612442096074422\n",
      "-0.0012612524271011353\n",
      "-0.0012612661520640055\n",
      "-0.001261318333943685\n",
      "-0.0012611812591552735\n",
      "-0.0012611590147018432\n",
      "-0.0012612443288167318\n",
      "-0.0012611353556315104\n",
      "-0.0012607727289199828\n",
      "-0.0012612035195032755\n",
      "-0.0012612682580947876\n",
      "-0.0012612807989120483\n",
      "-0.001261296518643697\n",
      "-0.0012613151868184408\n",
      "-0.0012613173961639405\n",
      "-0.001261300261815389\n",
      "-0.0012612212419509888\n",
      "-0.0012607719739278157\n",
      "-0.0012611397822697957\n",
      "-0.0012612159729003907\n",
      "-0.0012612719456354777\n",
      "-0.0012612934668858847\n",
      "-0.0012613140185674032\n",
      "-0.0012611725648244221\n",
      "-0.001261059856414795\n",
      "-0.0012612685283025105\n",
      "-0.001261299967765808\n",
      "-0.0012611498991648355\n",
      "-0.0012603269338607788\n",
      "-0.0012610660552978515\n",
      "-0.0012612013339996337\n",
      "-0.0012612846374511718\n",
      "-0.0012613112290700277\n",
      "-0.0012613109191258749\n",
      "-0.0012613182544708253\n",
      "-0.0012613167603810628\n",
      "-0.0012613250255584718\n",
      "-0.0012613140185674032\n",
      "-0.0012613253196080525\n",
      "-0.0012613359928131103\n",
      "-0.0012613179365793864\n",
      "-0.0012611238876978556\n",
      "-0.00126034251054128\n",
      "-0.0012610695997873943\n",
      "-0.0012612103541692098\n",
      "-0.001261288849512736\n",
      "-0.001261312174797058\n",
      "-0.0012613237063090006\n",
      "-0.001261292807261149\n",
      "-0.001261162535349528\n",
      "-0.0012612801551818847\n",
      "-0.001261315099398295\n",
      "-0.0012613279581069946\n",
      "-0.0012613207181294758\n",
      "-0.0012613195260365805\n",
      "-0.0012612461566925048\n",
      "-0.001260382620493571\n",
      "-0.0012609841505686443\n",
      "-0.0012612085501352945\n",
      "-0.0012612850427627565\n",
      "-0.001261303965250651\n",
      "-0.001261314312616984\n",
      "-0.0012613258123397827\n",
      "-0.001261330811182658\n",
      "-0.0012613109827041627\n",
      "-0.001261056089401245\n",
      "-0.0012612287918726604\n",
      "-0.0012612717469533284\n",
      "-0.0012613283793131511\n",
      "-0.001261304744084676\n",
      "-0.0012607796907424928\n",
      "-0.0012605584541956585\n",
      "-0.001261068844795227\n",
      "-0.00126125545501709\n",
      "-0.0012613032658894856\n",
      "-0.0012613247315088909\n",
      "-0.0012613339026769001\n",
      "-0.00126131591796875\n",
      "-0.0012613265673319498\n",
      "-0.0012613228797912599\n",
      "-0.0012613246997197469\n",
      "-0.00126132333278656\n",
      "-0.0012613216956456502\n",
      "-0.001261327330271403\n",
      "-0.0012611937046051026\n",
      "-0.0012603550513585408\n",
      "-0.0012610440095265707\n",
      "-0.0012611769199371338\n",
      "-0.0012612802267074584\n",
      "-0.00126129732131958\n",
      "-0.001261312182744344\n",
      "-0.001261323857307434\n",
      "-0.0012613239844640096\n",
      "-0.001261321512858073\n",
      "-0.0012613075176874796\n",
      "-0.0012611191908518473\n",
      "-0.001261263664563497\n",
      "-0.0012612933079401653\n",
      "-0.0012610363006591797\n",
      "-0.0012605324188868204\n",
      "-0.0012610952774683634\n",
      "-0.0012612241983413696\n",
      "-0.0012612910906473795\n",
      "-0.0012613183736801148\n",
      "-0.0012613179047902426\n",
      "-0.0012613208452860515\n",
      "-0.0012613266070683796\n",
      "-0.0012613162120183308\n",
      "-0.0012613195260365805\n",
      "-0.001261320964495341\n",
      "-0.0012612600406010945\n",
      "-0.0012605461359024048\n",
      "-0.0012608790715535481\n",
      "-0.0012611552079518635\n",
      "-0.0012612603823343913\n",
      "-0.0012613122542699177\n",
      "-0.0012613177061080933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00126132230758667\n",
      "-0.0012613242467244465\n",
      "-0.0012612505197525025\n",
      "-0.0012612372716267904\n",
      "-0.0012612959782282512\n",
      "-0.0012613145430882772\n",
      "-0.0012612889528274536\n",
      "-0.001260586134592692\n",
      "-0.001260830775896708\n",
      "-0.0012611480156580607\n",
      "-0.001261268162727356\n",
      "-0.0012613110701243083\n",
      "-0.0012613127390543619\n",
      "-0.001261324644088745\n",
      "-0.0012613219102223714\n",
      "-0.0012612838347752888\n",
      "-0.0012611372788747153\n",
      "-0.0012612641016642252\n",
      "-0.001261299188931783\n",
      "-0.0012613286018371582\n",
      "-0.0012613211790720622\n",
      "-0.0012612303733825683\n",
      "-0.0012598822673161824\n",
      "-0.0012609342575073243\n",
      "-0.0012611198027928671\n",
      "-0.0012612773100535075\n",
      "-0.0012613057295481364\n",
      "-0.0012613277355829876\n",
      "-0.0012613182226816812\n",
      "-0.0012613306999206544\n",
      "-0.0012613273779551188\n",
      "-0.001261326003074646\n",
      "-0.00126131698290507\n",
      "-0.0012613252639770507\n",
      "-0.0012613234122594197\n",
      "-0.0012611978689829508\n",
      "-0.0012610707521438598\n",
      "-0.0012612746556599935\n",
      "-0.001261302391688029\n",
      "-0.0012613036553064982\n",
      "-0.0012609695990880331\n",
      "-0.001260546056429545\n",
      "-0.001261091430981954\n",
      "-0.0012612316528956096\n",
      "-0.0012613053083419799\n",
      "-0.0012613072792689005\n",
      "-0.0012613183577855427\n",
      "-0.00126132603486379\n",
      "-0.0012613178491592406\n",
      "-0.0012613174041112263\n",
      "-0.0012613216718037922\n",
      "-0.0012613260984420777\n",
      "-0.001261327616373698\n",
      "-0.0012612935622533163\n",
      "-0.001260449242591858\n",
      "-0.0012607096751530965\n",
      "-0.001261144717534383\n",
      "-0.001261275601387024\n",
      "-0.0012613022009531657\n",
      "-0.0012613028764724731\n",
      "-0.0012612446705500284\n",
      "-0.0012613056341807048\n",
      "-0.0012613078594207764\n",
      "-0.0012613233884175617\n",
      "-0.0012613226413726807\n",
      "-0.0012613191445668538\n",
      "-0.0012613274733225504\n",
      "-0.0012612876335779827\n",
      "-0.0012611631472905476\n",
      "-0.0012609431266784668\n",
      "-0.0012607179959615072\n",
      "-0.0012611285050710041\n",
      "-0.0012612515211105346\n",
      "-0.0012612929026285808\n",
      "-0.0012613087177276612\n",
      "-0.0012613291025161744\n",
      "-0.0012613313754399618\n",
      "-0.0012613208691279094\n",
      "-0.0012613166173299154\n",
      "-0.0012613215366999307\n",
      "-0.0012612324953079224\n",
      "-0.001260459081331889\n",
      "-0.0012610317468643188\n",
      "-0.0012612099726994831\n",
      "-0.0012612822373708088\n",
      "-0.0012613103151321412\n",
      "-0.0012613189856211344\n",
      "-0.0012613276402155557\n",
      "-0.00126131755510966\n",
      "-0.0012613187948862712\n",
      "-0.001261322331428528\n",
      "-0.0012612649520238241\n",
      "-0.0012602938652038573\n",
      "-0.0012608721097310384\n",
      "-0.0012611725966135661\n",
      "-0.001261280393600464\n",
      "-0.0012613090753555297\n",
      "-0.0012613220453262328\n",
      "-0.0012613184531529746\n",
      "-0.0012613232930501301\n",
      "-0.0012613258361816406\n",
      "-0.0012613146384557088\n",
      "-0.0012613257010777792\n",
      "-0.0012613180001576742\n",
      "-0.0012613226493199666\n",
      "-0.0012612699588139852\n",
      "-0.0012605364799499511\n",
      "-0.0012608356714248657\n",
      "-0.0012611547549565634\n",
      "-0.0012612656911214193\n",
      "-0.0012612933238347372\n",
      "-0.001261321465174357\n",
      "-0.001261327838897705\n",
      "-0.0012613261143366496\n",
      "-0.0012613219022750855\n",
      "-0.001261316967010498\n",
      "-0.0012612872918446858\n",
      "-0.0012609829584757488\n",
      "-0.0012608035882314046\n",
      "-0.0012611628691355386\n",
      "-0.0012612602551778157\n",
      "-0.0012612979968388875\n",
      "-0.0012613196849822997\n",
      "-0.0012613184611002604\n",
      "-0.001261316196123759\n",
      "-0.0012613234996795655\n",
      "-0.0012613273859024049\n",
      "-0.0012612994273503621\n",
      "-0.00126084094842275\n",
      "-0.0012606935103734334\n",
      "-0.0012611239592234293\n",
      "-0.0012612525860468547\n",
      "-0.0012612972736358642\n",
      "-0.0012613128980000813\n",
      "-0.0012613105376561482\n",
      "-0.001261319955190023\n",
      "-0.0012613180716832479\n",
      "-0.0012613116979599\n",
      "-0.0012612298091252644\n",
      "-0.0012605069160461426\n",
      "-0.0012610738118489584\n",
      "-0.001261218778292338\n",
      "-0.0012612806797027587\n",
      "-0.0012613145510355632\n",
      "-0.001261323094367981\n",
      "-0.0012613225698471068\n",
      "-0.0012613220850626627\n",
      "-0.0012613187789916993\n",
      "-0.0012613166411717732\n",
      "-0.0012613195816675823\n",
      "-0.0012611565033594768\n",
      "-0.001260509244600932\n",
      "-0.001261082394917806\n",
      "-0.0012612162113189698\n",
      "-0.0012612876097361247\n",
      "-0.0012612878243128459\n",
      "-0.0012613187313079833\n",
      "-0.001261322792371114\n",
      "-0.001261328673362732\n",
      "-0.0012613226970036825\n",
      "-0.0012613113164901732\n",
      "-0.0012608078002929687\n",
      "-0.0012605495691299438\n",
      "-0.001261052640279134\n",
      "-0.0012612566391626994\n",
      "-0.001261308725674947\n",
      "-0.0012613212744394938\n",
      "-0.0012613188187281291\n",
      "-0.0012613248030344645\n",
      "-0.0012613215843836467\n",
      "-0.0012613229990005492\n",
      "-0.0012613255023956299\n",
      "-0.0012613178253173829\n",
      "-0.00126114501953125\n",
      "-0.0012612134377161662\n",
      "-0.0012609503110249838\n",
      "-0.0012609118064244589\n",
      "-0.0012611706256866455\n",
      "-0.0012612759669621786\n",
      "-0.0012613057454427083\n",
      "-0.0012612968683242797\n",
      "-0.001261310029029846\n",
      "-0.001261318556467692\n",
      "-0.0012613167603810628\n",
      "-0.0012613247235616048\n",
      "-0.001261221448580424\n",
      "-0.001260170062383016\n",
      "-0.001261018657684326\n",
      "-0.001261192242304484\n",
      "-0.0012612647612889607\n",
      "-0.0012613110303878784\n",
      "-0.0012613133827845254\n",
      "-0.0012612745841344198\n",
      "-0.0012612942854563396\n",
      "-0.0012613214095433553\n",
      "-0.001261315647761027\n",
      "-0.0012612624009450276\n",
      "-0.0012613142331441244\n",
      "-0.0012613195975621542\n",
      "-0.0012613159577051797\n",
      "-0.0012613187630971274\n",
      "-0.0012610278685887654\n",
      "-0.0012604083061218262\n",
      "-0.0012610192696253459\n",
      "-0.0012611954689025879\n",
      "-0.0012613016287485758\n",
      "-0.0012613206148147584\n",
      "-0.0012613109827041627\n",
      "-0.0012613090117772421\n",
      "-0.0012613183895746867\n",
      "-0.0012613214333852133\n",
      "-0.0012613316535949708\n",
      "-0.0012613227685292561\n",
      "-0.0012611557324727375\n",
      "-0.0012611713488896688\n",
      "-0.001261238145828247\n",
      "-0.0012607092539469401\n",
      "-0.0012610009670257567\n",
      "-0.0012611775557200113\n",
      "-0.0012612696329752605\n",
      "-0.0012612946192423504\n",
      "-0.0012613134860992431\n",
      "-0.0012613189458847045\n",
      "-0.0012613224585851033\n",
      "-0.0012613247235616048\n",
      "-0.0012613221883773804\n",
      "-0.0012613208373387654\n",
      "-0.0012612699111302693\n",
      "-0.0012601412852605183\n",
      "-0.0012608714421590169\n",
      "-0.0012611751317977906\n",
      "-0.001261258832613627\n",
      "-0.0012613023360570272\n",
      "-0.0012613217035929362\n",
      "-0.001261321528752645\n",
      "-0.0012613107840220134\n",
      "-0.0012610898017883301\n",
      "-0.0012612648566563923\n",
      "-0.0012612948099772136\n",
      "-0.0012613199710845948\n",
      "-0.0012613197247187296\n",
      "-0.0012613200823465983\n",
      "-0.0012613165140151978\n",
      "-0.001261323634783427\n",
      "-0.0012612417380015056\n",
      "-0.0012603222211201987\n",
      "-0.0012610054095586142\n",
      "-0.0012612021366755167\n",
      "-0.0012612767060597738\n",
      "-0.0012613033215204874\n",
      "-0.0012613205989201865\n",
      "-0.0012613218863805134\n",
      "-0.0012613139629364014\n",
      "-0.0012613238970438638\n",
      "-0.0012613220691680908\n",
      "-0.0012613287130991617\n",
      "-0.0012612160285313924\n",
      "-0.0012610835790634156\n",
      "-0.0012612721999486287\n",
      "-0.0012612122933069864\n",
      "-0.001260202137629191\n",
      "-0.0012609968821207683\n",
      "-0.001261192234357198\n",
      "-0.0012612661838531493\n",
      "-0.0012613115072250367\n",
      "-0.0012613184531529746\n",
      "-0.0012613289038340251\n",
      "-0.0012613178412119548\n",
      "-0.0012613260825475056\n",
      "-0.0012613200743993123\n",
      "-0.001261327846844991\n",
      "-0.0012613284508387248\n",
      "-0.0012613231658935547\n",
      "-0.0012613250414530437\n",
      "-0.0012612985054651896\n",
      "-0.0012606231689453126\n",
      "-0.0012606419483820598\n",
      "-0.0012611123164494832\n",
      "-0.0012612760066986084\n",
      "-0.0012613042990366617\n",
      "-0.0012613108396530152\n",
      "-0.0012611345370610555\n",
      "-0.0012612301508585612\n",
      "-0.0012612977027893066\n",
      "-0.0012613193909327188\n",
      "-0.001261319335301717\n",
      "-0.0012613264242808024\n",
      "-0.0012613179922103882\n",
      "-0.0012613261540730795\n",
      "-0.001261325208346049\n",
      "-0.0012611422379811604\n",
      "-0.0012602970123291016\n",
      "-0.0012610602617263793\n",
      "-0.0012611968835194907\n",
      "-0.0012612801790237427\n",
      "-0.0012613217751185099\n",
      "-0.0012613250176111857\n",
      "-0.0012613244454065958\n",
      "-0.0012613287051518758\n",
      "-0.0012612423261006674\n",
      "-0.0012611452182133991\n",
      "-0.0012612661441167195\n",
      "-0.0012613117456436156\n",
      "-0.0012613213221232095\n",
      "-0.0012613260825475056\n",
      "-0.0012612641493479411\n",
      "-0.0012602474133173625\n",
      "-0.001260910701751709\n",
      "-0.0012611912806828817\n",
      "-0.0012612622340520223\n",
      "-0.0012613007545471192\n",
      "-0.0012613162755966186\n",
      "-0.0012613192081451416\n",
      "-0.001261324143409729\n",
      "-0.0012613229195276897\n",
      "-0.0012613278786341349\n",
      "-0.001261324183146159\n",
      "-0.00126132706006368\n",
      "-0.0012613252480824788\n",
      "-0.001261181386311849\n",
      "-0.0012611112197240195\n",
      "-0.0012611363887786865\n",
      "-0.0012604300737380982\n",
      "-0.0012610969146092733\n",
      "-0.0012612228552500407\n",
      "-0.001261281402905782\n",
      "-0.0012613063097000122\n",
      "-0.0012613129138946534\n",
      "-0.0012613211234410605\n",
      "-0.0012613234043121339\n",
      "-0.0012613207499186198\n",
      "-0.0012613268534342448\n",
      "-0.001261323587099711\n",
      "-0.0012613012234369914\n",
      "-0.0012609707355499267\n",
      "-0.0012606601635615031\n",
      "-0.0012611189603805543\n",
      "-0.001261241046587626\n",
      "-0.0012612937053044636\n",
      "-0.0012613192399342854\n",
      "-0.0012613222201665242\n",
      "-0.0012613217910130818\n",
      "-0.0012613207181294758\n",
      "-0.0012612517436345419\n",
      "-0.001261122218767802\n",
      "-0.0012612561146418254\n",
      "-0.0012613090912501018\n",
      "-0.0012610976775487264\n",
      "-0.0012603481769561768\n",
      "-0.0012610579967498778\n",
      "-0.0012612018744150798\n",
      "-0.001261296812693278\n",
      "-0.0012613332827885945\n",
      "-0.0012613207260767619\n",
      "-0.0012613219579060873\n",
      "-0.00126131858030955\n",
      "-0.001261321465174357\n",
      "-0.0012613169511159262\n",
      "-0.001261314900716146\n",
      "-0.0012613160689671835\n",
      "-0.0012613199869791667\n",
      "-0.0012612659136454264\n",
      "-0.0012604459285736084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012609307924906412\n",
      "-0.001261154596010844\n",
      "-0.0012612628936767578\n",
      "-0.0012613080422083536\n",
      "-0.0012613234599431356\n",
      "-0.0012613284349441529\n",
      "-0.0012613274892171223\n",
      "-0.0012613071839014688\n",
      "-0.0012612229665120442\n",
      "-0.0012612976710001628\n",
      "-0.0012613118171691895\n",
      "-0.0012613263448079427\n",
      "-0.0012612101475397745\n",
      "-0.0012603353182474772\n",
      "-0.0012610674937566122\n",
      "-0.0012612205823262532\n",
      "-0.001261284327507019\n",
      "-0.0012613110383351645\n",
      "-0.0012613258679707844\n",
      "-0.0012613110939661662\n",
      "-0.001261186973253886\n",
      "-0.0012612725496292114\n",
      "-0.0012613144000371296\n",
      "-0.0012613158305486043\n",
      "-0.001261317483584086\n",
      "-0.0012613219340642293\n",
      "-0.0012612482865651448\n",
      "-0.0012604036569595336\n",
      "-0.0012610119422276815\n",
      "-0.0012612060546875\n",
      "-0.001261290661493937\n",
      "-0.0012613076368967692\n",
      "-0.0012613235553105672\n",
      "-0.0012613148848215738\n",
      "-0.001261224627494812\n",
      "-0.0012611441214879354\n",
      "-0.0012612680276234945\n",
      "-0.0012613101720809936\n",
      "-0.001261316688855489\n",
      "-0.0012613155682881672\n",
      "-0.0012612932205200195\n",
      "-0.0012605411211649576\n",
      "-0.0012606868346532185\n",
      "-0.0012611336628595988\n",
      "-0.0012612643241882325\n",
      "-0.0012612983226776124\n",
      "-0.0012613170305887857\n",
      "-0.0012613134066263834\n",
      "-0.0012613140424092611\n",
      "-0.001261325740814209\n",
      "-0.0012613235632578533\n",
      "-0.0012613243420918783\n",
      "-0.001261323587099711\n",
      "-0.0012613234281539916\n",
      "-0.0012613239447275798\n",
      "-0.001261094363530477\n",
      "-0.0012609726985295614\n",
      "-0.0012604288736979166\n",
      "-0.001261095094680786\n",
      "-0.0012612236897150675\n",
      "-0.0012612848917643229\n",
      "-0.0012613181591033936\n",
      "-0.0012613192081451416\n",
      "-0.0012613231658935547\n",
      "-0.0012613286336263022\n",
      "-0.0012613179365793864\n",
      "-0.0012613207181294758\n",
      "-0.0012613222996393839\n",
      "-0.0012613017559051515\n",
      "-0.0012609503587086995\n",
      "-0.0012606547117233276\n",
      "-0.0012611125310262043\n",
      "-0.0012612427473068237\n",
      "-0.0012613038221995037\n",
      "-0.0012613190094629924\n",
      "-0.0012613154331843057\n",
      "-0.0012613197485605875\n",
      "-0.0012613186915715535\n",
      "-0.001261326313018799\n",
      "-0.0012613306681315104\n",
      "-0.0012613087177276612\n",
      "-0.0012610562562942506\n",
      "-0.0012607819477717082\n",
      "-0.0012608163913091024\n",
      "-0.0012611557801564535\n",
      "-0.0012612521648406982\n",
      "-0.0012612958113352457\n",
      "-0.0012613229831059773\n",
      "-0.0012613197565078736\n",
      "-0.0012613136132558188\n",
      "-0.0012613208055496216\n",
      "-0.0012613222281138102\n",
      "-0.0012613211711247762\n",
      "-0.0012612475633621216\n",
      "-0.0012604502121607462\n",
      "-0.0012610188166300455\n",
      "-0.0012612077156702677\n",
      "-0.0012612786451975506\n",
      "-0.0012613150676091512\n",
      "-0.0012613194942474365\n",
      "-0.001261318850517273\n",
      "-0.001261329444249471\n",
      "-0.001261245044072469\n",
      "-0.0012611813068389892\n",
      "-0.0012612805604934692\n",
      "-0.0012613163948059082\n",
      "-0.0012613140185674032\n",
      "-0.001261084032058716\n",
      "-0.00126040248076121\n",
      "-0.0012610658407211303\n",
      "-0.0012612089236577351\n",
      "-0.0012612919807434082\n",
      "-0.0012613215525945029\n",
      "-0.0012613155047098796\n",
      "-0.0012613213221232095\n",
      "-0.001261317801475525\n",
      "-0.0012613136688868205\n",
      "-0.0012613220850626627\n",
      "-0.001261313279469808\n",
      "-0.0012613184611002604\n",
      "-0.0012611021518707276\n",
      "-0.0012610256512959799\n",
      "-0.0012606111685434978\n",
      "-0.0012611437638600667\n",
      "-0.0012612448692321777\n",
      "-0.0012612865686416625\n",
      "-0.0012613219817479452\n",
      "-0.001261315353711446\n",
      "-0.0012613207896550497\n",
      "-0.0012613181511561076\n",
      "-0.0012613178968429565\n",
      "-0.001261318055788676\n",
      "-0.0012611991246541342\n",
      "-0.001260635232925415\n",
      "-0.0012611438671747844\n",
      "-0.0012612386067708333\n",
      "-0.0012612825949986776\n",
      "-0.001261306627591451\n",
      "-0.001261318278312683\n",
      "-0.0012613243420918783\n",
      "-0.0012613215843836467\n",
      "-0.0012612287918726604\n",
      "-0.0012611064751942953\n",
      "-0.0012611703236897786\n",
      "-0.001260690975189209\n",
      "-0.0012611398696899415\n",
      "-0.0012612314144770304\n",
      "-0.0012612752278645834\n",
      "-0.001261292290687561\n",
      "-0.0012613167762756347\n",
      "-0.0012613179683685302\n",
      "-0.0012613195896148681\n",
      "-0.0012613192081451416\n",
      "-0.0012612693945566814\n",
      "-0.0012608301083246868\n",
      "-0.0012609780311584472\n",
      "-0.001261181624730428\n",
      "-0.0012612664063771566\n",
      "-0.0012612987200419108\n",
      "-0.0012613206466039022\n",
      "-0.0012613197167714437\n",
      "-0.0012613166650136311\n",
      "-0.0012613261540730795\n",
      "-0.0012613089323043824\n",
      "-0.00126105268796285\n",
      "-0.0012608177185058594\n",
      "-0.0012608197291692099\n",
      "-0.001261142635345459\n",
      "-0.0012612597306569417\n",
      "-0.0012612966458002727\n",
      "-0.0012613125244776407\n",
      "-0.0012613245010375976\n",
      "-0.0012613141298294067\n",
      "-0.0012613336086273192\n",
      "-0.0012613199472427368\n",
      "-0.0012612945238749185\n",
      "-0.0012610402981440227\n",
      "-0.0012606979131698608\n",
      "-0.0012611339886983235\n",
      "-0.001261245369911194\n",
      "-0.001261298712094625\n",
      "-0.001261314598719279\n",
      "-0.0012613248348236083\n",
      "-0.0012613251606623333\n",
      "-0.001261313525835673\n",
      "-0.001261167844136556\n",
      "-0.001261277969678243\n",
      "-0.0012613056659698486\n",
      "-0.0012611489057540893\n",
      "-0.0012602968613306682\n",
      "-0.0012610541264216105\n",
      "-0.0012612058480580648\n",
      "-0.001261286481221517\n",
      "-0.0012613120079040527\n",
      "-0.0012613216241200765\n",
      "-0.0012613183418909708\n",
      "-0.0012613210519154866\n",
      "-0.001261323618888855\n",
      "-0.0012613301992416381\n",
      "-0.0012613161325454713\n",
      "-0.00126121776898702\n",
      "-0.0012611959139506022\n",
      "-0.0012612770795822144\n",
      "-0.0012611709594726563\n",
      "-0.0012603837887446086\n",
      "-0.0012610944112141926\n",
      "-0.0012612293243408203\n",
      "-0.0012612843672434489\n",
      "-0.001261303424835205\n",
      "-0.0012613229354222616\n",
      "-0.0012613232056299846\n",
      "-0.0012613227287928263\n",
      "-0.001261313001314799\n",
      "-0.0012613286972045898\n",
      "-0.0012613230069478353\n",
      "-0.001261319080988566\n",
      "-0.0012613159815470377\n",
      "-0.0012611045281092326\n",
      "-0.0012602369546890258\n",
      "-0.0012608607133229573\n",
      "-0.0012611478408177694\n",
      "-0.001261286703745524\n",
      "-0.001261320169766744\n",
      "-0.0012613176981608072\n",
      "-0.0012613199234008789\n",
      "-0.001261327862739563\n",
      "-0.0012613173087437947\n",
      "-0.0012613193114598593\n",
      "-0.0012613189776738486\n",
      "-0.0012613282203674317\n",
      "-0.0012613220453262328\n",
      "-0.0012612034797668456\n",
      "-0.0012611680348714193\n",
      "-0.0012611766815185546\n",
      "-0.0012604245901107788\n",
      "-0.0012610684951146443\n",
      "-0.0012612239519755046\n",
      "-0.0012612775087356568\n",
      "-0.001261314304669698\n",
      "-0.0012613279183705647\n",
      "-0.0012613306204477947\n",
      "-0.0012613358577092488\n",
      "-0.0012613209486007691\n",
      "-0.0012613285382588703\n",
      "-0.0012613206704457601\n",
      "-0.0012613174517949423\n",
      "-0.0012611764272054036\n",
      "-0.001260463237762451\n",
      "-0.0012610960960388184\n",
      "-0.001261225660641988\n",
      "-0.0012612770318984985\n",
      "-0.0012612695614496866\n",
      "-0.0012612903833389283\n",
      "-0.0012613186597824097\n",
      "-0.0012613098700841269\n",
      "-0.001261326257387797\n",
      "-0.001261326559384664\n",
      "-0.0012613152186075846\n",
      "-0.0012612886826197307\n",
      "-0.0012609792470932007\n",
      "-0.0012605377833048502\n",
      "-0.0012611210982004801\n",
      "-0.0012612361192703247\n",
      "-0.001261291790008545\n",
      "-0.0012613122781117757\n",
      "-0.001261322553952535\n",
      "-0.0012613202651341757\n",
      "-0.0012613237380981446\n",
      "-0.001261321481068929\n",
      "-0.0012613207976023355\n",
      "-0.0012613192319869996\n",
      "-0.0012611461639404297\n",
      "-0.0012603321234385173\n",
      "-0.001261073366800944\n",
      "-0.0012612118323644002\n",
      "-0.0012612897793451946\n",
      "-0.0012613197088241577\n",
      "-0.001261247706413269\n",
      "-0.001261211895942688\n",
      "-0.0012612914164861044\n",
      "-0.001261320940653483\n",
      "-0.0012613259394963583\n",
      "-0.0012613221565882366\n",
      "-0.001261324707667033\n",
      "-0.0012613243420918783\n",
      "-0.0012613149722417196\n",
      "-0.0012608768781026205\n",
      "-0.0012606311003367106\n",
      "-0.0012610900322596232\n",
      "-0.001261250368754069\n",
      "-0.0012613019704818726\n",
      "-0.0012613235155741374\n",
      "-0.0012613016287485758\n",
      "-0.001261097224553426\n",
      "-0.0012612611691157024\n",
      "-0.0012612950960795084\n",
      "-0.0012613292217254638\n",
      "-0.0012613242944081624\n",
      "-0.0012613203128178914\n",
      "-0.00126131378809611\n",
      "-0.0012612669626871744\n",
      "-0.0012601999521255492\n",
      "-0.0012608985344568888\n",
      "-0.0012611814498901368\n",
      "-0.0012612512111663818\n",
      "-0.0012613130569458007\n",
      "-0.0012613183180491129\n",
      "-0.0012613288402557373\n",
      "-0.0012613271713256837\n",
      "-0.0012613226811091106\n",
      "-0.001261327290534973\n",
      "-0.0012613226175308228\n",
      "-0.001261313796043396\n",
      "-0.0012611087242762249\n",
      "-0.0012612664937973023\n",
      "-0.001261294388771057\n",
      "-0.0012613090674082439\n",
      "-0.0012611058712005616\n",
      "-0.0012604994058609009\n",
      "-0.0012610960642496746\n",
      "-0.0012612273375193278\n",
      "-0.00126128888130188\n",
      "-0.001261315631866455\n",
      "-0.0012613199392954508\n",
      "-0.0012613242387771607\n",
      "-0.0012613176822662353\n",
      "-0.0012613229274749756\n",
      "-0.0012613176345825194\n",
      "-0.001261320169766744\n",
      "-0.001261316482226054\n",
      "-0.001261189874013265\n",
      "-0.0012603137254714966\n",
      "-0.001260963773727417\n",
      "-0.001261187752087911\n",
      "-0.0012612744092941285\n",
      "-0.0012613178412119548\n",
      "-0.0012613175630569458\n",
      "-0.0012613145192464192\n",
      "-0.0012613275210062664\n",
      "-0.0012613257487614949\n",
      "-0.00126133021513621\n",
      "-0.0012612564007441203\n",
      "-0.001261213239034017\n",
      "-0.0012612906217575072\n",
      "-0.0012613032023111978\n",
      "-0.0012610125462214153\n",
      "-0.0012605468273162842\n",
      "-0.0012610899845759073\n",
      "-0.0012612325191497802\n",
      "-0.0012612947225570678\n",
      "-0.0012613205989201865\n",
      "-0.0012613251765569052\n",
      "-0.0012613222440083821\n",
      "-0.001261325216293335\n",
      "-0.0012613249778747559\n",
      "-0.0012613275766372681\n",
      "-0.0012612377325693766\n",
      "-0.001261176856358846\n",
      "-0.0012610538323720296\n",
      "-0.0012607619603474936\n",
      "-0.0012611763159434\n",
      "-0.0012612608909606934\n",
      "-0.0012613046566645305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012613115549087524\n",
      "-0.0012613178888956705\n",
      "-0.0012613197882970174\n",
      "-0.0012613301436106364\n",
      "-0.0012613109668095906\n",
      "-0.0012611867348353068\n",
      "-0.0012606117645899454\n",
      "-0.0012611302216847737\n",
      "-0.0012612404664357503\n",
      "-0.0012612911144892374\n",
      "-0.001261311133702596\n",
      "-0.0012612740993499757\n",
      "-0.0012611730972925822\n",
      "-0.0012612815539042154\n",
      "-0.001261314328511556\n",
      "-0.0012613221486409505\n",
      "-0.0012613202889760334\n",
      "-0.001261206038792928\n",
      "-0.0012603111743927002\n",
      "-0.0012610543966293335\n",
      "-0.0012612123886744182\n",
      "-0.00126127982934316\n",
      "-0.0012613084236780803\n",
      "-0.0012613203525543213\n",
      "-0.0012613226334253947\n",
      "-0.0012613213698069255\n",
      "-0.0012613263765970865\n",
      "-0.0012612011273701986\n",
      "-0.0012612414280573528\n",
      "-0.0012613024870554606\n",
      "-0.001261321512858073\n",
      "-0.0012613263686498007\n",
      "-0.0012611411809921264\n",
      "-0.001260328205426534\n",
      "-0.0012610622485478718\n",
      "-0.0012612097342809042\n",
      "-0.0012612936894098917\n",
      "-0.0012613146543502807\n",
      "-0.0012613255500793458\n",
      "-0.0012613184611002604\n",
      "-0.0012613203128178914\n",
      "-0.001261324683825175\n",
      "-0.001261290454864502\n",
      "-0.0012611475547154745\n",
      "-0.001261277993520101\n",
      "-0.0012613098859786988\n",
      "-0.0012613232771555582\n",
      "-0.0012612225532531737\n",
      "-0.00126033562819163\n",
      "-0.0012610477685928345\n",
      "-0.0012612157742182414\n",
      "-0.0012612772305806478\n",
      "-0.0012613097508748373\n",
      "-0.001261320424079895\n",
      "-0.0012613250176111857\n",
      "-0.0012613142172495525\n",
      "-0.0012613191843032837\n",
      "-0.0012613221168518067\n",
      "-0.0012613197326660156\n",
      "-0.0012613228638966877\n",
      "-0.001261320988337199\n",
      "-0.0012610072294871012\n",
      "-0.0012607116142908732\n",
      "-0.0012608066082000732\n",
      "-0.0012611462195714314\n",
      "-0.001261266819636027\n",
      "-0.0012613047202428183\n",
      "-0.0012613158226013183\n",
      "-0.001261322832107544\n",
      "-0.0012613246520360312\n",
      "-0.001261327870686849\n",
      "-0.0012613244533538819\n",
      "-0.0012613253911336263\n",
      "-0.0012613219976425171\n",
      "-0.0012612924655278524\n",
      "-0.0012602818489074706\n",
      "-0.0012606499036153158\n",
      "-0.0012611653884251912\n",
      "-0.0012612408876419067\n",
      "-0.0012613101641337078\n",
      "-0.0012613231182098388\n",
      "-0.0012613187074661254\n",
      "-0.0012613210757573446\n",
      "-0.0012613144954045613\n",
      "-0.0012613258441289267\n",
      "-0.0012613130728403726\n",
      "-0.0012613259236017864\n",
      "-0.00126132386525472\n",
      "-0.0012613281806310018\n",
      "-0.0012610925912857056\n",
      "-0.0012611789226531982\n",
      "-0.0012612481594085694\n",
      "-0.001261134632428487\n",
      "-0.001260836927096049\n",
      "-0.0012612290382385254\n",
      "-0.0012612605174382528\n",
      "-0.0012612776835759481\n",
      "-0.0012613078673680623\n",
      "-0.0012613084475199383\n",
      "-0.0012613244533538819\n",
      "-0.0012613232930501301\n",
      "-0.0012612860679626465\n",
      "-0.0012605552752812704\n",
      "-0.0012608527501424154\n",
      "-0.0012611589749654134\n",
      "-0.0012612774848937988\n",
      "-0.0012613095919291179\n",
      "-0.001261322514216105\n",
      "-0.0012613189935684205\n",
      "-0.0012613251765569052\n",
      "-0.0012613176107406617\n",
      "-0.0012613217989603678\n",
      "-0.0012613210519154866\n",
      "-0.0012613147417704265\n",
      "-0.0012611090103785197\n",
      "-0.0012612390200297037\n",
      "-0.001261008644104004\n",
      "-0.001260491140683492\n",
      "-0.001261082649230957\n",
      "-0.0012612279256184896\n",
      "-0.001261297313372294\n",
      "-0.0012613168001174926\n",
      "-0.001261321004231771\n",
      "-0.0012613207817077636\n",
      "-0.0012613283554712932\n",
      "-0.0012613203525543213\n",
      "-0.0012613207976023355\n",
      "-0.0012613209009170532\n",
      "-0.0012613183577855427\n",
      "-0.001261247722307841\n",
      "-0.0012605173190434775\n",
      "-0.001261029815673828\n",
      "-0.0012612104495366414\n",
      "-0.0012612765789031981\n",
      "-0.0012613076368967692\n",
      "-0.0012613190015157063\n",
      "-0.001261248747507731\n",
      "-0.0012611456314722696\n",
      "-0.0012612709045410155\n",
      "-0.0012613140106201171\n",
      "-0.001261324119567871\n",
      "-0.0012613200505574545\n",
      "-0.0012612550258636476\n",
      "-0.0012605900128682455\n",
      "-0.00126101553440094\n",
      "-0.0012612033367156983\n",
      "-0.0012612825155258179\n",
      "-0.0012613115708033243\n",
      "-0.0012613117774327596\n",
      "-0.0012613194386164347\n",
      "-0.0012613213141759237\n",
      "-0.0012613232533137003\n",
      "-0.0012613294124603272\n",
      "-0.0012613191207249959\n",
      "-0.0012613201459248861\n",
      "-0.0012613079388936362\n",
      "-0.001260584020614624\n",
      "-0.0012601842482884726\n",
      "-0.0012610503991444906\n",
      "-0.0012612093925476074\n",
      "-0.0012613014936447143\n",
      "-0.0012613154411315918\n",
      "-0.001261323626836141\n",
      "-0.0012613204638163248\n",
      "-0.0012613196690877278\n",
      "-0.0012613240242004395\n",
      "-0.001261328673362732\n",
      "-0.0012613230307896932\n",
      "-0.001261324954032898\n",
      "-0.0012613298813501995\n",
      "-0.0012613165616989135\n",
      "-0.0012610743522644043\n",
      "-0.001261271095275879\n",
      "-0.0012612841208775839\n",
      "-0.0012612137556076049\n",
      "-0.0012606857299804688\n",
      "-0.0012611307859420775\n",
      "-0.0012612337509791056\n",
      "-0.0012612856944402059\n",
      "-0.0012613040129343669\n",
      "-0.0012613086064656575\n",
      "-0.0012613269805908202\n",
      "-0.001261316442489624\n",
      "-0.0012613210439682008\n",
      "-0.0012613253196080525\n",
      "-0.0012612792094548544\n",
      "-0.0012604904174804688\n",
      "-0.001260892669359843\n",
      "-0.0012611817836761475\n",
      "-0.001261283278465271\n",
      "-0.0012613123496373493\n",
      "-0.0012613205512364706\n",
      "-0.0012612137953440347\n",
      "-0.0012612213055292764\n",
      "-0.0012612982114156087\n",
      "-0.0012613155841827393\n",
      "-0.0012613194624582927\n",
      "-0.001261317213376363\n",
      "-0.0012613283077875773\n",
      "-0.0012613288482030234\n",
      "-0.0012611936887105305\n",
      "-0.0012603456656138101\n",
      "-0.0012610815048217773\n",
      "-0.0012612231334050497\n",
      "-0.001261280083656311\n",
      "-0.0012613083680470785\n",
      "-0.0012613196849822997\n",
      "-0.0012613305966059367\n",
      "-0.0012612812995910644\n",
      "-0.0012611531496047974\n",
      "-0.0012612762928009033\n",
      "-0.0012613068103790282\n",
      "-0.0012613163073857626\n",
      "-0.0012613242149353027\n",
      "-0.0012612760305404664\n",
      "-0.00126064403851827\n",
      "-0.0012609466632207236\n",
      "-0.0012611757357915243\n",
      "-0.0012612749973932903\n",
      "-0.0012613114595413208\n",
      "-0.0012613151629765828\n",
      "-0.001261326766014099\n",
      "-0.001261316752433777\n",
      "-0.001261321218808492\n",
      "-0.0012613178173700968\n",
      "-0.0012613043705622356\n",
      "-0.0012611025015513103\n",
      "-0.0012612601200739542\n",
      "-0.00126109885374705\n",
      "-0.0012603156566619873\n",
      "-0.0012610490163167318\n",
      "-0.0012612010637919108\n",
      "-0.0012612966140111287\n",
      "-0.0012613152424494425\n",
      "-0.0012613243818283082\n",
      "-0.001261320439974467\n",
      "-0.0012613250970840454\n",
      "-0.0012613191922505697\n",
      "-0.0012613242705663045\n",
      "-0.0012613229036331176\n",
      "-0.0012613165299097697\n",
      "-0.0012613228638966877\n",
      "-0.0012613175392150878\n",
      "-0.0012611344416936239\n",
      "-0.0012603599230448404\n",
      "-0.0012610051552454631\n",
      "-0.0012611894766489664\n",
      "-0.001261278820037842\n",
      "-0.001261322585741679\n",
      "-0.0012613168160120645\n",
      "-0.0012613142410914102\n",
      "-0.0012613162676493328\n",
      "-0.001261320153872172\n",
      "-0.0012612229665120442\n",
      "-0.0012612165609995525\n",
      "-0.0012612879594167074\n",
      "-0.0012613154172897338\n",
      "-0.0012613182703653972\n",
      "-0.0012610785722732543\n",
      "-0.0012604916334152223\n",
      "-0.0012610872824986776\n",
      "-0.0012612241586049398\n",
      "-0.0012612949053446452\n",
      "-0.001261316164334615\n",
      "-0.0012613253275553385\n",
      "-0.0012613208134969077\n",
      "-0.0012613250176111857\n",
      "-0.001261323841412862\n",
      "-0.0012613207260767619\n",
      "-0.00126125172773997\n",
      "-0.001261180289586385\n",
      "-0.00126126123269399\n",
      "-0.0012610114812850951\n",
      "-0.0012606281518936157\n",
      "-0.0012611112435658772\n",
      "-0.0012612489541371664\n",
      "-0.0012612948815027873\n",
      "-0.0012613190571467083\n",
      "-0.0012613269726435344\n",
      "-0.0012613203684488932\n",
      "-0.0012613247553507486\n",
      "-0.0012613235791524252\n",
      "-0.0012613240003585815\n",
      "-0.0012613200028737386\n",
      "-0.0012613207658131917\n",
      "-0.0012612020015716552\n",
      "-0.0012601011594136557\n",
      "-0.0012609984318415323\n",
      "-0.0012611258506774901\n",
      "-0.0012611323833465577\n",
      "-0.0012612692912419637\n",
      "-0.0012613114356994628\n",
      "-0.0012613183895746867\n",
      "-0.0012613252480824788\n",
      "-0.0012613292773564658\n",
      "-0.0012613173166910808\n",
      "-0.001261322029431661\n",
      "-0.0012613218943277995\n",
      "-0.0012613210995992025\n",
      "-0.0012613258361816406\n",
      "-0.0012613239526748658\n",
      "-0.0012613228638966877\n",
      "-0.0012612576087315877\n",
      "-0.001260405135154724\n",
      "-0.0012608046293258667\n",
      "-0.0012611548821131388\n",
      "-0.0012612714926401774\n",
      "-0.0012613193988800049\n",
      "-0.001261327568689982\n",
      "-0.0012613269329071045\n",
      "-0.0012613242149353027\n",
      "-0.001261320161819458\n",
      "-0.001261321989695231\n",
      "-0.0012613276878992716\n",
      "-0.0012613189776738486\n",
      "-0.0012612754265467327\n",
      "-0.0012605955839157105\n",
      "-0.0012609666347503663\n",
      "-0.0012611916303634644\n",
      "-0.001261283818880717\n",
      "-0.001261312222480774\n",
      "-0.0012613253037134806\n",
      "-0.0012613221009572346\n",
      "-0.0012613269011179605\n",
      "-0.0012613256216049194\n",
      "-0.0012613187948862712\n",
      "-0.0012613168319066366\n",
      "-0.0012612082878748576\n",
      "-0.0012611006339391072\n",
      "-0.001260807474454244\n",
      "-0.0012608107089996338\n",
      "-0.0012611411492029826\n",
      "-0.0012612678289413452\n",
      "-0.0012612968921661377\n",
      "-0.0012613157192866008\n",
      "-0.0012613202889760334\n",
      "-0.0012613205989201865\n",
      "-0.0012613187313079833\n",
      "-0.0012613263289133708\n",
      "-0.0012613229513168335\n",
      "-0.001261322808265686\n",
      "-0.0012613168319066366\n",
      "-0.0012609913190205892\n",
      "-0.0012601388772328695\n",
      "-0.0012609148025512695\n",
      "-0.0012612526655197145\n",
      "-0.001261275299390157\n",
      "-0.0012613158941268921\n",
      "-0.0012613192001978556\n",
      "-0.001261323912938436\n",
      "-0.0012613144238789876\n",
      "-0.001261199164390564\n",
      "-0.0012612857898076375\n",
      "-0.0012613099813461304\n",
      "-0.0012613208214441935\n",
      "-0.0012613230307896932\n",
      "-0.0012613200585047404\n",
      "-0.0012613238175710041\n",
      "-0.0012613228956858318\n",
      "-0.001261320455869039\n",
      "-0.001261293339729309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001260442344347636\n",
      "-0.001260753830273946\n",
      "-0.0012611632347106934\n",
      "-0.001261281402905782\n",
      "-0.0012612974484761555\n",
      "-0.001261326281229655\n",
      "-0.0012613248348236083\n",
      "-0.0012613245884577433\n",
      "-0.0012612071752548218\n",
      "-0.001261242135365804\n",
      "-0.0012613020817438762\n",
      "-0.0012613224267959595\n",
      "-0.0012613162358601888\n",
      "-0.001261323356628418\n",
      "-0.0012613229433695475\n",
      "-0.0012613219817479452\n",
      "-0.0012612706661224366\n",
      "-0.0012598430554072062\n",
      "-0.0012608863512674968\n",
      "-0.001261100713411967\n",
      "-0.0012612737735112507\n",
      "-0.0012613013505935668\n",
      "-0.0012613118251164753\n",
      "-0.0012613148848215738\n",
      "-0.0012613292773564658\n",
      "-0.0012612456639607748\n",
      "-0.0012612209558486938\n",
      "-0.0012612945079803466\n",
      "-0.0012613229115804037\n",
      "-0.0012613253752390544\n",
      "-0.0012613206307093303\n",
      "-0.0012613213141759237\n",
      "-0.0012613224744796752\n",
      "-0.0012613206386566161\n",
      "-0.001261322037378947\n",
      "-0.001261323912938436\n",
      "-0.0012612871328989664\n",
      "-0.0012605288902918497\n",
      "-0.0012606920878092448\n",
      "-0.0012611257155736287\n",
      "-0.0012612743616104126\n",
      "-0.0012612953344980876\n",
      "-0.001261312222480774\n",
      "-0.0012613214731216431\n",
      "-0.0012613221883773804\n",
      "-0.0012613300005594888\n",
      "-0.001261310052871704\n",
      "-0.0012611862421035766\n",
      "-0.001261276650428772\n",
      "-0.0012613161404927571\n",
      "-0.001261322585741679\n",
      "-0.0012613185962041219\n",
      "-0.0012611345688501993\n",
      "-0.0012603933890660605\n",
      "-0.0012610805749893189\n",
      "-0.0012612162510553997\n",
      "-0.0012612829367319742\n",
      "-0.001261319931348165\n",
      "-0.0012613257090250652\n",
      "-0.0012613232851028443\n",
      "-0.0012613152901331584\n",
      "-0.0012611818631490072\n",
      "-0.0012612460851669312\n",
      "-0.0012612996260325114\n",
      "-0.0012613159418106078\n",
      "-0.0012613191445668538\n",
      "-0.0012612894455591838\n",
      "-0.0012607750097910562\n",
      "-0.0012608795881271363\n",
      "-0.0012611666520436606\n",
      "-0.0012612655798594156\n",
      "-0.0012613126834233601\n",
      "-0.0012613194465637208\n",
      "-0.001261322291692098\n",
      "-0.0012613246997197469\n",
      "-0.001261328935623169\n",
      "-0.0012613241751988729\n",
      "-0.0012612958510716755\n",
      "-0.0012611141284306843\n",
      "-0.0012610350052515665\n",
      "-0.0012606101751327515\n",
      "-0.0012611313978830973\n",
      "-0.0012612483501434327\n",
      "-0.0012612926403681437\n",
      "-0.0012613234519958496\n",
      "-0.0012613259394963583\n",
      "-0.0012613268772761028\n",
      "-0.0012613254229227701\n",
      "-0.001261326519648234\n",
      "-0.001261322808265686\n",
      "-0.0012613279740015665\n",
      "-0.0012613086700439453\n",
      "-0.0012606703281402589\n",
      "-0.0012605348507563274\n",
      "-0.0012611003081003825\n",
      "-0.0012612650712331135\n",
      "-0.0012612952868143717\n",
      "-0.0012613166014353433\n",
      "-0.001261319335301717\n",
      "-0.0012612444400787353\n",
      "-0.0012612236420313518\n",
      "-0.0012612846851348877\n",
      "-0.0012613218545913696\n",
      "-0.0012613223393758137\n",
      "-0.0012613260746002198\n",
      "-0.0012613220453262328\n",
      "-0.001261325510342916\n",
      "-0.00126131378809611\n",
      "-0.0012612666447957357\n",
      "-0.0012601910750071209\n",
      "-0.001260896857579549\n",
      "-0.0012611805518468221\n",
      "-0.001261243971188863\n",
      "-0.00126131645043691\n",
      "-0.0012613202412923177\n",
      "-0.0012612828969955445\n",
      "-0.0012612746000289917\n",
      "-0.0012613192796707153\n",
      "-0.001261313549677531\n",
      "-0.0012613224824269613\n",
      "-0.0012613220771153769\n",
      "-0.0012613200187683105\n",
      "-0.0012612565358479818\n",
      "-0.0012612913370132447\n",
      "-0.0012613075971603393\n",
      "-0.0012612666447957357\n",
      "-0.0012604151328404745\n",
      "-0.0012609641710917155\n",
      "-0.0012612016598383586\n",
      "-0.0012612819115320841\n",
      "-0.0012613046169281006\n",
      "-0.0012613202730814615\n",
      "-0.0012612760861714682\n",
      "-0.0012612305402755737\n",
      "-0.0012612863938013712\n",
      "-0.0012613149404525758\n",
      "-0.001261325494448344\n",
      "-0.0012613260904947917\n",
      "-0.0012613213618596394\n",
      "-0.0012613195896148681\n",
      "-0.0012612928708394368\n",
      "-0.001260324764251709\n",
      "-0.0012607168594996135\n",
      "-0.0012611710468928019\n",
      "-0.0012612450838088989\n",
      "-0.0012613070011138917\n",
      "-0.0012612635691960652\n",
      "-0.0012612590948740642\n",
      "-0.0012613069693247477\n",
      "-0.001261319406827291\n",
      "-0.0012613245010375976\n",
      "-0.0012613192081451416\n",
      "-0.0012613171180089315\n",
      "-0.0012613287210464477\n",
      "-0.0012613238334655762\n",
      "-0.001261273725827535\n",
      "-0.0012611945708592733\n",
      "-0.0012612120628356933\n",
      "-0.0012607607920964558\n",
      "-0.0012611452182133991\n",
      "-0.0012612404266993205\n",
      "-0.0012612721602121989\n",
      "-0.0012612976789474488\n",
      "-0.0012613145351409911\n",
      "-0.0012613128900527955\n",
      "-0.0012613278071085612\n",
      "-0.0012613276799519856\n",
      "-0.0012612526098887125\n",
      "-0.0012606632312138876\n",
      "-0.0012610536257425945\n",
      "-0.0012612044731775919\n",
      "-0.0012612788915634156\n",
      "-0.0012612951358159383\n",
      "-0.0012613187154134114\n",
      "-0.0012613173405329387\n",
      "-0.001261249876022339\n",
      "-0.0012611898104349772\n",
      "-0.0012612611929575602\n",
      "-0.0012612619717915852\n",
      "-0.0012610718568166097\n",
      "-0.0012609694321950276\n",
      "-0.0012612276156743368\n",
      "-0.001261291495958964\n",
      "-0.0012612828731536866\n",
      "-0.0012613180239995322\n",
      "-0.0012613112052281697\n",
      "-0.0012613166332244873\n",
      "-0.0012612475633621216\n",
      "-0.0012607515573501587\n",
      "-0.001261107850074768\n",
      "-0.001261211371421814\n",
      "-0.0012612782955169677\n",
      "-0.0012612991333007812\n",
      "-0.0012613123019536336\n",
      "-0.0012611993312835693\n",
      "-0.0012612718264261881\n",
      "-0.0012613041162490846\n",
      "-0.0012613148689270019\n",
      "-0.0012612237056096394\n",
      "-0.001260564136505127\n",
      "-0.001261093513170878\n",
      "-0.0012612189372380575\n",
      "-0.0012612868626912434\n",
      "-0.0012613149483998616\n",
      "-0.001261323062578837\n",
      "-0.0012613195021947226\n",
      "-0.001261317459742228\n",
      "-0.001261323634783427\n",
      "-0.0012613176743189493\n",
      "-0.0012610823472340902\n",
      "-0.0012612067858378093\n",
      "-0.0012608342011769613\n",
      "-0.0012609096686045328\n",
      "-0.0012611694653828938\n",
      "-0.0012612602551778157\n",
      "-0.0012613009452819824\n",
      "-0.0012613186995188395\n",
      "-0.0012613212505976359\n",
      "-0.0012613193909327188\n",
      "-0.001261321210861206\n",
      "-0.0012613231261571249\n",
      "-0.0012612899700800578\n",
      "-0.0012609596967697143\n",
      "-0.0012609078168869018\n",
      "-0.0012611738602320353\n",
      "-0.0012612588167190551\n",
      "-0.0012612974882125854\n",
      "-0.0012613226493199666\n",
      "-0.0012613175948460898\n",
      "-0.0012613213698069255\n",
      "-0.0012613210916519165\n",
      "-0.0012613133986790976\n",
      "-0.0012612091779708862\n",
      "-0.0012610668182373047\n",
      "-0.0012604545831680298\n",
      "-0.001261074701944987\n",
      "-0.001261229109764099\n",
      "-0.0012613034327824911\n",
      "-0.0012613272984822591\n",
      "-0.0012613267421722413\n",
      "-0.0012613237460454305\n",
      "-0.0012613213777542115\n",
      "-0.0012613190253575643\n",
      "-0.0012613271236419678\n",
      "-0.0012613198359807333\n",
      "-0.0012613282521565755\n",
      "-0.001261316180229187\n",
      "-0.0012607956329981486\n",
      "-0.0012605051040649414\n",
      "-0.0012610612074534099\n",
      "-0.0012612421035766601\n",
      "-0.0012613086382548015\n",
      "-0.0012613272269566855\n",
      "-0.0012613241990407308\n",
      "-0.001261323618888855\n",
      "-0.0012613306681315104\n",
      "-0.001261320734024048\n",
      "-0.001261328069368998\n",
      "-0.001261324691772461\n",
      "-0.0012613204956054688\n",
      "-0.0012613322099049886\n",
      "-0.001261324151357015\n",
      "-0.0012612999439239503\n",
      "-0.0012597585995992025\n",
      "-0.001260661021868388\n",
      "-0.0012610248406728108\n",
      "-0.0012612358490626018\n",
      "-0.001261306071281433\n",
      "-0.0012613099813461304\n",
      "-0.001261299459139506\n",
      "-0.0012611888408660889\n",
      "-0.0012612768491109213\n",
      "-0.0012613195021947226\n",
      "-0.0012613299051920572\n",
      "-0.0012613187154134114\n",
      "-0.0012613259553909302\n",
      "-0.0012613194624582927\n",
      "-0.001261326789855957\n",
      "-0.001261322816212972\n",
      "-0.0012613271156946817\n",
      "-0.001261328673362732\n",
      "-0.0012613030513127644\n",
      "-0.0012611523787180582\n",
      "-0.0012612625042597453\n",
      "-0.0012613068342208862\n",
      "-0.001261312429110209\n",
      "-0.0012611104647318521\n",
      "-0.0012605823596318563\n",
      "-0.0012611243724822997\n",
      "-0.0012612450838088989\n",
      "-0.0012612924098968506\n",
      "-0.0012613177935282389\n",
      "-0.001261319859822591\n",
      "-0.0012613242626190186\n",
      "-0.001261319406827291\n",
      "-0.0012613319873809815\n",
      "-0.0012613162914911905\n",
      "-0.001261321465174357\n",
      "-0.0012612359762191772\n",
      "-0.0012604523181915283\n",
      "-0.001260988958676656\n",
      "-0.0012611857573191325\n",
      "-0.001261285146077474\n",
      "-0.001261314312616984\n",
      "-0.0012613208134969077\n",
      "-0.001261320980389913\n",
      "-0.0012613280296325685\n",
      "-0.0012613271554311116\n",
      "-0.0012613219658533731\n",
      "-0.0012613230148951213\n",
      "-0.0012613247791926066\n",
      "-0.0012613178968429565\n",
      "-0.0012606844981511434\n",
      "-0.001260339339574178\n",
      "-0.0012610865831375123\n",
      "-0.0012612430413564046\n",
      "-0.001261294158299764\n",
      "-0.001261300746599833\n",
      "-0.0012612277348836264\n",
      "-0.0012613036473592123\n",
      "-0.0012613149881362915\n",
      "-0.0012613211393356324\n",
      "-0.0012613253037134806\n",
      "-0.0012613205115000407\n",
      "-0.0012613197724024455\n",
      "-0.0012613200108210247\n",
      "-0.0012613179604212444\n",
      "-0.0012612199465433756\n",
      "-0.0012612725178400676\n",
      "-0.001261242914199829\n",
      "-0.0012609443346659342\n",
      "-0.0012610602140426636\n",
      "-0.001261200475692749\n",
      "-0.0012612784465154013\n",
      "-0.0012613147258758546\n",
      "-0.001261307692527771\n",
      "-0.0012613253593444823\n",
      "-0.0012613224347432454\n",
      "-0.0012613253196080525\n",
      "-0.0012612136363983155\n",
      "-0.001260444688796997\n",
      "-0.0012609249114990235\n",
      "-0.0012611775239308675\n",
      "-0.0012612792332967122\n",
      "-0.0012613088528315227\n",
      "-0.0012613195021947226\n",
      "-0.001261325184504191\n",
      "-0.001261327886581421\n",
      "-0.0012613223155339558\n",
      "-0.0012613196929295858\n",
      "-0.0012613282283147175\n",
      "-0.0012613236904144287\n",
      "-0.0012613112529118856\n",
      "-0.0012610198259353637\n",
      "-0.0012603672822316487\n",
      "-0.0012610026915868123\n",
      "-0.0012612266381581624\n",
      "-0.0012613053878148396\n",
      "-0.0012613091786702474\n",
      "-0.001261331836382548\n",
      "-0.0012612896919250488\n",
      "-0.001261166254679362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012612836440404256\n",
      "-0.0012613191445668538\n",
      "-0.0012613165855407714\n",
      "-0.0012613198757171632\n",
      "-0.001261322331428528\n",
      "-0.0012613214095433553\n",
      "-0.0012613236506779989\n",
      "-0.0012613121668497722\n",
      "-0.001260781208674113\n",
      "-0.001260355814297994\n",
      "-0.0012610487143198648\n",
      "-0.0012612608353296916\n",
      "-0.001261295509338379\n",
      "-0.0012613194783528646\n",
      "-0.0012613212664922078\n",
      "-0.0012612931648890177\n",
      "-0.001261217427253723\n",
      "-0.0012612971544265748\n"
     ]
    }
   ],
   "source": [
    "nb_of_epochs=4000\n",
    "batch_size=int(x_variable.size(0)/10)\n",
    "\n",
    "for epoch in range(nb_of_epochs):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        recon_batch,mu,logvar = model(x_variable.narrow(0,b,batch_size))                \n",
    "        loss = loss_function(recon_batch, y_variable.narrow(0,b,batch_size),mu,logvar)     \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "        train_loss+=loss.data[0]\n",
    "    print(train_loss/x_variable.size(0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VAE(\n",
       "   (fc1): Linear(in_features=24, out_features=14, bias=True)\n",
       "   (fc21): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc22): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc3): Linear(in_features=2, out_features=14, bias=True)\n",
       "   (fc4): Linear(in_features=14, out_features=24, bias=True)\n",
       " ), Variable containing:\n",
       " (  0  ,.,.) = \n",
       "   7.2590e-01  1.7160e-01  8.4208e-01  ...  -8.6179e-01  6.2929e-01 -2.9497e-01\n",
       " \n",
       " (  1  ,.,.) = \n",
       "   1.1056e+00  8.5052e-02  1.0595e+00  ...  -8.8101e-01  5.4964e-01 -4.5626e-01\n",
       " \n",
       " (  2  ,.,.) = \n",
       "   1.3043e+00 -1.6374e-01  7.1454e-01  ...  -1.0654e+00  3.5652e-01 -3.6640e-01\n",
       "  ...  \n",
       " \n",
       " (59997,.,.) = \n",
       "   1.0052e+00  1.3537e-01  7.4466e-01  ...  -8.4981e-01  6.0202e-01 -5.4357e-01\n",
       " \n",
       " (59998,.,.) = \n",
       "   7.9475e-01 -1.6893e-01  4.9571e-01  ...  -4.6446e-01  1.2147e+00 -5.7865e-01\n",
       " \n",
       " (59999,.,.) = \n",
       "   1.3487e+00  1.5864e-02  7.7996e-01  ...  -4.4117e-01  5.4940e-01 -2.1820e-01\n",
       " [torch.cuda.FloatTensor of size 60000x1x24 (GPU 0)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable=model.cuda(),x_variable.cuda()\n",
    "model.eval()\n",
    "model,x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1fa97f21dbca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-14edde0d2c3a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparametrize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-14edde0d2c3a>\u001b[0m in \u001b[0;36mreparametrize\u001b[1;34m(self, mu, logvar)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'other'"
     ]
    }
   ],
   "source": [
    "polygon_prediction=model(x_variable)\n",
    "polygon_prediction=polygon_prediction[14].data.cpu()\n",
    "polygon_prediction=polygon_prediction.numpy()\n",
    "polygon_prediction=polygon_prediction.reshape(12,2)\n",
    "\n",
    "plot_contour(polygon_prediction)\n",
    "plot_contour(polygons[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VAE(\n",
       "   (fc1): Linear(in_features=24, out_features=14, bias=True)\n",
       "   (fc21): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc22): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc3): Linear(in_features=2, out_features=14, bias=True)\n",
       "   (fc4): Linear(in_features=14, out_features=24, bias=True)\n",
       " ), Variable containing:\n",
       " (  0  ,.,.) = \n",
       "   7.2590e-01  1.7160e-01  8.4208e-01  ...  -8.6179e-01  6.2929e-01 -2.9497e-01\n",
       " \n",
       " (  1  ,.,.) = \n",
       "   1.1056e+00  8.5052e-02  1.0595e+00  ...  -8.8101e-01  5.4964e-01 -4.5626e-01\n",
       " \n",
       " (  2  ,.,.) = \n",
       "   1.3043e+00 -1.6374e-01  7.1454e-01  ...  -1.0654e+00  3.5652e-01 -3.6640e-01\n",
       "  ...  \n",
       " \n",
       " (59997,.,.) = \n",
       "   1.0052e+00  1.3537e-01  7.4466e-01  ...  -8.4981e-01  6.0202e-01 -5.4357e-01\n",
       " \n",
       " (59998,.,.) = \n",
       "   7.9475e-01 -1.6893e-01  4.9571e-01  ...  -4.6446e-01  1.2147e+00 -5.7865e-01\n",
       " \n",
       " (59999,.,.) = \n",
       "   1.3487e+00  1.5864e-02  7.7996e-01  ...  -4.4117e-01  5.4940e-01 -2.1820e-01\n",
       " [torch.FloatTensor of size 60000x1x24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
