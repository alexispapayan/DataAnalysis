{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.tensor as Tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from Triangulation import *\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons=load_dataset('12_polygons.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped=[]\n",
    "for polygon in polygons:\n",
    "    polygons_reshaped.append(polygon.reshape(1,2*12))\n",
    "\n",
    "polygons_reshaped=np.array(polygons_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons_reshaped=polygons_reshaped.reshape(polygons_reshaped.shape[0],1,2*12)\n",
    "\n",
    "#polygons_reshaped=polygons_reshaped.reshape(polygons_reshaped.shape[0],2*12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=torch.from_numpy(polygons_reshaped).type(torch.FloatTensor)\n",
    "x_variable,y_variable=Variable(x_tensor).cuda(),Variable(x_tensor).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This auto encoder uses linear connections (Results may be the same as using PCA)\n",
    "class simple_autoencoder(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(simple_autoencoder,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "        nn.Linear(2*12,12),\n",
    "        nn.BatchNorm1d(12,momentum=0.5),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(12,6),\n",
    "        nn.BatchNorm1d(6,momentum=0.5),\n",
    "        nn.ReLU(True),      \n",
    "        nn.Linear(6,2),\n",
    "        nn.BatchNorm1d(2,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        nn.Linear(2,6),\n",
    "        nn.BatchNorm1d(6,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(6,12),\n",
    "        nn.BatchNorm1d(12,momentum=0.5),\n",
    "\n",
    "        nn.ReLU(True)    ,\n",
    "        nn.Linear(12,2*12)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Convolutional autoencoder\n",
    "class conv_autoencoder(nn.Module):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super(conv_autoencoder,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "        nn.Conv1d(1,12,kernel_size=2,stride=2),# d 24->12 stride=2 , filters =3\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv1d(12,24,kernel_size=2,stride=2),# d 12->6 stride=2, filters=6\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv1d(24,2,kernel_size=6,stride=1),#d  6->1 stride=1 ,filters=2   \n",
    "        )\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "        nn.ReLU(True),\n",
    "\n",
    "        nn.ConvTranspose1d(2,24,kernel_size=6,stride=1),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose1d(24,12,kernel_size=2,stride=2),\n",
    "        nn.ReLU(True),\n",
    "        nn.ConvTranspose1d(12,1,kernel_size=2,stride=2),\n",
    "\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=conv_autoencoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(\n",
    "model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.852904947916667\n",
      "10.485692651367188\n",
      "9.160875577799478\n",
      "5.76312891438802\n",
      "2.161543145751953\n",
      "1.2376622975667317\n",
      "1.0547772125244141\n",
      "1.0180186177571615\n",
      "1.0119929412841797\n",
      "1.011035595703125\n",
      "1.0108834320068358\n",
      "1.0108516235351563\n",
      "1.0108370422363282\n",
      "1.0108245391845703\n",
      "1.0108118153889973\n",
      "1.0107979044596354\n",
      "1.0107751169840495\n",
      "1.01073483988444\n",
      "1.0106841817220051\n",
      "1.0106299825032552\n",
      "1.010574951171875\n",
      "1.0105195383707681\n",
      "1.010461595662435\n",
      "1.0104009104410807\n",
      "1.0103416646321615\n",
      "1.0102793345133463\n",
      "1.010197661336263\n",
      "1.0101168986002604\n",
      "1.010040000406901\n",
      "1.0099657023111979\n",
      "1.0098911692301433\n",
      "1.0098235646565754\n",
      "1.0097573506673176\n",
      "1.0096921864827475\n",
      "1.0096276916503906\n",
      "1.0095635721842449\n",
      "1.009499688720703\n",
      "1.0094360748291016\n",
      "1.0093725880940756\n",
      "1.0093091552734375\n",
      "1.0092456573486328\n",
      "1.009182064819336\n",
      "1.0091183451334635\n",
      "1.0090543782552084\n",
      "1.0089901072184244\n",
      "1.0089255106608073\n",
      "1.0088606272379557\n",
      "1.0087892720540366\n",
      "1.0086877258300782\n",
      "1.0086062459309897\n",
      "1.0085306447347004\n",
      "1.0084563161214193\n",
      "1.008383169555664\n",
      "1.0083113108317057\n",
      "1.0082406056722004\n",
      "1.0081707143147787\n",
      "1.0081011006673177\n",
      "1.0080316660563151\n",
      "1.0079621571858723\n",
      "1.0078925201416016\n",
      "1.0078228251139323\n",
      "1.0077529724121095\n",
      "1.0076830637613932\n",
      "1.007613121541341\n",
      "1.0075430806477865\n",
      "1.0074731821695964\n",
      "1.0074037017822266\n",
      "1.0073345682779948\n",
      "1.0072656850179036\n",
      "1.0071969614664713\n",
      "1.0071260426839193\n",
      "1.007054926554362\n",
      "1.006985423787435\n",
      "1.0069161549886068\n",
      "1.006846132405599\n",
      "1.0067753896077474\n",
      "1.0067054168701173\n",
      "1.00663642578125\n",
      "1.0065447886149088\n",
      "1.006465434773763\n",
      "1.0063971588134766\n",
      "1.006330008951823\n",
      "1.0062631103515625\n",
      "1.0061964660644531\n",
      "1.006130335489909\n",
      "1.0060646881103517\n",
      "1.0059989267985026\n",
      "1.005934673055013\n",
      "1.005871219889323\n",
      "1.0058092956542968\n",
      "1.005748984781901\n",
      "1.0056901092529298\n",
      "1.0056328389485678\n",
      "1.0055775146484376\n",
      "1.0055242421468098\n",
      "1.0054729665120443\n",
      "1.0054237314860026\n",
      "1.0053767415364583\n",
      "1.0053317637125652\n",
      "1.0052888437906902\n",
      "1.0052481740315755\n",
      "1.0052098876953126\n",
      "1.005173887125651\n",
      "1.0051402201334636\n",
      "1.0051083974202475\n",
      "1.005078716023763\n",
      "1.0050511555989583\n",
      "1.005025601196289\n",
      "1.005000678507487\n",
      "1.004983352661133\n",
      "1.0049615427652996\n",
      "1.0049422393798828\n",
      "1.0049244160970052\n",
      "1.0049080759684246\n",
      "1.0048935434977213\n",
      "1.0048792185465494\n",
      "1.0048664276123047\n",
      "1.0048545989990234\n",
      "1.0048436635335287\n",
      "1.0048334340413412\n",
      "1.0048238403320313\n",
      "1.0048148264567058\n",
      "1.0048090087890624\n",
      "1.0047998107910157\n",
      "1.004791783650716\n",
      "1.004784184773763\n",
      "1.0047765106201172\n",
      "1.004769950358073\n",
      "1.0047632354736329\n",
      "1.0047566884358723\n",
      "1.0047503000895182\n",
      "1.0047440622965496\n",
      "1.0047379669189453\n",
      "1.0047319356282551\n",
      "1.0047260142008463\n",
      "1.004720273844401\n",
      "1.0047145497639973\n",
      "1.0047088887532551\n",
      "1.0047032155354818\n",
      "1.0046976633707683\n",
      "1.0046912618001302\n",
      "1.004687787882487\n",
      "1.0046813944498698\n",
      "1.004675848388672\n",
      "1.0046703348795574\n",
      "1.0046647786458334\n",
      "1.0040719248453776\n",
      "0.9888406412760417\n",
      "0.9571629984537761\n",
      "0.9345436859130859\n",
      "0.9281753946940104\n",
      "0.9270605651855469\n",
      "0.926520029703776\n",
      "0.926148432413737\n",
      "0.9258630645751953\n",
      "0.9256335988362631\n",
      "0.9254429738362631\n",
      "0.925280717976888\n",
      "0.9251414143880209\n",
      "0.9250227681477865\n",
      "0.9249207000732422\n",
      "0.9248309895833333\n",
      "0.9247529795328776\n",
      "0.9246851918538411\n",
      "0.9246256581624349\n",
      "0.9245735076904297\n",
      "0.9245272725423177\n",
      "0.9244855580647786\n",
      "0.9244482452392578\n",
      "0.9244141021728516\n",
      "0.9243840789794922\n",
      "0.9243548645019531\n",
      "0.9243276519775391\n",
      "0.9243033376057943\n",
      "0.9242798756917318\n",
      "0.9242592071533203\n",
      "0.9242382771809896\n",
      "0.9242186411539713\n",
      "0.9241991790771484\n",
      "0.924179936726888\n",
      "0.9241611002604166\n",
      "0.9241428527832031\n",
      "0.9241251953125\n",
      "0.9241083048502604\n",
      "0.9240920654296875\n",
      "0.9240756632486979\n",
      "0.9240597971598308\n",
      "0.924043901570638\n",
      "0.9240281768798828\n",
      "0.9240123453776041\n",
      "0.923997060139974\n",
      "0.9239824137369792\n",
      "0.9239684580485026\n",
      "0.923952436319987\n",
      "0.9239374287923177\n",
      "0.9239227966308594\n",
      "0.9239084259033203\n",
      "0.9238941680908204\n",
      "0.9238799845377604\n",
      "0.923865874226888\n",
      "0.9238516459147136\n",
      "0.9238373026529948\n",
      "0.9238228678385416\n",
      "0.9238085093180338\n",
      "0.9237943277994791\n",
      "0.9237800048828125\n",
      "0.9237656921386719\n",
      "0.9237510091145833\n",
      "0.9237364440917969\n",
      "0.9237229390462239\n",
      "0.9237088155110678\n",
      "0.9236931549072266\n",
      "0.9236777750651042\n",
      "0.9236623606363932\n",
      "0.9236467885335287\n",
      "0.9236311503092448\n",
      "0.9236153584798177\n",
      "0.9235993347167969\n",
      "0.9235833079020183\n",
      "0.9235668589274089\n",
      "0.9235503845214844\n",
      "0.9235338185628256\n",
      "0.9235168375651042\n",
      "0.9234995635986328\n",
      "0.9234821299235026\n",
      "0.9234643809000651\n",
      "0.9234463358561198\n",
      "0.9234276153564454\n",
      "0.9234090677897135\n",
      "0.9233902069091797\n",
      "0.9233709238688151\n",
      "0.9233513610839844\n",
      "0.9233313079833985\n",
      "0.9233111775716146\n",
      "0.9232906077067057\n",
      "0.9232694417317708\n",
      "0.9232480112711589\n",
      "0.9232262196858724\n",
      "0.9232155354817708\n",
      "0.9231866760253906\n",
      "0.9231514251708984\n",
      "0.9231246541341146\n",
      "0.9230991394042969\n",
      "0.9230732004801432\n",
      "0.9230461293538411\n",
      "0.9230209635416666\n",
      "0.9229936696370443\n",
      "0.9229653350830078\n",
      "0.9229358551025391\n",
      "0.922904985555013\n",
      "0.9228726928710937\n",
      "0.9228416900634766\n",
      "0.9228092030843099\n",
      "0.9227759541829427\n",
      "0.9227420928955078\n",
      "0.9227070658365886\n",
      "0.9226708323160807\n",
      "0.9226340555826823\n",
      "0.9225958607991537\n",
      "0.9225562377929688\n",
      "0.9225152170817057\n",
      "0.9224734517415365\n",
      "0.9224300201416016\n",
      "0.9223851643880209\n",
      "0.9223390197753907\n",
      "0.9222914662679036\n",
      "0.922241874186198\n",
      "0.9221904256184896\n",
      "0.9221380584716797\n",
      "0.9220825154622396\n",
      "0.9220248484293619\n",
      "0.9219647288004558\n",
      "0.9219032633463542\n",
      "0.9218392100016276\n",
      "0.9217727193196614\n",
      "0.9217042287190755\n",
      "0.9216307932535808\n",
      "0.921556319173177\n",
      "0.9214776835123698\n",
      "0.9213949686686198\n",
      "0.9213099650065104\n",
      "0.9212206207275391\n",
      "0.9211066640218099\n",
      "0.9209606150309245\n",
      "0.920853119913737\n",
      "0.9207422251383464\n",
      "0.9206263610839843\n",
      "0.9205049224853515\n",
      "0.9203769256591797\n",
      "0.9202421783447265\n",
      "0.9200886464436849\n",
      "0.9199287343343099\n",
      "0.9197683878580729\n",
      "0.9196001322428385\n",
      "0.9194223917643229\n",
      "0.9192291249593099\n",
      "0.9190306528727213\n",
      "0.9188217071533203\n",
      "0.9185663208007813\n",
      "0.9181310526529948\n",
      "0.9178498463948568\n",
      "0.9175586537679037\n",
      "0.9172519683837891\n",
      "0.9169231770833334\n",
      "0.9165757548014323\n",
      "0.916210395304362\n",
      "0.9158236551920573\n",
      "0.9154136037190755\n",
      "0.9149785441080729\n",
      "0.9145170450846354\n",
      "0.9140282368977865\n",
      "0.9135155395507812\n",
      "0.9129636159261068\n",
      "0.9123769185384114\n",
      "0.9117517608642578\n",
      "0.9110886454264323\n",
      "0.9103699737548828\n",
      "0.909533676147461\n",
      "0.9087025970458984\n",
      "0.9078409566243489\n",
      "0.9069511027018229\n",
      "0.9060355560302734\n",
      "0.90509189453125\n",
      "0.904113545735677\n",
      "0.9030344645182292\n",
      "0.9019727884928386\n",
      "0.9009085378011068\n",
      "0.8998244883219401\n",
      "0.8987228668212891\n",
      "0.8976074391682942\n",
      "0.8964281453450521\n",
      "0.895274755859375\n",
      "0.8941242004394532\n",
      "0.8929732340494791\n",
      "0.8918230346679688\n",
      "0.8906785196940105\n",
      "0.8895433553059896\n",
      "0.8884191375732422\n",
      "0.8873074513753255\n",
      "0.8862039815266927\n",
      "0.8851108968098959\n",
      "0.8840319264729818\n",
      "0.8829633209228516\n",
      "0.8819060852050781\n",
      "0.8808750162760417\n",
      "0.8798565724690756\n",
      "0.8788542236328125\n",
      "0.8778860270182292\n",
      "0.8769537862141927\n",
      "0.8760558420817057\n",
      "0.8751838053385417\n",
      "0.8743444396972656\n",
      "0.8735269317626954\n",
      "0.872737363688151\n",
      "0.8719770665486654\n",
      "0.8712499120076498\n",
      "0.8705518274943034\n",
      "0.8698771347045898\n",
      "0.8692305363972982\n",
      "0.8686081059773763\n",
      "0.8680110254923502\n",
      "0.8674359817504883\n",
      "0.8668848058064779\n",
      "0.8663543095906575\n",
      "0.8658438456217448\n",
      "0.8653522252400716\n",
      "0.8648783137003581\n",
      "0.8644245900472005\n",
      "0.8639844273885091\n",
      "0.863562028503418\n",
      "0.8631543197631836\n",
      "0.8627620178222656\n",
      "0.8623817301432292\n",
      "0.8620151789347331\n",
      "0.8616595357259115\n",
      "0.8613182362874349\n",
      "0.8609904485066732\n",
      "0.860671901957194\n",
      "0.8603674825032552\n",
      "0.8600733423868815\n",
      "0.8597890864054362\n",
      "0.8595151728312175\n",
      "0.8592512456258138\n",
      "0.8589951217651367\n",
      "0.8587480728149414\n",
      "0.858510004679362\n",
      "0.8582797007242838\n",
      "0.8580556864420573\n",
      "0.8578403676350912\n",
      "0.8576320912679036\n",
      "0.8574284489949544\n",
      "0.8572323649088541\n",
      "0.8570418126424154\n",
      "0.8568547063191732\n",
      "0.8566714579264323\n",
      "0.8564950373331706\n",
      "0.856322280883789\n",
      "0.8561550938924154\n",
      "0.8559959462483724\n",
      "0.855841949971517\n",
      "0.855691551208496\n",
      "0.8555456837972005\n",
      "0.8554042455037435\n",
      "0.8552671956380208\n",
      "0.8551338180541992\n",
      "0.8550048253377278\n",
      "0.8548791534423829\n",
      "0.8547575205485026\n",
      "0.8546386377970377\n",
      "0.8545229359944662\n",
      "0.8544109680175781\n",
      "0.8543010452270507\n",
      "0.8541944366455078\n",
      "0.8540917332967123\n",
      "0.8539918823242187\n",
      "0.8538931996663411\n",
      "0.8537971878051758\n",
      "0.8537054361979166\n",
      "0.853613405863444\n",
      "0.853526351928711\n",
      "0.853440793355306\n",
      "0.8533563191731771\n",
      "0.8532745971679687\n",
      "0.8531973098754883\n",
      "0.8531187795003256\n",
      "0.8530452824910482\n",
      "0.8529699086507161\n",
      "0.8528970362345377\n",
      "0.8528247472127278\n",
      "0.8527539159138997\n",
      "0.8526860712687174\n",
      "0.8526195907592773\n",
      "0.8525554499308268\n",
      "0.8524933191935221\n",
      "0.8524320978800456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8523728088378906\n",
      "0.8523144933064779\n",
      "0.8522572875976563\n",
      "0.8522036931355794\n",
      "0.8521491739908854\n",
      "0.8520974451700847\n",
      "0.8520469278971354\n",
      "0.851995915222168\n",
      "0.851947787475586\n",
      "0.8519010421752929\n",
      "0.8518543375651042\n",
      "0.8518090026855468\n",
      "0.8517645853678385\n",
      "0.8517212687174479\n",
      "0.8516784052530925\n",
      "0.8516361790974935\n",
      "0.8515958587646484\n",
      "0.8515565292358398\n",
      "0.8515173706054687\n",
      "0.8514800048828125\n",
      "0.8514432083129883\n",
      "0.8514073059082031\n",
      "0.8513722569783528\n",
      "0.851337434387207\n",
      "0.8513029174804687\n",
      "0.851270356241862\n",
      "0.8512375681559244\n",
      "0.8512063776652018\n",
      "0.8511762730916341\n",
      "0.8511439371744791\n",
      "0.8511152160644532\n",
      "0.8510848932902019\n",
      "0.8510551122029623\n",
      "0.8510279881795247\n",
      "0.8510011103312175\n",
      "0.8509738972981771\n",
      "0.8509472813924154\n",
      "0.8509212793986003\n",
      "0.8508948883056641\n",
      "0.8508701599121093\n",
      "0.850846467590332\n",
      "0.8508213190714519\n",
      "0.8507976313273112\n",
      "0.8507738240559896\n",
      "0.8507525126139323\n",
      "0.8507303202311198\n",
      "0.8507083887736002\n",
      "0.850686925760905\n",
      "0.8506663106282553\n",
      "0.850645258585612\n",
      "0.8506257802327474\n",
      "0.8506060963948567\n",
      "0.8505865315755209\n",
      "0.8505675572713216\n",
      "0.8505484614054362\n",
      "0.8505303278605143\n",
      "0.8505122095743816\n",
      "0.850494160970052\n",
      "0.8504772516886393\n",
      "0.8504599472045898\n",
      "0.8504434753417969\n",
      "0.850427197265625\n",
      "0.8504106618245443\n",
      "0.8503939341227214\n",
      "0.8503773259480795\n",
      "0.850361403910319\n",
      "0.8503467488606771\n",
      "0.8503301676432292\n",
      "0.8503142491658529\n",
      "0.8502983642578125\n",
      "0.8502838099161784\n",
      "0.8502684193929037\n",
      "0.8502538472493489\n",
      "0.8502387552897135\n",
      "0.8502254241943359\n",
      "0.8502114415486653\n",
      "0.8501978922526041\n",
      "0.850184558614095\n",
      "0.8501718922932943\n",
      "0.8501584167480469\n",
      "0.8501458501180013\n",
      "0.8501337987263997\n",
      "0.8501214294433593\n",
      "0.8501090703328451\n",
      "0.8500956609090169\n",
      "0.8500810526529948\n",
      "0.8500674194335938\n",
      "0.850055132039388\n",
      "0.8500417256673177\n",
      "0.8500293655395508\n",
      "0.8500169260660807\n",
      "0.8500052642822266\n",
      "0.8499925237019856\n",
      "0.8499804275512696\n",
      "0.8499684087117513\n",
      "0.8499567052205403\n",
      "0.8499450210571289\n",
      "0.8499339884440105\n",
      "0.8499231847127279\n",
      "0.8499122334798177\n",
      "0.849901101175944\n",
      "0.8498905405680338\n",
      "0.8498800710042318\n",
      "0.8498693618774414\n",
      "0.8498595296223959\n",
      "0.8498488093058268\n",
      "0.8498383280436198\n",
      "0.8498276814778646\n",
      "0.8498179758707682\n",
      "0.8498081782023112\n",
      "0.8497980707804362\n",
      "0.849788932800293\n",
      "0.849779089864095\n",
      "0.8497700937906901\n",
      "0.8497598139444987\n",
      "0.8497504872639974\n",
      "0.8497418599446614\n",
      "0.8497326507568359\n",
      "0.8497245239257812\n",
      "0.8497156407674153\n",
      "0.8497070398966471\n",
      "0.8496990819295247\n",
      "0.8496894170125325\n",
      "0.8496816055297851\n",
      "0.8496725199381511\n",
      "0.8496649098714193\n",
      "0.8496568313598633\n",
      "0.8496491607666016\n",
      "0.8496410252888997\n",
      "0.8496330912272135\n",
      "0.8496226038614909\n",
      "0.8496091232299805\n",
      "0.8495924875895182\n",
      "0.8495813420613607\n",
      "0.8495701212565104\n",
      "0.8495665186564128\n",
      "0.8495534271240235\n",
      "0.8495456181844075\n",
      "0.8495373174031575\n",
      "0.8495299331665039\n",
      "0.8495225443522135\n",
      "0.8495162684122721\n",
      "0.8495093165079752\n",
      "0.8495021301269531\n",
      "0.8494952519734701\n",
      "0.8494875005086263\n",
      "0.8494818552652995\n",
      "0.8494756627400716\n",
      "0.8494689193725586\n",
      "0.8494614120483398\n",
      "0.8494542668660482\n",
      "0.8494457244873047\n",
      "0.8494391296386719\n",
      "0.8494288050333659\n",
      "0.8494194279988607\n",
      "0.8494105036417643\n",
      "0.8494005304972331\n",
      "0.8493929189046224\n",
      "0.8493852259318034\n",
      "0.8493771911621094\n",
      "0.8493703684488932\n",
      "0.8493636199951172\n",
      "0.8493572580973308\n",
      "0.8493512685139974\n",
      "0.8493452982584635\n",
      "0.8493400680541993\n",
      "0.8493333343505859\n",
      "0.8493282190958659\n",
      "0.8493218882242839\n",
      "0.8493158594767253\n",
      "0.8493123723347982\n",
      "0.8493040252685546\n",
      "0.8492985371907552\n",
      "0.8492930068969726\n",
      "0.8492877354939778\n",
      "0.8492796376546224\n",
      "0.849274674987793\n",
      "0.8492688369750977\n",
      "0.8492626393636068\n",
      "0.8492582667032877\n",
      "0.8492531295776368\n",
      "0.8492482350667317\n",
      "0.849243901570638\n",
      "0.8492380447387695\n",
      "0.8492336130777994\n",
      "0.8492293492635091\n",
      "0.8492246256510416\n",
      "0.8492198750813802\n",
      "0.849215547688802\n",
      "0.8492113301595052\n",
      "0.8492076192220052\n",
      "0.8492027201334635\n",
      "0.8491980631510416\n",
      "0.8491950073242187\n",
      "0.8491906712849935\n",
      "0.8491849914550781\n",
      "0.8491811279296875\n",
      "0.8491759470621745\n",
      "0.8491719106038411\n",
      "0.849169064839681\n",
      "0.849164512125651\n",
      "0.8491603352864583\n",
      "0.8491559997558594\n",
      "0.8491525767008463\n",
      "0.8491472106933594\n",
      "0.8491442220052083\n",
      "0.8491400034586588\n",
      "0.84913646291097\n",
      "0.8491324203491211\n",
      "0.8491293899536133\n",
      "0.8491253087361653\n",
      "0.8491215627034505\n",
      "0.849118349202474\n",
      "0.8491147003173828\n",
      "0.8491110707600912\n",
      "0.8491070551554362\n",
      "0.849104231262207\n",
      "0.8491008122762044\n",
      "0.8490969065348307\n",
      "0.8490942911783854\n",
      "0.8490907572428386\n",
      "0.8490866780598958\n",
      "0.8490838694254558\n",
      "0.8490804229736328\n",
      "0.8490773376464844\n",
      "0.8490731109619141\n",
      "0.8490704157511393\n",
      "0.8490665079752604\n",
      "0.8490637685139973\n",
      "0.8490595860799154\n",
      "0.8490570002237956\n",
      "0.8490545750935873\n",
      "0.8490494466145834\n",
      "0.8490473958333333\n",
      "0.849044135538737\n",
      "0.849041172281901\n",
      "0.8490380137125652\n",
      "0.8490341603597005\n",
      "0.8490321248372396\n",
      "0.8490295033772787\n",
      "0.8490253229777018\n",
      "0.849021701558431\n",
      "0.8490198776245117\n",
      "0.8490148885091146\n",
      "0.849012082417806\n",
      "0.8490099614461263\n",
      "0.8490062922159831\n",
      "0.8490027572631836\n",
      "0.8489997146606445\n",
      "0.8489955627441407\n",
      "0.8489932678222656\n",
      "0.8489885986328125\n",
      "0.8489854629516601\n",
      "0.8489822479248047\n",
      "0.8489792282104492\n",
      "0.8489769159952799\n",
      "0.8489746739705404\n",
      "0.8489709777832031\n",
      "0.8489675206502278\n",
      "0.8489645904541016\n",
      "0.8489606689453125\n",
      "0.848957743326823\n",
      "0.8489547353108724\n",
      "0.8489518513997396\n",
      "0.8489491734822591\n",
      "0.8489447667439779\n",
      "0.848941318766276\n",
      "0.8489377283732097\n",
      "0.8489327621459961\n",
      "0.848929623413086\n",
      "0.8489262751261393\n",
      "0.8489214752197266\n",
      "0.8489174575805664\n",
      "0.8489121027628581\n",
      "0.848907867940267\n",
      "0.8489032491048177\n",
      "0.8488986882527669\n",
      "0.8488944239298503\n",
      "0.8488885935465494\n",
      "0.8488843928019205\n",
      "0.848879904683431\n",
      "0.8488766042073568\n",
      "0.8488721603393554\n",
      "0.8488693756103516\n",
      "0.8488650868733724\n",
      "0.8488618001302083\n",
      "0.848859201558431\n",
      "0.8488563379923503\n",
      "0.8488526657104493\n",
      "0.8488501647949219\n",
      "0.8488469411214192\n",
      "0.8488441177368165\n",
      "0.8488407353719075\n",
      "0.8488379206339518\n",
      "0.8488343541463216\n",
      "0.8488312850952149\n",
      "0.8488286819458007\n",
      "0.8488260106404623\n",
      "0.8488229487101236\n",
      "0.8488194152832031\n",
      "0.8488170232137044\n",
      "0.8488137440999349\n",
      "0.84881087697347\n",
      "0.8488077794392903\n",
      "0.8488049184163411\n",
      "0.8488020253499349\n",
      "0.8487997863769531\n",
      "0.8487961018880208\n",
      "0.8487933517456054\n",
      "0.8487896082560221\n",
      "0.8487869837443034\n",
      "0.8487837626139323\n",
      "0.8487811192830403\n",
      "0.8487780715942382\n",
      "0.8487755462646485\n",
      "0.8487719365437826\n",
      "0.8487687652587891\n",
      "0.8487661056518555\n",
      "0.848762863667806\n",
      "0.8487598856608073\n",
      "0.8487570185343425\n",
      "0.8487538396199544\n",
      "0.8487510813395183\n",
      "0.8487446192423502\n",
      "0.8487388631184896\n",
      "0.8487347407023113\n",
      "0.8487313969930013\n",
      "0.8487272501627604\n",
      "0.8487247182210287\n",
      "0.8487196258544922\n",
      "0.8487162470499674\n",
      "0.8487134465535482\n",
      "0.8487096527099609\n",
      "0.8487062779744466\n",
      "0.8487031509399414\n",
      "0.8486986323038737\n",
      "0.8486943598429362\n",
      "0.8486922200520833\n",
      "0.8486881266276042\n",
      "0.8486848841349284\n",
      "0.8486818069458008\n",
      "0.8486787165323894\n",
      "0.8486759145100912\n",
      "0.8486726425170898\n",
      "0.8486688980102539\n",
      "0.8486656692504883\n",
      "0.8486618235270182\n",
      "0.848659223429362\n",
      "0.8486562952677409\n",
      "0.8486535135904948\n",
      "0.8486497243245442\n",
      "0.8486462229410807\n",
      "0.8486437947591146\n",
      "0.8486401138305664\n",
      "0.8486372431437175\n",
      "0.8486343617757162\n",
      "0.8486311391194662\n",
      "0.8486279576619467\n",
      "0.8486254099527994\n",
      "0.848622201538086\n",
      "0.848618972269694\n",
      "0.848615641784668\n",
      "0.8486128194173177\n",
      "0.8486094319661458\n",
      "0.8486066309611002\n",
      "0.8486035934448242\n",
      "0.8486010218302409\n",
      "0.8485977966308593\n",
      "0.8485947896321615\n",
      "0.8485920008341471\n",
      "0.8485885203043619\n",
      "0.8485851969401041\n",
      "0.8485829081217448\n",
      "0.8485793914794922\n",
      "0.8485766728719075\n",
      "0.8485730484008789\n",
      "0.8485710276285807\n",
      "0.8485669921875\n",
      "0.8485648193359375\n",
      "0.8485609827677408\n",
      "0.8485577092488606\n",
      "0.8485547953287761\n",
      "0.848551405843099\n",
      "0.848548870340983\n",
      "0.848545962524414\n",
      "0.8485427708943685\n",
      "0.8485396891276041\n",
      "0.8485367411295572\n",
      "0.8485326090494791\n",
      "0.8485304524739583\n",
      "0.8485276051839192\n",
      "0.848524420674642\n",
      "0.8485209737141927\n",
      "0.8485181681315104\n",
      "0.8485153727213541\n",
      "0.8485133651733399\n",
      "0.8485098419189453\n",
      "0.8485061401367188\n",
      "0.848503564453125\n",
      "0.8485005559285482\n",
      "0.8484964960734049\n",
      "0.8484930089314778\n",
      "0.8484880472819011\n",
      "0.8484859751383463\n",
      "0.8484834370930989\n",
      "0.8484800359090169\n",
      "0.8484761795043946\n",
      "0.848472905476888\n",
      "0.8484696899414063\n",
      "0.8484660827636719\n",
      "0.8484630004882813\n",
      "0.8484590347290039\n",
      "0.8484544494628906\n",
      "0.8484496266682943\n",
      "0.8484458902994791\n",
      "0.8484427317301432\n",
      "0.8484381958007813\n",
      "0.8484337641398112\n",
      "0.8484310343424479\n",
      "0.8484280090332031\n",
      "0.8484241460164388\n",
      "0.8484195541381836\n",
      "0.8484161259969075\n",
      "0.8484123723347982\n",
      "0.8484088450113932\n",
      "0.8484056325276693\n",
      "0.8484019454956054\n",
      "0.8483982788085938\n",
      "0.8483953770955404\n",
      "0.8483914240519206\n",
      "0.8483885925292969\n",
      "0.8483851216634115\n",
      "0.8483818379720052\n",
      "0.8483783218383789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8483755828857422\n",
      "0.848372272237142\n",
      "0.8483691609700521\n",
      "0.8483656453450521\n",
      "0.8483622131347657\n",
      "0.8483587000528972\n",
      "0.8483552179972331\n",
      "0.8483515075683594\n",
      "0.8483485717773438\n",
      "0.8483446192423503\n",
      "0.8483416320800782\n",
      "0.8483376637776693\n",
      "0.8483335301717122\n",
      "0.8483299947102865\n",
      "0.8483264790852865\n",
      "0.8483226338704427\n",
      "0.8483194442749024\n",
      "0.8483156834920247\n",
      "0.84831204884847\n",
      "0.8483080937703451\n",
      "0.8483044865926107\n",
      "0.8483010945638021\n",
      "0.8482979532877604\n",
      "0.8482941696166992\n",
      "0.8482902877807618\n",
      "0.8482871576944987\n",
      "0.8482844065348307\n",
      "0.8482806564331055\n",
      "0.8482773473103841\n",
      "0.8482738093058269\n",
      "0.8482706395467122\n",
      "0.8482663630167643\n",
      "0.8482630564371745\n",
      "0.8482595993041993\n",
      "0.8482552764892578\n",
      "0.848252500406901\n",
      "0.8482488983154297\n",
      "0.8482444636027018\n",
      "0.8482410964965821\n",
      "0.8482377512613932\n",
      "0.8482339070638021\n",
      "0.8482305150349935\n",
      "0.8482266281127929\n",
      "0.8482220219930013\n",
      "0.8482186828613282\n",
      "0.8482150161743164\n",
      "0.8482110382080078\n",
      "0.8482074666341146\n",
      "0.8482042694091797\n",
      "0.8482006815592448\n",
      "0.848196810913086\n",
      "0.8481936269124349\n",
      "0.8481889745076497\n",
      "0.8481855555216471\n",
      "0.8481823287963867\n",
      "0.8481785369873047\n",
      "0.8481747283935547\n",
      "0.8481712895711263\n",
      "0.848166855875651\n",
      "0.8481625600179037\n",
      "0.8481585723876953\n",
      "0.8481546869913738\n",
      "0.8481506866455079\n",
      "0.8481465077718099\n",
      "0.8481424672444662\n",
      "0.8481384282430013\n",
      "0.8481352834065755\n",
      "0.8481307118733724\n",
      "0.8481271794637044\n",
      "0.8481227289835612\n",
      "0.8481184321085612\n",
      "0.8481147883097331\n",
      "0.8481110534667968\n",
      "0.8481066202799479\n",
      "0.8481027679443359\n",
      "0.8480984659830729\n",
      "0.8480942647298177\n",
      "0.8480906677246094\n",
      "0.8480860834757487\n",
      "0.8480815790812174\n",
      "0.8480774383544922\n",
      "0.8480731145222982\n",
      "0.8480691889444987\n",
      "0.8480644688924154\n",
      "0.8480602605183919\n",
      "0.8480550552368165\n",
      "0.8480516286214193\n",
      "0.8480471806844075\n",
      "0.8480422470092773\n",
      "0.8480389032999675\n",
      "0.8480344685872396\n",
      "0.8480294881184895\n",
      "0.8480258595784506\n",
      "0.8480221435546875\n",
      "0.8480168162027995\n",
      "0.8480115264892578\n",
      "0.848007073465983\n",
      "0.8480024490356445\n",
      "0.847997880045573\n",
      "0.8479932393391927\n",
      "0.8479881225585938\n",
      "0.8479840225219727\n",
      "0.847978982035319\n",
      "0.847974126180013\n",
      "0.8479697535196941\n",
      "0.8479655695597331\n",
      "0.847960540262858\n",
      "0.8479560770670573\n",
      "0.8479508326212565\n",
      "0.8479445755004883\n",
      "0.8479409225463868\n",
      "0.8479360224405924\n",
      "0.8479302439371745\n",
      "0.8479254694620768\n",
      "0.8479206720987956\n",
      "0.8479156010945638\n",
      "0.8479101201375325\n",
      "0.8479047566731771\n",
      "0.8478995666503907\n",
      "0.8478933024088542\n",
      "0.8478877044677734\n",
      "0.8478826029459635\n",
      "0.8478779693603515\n",
      "0.8478728571573894\n",
      "0.8478669759114583\n",
      "0.8478608688354492\n",
      "0.8478541742960612\n",
      "0.8478482147216797\n",
      "0.8478432662963867\n",
      "0.8478369084676107\n",
      "0.8478315663655599\n",
      "0.8478256067911784\n",
      "0.8478190836588542\n",
      "0.8478120239257813\n",
      "0.8478062001546224\n",
      "0.8477984588623046\n",
      "0.8477918294270833\n",
      "0.8477860015869141\n",
      "0.8477787953694661\n",
      "0.8477723434448242\n",
      "0.8477664077758789\n",
      "0.8477590220133464\n",
      "0.847753407796224\n",
      "0.8477471430460612\n",
      "0.8477414749145508\n",
      "0.8477349792480469\n",
      "0.8477278091430664\n",
      "0.8477216471354166\n",
      "0.8477144999186198\n",
      "0.8477081090291341\n",
      "0.8477018366495768\n",
      "0.8476944676717122\n",
      "0.8476892532348633\n",
      "0.847682258605957\n",
      "0.8476757807413737\n",
      "0.8476687820434571\n",
      "0.8476627293904623\n",
      "0.8476561147054037\n",
      "0.847649799601237\n",
      "0.8476436411539714\n",
      "0.8476369099934896\n",
      "0.847630989074707\n",
      "0.8476248041788736\n",
      "0.8476182754516601\n",
      "0.8476121576944987\n",
      "0.8476062032063802\n",
      "0.8475995244344076\n",
      "0.8475917470296224\n",
      "0.8475859242757161\n",
      "0.8475787063598633\n",
      "0.8475725977579752\n",
      "0.8475661783854167\n",
      "0.8475599655151367\n",
      "0.8475530665079752\n",
      "0.8475457951863606\n",
      "0.8475376490275065\n",
      "0.8475323359171549\n",
      "0.8475254653930664\n",
      "0.8475183868408203\n",
      "0.8475117706298828\n",
      "0.8475041208902995\n",
      "0.8474963302612305\n",
      "0.8474891057332357\n",
      "0.8474826156616211\n",
      "0.8474750162760417\n",
      "0.8474680074055989\n",
      "0.847461469523112\n",
      "0.8474530634562174\n",
      "0.8474454045613606\n",
      "0.8474378580729167\n",
      "0.8474311406453451\n",
      "0.8474243794759114\n",
      "0.8474159001668294\n",
      "0.8474096501668295\n",
      "0.8474010437011719\n",
      "0.8473929021199544\n",
      "0.847386506652832\n",
      "0.8473781341552734\n",
      "0.8473697580973307\n",
      "0.847362772623698\n",
      "0.8473549474080404\n",
      "0.8473468902587891\n",
      "0.8473390218098958\n",
      "0.8473305898030599\n",
      "0.8473220911661784\n",
      "0.8473126963297526\n",
      "0.8473035944620768\n",
      "0.84729501953125\n",
      "0.8472871368408204\n",
      "0.8472789494832357\n",
      "0.8472698267618816\n",
      "0.8472601359049479\n",
      "0.8472497650146484\n",
      "0.8472394912719726\n",
      "0.8472306040445964\n",
      "0.847219147237142\n",
      "0.8472098312377929\n",
      "0.8472002329508463\n",
      "0.8471901952107748\n",
      "0.8471772771199544\n",
      "0.8471680557250977\n",
      "0.8471551035563151\n",
      "0.8471393218994141\n",
      "0.8471278544108073\n",
      "0.8471156026204427\n",
      "0.8471037089029948\n",
      "0.8470934636433919\n",
      "0.8470816858927409\n",
      "0.8470694458007813\n",
      "0.8470578770955404\n",
      "0.8470471221923828\n",
      "0.8470362970987956\n",
      "0.8470252665201823\n",
      "0.8470141174316407\n",
      "0.8470020289103191\n",
      "0.84699130859375\n",
      "0.8469794998168946\n",
      "0.8469687657674153\n",
      "0.8469573603312175\n",
      "0.846947157796224\n",
      "0.8469358139038086\n",
      "0.8469235193888346\n",
      "0.846912814839681\n",
      "0.8469009287516276\n",
      "0.8468893910725911\n",
      "0.8468773844401042\n",
      "0.8468654266357422\n",
      "0.8468522303263346\n",
      "0.8468404907226562\n",
      "0.8468275563557943\n",
      "0.8468150563557942\n",
      "0.8468036204020182\n",
      "0.846790739440918\n",
      "0.8467793874104818\n",
      "0.8467667388916016\n",
      "0.8467544484456381\n",
      "0.8467426035563151\n",
      "0.8467288299560547\n",
      "0.8467156748453776\n",
      "0.8467014353434245\n",
      "0.8466866658528646\n",
      "0.8466730239868164\n",
      "0.8466592722574869\n",
      "0.8466439763387045\n",
      "0.8466301727294921\n",
      "0.8466150232950846\n",
      "0.8466003219604492\n",
      "0.8465848073323567\n",
      "0.8465693008422852\n",
      "0.8465548909505208\n",
      "0.846539774576823\n",
      "0.8465260177612305\n",
      "0.8465107940673828\n",
      "0.846496262105306\n",
      "0.8464821990966797\n",
      "0.8464665776570638\n",
      "0.8464509908040364\n",
      "0.8464350713094075\n",
      "0.846419948832194\n",
      "0.8464058461507161\n",
      "0.8463890614827474\n",
      "0.8463738520304362\n",
      "0.8463575551350911\n",
      "0.8463416961669922\n",
      "0.8463253829956054\n",
      "0.8463092041015625\n",
      "0.8462935511271159\n",
      "0.8462766026814779\n",
      "0.8462603159586588\n",
      "0.8462436177571615\n",
      "0.8462275466918945\n",
      "0.8462122675577799\n",
      "0.8461950892130534\n",
      "0.8461786183675131\n",
      "0.8461626358032227\n",
      "0.8461454386393229\n",
      "0.8461291809082031\n",
      "0.8461102605183919\n",
      "0.8460946436564127\n",
      "0.8460776301066081\n",
      "0.8460591740926107\n",
      "0.8460414021809896\n",
      "0.8460234553019206\n",
      "0.8460044026692708\n",
      "0.845984466044108\n",
      "0.8459667302449544\n",
      "0.8459491495768229\n",
      "0.8459284652709961\n",
      "0.8459099527994791\n",
      "0.8458907292683919\n",
      "0.8458712427775065\n",
      "0.8458525329589843\n",
      "0.8458334030151368\n",
      "0.8458141337076823\n",
      "0.8457943806966146\n",
      "0.8457754750569662\n",
      "0.8457553426106771\n",
      "0.8457357457478841\n",
      "0.8457158548990885\n",
      "0.8456949696858724\n",
      "0.8456755900065104\n",
      "0.8456550760904948\n",
      "0.8456339935302735\n",
      "0.8456128255208334\n",
      "0.8455920252482096\n",
      "0.8455723744710286\n",
      "0.8455519022623698\n",
      "0.8455310022989909\n",
      "0.8455101618448894\n",
      "0.8454897527058919\n",
      "0.8454682815551757\n",
      "0.8454454915364583\n",
      "0.8454232167561849\n",
      "0.8454020955403646\n",
      "0.8453801549275716\n",
      "0.8453577336629232\n",
      "0.8453353947957357\n",
      "0.8453141036987305\n",
      "0.845291807047526\n",
      "0.8452693659464519\n",
      "0.8452461486816406\n",
      "0.8452211303710937\n",
      "0.8451990315755208\n",
      "0.8451741612752278\n",
      "0.8451509735107422\n",
      "0.8451258880615234\n",
      "0.8451026814778646\n",
      "0.8450778574625651\n",
      "0.8450524597167969\n",
      "0.8450272618611654\n",
      "0.8450025115966797\n",
      "0.8449767486572266\n",
      "0.8449521443684895\n",
      "0.844926714070638\n",
      "0.844901035563151\n",
      "0.8448745854695638\n",
      "0.8448490544637044\n",
      "0.8448231307983398\n",
      "0.8447962763468424\n",
      "0.8447685394287109\n",
      "0.844741417948405\n",
      "0.8447145670572916\n",
      "0.8446876958211263\n",
      "0.8446615010579427\n",
      "0.8446335947672526\n",
      "0.8446054117838542\n",
      "0.8445786722819011\n",
      "0.8445499120076497\n",
      "0.8445208958943685\n",
      "0.8444928359985352\n",
      "0.844464741007487\n",
      "0.844434933980306\n",
      "0.8444085764567058\n",
      "0.8443778294881185\n",
      "0.8443488708496094\n",
      "0.8443188181559245\n",
      "0.844290702311198\n",
      "0.8442588490804036\n",
      "0.8442293172200521\n",
      "0.8442005828857422\n",
      "0.8441684748331706\n",
      "0.8441380030314127\n",
      "0.8441069803873698\n",
      "0.8440774403889973\n",
      "0.8440460642496744\n",
      "0.8440144332885742\n",
      "0.8439839294433594\n",
      "0.8439519571940104\n",
      "0.8439199773152669\n",
      "0.8438871032714844\n",
      "0.8438548263549804\n",
      "0.8438218383789062\n",
      "0.843789821879069\n",
      "0.8437548049926757\n",
      "0.8437197606404623\n",
      "0.8436845748901367\n",
      "0.8436499481201172\n",
      "0.8436167541503906\n",
      "0.8435817347208658\n",
      "0.8435452728271484\n",
      "0.8435097274780273\n",
      "0.8434605021158854\n",
      "0.84341195119222\n",
      "0.8433677241007487\n",
      "0.8433280461629232\n",
      "0.8432907536824544\n",
      "0.8432538136800131\n",
      "0.8432178227742513\n",
      "0.8431786870320638\n",
      "0.8431429204305013\n",
      "0.8431074762980143\n",
      "0.8430709615071614\n",
      "0.8430347356160481\n",
      "0.842994846089681\n",
      "0.8429577458699544\n",
      "0.8429219243367513\n",
      "0.8428843770345052\n",
      "0.8428473200480143\n",
      "0.8428106104532878\n",
      "0.8427737503051758\n",
      "0.8427360509236653\n",
      "0.8426980656941732\n",
      "0.8426618184407552\n",
      "0.8426218434651692\n",
      "0.8425845392862956\n",
      "0.8425459218343099\n",
      "0.8425049763997395\n",
      "0.8424643951416015\n",
      "0.8424268692016602\n",
      "0.8423882074991862\n",
      "0.8423498275756836\n",
      "0.8423101669311523\n",
      "0.8422708012898763\n",
      "0.8422305552164714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421905919392904\n",
      "0.8421491485595703\n",
      "0.8421087020874023\n",
      "0.8420701548258464\n",
      "0.8420309387207031\n",
      "0.8419896326700846\n",
      "0.8419498036702474\n",
      "0.8419096059163411\n",
      "0.8418681004842122\n",
      "0.8418282953898112\n",
      "0.841787614440918\n",
      "0.8417467936197917\n",
      "0.8417069620768229\n",
      "0.841665500386556\n",
      "0.8416239552815755\n",
      "0.8415831400553385\n",
      "0.8415409378051758\n",
      "0.8414979532877604\n",
      "0.8414595687866211\n",
      "0.8414169794718425\n",
      "0.841374478149414\n",
      "0.8413337478637696\n",
      "0.8412915364583333\n",
      "0.8412498352050781\n",
      "0.84120706837972\n",
      "0.841164469909668\n",
      "0.8411218531290691\n",
      "0.8410811518351237\n",
      "0.8410373794555664\n",
      "0.8409947682698568\n",
      "0.8409534057617187\n",
      "0.8409121668497721\n",
      "0.8408695765177409\n",
      "0.8408282887776692\n",
      "0.8407875498453776\n",
      "0.8407436711629231\n",
      "0.8407021784464518\n",
      "0.840656477355957\n",
      "0.8406147323608398\n",
      "0.8405723973592122\n",
      "0.8405274490356446\n",
      "0.8404849904378255\n",
      "0.8404403106689453\n",
      "0.8403977091471354\n",
      "0.8403581766764323\n",
      "0.8403138544718425\n",
      "0.8402698211669922\n",
      "0.8402252131144206\n",
      "0.8401842122395833\n",
      "0.8401386525472005\n",
      "0.8400957010904948\n",
      "0.8400510655721029\n",
      "0.8400073420206706\n",
      "0.8399658192952474\n",
      "0.8399195297241211\n",
      "0.8398749354044597\n",
      "0.8398281794230144\n",
      "0.8397811014811198\n",
      "0.839737446085612\n",
      "0.8396761672973633\n",
      "0.8396254857381185\n",
      "0.8395780507405599\n",
      "0.8395282801310221\n",
      "0.8394789047241211\n",
      "0.8394344131469726\n",
      "0.8393855336507161\n",
      "0.8393421076456705\n",
      "0.8392942891438802\n",
      "0.8392448404947916\n",
      "0.8391962346394857\n",
      "0.8391459218343099\n",
      "0.8391011484781901\n",
      "0.8390504994710286\n",
      "0.8389996597290039\n",
      "0.8389517247517904\n",
      "0.8388992360432943\n",
      "0.8388523020426433\n",
      "0.8388069208780925\n",
      "0.8387573623657226\n",
      "0.8387096959431967\n",
      "0.8386609756469726\n",
      "0.8386141153971354\n",
      "0.8385633560180664\n",
      "0.8385109893798828\n",
      "0.8384645182291667\n",
      "0.8384148325602213\n",
      "0.838368056233724\n",
      "0.8383186798095703\n",
      "0.8382683115641276\n",
      "0.8382192662556966\n",
      "0.8381693394978841\n",
      "0.838120881652832\n",
      "0.8380708567301433\n",
      "0.8380239700317382\n",
      "0.8379749170939128\n",
      "0.8379279993693034\n",
      "0.8378823262532552\n",
      "0.8378341512044271\n",
      "0.837787880452474\n",
      "0.8377403472900391\n",
      "0.8376920700073243\n",
      "0.8376447642008463\n",
      "0.8375947336832682\n",
      "0.8375466837565104\n",
      "0.8375017023722331\n",
      "0.8374552291870118\n",
      "0.8374068918863933\n",
      "0.8373639195760091\n",
      "0.8373107192993164\n",
      "0.8372687744140626\n",
      "0.8372185145060221\n",
      "0.8371691360473633\n",
      "0.8371245361328125\n",
      "0.8370786493937175\n",
      "0.8370323201497396\n",
      "0.8369804336547851\n",
      "0.8369333257039389\n",
      "0.8368841684977214\n",
      "0.8368394485473633\n",
      "0.8367966456095378\n",
      "0.8367534764607747\n",
      "0.8367085184733073\n",
      "0.8366631805419922\n",
      "0.8366156397501627\n",
      "0.8365761983235677\n",
      "0.836523742167155\n",
      "0.8364819112141927\n",
      "0.8364397867838541\n",
      "0.8363962855021159\n",
      "0.8363598627726238\n",
      "0.8363125651041666\n",
      "0.8362679885864258\n",
      "0.8362285669962565\n",
      "0.8361838612874349\n",
      "0.8361384735107422\n",
      "0.8360933995564779\n",
      "0.8360504404703776\n",
      "0.8360072555541992\n",
      "0.8359626419067383\n",
      "0.8359241236368815\n",
      "0.8358807713826497\n",
      "0.8358449554443359\n",
      "0.8358035629272461\n",
      "0.8357595403035482\n",
      "0.8357133275349935\n",
      "0.8356744272867839\n",
      "0.8356312835693359\n",
      "0.8355953521728515\n",
      "0.8355543807983399\n",
      "0.835512343343099\n",
      "0.8354748524983724\n",
      "0.8354372863769531\n",
      "0.8354017171223959\n",
      "0.8353613367716471\n",
      "0.8353186147054037\n",
      "0.8352778798421224\n",
      "0.8352394337972006\n",
      "0.835201372273763\n",
      "0.8351627487182617\n",
      "0.8351246256510416\n",
      "0.8350891967773437\n",
      "0.8350497177124023\n",
      "0.8350128779093424\n",
      "0.8349775466918945\n",
      "0.8349435719807943\n",
      "0.8349073669433594\n",
      "0.8348692169189453\n",
      "0.8348296279907227\n",
      "0.8347928654988607\n",
      "0.8347548065185547\n",
      "0.8347176940917969\n",
      "0.8346823989868164\n",
      "0.8346470291137695\n",
      "0.8346102584838867\n",
      "0.8345742472330729\n",
      "0.8345403533935547\n",
      "0.8345055953979492\n",
      "0.8344713256835937\n",
      "0.8344353551228841\n",
      "0.8344008850097656\n",
      "0.8343685704549154\n",
      "0.8343340560913086\n",
      "0.8342998143513998\n",
      "0.8342656855265299\n",
      "0.8342332936604818\n",
      "0.8342018742879231\n",
      "0.8341695882161458\n",
      "0.8341383015950521\n",
      "0.8341046900431315\n",
      "0.834073199971517\n",
      "0.8340391301472981\n",
      "0.8340100341796876\n",
      "0.8339719299316406\n",
      "0.833941433207194\n",
      "0.833910513305664\n",
      "0.8338775238037109\n",
      "0.8338451293945313\n",
      "0.8338114023844401\n",
      "0.8337822148640951\n",
      "0.8337522791544596\n",
      "0.8337224634806315\n",
      "0.8336944905598959\n",
      "0.8336620961507162\n",
      "0.8336325459798177\n",
      "0.8336006902058919\n",
      "0.833567377726237\n",
      "0.8335374577840169\n",
      "0.8335079823811848\n",
      "0.8334807662963867\n",
      "0.8334482284545899\n",
      "0.8334203389485677\n",
      "0.833395187886556\n",
      "0.8333667353312174\n",
      "0.8333393936157226\n",
      "0.8333065200805664\n",
      "0.8332780771891276\n",
      "0.8332527648925782\n",
      "0.8332267537434895\n",
      "0.8331991353352864\n",
      "0.8331713033040364\n",
      "0.833145127360026\n",
      "0.8331179829915365\n",
      "0.8330853500366211\n",
      "0.8330624821980794\n",
      "0.8330319610595703\n",
      "0.83300625\n",
      "0.8329798166910807\n",
      "0.8329459660847982\n",
      "0.8329241882324219\n",
      "0.8328979324340821\n",
      "0.8328753875732422\n",
      "0.8328465067545573\n",
      "0.8328225468953451\n",
      "0.8327982177734375\n",
      "0.8327725880940755\n",
      "0.8327492101033529\n",
      "0.8327229415893554\n",
      "0.8326975006103515\n",
      "0.8326698064168294\n",
      "0.8326441594441731\n",
      "0.8326229080200195\n",
      "0.8325938400268554\n",
      "0.8325695388793946\n",
      "0.832546224975586\n",
      "0.8325251993815104\n",
      "0.8325030125935873\n",
      "0.8324799748738607\n",
      "0.8324561238606771\n",
      "0.8324319081624348\n",
      "0.8324060399373372\n",
      "0.832384917195638\n",
      "0.8323630788167318\n",
      "0.8323416585286458\n",
      "0.8323190373738607\n",
      "0.8322950836181641\n",
      "0.832275092569987\n",
      "0.8322523076375326\n",
      "0.8322328033447266\n",
      "0.8322081726074219\n",
      "0.8321854400634766\n",
      "0.8321678263346354\n",
      "0.8321477961222331\n",
      "0.8321308222452799\n",
      "0.8321014958699544\n",
      "0.8320785929361979\n",
      "0.8320543080647786\n",
      "0.8320315678914388\n",
      "0.8320107533772787\n",
      "0.8319893407185872\n",
      "0.8319692072550455\n",
      "0.8319513570149739\n",
      "0.8319251556396484\n",
      "0.8318979242960612\n",
      "0.8318778325398763\n",
      "0.8318526936848958\n",
      "0.8318329350789389\n",
      "0.8318127176920573\n",
      "0.8317940973917644\n",
      "0.8317717849731445\n",
      "0.8317517735799154\n",
      "0.8317302174886068\n",
      "0.8317105336507161\n",
      "0.8316865198771158\n",
      "0.8316664128621419\n",
      "0.8316469802856445\n",
      "0.8316270833333334\n",
      "0.8316051869710287\n",
      "0.8315858922322591\n",
      "0.8315697219848632\n",
      "0.8315497665405274\n",
      "0.831528992207845\n",
      "0.8315095789591471\n",
      "0.8314872548421224\n",
      "0.8314645202636719\n",
      "0.8314415405273438\n",
      "0.8314214248657227\n",
      "0.831396945699056\n",
      "0.831381733194987\n",
      "0.8313663721720378\n",
      "0.8313525644938151\n",
      "0.8313368240356446\n",
      "0.8313197697957356\n",
      "0.8313000483194987\n",
      "0.8312827107747396\n",
      "0.8312679723103841\n",
      "0.8312530085245768\n",
      "0.8312341364542644\n",
      "0.8312085032145182\n",
      "0.8311854461669922\n",
      "0.8311573867797851\n",
      "0.8311409657796224\n",
      "0.8311263117472331\n",
      "0.8311046915690105\n",
      "0.8310872955322266\n",
      "0.8310713439941406\n",
      "0.8310536661783854\n",
      "0.8310344451904297\n",
      "0.8310165715535481\n",
      "0.8309986216227213\n",
      "0.8309798990885416\n",
      "0.8309622945149739\n",
      "0.8309406565348307\n",
      "0.8309251302083334\n",
      "0.8309079569498697\n",
      "0.8308923594156901\n",
      "0.8308755310058594\n",
      "0.8308594746907552\n",
      "0.8308451975504557\n",
      "0.8308269841512044\n",
      "0.8308095891316731\n",
      "0.8307914916992187\n",
      "0.8307827362060547\n",
      "0.8307617614746093\n",
      "0.830740781656901\n",
      "0.8307288111368815\n",
      "0.8307145751953126\n",
      "0.8306994196573894\n",
      "0.830682383219401\n",
      "0.830668473815918\n",
      "0.830649658203125\n",
      "0.8306349319458007\n",
      "0.8306207784016927\n",
      "0.8306059117635091\n",
      "0.8305875595092773\n",
      "0.8305760101318359\n",
      "0.8305604807535807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-1ca7b3be8cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# clear gradients for next train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# backpropagation, compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# apply gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Users\\papagian\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=model.cuda()\n",
    "nb_of_epochs=4000\n",
    "batch_size=int(x_variable.size(0)/100)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(nb_of_epochs):\n",
    "    sum_loss=0\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        out = model(x_variable.narrow(0,b,batch_size))                \n",
    "        loss = criterion(out, y_variable.narrow(0,b,batch_size))     \n",
    "        sum_loss+=loss.data[0]\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "    print(sum_loss/x_variable.size(0))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv1d(1, 12, kernel_size=(2,), stride=(2,))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv1d(12, 24, kernel_size=(2,), stride=(2,))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): Conv1d(24, 2, kernel_size=(6,), stride=(1,))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ReLU(inplace)\n",
       "    (1): ConvTranspose1d(2, 24, kernel_size=(6,), stride=(1,))\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose1d(24, 12, kernel_size=(2,), stride=(2,))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): ConvTranspose1d(12, 1, kernel_size=(2,), stride=(2,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable=model.cpu(),x_variable.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(  0  ,.,.) = \n",
       "  7.2590e-01  1.7160e-01  8.4208e-01  ...  -8.6179e-01  6.2929e-01 -2.9497e-01\n",
       "\n",
       "(  1  ,.,.) = \n",
       "  1.1056e+00  8.5052e-02  1.0595e+00  ...  -8.8101e-01  5.4964e-01 -4.5626e-01\n",
       "\n",
       "(  2  ,.,.) = \n",
       "  1.3043e+00 -1.6374e-01  7.1454e-01  ...  -1.0654e+00  3.5652e-01 -3.6640e-01\n",
       " ...  \n",
       "\n",
       "(59997,.,.) = \n",
       "  1.0052e+00  1.3537e-01  7.4466e-01  ...  -8.4981e-01  6.0202e-01 -5.4357e-01\n",
       "\n",
       "(59998,.,.) = \n",
       "  7.9475e-01 -1.6893e-01  4.9571e-01  ...  -4.6446e-01  1.2147e+00 -5.7865e-01\n",
       "\n",
       "(59999,.,.) = \n",
       "  1.3487e+00  1.5864e-02  7.7996e-01  ...  -4.4117e-01  5.4940e-01 -2.1820e-01\n",
       "[torch.FloatTensor of size 60000x1x24]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polygon_prediction=model(x_variable)\n",
    "polygon_prediction=polygon_prediction[89].data.cpu()\n",
    "polygon_prediction=polygon_prediction.numpy()\n",
    "polygon_prediction=polygon_prediction.reshape(12,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11205034048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_contour(polygon_prediction)\n",
    "plot_contour(polygons[89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv1d in module torch.nn.modules.conv:\n",
      "\n",
      "class Conv1d(_ConvNd)\n",
      " |  Applies a 1D convolution over an input signal composed of several input\n",
      " |  planes.\n",
      " |  \n",
      " |  In the simplest case, the output value of the layer with input size\n",
      " |  :math:`(N, C_{in}, L)` and output :math:`(N, C_{out}, L_{out})` can be\n",
      " |  precisely described as:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      \\begin{array}{ll}\n",
      " |      out(N_i, C_{out_j})  = bias(C_{out_j})\n",
      " |                     + \\sum_{{k}=0}^{C_{in}-1} weight(C_{out_j}, k)  \\star input(N_i, k)\n",
      " |      \\end{array}\n",
      " |  \n",
      " |  where :math:`\\star` is the valid `cross-correlation`_ operator,\n",
      " |  :math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
      " |  :math:`L` is a length of signal sequence.\n",
      " |  \n",
      " |  | :attr:`stride` controls the stride for the cross-correlation, a single\n",
      " |    number or a one-element tuple.\n",
      " |  | :attr:`padding` controls the amount of implicit zero-paddings on both\n",
      " |  |  sides for :attr:`padding` number of points.\n",
      " |  | :attr:`dilation` controls the spacing between the kernel points; also\n",
      " |    known as the à trous algorithm. It is harder to describe, but this `link`_\n",
      " |    has a nice visualization of what :attr:`dilation` does.\n",
      " |  | :attr:`groups` controls the connections between inputs and outputs.\n",
      " |    `in_channels` and `out_channels` must both be divisible by `groups`.\n",
      " |  |       At groups=1, all inputs are convolved to all outputs.\n",
      " |  |       At groups=2, the operation becomes equivalent to having two conv\n",
      " |               layers side by side, each seeing half the input channels,\n",
      " |               and producing half the output channels, and both subsequently\n",
      " |               concatenated.\n",
      " |          At groups=`in_channels`, each input channel is convolved with its\n",
      " |               own set of filters (of size `out_channels // in_channels`).\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |       Depending of the size of your kernel, several (of the last)\n",
      " |       columns of the input might be lost, because it is a valid\n",
      " |       `cross-correlation`_, and not a full `cross-correlation`_.\n",
      " |       It is up to the user to add proper padding.\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |       The configuration when `groups == in_channels` and `out_channels = K * in_channels`\n",
      " |       where `K` is a positive integer is termed in literature as depthwise convolution.\n",
      " |  \n",
      " |       In other words, for an input of size :math:`(N, C_{in}, L_{in})`, if you want a\n",
      " |       depthwise convolution with a depthwise multiplier `K`,\n",
      " |       then you use the constructor arguments\n",
      " |       :math:`(in\\_channels=C_{in}, out\\_channels=C_{in} * K, ..., groups=C_{in})`\n",
      " |  \n",
      " |  Args:\n",
      " |      in_channels (int): Number of channels in the input image\n",
      " |      out_channels (int): Number of channels produced by the convolution\n",
      " |      kernel_size (int or tuple): Size of the convolving kernel\n",
      " |      stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      " |      padding (int or tuple, optional): Zero-padding added to both sides of\n",
      " |          the input. Default: 0\n",
      " |      dilation (int or tuple, optional): Spacing between kernel\n",
      " |          elements. Default: 1\n",
      " |      groups (int, optional): Number of blocked connections from input\n",
      " |          channels to output channels. Default: 1\n",
      " |      bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C_{in}, L_{in})`\n",
      " |      - Output: :math:`(N, C_{out}, L_{out})` where\n",
      " |        :math:`L_{out} = floor((L_{in}  + 2 * padding - dilation * (kernel\\_size - 1) - 1) / stride + 1)`\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight (Tensor): the learnable weights of the module of shape\n",
      " |          (out_channels, in_channels, kernel_size)\n",
      " |      bias (Tensor):   the learnable bias of the module of shape\n",
      " |          (out_channels)\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Conv1d(16, 33, 3, stride=2)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 16, 50))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  .. _cross-correlation:\n",
      " |      https://en.wikipedia.org/wiki/Cross-correlation\n",
      " |  \n",
      " |  .. _link:\n",
      " |      https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Conv1d\n",
      " |      _ConvNd\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _ConvNd:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Conv1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97967017,  0.00463816],\n",
       "       [ 0.84873545,  0.50282168],\n",
       "       [ 0.38454553,  0.68619001],\n",
       "       [-0.00177162,  0.84484351],\n",
       "       [-0.40613338,  0.69851059],\n",
       "       [-0.9370811 ,  0.56263232],\n",
       "       [-0.90546513,  0.00325594],\n",
       "       [-0.76080012, -0.44539672],\n",
       "       [-0.39749897, -0.67931378],\n",
       "       [ 0.00169786, -0.92938364],\n",
       "       [ 0.49300653, -0.82771182],\n",
       "       [ 0.70441866, -0.41366461]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNorm1d in module torch.nn.modules.batchnorm:\n",
      "\n",
      "class BatchNorm1d(_BatchNorm)\n",
      " |  Applies Batch Normalization over a 2d or 3d input that is seen as a\n",
      " |  mini-batch.\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated per-dimension over\n",
      " |  the mini-batches and gamma and beta are learnable parameter vectors\n",
      " |  of size C (where C is the input size).\n",
      " |  \n",
      " |  During training, this layer keeps a running estimate of its computed mean\n",
      " |  and variance. The running sum is kept with a default momentum of 0.1.\n",
      " |  \n",
      " |  During evaluation, this running mean/variance is used for normalization.\n",
      " |  \n",
      " |  Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      " |  on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
      " |  \n",
      " |  Args:\n",
      " |      num_features: num_features from an expected input of size\n",
      " |          `batch_size x num_features [x width]`\n",
      " |      eps: a value added to the denominator for numerical stability.\n",
      " |          Default: 1e-5\n",
      " |      momentum: the value used for the running_mean and running_var\n",
      " |          computation. Default: 0.1\n",
      " |      affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      " |          affine parameters. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
      " |      - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100)\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100, affine=False)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 100))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNorm1d\n",
      " |      _BatchNorm\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _BatchNorm:\n",
      " |  \n",
      " |  __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BatchNorm1d in module torch.nn.modules.batchnorm:\n",
      "\n",
      "class BatchNorm1d(_BatchNorm)\n",
      " |  Applies Batch Normalization over a 2d or 3d input that is seen as a\n",
      " |  mini-batch.\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated per-dimension over\n",
      " |  the mini-batches and gamma and beta are learnable parameter vectors\n",
      " |  of size C (where C is the input size).\n",
      " |  \n",
      " |  During training, this layer keeps a running estimate of its computed mean\n",
      " |  and variance. The running sum is kept with a default momentum of 0.1.\n",
      " |  \n",
      " |  During evaluation, this running mean/variance is used for normalization.\n",
      " |  \n",
      " |  Because the BatchNorm is done over the `C` dimension, computing statistics\n",
      " |  on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
      " |  \n",
      " |  Args:\n",
      " |      num_features: num_features from an expected input of size\n",
      " |          `batch_size x num_features [x width]`\n",
      " |      eps: a value added to the denominator for numerical stability.\n",
      " |          Default: 1e-5\n",
      " |      momentum: the value used for the running_mean and running_var\n",
      " |          computation. Default: 0.1\n",
      " |      affine: a boolean value that when set to ``True``, gives the layer learnable\n",
      " |          affine parameters. Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
      " |      - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
      " |  \n",
      " |  Examples:\n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100)\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.BatchNorm1d(100, affine=False)\n",
      " |      >>> input = autograd.Variable(torch.randn(20, 100))\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BatchNorm1d\n",
      " |      _BatchNorm\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _BatchNorm:\n",
      " |  \n",
      " |  __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  forward(self, input)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>>\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Linear (2 -> 2)\n",
      " |          Parameter containing:\n",
      " |           1  1\n",
      " |           1  1\n",
      " |          [torch.FloatTensor of size 2x2]\n",
      " |          Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True`` then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :func:`state_dict()` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n",
      " |              match the keys returned by this module's `:func:`state_dict()`\n",
      " |              function.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      When keep_vars is ``True``, it returns a Variable for each parameter\n",
      " |      (rather than a Tensor).\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional):\n",
      " |              if not None, the return dictionary is stored into destination.\n",
      " |              Default: None\n",
      " |          prefix (string, optional): Adds a prefix to the key (name) of every\n",
      " |              parameter and buffer in the result dictionary. Default: ''\n",
      " |          keep_vars (bool, optional): if ``True``, returns a Variable for each\n",
      " |              parameter. If ``False``, returns a Tensor for each parameter.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to dst_type.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Variable(torch.FloatTensor([[[0,0,3]]]))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 24])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = nn.Conv1d(in_channels=1,out_channels=2,kernel_size=2)\n",
    "x_variable=x_variable.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.2492 -1.6630\n",
       "  0.1763  0.8156\n",
       "[torch.FloatTensor of size 1x2x2]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 24])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(in_channels=1,out_channels=1,kernel_size=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (CUDAFloatTensor) and weight type (CPUFloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c33b0b8d459e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 168\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0m_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (CUDAFloatTensor) and weight type (CPUFloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "m(x_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "func=nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ba008a11b8a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "func(m(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variable=x_variable.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(24, 14)\n",
    "        self.fc21 = nn.Linear(14, 2)\n",
    "        self.fc22 = nn.Linear(14, 2)\n",
    "        self.fc3 = nn.Linear(2, 14)\n",
    "        self.fc4 = nn.Linear(14, 24)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "reconstruction_function = nn.BCELoss()\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    BCE = reconstruction_function(recon_x, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08427379811604818\n",
      "0.05906171976725261\n",
      "0.042195732879638674\n",
      "0.02583740832010905\n",
      "0.012065117899576823\n",
      "0.003920155477523803\n",
      "0.0017312102317810058\n",
      "0.0017457127412160238\n",
      "0.0014478217919667562\n",
      "0.0011840258439381917\n",
      "0.0010786115248998006\n",
      "0.0009954208215077719\n",
      "0.0009241549412409465\n",
      "0.0008680975914001465\n",
      "0.0008176822662353516\n",
      "0.0007710820436477661\n",
      "0.0007279782851537069\n",
      "0.000687526289621989\n",
      "0.0006492488662401835\n",
      "0.0006128788749376932\n",
      "0.00057838667233785\n",
      "0.0005447821577390034\n",
      "0.0005132324814796448\n",
      "0.0004823383053143819\n",
      "0.00045259085496266685\n",
      "0.0004236271778742472\n",
      "0.0003961742361386617\n",
      "0.0003692155043284098\n",
      "0.00034255663951237995\n",
      "0.0003169219692548116\n",
      "0.00029175037145614624\n",
      "0.00026713016231854757\n",
      "0.00024306388894716898\n",
      "0.00021923057238260906\n",
      "0.00019581817785898844\n",
      "0.00017181203365325927\n",
      "0.00014876575271288554\n",
      "0.0001262182076772054\n",
      "0.00010323645273844401\n",
      "8.076347708702088e-05\n",
      "5.763450264930725e-05\n",
      "3.502514958381653e-05\n",
      "1.3118181626001994e-05\n",
      "-9.948378801345826e-06\n",
      "-3.226481874783834e-05\n",
      "-5.5160810550053915e-05\n",
      "-7.773837248484293e-05\n",
      "-0.00010087252259254456\n",
      "-0.0001245050589243571\n",
      "-0.00014702658255894978\n",
      "-0.00016969714959462484\n",
      "-0.00019413987596829732\n",
      "-0.00021716567675272624\n",
      "-0.00024070487419764201\n",
      "-0.0002648278892040253\n",
      "-0.00028889471093813577\n",
      "-0.0003140231390794118\n",
      "-0.000338344810406367\n",
      "-0.00036256824334462485\n",
      "-0.00038730589548746744\n",
      "-0.00041285643180211383\n",
      "-0.00043880128463109333\n",
      "-0.0004643892248471578\n",
      "-0.0004898603439331055\n",
      "-0.0005171652634938558\n",
      "-0.0005414639512697855\n",
      "-0.0005675115744272868\n",
      "-0.00059354194800059\n",
      "-0.00061952512661616\n",
      "-0.000646462603410085\n",
      "-0.000672138520081838\n",
      "-0.0006985012372334798\n",
      "-0.0007241184949874878\n",
      "-0.0007483807245890299\n",
      "-0.0007721872329711914\n",
      "-0.0007963706572850545\n",
      "-0.0008200628836949667\n",
      "-0.0008421722094217936\n",
      "-0.0008645760297775268\n",
      "-0.0008869690497716267\n",
      "-0.000906771961847941\n",
      "-0.0009258384466171264\n",
      "-0.0009429646094640096\n",
      "-0.0009616099754969279\n",
      "-0.000977546739578247\n",
      "-0.0009956653197606405\n",
      "-0.0010086373805999755\n",
      "-0.0010235898812611898\n",
      "-0.0010366857290267945\n",
      "-0.001048962680498759\n",
      "-0.0010609579881032307\n",
      "-0.0010727603594462077\n",
      "-0.0010831118106842041\n",
      "-0.001092466974258423\n",
      "-0.0011020208994547526\n",
      "-0.0011104041735331218\n",
      "-0.0011187591552734375\n",
      "-0.0011256612300872802\n",
      "-0.0011327897389729817\n",
      "-0.001139420970280965\n",
      "-0.0011452138423919678\n",
      "-0.001151212207476298\n",
      "-0.001156295649210612\n",
      "-0.0011609936793645222\n",
      "-0.0011651495377222698\n",
      "-0.0011696418444315593\n",
      "-0.0011731675068537395\n",
      "-0.0011766764958699545\n",
      "-0.0011799752871195475\n",
      "-0.0011829618215560913\n",
      "-0.001185554536183675\n",
      "-0.0011882928450902303\n",
      "-0.0011904172897338867\n",
      "-0.0011926185846328735\n",
      "-0.0011944268067677817\n",
      "-0.0011962531248728434\n",
      "-0.0011979233741760255\n",
      "-0.0011995075702667236\n",
      "-0.001200924770037333\n",
      "-0.001202187689145406\n",
      "-0.0012035684585571288\n",
      "-0.0012047712961832683\n",
      "-0.0012058857361475627\n",
      "-0.001207015617688497\n",
      "-0.0012079926490783691\n",
      "-0.0012090163866678874\n",
      "-0.001209925373395284\n",
      "-0.0012107817729314168\n",
      "-0.0012115832726160686\n",
      "-0.0012124318599700927\n",
      "-0.001213224705060323\n",
      "-0.001214000129699707\n",
      "-0.0012147220055262248\n",
      "-0.0012153810660044352\n",
      "-0.0012160804669062296\n",
      "-0.0012167289733886718\n",
      "-0.0012173409700393678\n",
      "-0.0012179548978805542\n",
      "-0.0012185983022054037\n",
      "-0.0012191402594248454\n",
      "-0.0012196829319000243\n",
      "-0.001220228640238444\n",
      "-0.0012207616647084555\n",
      "-0.0012212678988774617\n",
      "-0.0012217735131581624\n",
      "-0.00122223223845164\n",
      "-0.0012227161566416424\n",
      "-0.001223235861460368\n",
      "-0.0012236465613047283\n",
      "-0.0012240599632263184\n",
      "-0.0012245021184285481\n",
      "-0.0012249342838923137\n",
      "-0.0012252930800120037\n",
      "-0.0012257636626561482\n",
      "-0.0012260949373245238\n",
      "-0.0012264157613118489\n",
      "-0.0012268179416656494\n",
      "-0.0012272163073221844\n",
      "-0.0012275152683258056\n",
      "-0.0012278796275456746\n",
      "-0.0012282392342885336\n",
      "-0.0012285046100616456\n",
      "-0.0012288379748662313\n",
      "-0.001229147736231486\n",
      "-0.0012294867277145385\n",
      "-0.0012297534227371217\n",
      "-0.0012300935427347818\n",
      "-0.001230324912071228\n",
      "-0.0012306315342585246\n",
      "-0.001230921181042989\n",
      "-0.00123118683497111\n",
      "-0.0012314383506774901\n",
      "-0.001231650455792745\n",
      "-0.0012319421370824179\n",
      "-0.0012321635643641154\n",
      "-0.0012324432293574015\n",
      "-0.0012326678276062012\n",
      "-0.0012329065720240275\n",
      "-0.001233125376701355\n",
      "-0.001233336369196574\n",
      "-0.0012335424900054932\n",
      "-0.0012337837616602579\n",
      "-0.0012339744329452514\n",
      "-0.0012341950575510661\n",
      "-0.0012344226996103922\n",
      "-0.0012346192121505738\n",
      "-0.0012348027467727661\n",
      "-0.0012350183327992758\n",
      "-0.0012351822137832641\n",
      "-0.0012353712956110637\n",
      "-0.0012355668306350707\n",
      "-0.001235733962059021\n",
      "-0.0012359388113021852\n",
      "-0.0012360803842544556\n",
      "-0.0012362808704376221\n",
      "-0.001236451546351115\n",
      "-0.001236619488398234\n",
      "-0.0012367979288101197\n",
      "-0.001236928701400757\n",
      "-0.0012371576070785522\n",
      "-0.0012372647762298585\n",
      "-0.0012374217907587686\n",
      "-0.001237554955482483\n",
      "-0.0012377196232477823\n",
      "-0.0012378894885381063\n",
      "-0.0012380483547846477\n",
      "-0.0012381714741388956\n",
      "-0.0012383075078328451\n",
      "-0.001238483476638794\n",
      "-0.0012385749816894531\n",
      "-0.0012387377580006917\n",
      "-0.0012388960043589275\n",
      "-0.0012389691511789957\n",
      "-0.0012391486962636312\n",
      "-0.0012392761945724487\n",
      "-0.0012393794298171997\n",
      "-0.0012395119190216064\n",
      "-0.0012396649916966756\n",
      "-0.0012397494713465372\n",
      "-0.0012398880958557129\n",
      "-0.0012399969021479288\n",
      "-0.0012401222387949625\n",
      "-0.0012402298291524251\n",
      "-0.0012403820276260377\n",
      "-0.0012404666503270467\n",
      "-0.0012405905405680339\n",
      "-0.0012407252073287963\n",
      "-0.0012408049583435058\n",
      "-0.00124096941947937\n",
      "-0.0012410502036412556\n",
      "-0.0012411648750305176\n",
      "-0.0012412681579589844\n",
      "-0.00124139297803243\n",
      "-0.0012414655367533366\n",
      "-0.0012415517171223958\n",
      "-0.001241709089279175\n",
      "-0.0012417890469233195\n",
      "-0.0012419016440709432\n",
      "-0.0012420185327529907\n",
      "-0.001242108416557312\n",
      "-0.0012422093868255616\n",
      "-0.0012423048575719198\n",
      "-0.0012423940499623617\n",
      "-0.0012425136168797812\n",
      "-0.0012426137129465738\n",
      "-0.0012427076657613118\n",
      "-0.0012428005456924438\n",
      "-0.0012429101228713989\n",
      "-0.001242996350924174\n",
      "-0.0012430423418680826\n",
      "-0.0012431825717290242\n",
      "-0.0012432677666346231\n",
      "-0.0012433658043543498\n",
      "-0.0012434782981872558\n",
      "-0.0012435287157694498\n",
      "-0.0012436463673909505\n",
      "-0.0012437087138493855\n",
      "-0.001243823226292928\n",
      "-0.0012438987255096435\n",
      "-0.0012439783652623494\n",
      "-0.0012440734386444093\n",
      "-0.0012441547632217407\n",
      "-0.0012442294200261433\n",
      "-0.0012443506797154744\n",
      "-0.0012444143851598104\n",
      "-0.0012444582303365072\n",
      "-0.0012445868810017904\n",
      "-0.0012446454048156738\n",
      "-0.0012447481632232667\n",
      "-0.0012448442379633586\n",
      "-0.0012448943694432577\n",
      "-0.001245007586479187\n",
      "-0.001245080065727234\n",
      "-0.001245167318979899\n",
      "-0.001245264450709025\n",
      "-0.0012453317721684773\n",
      "-0.0012453942696253458\n",
      "-0.0012454755783081054\n",
      "-0.0012455838521321615\n",
      "-0.0012456401427586874\n",
      "-0.0012457578976949055\n",
      "-0.0012457786242167155\n",
      "-0.0012458621501922607\n",
      "-0.0012459725220998127\n",
      "-0.0012460209210713705\n",
      "-0.0012460956811904907\n",
      "-0.0012461942911148071\n",
      "-0.001246280352274577\n",
      "-0.0012463392575581868\n",
      "-0.001246423554420471\n",
      "-0.0012464807192484539\n",
      "-0.0012465386788050334\n",
      "-0.0012466282367706299\n",
      "-0.0012466727733612061\n",
      "-0.0012467590729395548\n",
      "-0.0012468356529871623\n",
      "-0.0012469284057617187\n",
      "-0.0012470382610956828\n",
      "-0.0012470735788345337\n",
      "-0.001247126038869222\n",
      "-0.0012472362279891968\n",
      "-0.0012472904920578004\n",
      "-0.0012473863124847413\n",
      "-0.0012474538803100585\n",
      "-0.0012475524425506592\n",
      "-0.0012475937604904176\n",
      "-0.001247694993019104\n",
      "-0.0012477683067321776\n",
      "-0.0012478516578674317\n",
      "-0.0012478532552719116\n",
      "-0.0012479936997095743\n",
      "-0.001248007567723592\n",
      "-0.0012481063604354858\n",
      "-0.0012481571594874063\n",
      "-0.0012482614596684774\n",
      "-0.0012483050028483072\n",
      "-0.0012484041929244996\n",
      "-0.0012484810829162599\n",
      "-0.001248557432492574\n",
      "-0.0012486231883366903\n",
      "-0.0012486682891845703\n",
      "-0.0012487359523773193\n",
      "-0.0012488667090733846\n",
      "-0.001248843820889791\n",
      "-0.001248946777979533\n",
      "-0.001248987913131714\n",
      "-0.001249081301689148\n",
      "-0.0012491668462753297\n",
      "-0.0012492084741592406\n",
      "-0.0012492985010147096\n",
      "-0.0012493875900904338\n",
      "-0.0012494199434916178\n",
      "-0.0012495031674702962\n",
      "-0.001249535576502482\n",
      "-0.0012496412595113119\n",
      "-0.0012497000376383463\n",
      "-0.0012497437477111816\n",
      "-0.0012498330911000569\n",
      "-0.0012498919248580933\n",
      "-0.0012499701182047526\n",
      "-0.0012500519752502441\n",
      "-0.0012501142183939616\n",
      "-0.0012501633723576865\n",
      "-0.0012502117872238159\n",
      "-0.0012502467393875122\n",
      "-0.0012503542025883991\n",
      "-0.0012504365046819052\n",
      "-0.0012504802942276002\n",
      "-0.0012505703767140706\n",
      "-0.0012506165186564127\n",
      "-0.0012506740729014078\n",
      "-0.0012507869164148967\n",
      "-0.0012508083661397298\n",
      "-0.0012509148041407268\n",
      "-0.001250914478302002\n",
      "-0.0012509743928909303\n",
      "-0.0012510811726252238\n",
      "-0.001251150870323181\n",
      "-0.0012511801958084107\n",
      "-0.0012512809673945109\n",
      "-0.0012513704856236775\n",
      "-0.0012514143864313762\n",
      "-0.0012514701684316\n",
      "-0.0012515354951222737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012515852053960165\n",
      "-0.0012516491492589315\n",
      "-0.0012517378568649292\n",
      "-0.0012517849524815877\n",
      "-0.0012518394072850545\n",
      "-0.001251940401395162\n",
      "-0.0012519787073135375\n",
      "-0.0012520352840423583\n",
      "-0.0012521003246307374\n",
      "-0.0012521643002827963\n",
      "-0.0012522292455037435\n",
      "-0.001252248771985372\n",
      "-0.001252344552675883\n",
      "-0.001252402949333191\n",
      "-0.0012524743239084879\n",
      "-0.00125256343682607\n",
      "-0.0012526012182235717\n",
      "-0.001252672553062439\n",
      "-0.001252714236577352\n",
      "-0.0012528040409088134\n",
      "-0.0012528634309768676\n",
      "-0.001252859934171041\n",
      "-0.0012529809554417928\n",
      "-0.0012529974301656087\n",
      "-0.0012530914862950643\n",
      "-0.0012531592766443888\n",
      "-0.0012532113472620645\n",
      "-0.0012532693703969319\n",
      "-0.0012533439874649048\n",
      "-0.0012533462444941203\n",
      "-0.0012534990946451822\n",
      "-0.0012535181522369385\n",
      "-0.0012535861571629841\n",
      "-0.0012536409457524617\n",
      "-0.0012537391185760498\n",
      "-0.0012537651062011718\n",
      "-0.0012538124561309815\n",
      "-0.001253877862294515\n",
      "-0.0012539216041564942\n",
      "-0.0012539740800857544\n",
      "-0.0012540358622868855\n",
      "-0.0012541396220525107\n",
      "-0.001254183602333069\n",
      "-0.001254201873143514\n",
      "-0.0012543163061141968\n",
      "-0.001254315416018168\n",
      "-0.001254396645228068\n",
      "-0.0012544441382090251\n",
      "-0.001254478136698405\n",
      "-0.001254572566350301\n",
      "-0.001254589581489563\n",
      "-0.0012546985546747842\n",
      "-0.0012547329902648926\n",
      "-0.0012548084576924643\n",
      "-0.0012548912366231282\n",
      "-0.001254892404874166\n",
      "-0.0012549559195836385\n",
      "-0.0012550140857696534\n",
      "-0.001255047877629598\n",
      "-0.0012550638119379679\n",
      "-0.0012551714340845743\n",
      "-0.0012552200078964233\n",
      "-0.0012552645444869996\n",
      "-0.0012552829424540201\n",
      "-0.0012553702036539713\n",
      "-0.0012554354747136435\n",
      "-0.0012555009206136067\n",
      "-0.0012555327812830608\n",
      "-0.0012556129455566407\n",
      "-0.0012556322733561199\n",
      "-0.0012556998729705811\n",
      "-0.0012557320435841878\n",
      "-0.0012558189551035563\n",
      "-0.0012558171033859253\n",
      "-0.0012558948357899984\n",
      "-0.001255942718187968\n",
      "-0.0012559601306915283\n",
      "-0.0012560522556304932\n",
      "-0.0012561034679412842\n",
      "-0.0012561666250228882\n",
      "-0.0012562058369318643\n",
      "-0.001256257685025533\n",
      "-0.001256278657913208\n",
      "-0.0012563753445943196\n",
      "-0.0012564125219980875\n",
      "-0.0012564541737238566\n",
      "-0.0012565122524897257\n",
      "-0.001256526811917623\n",
      "-0.0012566028356552125\n",
      "-0.0012566625038782754\n",
      "-0.001256672724088033\n",
      "-0.0012567298730214436\n",
      "-0.0012567548990249633\n",
      "-0.0012568252642949423\n",
      "-0.001256902281443278\n",
      "-0.0012569178183873494\n",
      "-0.0012569779793421428\n",
      "-0.001257033658027649\n",
      "-0.001257071844736735\n",
      "-0.0012570900599161783\n",
      "-0.0012571307023366291\n",
      "-0.0012571793874104817\n",
      "-0.001257226045926412\n",
      "-0.0012572858174641926\n",
      "-0.001257304294904073\n",
      "-0.001257357954978943\n",
      "-0.0012573967933654786\n",
      "-0.0012574463764826456\n",
      "-0.0012574628591537475\n",
      "-0.0012575455904006959\n",
      "-0.0012575699249903361\n",
      "-0.0012576123873392741\n",
      "-0.0012576103528340657\n",
      "-0.0012576875925064087\n",
      "-0.0012577220678329467\n",
      "-0.0012577943483988445\n",
      "-0.0012578356345494589\n",
      "-0.0012578316211700438\n",
      "-0.0012579135338465373\n",
      "-0.0012579407930374145\n",
      "-0.0012579456726710002\n",
      "-0.0012580350399017334\n",
      "-0.0012580711841583252\n",
      "-0.0012580925226211548\n",
      "-0.0012580955743789672\n",
      "-0.001258169945081075\n",
      "-0.0012581488291422526\n",
      "-0.0012582358439763388\n",
      "-0.0012582788546880086\n",
      "-0.0012582778056462606\n",
      "-0.0012583336114883423\n",
      "-0.0012583622137705484\n",
      "-0.0012584272066752116\n",
      "-0.0012584436178207398\n",
      "-0.0012584814548492432\n",
      "-0.0012585025548934937\n",
      "-0.0012585318883260091\n",
      "-0.0012585981289545696\n",
      "-0.0012585924863815308\n",
      "-0.0012586486180623372\n",
      "-0.0012586474180221558\n",
      "-0.0012586661577224732\n",
      "-0.001258730991681417\n",
      "-0.0012587418794631957\n",
      "-0.0012587705135345458\n",
      "-0.0012587881326675415\n",
      "-0.0012588635842005413\n",
      "-0.00125879062016805\n",
      "-0.0012588617960611979\n",
      "-0.0012589748620986938\n",
      "-0.001258957266807556\n",
      "-0.0012589823484420776\n",
      "-0.0012588920434315999\n",
      "-0.0012589467763900756\n",
      "-0.0012590702613194784\n",
      "-0.0012591113567352295\n",
      "-0.0012591260671615601\n",
      "-0.001259171716372172\n",
      "-0.00125920250415802\n",
      "-0.0012592170397440591\n",
      "-0.0012591983954111734\n",
      "-0.0012591799656550089\n",
      "-0.001259282914797465\n",
      "-0.0012593206882476806\n",
      "-0.001259374221165975\n",
      "-0.0012593659083048503\n",
      "-0.0012593790928522746\n",
      "-0.001258986496925354\n",
      "-0.001259317390124003\n",
      "-0.0012594175418217976\n",
      "-0.0012594813585281373\n",
      "-0.0012594972848892212\n",
      "-0.001259526522954305\n",
      "-0.0012595270077387492\n",
      "-0.0012595791578292846\n",
      "-0.0012596012512842813\n",
      "-0.0012596403042475382\n",
      "-0.0012596331119537354\n",
      "-0.0012596508105595908\n",
      "-0.001259625236193339\n",
      "-0.001259367283185323\n",
      "-0.001259652098019918\n",
      "-0.0012597532033920289\n",
      "-0.001259753934542338\n",
      "-0.0012597647905349732\n",
      "-0.001259799599647522\n",
      "-0.0012598477125167846\n",
      "-0.0012598567962646484\n",
      "-0.0012598876555760701\n",
      "-0.001259888219833374\n",
      "-0.0012599019368489583\n",
      "-0.001259747854868571\n",
      "-0.0012592697302500407\n",
      "-0.001259768009185791\n",
      "-0.0012599406719207764\n",
      "-0.0012599160432815552\n",
      "-0.001260000745455424\n",
      "-0.0012600125551223754\n",
      "-0.0012600631793340047\n",
      "-0.0012600241502126058\n",
      "-0.0012600638389587402\n",
      "-0.00126007186571757\n",
      "-0.001260101858774821\n",
      "-0.0012601098537445069\n",
      "-0.001260142421722412\n",
      "-0.0012601338227589925\n",
      "-0.0012601529439290366\n",
      "-0.0012601710637410483\n",
      "-0.0012601814190546672\n",
      "-0.001260197138786316\n",
      "-0.0012601953903834024\n",
      "-0.0012601422389348347\n",
      "-0.0012589783906936645\n",
      "-0.0012598117192586264\n",
      "-0.0012601711670557657\n",
      "-0.0012602538108825684\n",
      "-0.0012602849006652832\n",
      "-0.001260312255223592\n",
      "-0.0012603152831395467\n",
      "-0.0012603230635325113\n",
      "-0.0012603785117467244\n",
      "-0.001260386315981547\n",
      "-0.0012603911797205607\n",
      "-0.0012603941122690838\n",
      "-0.0012604053417841594\n",
      "-0.0012603979508082072\n",
      "-0.0012604052384694417\n",
      "-0.001260458485285441\n",
      "-0.0012604389508565268\n",
      "-0.0012604524374008178\n",
      "-0.0012604853709538778\n",
      "-0.0012604415814081828\n",
      "-0.0012604843457539876\n",
      "-0.001260497283935547\n",
      "-0.0012605131228764852\n",
      "-0.001260502862930298\n",
      "-0.0012605276981989542\n",
      "-0.001259797469774882\n",
      "-0.0012600415229797363\n",
      "-0.0012604388316472372\n",
      "-0.0012604888916015625\n",
      "-0.0012605205059051513\n",
      "-0.0012605817159016927\n",
      "-0.0012605940421422323\n",
      "-0.001260616914431254\n",
      "-0.0012606146971384684\n",
      "-0.0012606562852859498\n",
      "-0.0012606306870778401\n",
      "-0.0012606499354044596\n",
      "-0.0012606429020563762\n",
      "-0.0012606703122456867\n",
      "-0.001260668436686198\n",
      "-0.0012606672604878744\n",
      "-0.0012606884161631266\n",
      "-0.0012605853796005249\n",
      "-0.0012597787857055664\n",
      "-0.0012604778130849202\n",
      "-0.001260589114824931\n",
      "-0.0012606703678766887\n",
      "-0.0012607181628545125\n",
      "-0.001260724727312724\n",
      "-0.001260731037457784\n",
      "-0.0012607531944910685\n",
      "-0.0012607517639795938\n",
      "-0.0012607391675313313\n",
      "-0.0012607406775156657\n",
      "-0.0012608215729395548\n",
      "-0.0012607711553573608\n",
      "-0.001260785969098409\n",
      "-0.0012608130931854248\n",
      "-0.0012607850392659505\n",
      "-0.0012599170684814453\n",
      "-0.0012602992137273152\n",
      "-0.0012606984694798788\n",
      "-0.0012607420206069947\n",
      "-0.0012608150164286295\n",
      "-0.0012608271916707357\n",
      "-0.001260811177889506\n",
      "-0.0012608157475789388\n",
      "-0.0012608530282974242\n",
      "-0.001260880208015442\n",
      "-0.001260848037401835\n",
      "-0.0012608764251073202\n",
      "-0.0012608848651250203\n",
      "-0.0012608940442403157\n",
      "-0.0012608816623687743\n",
      "-0.001260902778307597\n",
      "-0.0012608998934427898\n",
      "-0.0012605419317881267\n",
      "-0.0012601253430048625\n",
      "-0.0012606746594111126\n",
      "-0.0012608273347218832\n",
      "-0.00126089559396108\n",
      "-0.001260927907625834\n",
      "-0.0012609430074691772\n",
      "-0.001260946544011434\n",
      "-0.0012609424670537313\n",
      "-0.001260936427116394\n",
      "-0.001260931412378947\n",
      "-0.00126095023949941\n",
      "-0.0012609535853068034\n",
      "-0.00126096826394399\n",
      "-0.0012609445730845134\n",
      "-0.00126045028368632\n",
      "-0.0012603166739145915\n",
      "-0.0012607593139012654\n",
      "-0.0012609209378560385\n",
      "-0.0012609650532404582\n",
      "-0.0012609640836715697\n",
      "-0.0012609668811162313\n",
      "-0.0012610003391901653\n",
      "-0.0012609975735346475\n",
      "-0.0012609878778457643\n",
      "-0.0012609899759292603\n",
      "-0.0012609922488530476\n",
      "-0.001261006212234497\n",
      "-0.0012609516302744548\n",
      "-0.0012603862524032594\n",
      "-0.0012606349388758341\n",
      "-0.0012609005769093832\n",
      "-0.001260973048210144\n",
      "-0.0012610177993774413\n",
      "-0.0012610350131988526\n",
      "-0.0012610208749771119\n",
      "-0.0012610317468643188\n",
      "-0.0012610722621281942\n",
      "-0.0012610635995864868\n",
      "-0.0012610604524612428\n",
      "-0.0012610374450683593\n",
      "-0.0012604126532872518\n",
      "-0.0012605784257253012\n",
      "-0.00126090513865153\n",
      "-0.0012610188086827597\n",
      "-0.0012610487778981527\n",
      "-0.0012611175855000813\n",
      "-0.0012610928694407144\n",
      "-0.0012610666116078695\n",
      "-0.0012610939979553224\n",
      "-0.0012610979000727337\n",
      "-0.0012610749085744221\n",
      "-0.0012610795974731445\n",
      "-0.0012609601736068726\n",
      "-0.0012603428920110067\n",
      "-0.0012609427611033122\n",
      "-0.0012610164006551107\n",
      "-0.0012610655625661215\n",
      "-0.0012610996882120769\n",
      "-0.001261098829905192\n",
      "-0.0012611077547073364\n",
      "-0.0012610937595367432\n",
      "-0.0012611122131347657\n",
      "-0.0012610753854115804\n",
      "-0.0012607493003209433\n",
      "-0.0012606952587763467\n",
      "-0.0012609874804814657\n",
      "-0.001261077086130778\n",
      "-0.001261129101117452\n",
      "-0.0012611283938090006\n",
      "-0.0012611280838648478\n",
      "-0.0012611411094665527\n",
      "-0.001261091717084249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001260905392964681\n",
      "-0.001260707139968872\n",
      "-0.0012610512653986614\n",
      "-0.0012611152410507201\n",
      "-0.0012611063559850057\n",
      "-0.0012611244678497314\n",
      "-0.0012611426671346028\n",
      "-0.0012611341794331868\n",
      "-0.0012611191511154175\n",
      "-0.0012605311632156373\n",
      "-0.0012606348911921184\n",
      "-0.0012609764337539672\n",
      "-0.0012611244837443035\n",
      "-0.0012611381928126018\n",
      "-0.0012611489216486614\n",
      "-0.0012611704508463542\n",
      "-0.0012611680269241333\n",
      "-0.001261158299446106\n",
      "-0.0012611669858296711\n",
      "-0.0012611825704574584\n",
      "-0.001261122194925944\n",
      "-0.0012606677214304605\n",
      "-0.0012608527898788453\n",
      "-0.0012610427618026734\n",
      "-0.001261119786898295\n",
      "-0.0012611679633458455\n",
      "-0.0012611983140309651\n",
      "-0.001261160675684611\n",
      "-0.0012611855824788412\n",
      "-0.0012611587603886922\n",
      "-0.0012609032074610393\n",
      "-0.0012607102553049724\n",
      "-0.0012610742966334026\n",
      "-0.0012611283699671427\n",
      "-0.0012611643075942994\n",
      "-0.0012612041234970092\n",
      "-0.001261186401049296\n",
      "-0.0012611685037612916\n",
      "-0.0012610745747884114\n",
      "-0.0012607531706492106\n",
      "-0.001261076283454895\n",
      "-0.0012611297448476156\n",
      "-0.0012611671288808187\n",
      "-0.0012611610968907673\n",
      "-0.0012611966848373414\n",
      "-0.001261195429166158\n",
      "-0.0012611932277679443\n",
      "-0.001260905933380127\n",
      "-0.0012602036476135254\n",
      "-0.001260919745763143\n",
      "-0.0012610986550649008\n",
      "-0.0012611577192942302\n",
      "-0.001261181942621867\n",
      "-0.0012612120469411214\n",
      "-0.0012612243175506592\n",
      "-0.001261231811841329\n",
      "-0.0012612200578053792\n",
      "-0.0012612329800923666\n",
      "-0.0012612041473388672\n",
      "-0.001261214820543925\n",
      "-0.0012611705859502157\n",
      "-0.001260771075884501\n",
      "-0.0012608955224355063\n",
      "-0.0012610978603363038\n",
      "-0.0012611739238103231\n",
      "-0.001261228624979655\n",
      "-0.001261218293507894\n",
      "-0.0012612144947052003\n",
      "-0.001261209193865458\n",
      "-0.0012610265811284383\n",
      "-0.0012607395728429158\n",
      "-0.0012611093362172444\n",
      "-0.0012611615657806396\n",
      "-0.0012611799478530884\n",
      "-0.0012612118005752564\n",
      "-0.001261230476697286\n",
      "-0.0012612367312113445\n",
      "-0.001261228632926941\n",
      "-0.0012606244723002116\n",
      "-0.0012603849649429322\n",
      "-0.0012609748840332032\n",
      "-0.0012611801862716676\n",
      "-0.0012611860116322835\n",
      "-0.0012612212260564167\n",
      "-0.0012611578543980917\n",
      "-0.0012612250169118245\n",
      "-0.0012612532059351604\n",
      "-0.001261239473025004\n",
      "-0.0012612417777379354\n",
      "-0.0012612505515416463\n",
      "-0.0012612548033396403\n",
      "-0.001261236834526062\n",
      "-0.0012610223134358724\n",
      "-0.0012606352885564168\n",
      "-0.001261126923561096\n",
      "-0.0012612087488174438\n",
      "-0.0012612027327219646\n",
      "-0.0012612300872802734\n",
      "-0.0012612597624460857\n",
      "-0.0012612420161565146\n",
      "-0.0012611250003178914\n",
      "-0.0012607310771942139\n",
      "-0.0012611062208811442\n",
      "-0.0012611478646596273\n",
      "-0.0012611738681793214\n",
      "-0.0012612463394800822\n",
      "-0.001261258856455485\n",
      "-0.001261245568593343\n",
      "-0.0012612429300944011\n",
      "-0.0012608940124511719\n",
      "-0.001260236692428589\n",
      "-0.0012609666347503663\n",
      "-0.001261172890663147\n",
      "-0.0012612473090489705\n",
      "-0.001261256504058838\n",
      "-0.0012612640301386516\n",
      "-0.0012612483819325765\n",
      "-0.0012612582127253214\n",
      "-0.0012612501780192057\n",
      "-0.0012612771034240724\n",
      "-0.0012612648089726766\n",
      "-0.001261215392748515\n",
      "-0.0012609100341796876\n",
      "-0.0012607671817143758\n",
      "-0.001261134910583496\n",
      "-0.001261203908920288\n",
      "-0.0012612422943115234\n",
      "-0.001261263863245646\n",
      "-0.0012612873474756876\n",
      "-0.0012612237930297852\n",
      "-0.0012610668420791626\n",
      "-0.0012607242822647094\n",
      "-0.0012611512263615925\n",
      "-0.0012612082004547118\n",
      "-0.001261219064394633\n",
      "-0.0012612413009007771\n",
      "-0.0012612819592158\n",
      "-0.001261265778541565\n",
      "-0.0012612579266230266\n",
      "-0.0012610921223958334\n",
      "-0.0012599446773529054\n",
      "-0.0012609074751536052\n",
      "-0.0012611197789510092\n",
      "-0.0012612379630406697\n",
      "-0.0012612628777821859\n",
      "-0.0012612922112147013\n",
      "-0.0012612716754277547\n",
      "-0.0012612568219502766\n",
      "-0.0012612574497858683\n",
      "-0.001261140775680542\n",
      "-0.0012612417856852213\n",
      "-0.0012612837235132853\n",
      "-0.0012612668673197428\n",
      "-0.0012612711747487387\n",
      "-0.001261154596010844\n",
      "-0.0012607714970906576\n",
      "-0.0012611602306365967\n",
      "-0.0012612012227376302\n",
      "-0.0012612302700678508\n",
      "-0.0012612238168716431\n",
      "-0.0012612738370895386\n",
      "-0.0012612778902053833\n",
      "-0.0012612862586975097\n",
      "-0.0012612675031026205\n",
      "-0.001260923926035563\n",
      "-0.0012597490549087524\n",
      "-0.001260753877957662\n",
      "-0.0012611506621042887\n",
      "-0.0012612340211868285\n",
      "-0.0012612648646036784\n",
      "-0.0012612751642862956\n",
      "-0.001261281156539917\n",
      "-0.0012612836678822835\n",
      "-0.0012612813949584962\n",
      "-0.0012612773180007936\n",
      "-0.001261269481976827\n",
      "-0.0012612799485524496\n",
      "-0.0012612846851348877\n",
      "-0.0012612174113591512\n",
      "-0.0012612331787745159\n",
      "-0.0012612590710322062\n",
      "-0.0012612205505371094\n",
      "-0.0012609171628952026\n",
      "-0.001260977554321289\n",
      "-0.0012611730893452961\n",
      "-0.0012612786769866944\n",
      "-0.001261210521062215\n",
      "-0.0012612728118896485\n",
      "-0.0012612844387690225\n",
      "-0.0012612783114115398\n",
      "-0.00126130424340566\n",
      "-0.0012612082004547118\n",
      "-0.0012596346696217855\n",
      "-0.0012605950435002646\n",
      "-0.0012610313653945923\n",
      "-0.0012612189610799155\n",
      "-0.0012612601280212403\n",
      "-0.0012612870772679646\n",
      "-0.0012612842241923013\n",
      "-0.0012612939834594726\n",
      "-0.0012612842877705891\n",
      "-0.0012612876812616984\n",
      "-0.0012612765073776245\n",
      "-0.0012612845420837402\n",
      "-0.0012612498919169109\n",
      "-0.0012612009207407634\n",
      "-0.0012612637599309286\n",
      "-0.0012612845897674561\n",
      "-0.00126129523118337\n",
      "-0.0012612544059753417\n",
      "-0.0012611009041468303\n",
      "-0.0012607525428136189\n",
      "-0.0012611966768900553\n",
      "-0.0012612391153971354\n",
      "-0.0012612419684727987\n",
      "-0.0012612720012664795\n",
      "-0.0012612892707188925\n",
      "-0.001261283016204834\n",
      "-0.0012612943172454834\n",
      "-0.0012610191106796264\n",
      "-0.0012602014223734538\n",
      "-0.0012609954913457234\n",
      "-0.0012611764748891195\n",
      "-0.0012612573464711508\n",
      "-0.0012613002300262452\n",
      "-0.0012612864176432292\n",
      "-0.001261312206586202\n",
      "-0.0012613047281901041\n",
      "-0.0012612964073816936\n",
      "-0.0012612971782684326\n",
      "-0.0012612892866134644\n",
      "-0.0012612485726674397\n",
      "-0.0012608140389124551\n",
      "-0.00126088019212087\n",
      "-0.0012611518144607543\n",
      "-0.0012612321376800537\n",
      "-0.001261292028427124\n",
      "-0.0012612884680430095\n",
      "-0.0012612955729166667\n",
      "-0.0012612404425938924\n",
      "-0.0012610470453898112\n",
      "-0.0012609878857930501\n",
      "-0.0012612714131673176\n",
      "-0.0012612204472223917\n",
      "-0.0012612009604771933\n",
      "-0.0012612359603246053\n",
      "-0.0012610722700754802\n",
      "-0.0012608089923858642\n",
      "-0.0012612149000167847\n",
      "-0.0012612346728642782\n",
      "-0.0012612337827682496\n",
      "-0.0012612539211908976\n",
      "-0.0012612951119740803\n",
      "-0.0012613061825434368\n",
      "-0.0012612926721572875\n",
      "-0.0012610231955846151\n",
      "-0.0012600234349568686\n",
      "-0.0012609269618988038\n",
      "-0.0012611570994059244\n",
      "-0.0012612685521443685\n",
      "-0.001261284875869751\n",
      "-0.0012612913290659586\n",
      "-0.0012612863381703694\n",
      "-0.0012612939993540445\n",
      "-0.0012612962007522582\n",
      "-0.001261310577392578\n",
      "-0.0012612907965977988\n",
      "-0.0012612934350967407\n",
      "-0.0012613085985183716\n",
      "-0.001261134696006775\n",
      "-0.0012605045874913533\n",
      "-0.001261104933420817\n",
      "-0.001261220105489095\n",
      "-0.001261259158452352\n",
      "-0.001261288102467855\n",
      "-0.0012612824122111002\n",
      "-0.0012612801631291708\n",
      "-0.0012612934271494548\n",
      "-0.0012613108158111572\n",
      "-0.0012612390279769898\n",
      "-0.0012603742043177286\n",
      "-0.0012608360608418782\n",
      "-0.0012611307144165039\n",
      "-0.0012612338701883951\n",
      "-0.0012612810055414837\n",
      "-0.0012612778584162395\n",
      "-0.0012613075097401938\n",
      "-0.00126129363377889\n",
      "-0.0012613176822662353\n",
      "-0.0012613049745559693\n",
      "-0.0012612936019897462\n",
      "-0.0012611925363540648\n",
      "-0.0012605053742726644\n",
      "-0.0012610619544982911\n",
      "-0.0012611964384714762\n",
      "-0.001261246116956075\n",
      "-0.0012612800280253093\n",
      "-0.0012613049189249674\n",
      "-0.001261310601234436\n",
      "-0.001261297845840454\n",
      "-0.0012613072633743286\n",
      "-0.0012612919092178344\n",
      "-0.0012610241333643596\n",
      "-0.0012600997845331828\n",
      "-0.0012609840949376423\n",
      "-0.001261163862546285\n",
      "-0.0012612597544987996\n",
      "-0.0012613030274709065\n",
      "-0.0012613053083419799\n",
      "-0.0012612895091374716\n",
      "-0.0012613194306691487\n",
      "-0.0012613187551498413\n",
      "-0.0012613072315851848\n",
      "-0.0012612923383712768\n",
      "-0.0012612931887308757\n",
      "-0.0012612502892812092\n",
      "-0.0012607303619384766\n",
      "-0.0012609317620595297\n",
      "-0.001261146887143453\n",
      "-0.001261224635442098\n",
      "-0.0012612957795461019\n",
      "-0.001261289930343628\n",
      "-0.0012612735827763875\n",
      "-0.0012612937053044636\n",
      "-0.0012612701813379922\n",
      "-0.0012610979795455932\n",
      "-0.0012606767098108926\n",
      "-0.0012611787716547648\n",
      "-0.0012612277746200562\n",
      "-0.0012612746953964233\n",
      "-0.001261291734377543\n",
      "-0.0012612890323003133\n",
      "-0.0012613102595011394\n",
      "-0.0012611918767293294\n",
      "-0.0012607764800389607\n",
      "-0.0012611133654912314\n",
      "-0.001261188824971517\n",
      "-0.001261255407333374\n",
      "-0.0012612874587376913\n",
      "-0.001261306921641032\n",
      "-0.0012612836281458537\n",
      "-0.001261309544245402\n",
      "-0.001261140775680542\n",
      "-0.0012602448066075644\n",
      "-0.0012610181252161662\n",
      "-0.0012612033923467\n",
      "-0.0012612660725911458\n",
      "-0.0012612876892089844\n",
      "-0.0012613088607788085\n",
      "-0.0012613112370173137\n",
      "-0.0012613070964813233\n",
      "-0.0012613170782725016\n",
      "-0.0012612974246342978\n",
      "-0.0012611634333928427\n",
      "-0.0012610983530680338\n",
      "-0.00126078675587972\n",
      "-0.0012612258911132812\n",
      "-0.0012612353801727295\n",
      "-0.0012612585544586183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012612788597742718\n",
      "-0.0012613097667694092\n",
      "-0.0012612845500310263\n",
      "-0.0012612536907196045\n",
      "-0.0012607269207636515\n",
      "-0.0012608058214187623\n",
      "-0.0012611128966013591\n",
      "-0.0012612457434336343\n",
      "-0.0012612795193990072\n",
      "-0.0012613061904907226\n",
      "-0.001261296033859253\n",
      "-0.001261315123240153\n",
      "-0.0012613049666086833\n",
      "-0.001261256202061971\n",
      "-0.0012610207955042522\n",
      "-0.001260685642560323\n",
      "-0.001261139678955078\n",
      "-0.001261234990755717\n",
      "-0.001261286155382792\n",
      "-0.0012612888892491658\n",
      "-0.0012612949927647908\n",
      "-0.0012612617333730063\n",
      "-0.0012612658818562826\n",
      "-0.0012611179987589518\n",
      "-0.0012608616352081299\n",
      "-0.0012612433751424154\n",
      "-0.001261206881205241\n",
      "-0.0012613155762354533\n",
      "-0.0012612513542175293\n",
      "-0.0012610725879669189\n",
      "-0.0012610313415527344\n",
      "-0.0012612770080566405\n",
      "-0.0012611806392669677\n",
      "-0.00126123472849528\n",
      "-0.0012612587531407673\n",
      "-0.001261049214998881\n",
      "-0.001260746971766154\n",
      "-0.001261193577448527\n",
      "-0.0012612579266230266\n",
      "-0.0012612594525019329\n",
      "-0.0012612969716389974\n",
      "-0.0012613173166910808\n",
      "-0.0012612961848576863\n",
      "-0.0012611980199813842\n",
      "-0.0012606989224751791\n",
      "-0.0012610321124394736\n",
      "-0.001261177388827006\n",
      "-0.0012612486441930136\n",
      "-0.0012613024552663168\n",
      "-0.0012613094727198283\n",
      "-0.0012612988392512003\n",
      "-0.0012612933715184529\n",
      "-0.0012612525145212809\n",
      "-0.0012609758297602335\n",
      "-0.0012607122898101806\n",
      "-0.001261153523127238\n",
      "-0.001261239473025004\n",
      "-0.0012612931569417317\n",
      "-0.0012613051335016885\n",
      "-0.0012613001585006715\n",
      "-0.0012613049745559693\n",
      "-0.0012612571080525717\n",
      "-0.0012610086520512898\n",
      "-0.0012608530044555665\n",
      "-0.0012612088521321615\n",
      "-0.001261258053779602\n",
      "-0.001261242945988973\n",
      "-0.0012613049904505412\n",
      "-0.0012613117218017579\n",
      "-0.001261288809776306\n",
      "-0.001261160628000895\n",
      "-0.001260427991549174\n",
      "-0.0012610633293787638\n",
      "-0.0012612023750940959\n",
      "-0.001261274472872416\n",
      "-0.0012612964073816936\n",
      "-0.00126130424340566\n",
      "-0.0012613094727198283\n",
      "-0.00126130797068278\n",
      "-0.0012613150199254355\n",
      "-0.0012612746715545654\n",
      "-0.0012611118872960408\n",
      "-0.0012604259411493938\n",
      "-0.0012609208742777507\n",
      "-0.00126116357644399\n",
      "-0.001261269172032674\n",
      "-0.0012613003412882487\n",
      "-0.0012613135655721028\n",
      "-0.0012613134860992431\n",
      "-0.0012613139629364014\n",
      "-0.0012613242626190186\n",
      "-0.0012612972815831502\n",
      "-0.0012613007307052612\n",
      "-0.0012612399260203044\n",
      "-0.0012603587071100871\n",
      "-0.0012609107573827109\n",
      "-0.0012611413876215617\n",
      "-0.0012612532059351604\n",
      "-0.0012613205273946126\n",
      "-0.0012613096555074057\n",
      "-0.0012613134463628133\n",
      "-0.0012613147735595703\n",
      "-0.0012613167683283488\n",
      "-0.0012613056262334187\n",
      "-0.0012613067388534546\n",
      "-0.0012612306435902914\n",
      "-0.0012605105400085449\n",
      "-0.001260953195889791\n",
      "-0.0012611510594685872\n",
      "-0.0012612532695134482\n",
      "-0.0012612834850947061\n",
      "-0.001261304235458374\n",
      "-0.001261326543490092\n",
      "-0.001261307724316915\n",
      "-0.0012613239765167236\n",
      "-0.0012612788359324138\n",
      "-0.0012610958814620972\n",
      "-0.001260692803064982\n",
      "-0.0012611295859018962\n",
      "-0.0012612438360850016\n",
      "-0.0012612519184748332\n",
      "-0.00126129359404246\n",
      "-0.001261311904589335\n",
      "-0.0012612848043441773\n",
      "-0.001261254628499349\n",
      "-0.001261121988296509\n",
      "-0.001260902460416158\n",
      "-0.001261176331837972\n",
      "-0.0012612382332483926\n",
      "-0.0012612906297047933\n",
      "-0.0012612807432810466\n",
      "-0.0012611976464589436\n",
      "-0.0012610270420710247\n",
      "-0.0012611961603164674\n",
      "-0.0012612819592158\n",
      "-0.001261280878384908\n",
      "-0.0012613015810648601\n",
      "-0.0012612141132354736\n",
      "-0.001260486658414205\n",
      "-0.001260889188448588\n",
      "-0.00126113068262736\n",
      "-0.0012612504482269287\n",
      "-0.0012613002061843872\n",
      "-0.001261303146680196\n",
      "-0.0012613098541895548\n",
      "-0.0012613163153330485\n",
      "-0.0012613251447677611\n",
      "-0.001261298934618632\n",
      "-0.0012613144636154175\n",
      "-0.0012610298951466878\n",
      "-0.001260343352953593\n",
      "-0.0012610681136449179\n",
      "-0.0012612035274505615\n",
      "-0.001261265468597412\n",
      "-0.0012613027254740397\n",
      "-0.0012613142331441244\n",
      "-0.0012613121906916301\n",
      "-0.001261328069368998\n",
      "-0.0012613021691640217\n",
      "-0.0012613130966822306\n",
      "-0.0012613076607386271\n",
      "-0.0012610968510309854\n",
      "-0.0012603516896565756\n",
      "-0.001261023211479187\n",
      "-0.0012611856937408447\n",
      "-0.0012612879037857056\n",
      "-0.0012613041877746582\n",
      "-0.0012613017002741495\n",
      "-0.001261320439974467\n",
      "-0.0012613175789515177\n",
      "-0.0012613072554270427\n",
      "-0.0012611688693364462\n",
      "-0.0012612839778264363\n",
      "-0.0012613194227218628\n",
      "-0.0012613155364990234\n",
      "-0.0012610472361246744\n",
      "-0.0012598597208658853\n",
      "-0.0012608057737350463\n",
      "-0.0012611905813217164\n",
      "-0.001261256750424703\n",
      "-0.001261305562655131\n",
      "-0.0012613140106201171\n",
      "-0.0012613384405771892\n",
      "-0.0012613116900126139\n",
      "-0.0012613167842229207\n",
      "-0.0012613202333450317\n",
      "-0.0012613149642944335\n",
      "-0.0012613125085830688\n",
      "-0.0012611541350682576\n",
      "-0.001261224643389384\n",
      "-0.001261299220720927\n",
      "-0.0012613141695658366\n",
      "-0.0012613038937250773\n",
      "-0.0012611579100290935\n",
      "-0.0012605497121810914\n",
      "-0.0012611344973246256\n",
      "-0.0012612157265345255\n",
      "-0.0012612728595733642\n",
      "-0.0012612915436426798\n",
      "-0.001261322577794393\n",
      "-0.0012613209247589112\n",
      "-0.0012613274892171223\n",
      "-0.001261281704902649\n",
      "-0.0012609256744384766\n",
      "-0.0012607424815495808\n",
      "-0.00126114292939504\n",
      "-0.0012612545172373455\n",
      "-0.001261309274037679\n",
      "-0.0012612934112548827\n",
      "-0.0012613027890523276\n",
      "-0.0012613187948862712\n",
      "-0.0012613141854604085\n",
      "-0.001261324707667033\n",
      "-0.0012612472852071125\n",
      "-0.0012601177295049032\n",
      "-0.0012607689539591471\n",
      "-0.0012611530065536499\n",
      "-0.0012612577676773071\n",
      "-0.0012612850745519003\n",
      "-0.0012613203207651775\n",
      "-0.0012613236824671427\n",
      "-0.001261324111620585\n",
      "-0.0012613128980000813\n",
      "-0.0012613089640935262\n",
      "-0.0012613080104192098\n",
      "-0.0012613308668136597\n",
      "-0.0012613195260365805\n",
      "-0.0012613070646921793\n",
      "-0.0012609697103500366\n",
      "-0.0012604618390401204\n",
      "-0.0012610523223876952\n",
      "-0.0012612228949864706\n",
      "-0.0012612744092941285\n",
      "-0.0012613001982371011\n",
      "-0.0012613124450047812\n",
      "-0.0012613075892130535\n",
      "-0.0012613363822301228\n",
      "-0.0012613117138544718\n",
      "-0.0012612648010253906\n",
      "-0.001260601003964742\n",
      "-0.0012609325965245565\n",
      "-0.001261161494255066\n",
      "-0.001261251425743103\n",
      "-0.0012612895568211873\n",
      "-0.0012613061825434368\n",
      "-0.0012613240798314412\n",
      "-0.001261317245165507\n",
      "-0.0012613198518753052\n",
      "-0.0012613004207611084\n",
      "-0.001261191479365031\n",
      "-0.0012605953613917032\n",
      "-0.0012611047108968098\n",
      "-0.001261194880803426\n",
      "-0.0012612604061762493\n",
      "-0.001261292282740275\n",
      "-0.0012613072395324706\n",
      "-0.0012613036870956422\n",
      "-0.0012613224267959595\n",
      "-0.0012613234202067058\n",
      "-0.001261303742726644\n",
      "-0.0012606701850891113\n",
      "-0.0012604848543802897\n",
      "-0.0012610361496607464\n",
      "-0.0012612399657567343\n",
      "-0.0012613130966822306\n",
      "-0.0012613105456034343\n",
      "-0.0012613235553105672\n",
      "-0.0012613214333852133\n",
      "-0.0012613193909327188\n",
      "-0.0012613189935684205\n",
      "-0.00126131591796875\n",
      "-0.0012613245407740274\n",
      "-0.0012613103389739991\n",
      "-0.0012610633293787638\n",
      "-0.001260424820582072\n",
      "-0.0012610718329747518\n",
      "-0.0012612241347630818\n",
      "-0.001261289389928182\n",
      "-0.0012612976948420207\n",
      "-0.001261313756306966\n",
      "-0.001261328689257304\n",
      "-0.001261312429110209\n",
      "-0.001261312182744344\n",
      "-0.001261266859372457\n",
      "-0.0012607808192571005\n",
      "-0.0012608747641245524\n",
      "-0.0012611323833465577\n",
      "-0.001261227798461914\n",
      "-0.001261294968922933\n",
      "-0.0012613274812698365\n",
      "-0.0012613178571065267\n",
      "-0.0012613232453664144\n",
      "-0.001261306095123291\n",
      "-0.0012613123337427774\n",
      "-0.0012613004366556803\n",
      "-0.001260629399617513\n",
      "-0.0012603314399719239\n",
      "-0.0012609828233718873\n",
      "-0.0012612381219863891\n",
      "-0.0012612847407658895\n",
      "-0.0012613166729609172\n",
      "-0.0012613147815068563\n",
      "-0.0012613118569056194\n",
      "-0.0012613174041112263\n",
      "-0.0012613186836242676\n",
      "-0.0012613135655721028\n",
      "-0.0012613282124201456\n",
      "-0.0012613271633783976\n",
      "-0.0012613024075826009\n",
      "-0.0012611268043518067\n",
      "-0.001260945963859558\n",
      "-0.0012605343023935954\n",
      "-0.001261108946800232\n",
      "-0.0012612199465433756\n",
      "-0.0012612834294637044\n",
      "-0.0012613105217615763\n",
      "-0.001261314829190572\n",
      "-0.0012613065083821615\n",
      "-0.0012613277037938435\n",
      "-0.0012613141536712647\n",
      "-0.00126132173538208\n",
      "-0.0012613072951634726\n",
      "-0.001261054507891337\n",
      "-0.0012603455384572347\n",
      "-0.0012610232512156169\n",
      "-0.0012611921946207682\n",
      "-0.001261274290084839\n",
      "-0.001261308757464091\n",
      "-0.0012613217989603678\n",
      "-0.0012613126595815024\n",
      "-0.0012613219261169433\n",
      "-0.0012613216559092203\n",
      "-0.0012611476500829061\n",
      "-0.0012611111958821615\n",
      "-0.001261187481880188\n",
      "-0.0012612242698669433\n",
      "-0.0012612168312072754\n",
      "-0.001261054555575053\n",
      "-0.0012611299912134806\n",
      "-0.0012612915198008218\n",
      "-0.0012612413962682088\n",
      "-0.0012613034884134929\n",
      "-0.001261314622561137\n",
      "-0.0012613269488016764\n",
      "-0.0012612991412480673\n",
      "-0.0012605851491292318\n",
      "-0.0012597938219706217\n",
      "-0.0012609767119089762\n",
      "-0.001261159054438273\n",
      "-0.0012612927675247191\n",
      "-0.0012613027175267537\n",
      "-0.0012613019863764445\n",
      "-0.0012613126913706462\n",
      "-0.0012613189776738486\n",
      "-0.0012613102038701374\n",
      "-0.0012613120714823406\n",
      "-0.001261311944325765\n",
      "-0.0012613227208455404\n",
      "-0.0012613189776738486\n",
      "-0.001261307470003764\n",
      "-0.0012612914562225342\n",
      "-0.0012611386934916178\n",
      "-0.0012611879587173462\n",
      "-0.0012612753629684449\n",
      "-0.0012613298018773398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012613094886144002\n",
      "-0.001261136031150818\n",
      "-0.001260197377204895\n",
      "-0.001261032287279765\n",
      "-0.0012612074851989746\n",
      "-0.0012612823804219564\n",
      "-0.001261323595046997\n",
      "-0.001261323062578837\n",
      "-0.0012613223393758137\n",
      "-0.001261320988337199\n",
      "-0.0012613139073053997\n",
      "-0.001261317459742228\n",
      "-0.0012613213936487834\n",
      "-0.001261318604151408\n",
      "-0.0012611668904622395\n",
      "-0.0012603310664494832\n",
      "-0.0012610600471496581\n",
      "-0.0012612011671066284\n",
      "-0.0012612761815388998\n",
      "-0.001261309774716695\n",
      "-0.0012613171497980753\n",
      "-0.0012613224903742473\n",
      "-0.0012613208293914796\n",
      "-0.0012613160928090414\n",
      "-0.0012613259792327882\n",
      "-0.0012613160451253255\n",
      "-0.001261288046836853\n",
      "-0.0012606056769688923\n",
      "-0.0012605525175730388\n",
      "-0.0012609931071599324\n",
      "-0.0012612492481867473\n",
      "-0.0012612878640492757\n",
      "-0.0012613258679707844\n",
      "-0.0012613147894541422\n",
      "-0.0012613333145777385\n",
      "-0.0012613056818644205\n",
      "-0.0012613064527511597\n",
      "-0.0012612211624781291\n",
      "-0.0012611501534779866\n",
      "-0.0012612662076950073\n",
      "-0.0012613113641738892\n",
      "-0.0012612822612126668\n",
      "-0.0012610477606455484\n",
      "-0.001260719648996989\n",
      "-0.0012611631155014038\n",
      "-0.0012612680435180664\n",
      "-0.0012612969636917114\n",
      "-0.0012612933476765951\n",
      "-0.0012613109827041627\n",
      "-0.0012613287369410196\n",
      "-0.001261315703392029\n",
      "-0.0012613230069478353\n",
      "-0.0012613065083821615\n",
      "-0.0012605586846669516\n",
      "-0.0012601587851842244\n",
      "-0.0012609846353530884\n",
      "-0.0012612524112065634\n",
      "-0.0012612888018290202\n",
      "-0.0012613135178883871\n",
      "-0.001261314304669698\n",
      "-0.001261326837539673\n",
      "-0.0012613242308298746\n",
      "-0.0012613279660542806\n",
      "-0.0012613240242004395\n",
      "-0.0012613124450047812\n",
      "-0.0012613311131795246\n",
      "-0.0012613215684890748\n",
      "-0.0012613146384557088\n",
      "-0.0012612814823786418\n",
      "-0.001260862668355306\n",
      "-0.0012606427987416585\n",
      "-0.0012611396710077922\n",
      "-0.001261248477300008\n",
      "-0.0012612990776697795\n",
      "-0.001261303424835205\n",
      "-0.0012613008260726928\n",
      "-0.0012613205194473268\n",
      "-0.0012613165616989135\n",
      "-0.0012613035758336384\n",
      "-0.0012612516244252523\n",
      "-0.0012605331818262735\n",
      "-0.0012608976602554321\n",
      "-0.0012611579577128092\n",
      "-0.0012612775325775147\n",
      "-0.0012612826426823935\n",
      "-0.0012613208373387654\n",
      "-0.0012613107919692992\n",
      "-0.001261318031946818\n",
      "-0.0012613172690073649\n",
      "-0.0012613110383351645\n",
      "-0.0012613185405731201\n",
      "-0.001261187473932902\n",
      "-0.0012602380037307738\n",
      "-0.001260925817489624\n",
      "-0.0012611451069513956\n",
      "-0.001261270546913147\n",
      "-0.0012613039414087932\n",
      "-0.0012613181193669638\n",
      "-0.0012613094091415405\n",
      "-0.0012613242149353027\n",
      "-0.001261328371365865\n",
      "-0.0012613155364990234\n",
      "-0.0012613207181294758\n",
      "-0.0012613031148910523\n",
      "-0.0012608503977457682\n",
      "-0.00126042320728302\n",
      "-0.0012610204855600994\n",
      "-0.0012612346967061361\n",
      "-0.0012613086064656575\n",
      "-0.0012613205194473268\n",
      "-0.0012613160371780395\n",
      "-0.0012613116184870402\n",
      "-0.0012613101959228516\n",
      "-0.00126132017771403\n",
      "-0.0012613171895345051\n",
      "-0.0012613217274347942\n",
      "-0.0012613236427307128\n",
      "-0.001261298664410909\n",
      "-0.0012607014258702596\n",
      "-0.0012606825908025106\n",
      "-0.0012611010471979777\n",
      "-0.001261241348584493\n",
      "-0.0012612961689631144\n",
      "-0.0012613274017969768\n",
      "-0.0012613258520762125\n",
      "-0.0012613293806711832\n",
      "-0.0012613181749979655\n",
      "-0.0012613286972045898\n",
      "-0.0012613228480021158\n",
      "-0.0012613163630167644\n",
      "-0.0012610193729400635\n",
      "-0.0012604684114456176\n",
      "-0.0012610794067382813\n",
      "-0.0012611337502797444\n",
      "-0.00126118799050649\n",
      "-0.001261295715967814\n",
      "-0.0012613170544306437\n",
      "-0.0012613102197647095\n",
      "-0.0012613136768341064\n",
      "-0.001261311411857605\n",
      "-0.0012613179683685302\n",
      "-0.001261313048998515\n",
      "-0.0012612993637720743\n",
      "-0.0012609915018081666\n",
      "-0.0012603865067164104\n",
      "-0.0012610537846883137\n",
      "-0.0012612284898757934\n",
      "-0.0012612974882125854\n",
      "-0.0012613224585851033\n",
      "-0.001261319367090861\n",
      "-0.0012613078673680623\n",
      "-0.0012613266309102376\n",
      "-0.0012613102436065673\n",
      "-0.001261316204071045\n",
      "-0.001261310601234436\n",
      "-0.001261169187227885\n",
      "-0.001260427729288737\n",
      "-0.0012610815684000651\n",
      "-0.0012612101554870605\n",
      "-0.0012612780253092447\n",
      "-0.001261294412612915\n",
      "-0.0012612100839614869\n",
      "-0.0012612967491149902\n",
      "-0.0012613163471221923\n",
      "-0.0012613286336263022\n",
      "-0.0012613220453262328\n",
      "-0.0012613063017527263\n",
      "-0.0012611374934514365\n",
      "-0.0012604192972183227\n",
      "-0.0012610913515090942\n",
      "-0.0012612274090449014\n",
      "-0.0012612846771876017\n",
      "-0.001261299443244934\n",
      "-0.0012613302310307821\n",
      "-0.0012613165299097697\n",
      "-0.0012613234678904215\n",
      "-0.0012613152503967286\n",
      "-0.0012611974080403647\n",
      "-0.0012612650235493978\n",
      "-0.0012610107342402141\n",
      "-0.0012605871041615805\n",
      "-0.0012611184040705362\n",
      "-0.0012612312237421672\n",
      "-0.001261295485496521\n",
      "-0.0012613060633341472\n",
      "-0.0012613167444864909\n",
      "-0.001261322577794393\n",
      "-0.001261322291692098\n",
      "-0.001261322061220805\n",
      "-0.0012613002061843872\n",
      "-0.0012610427141189575\n",
      "-0.0012606189489364624\n",
      "-0.001260958774884542\n",
      "-0.0012611772378285726\n",
      "-0.0012612614472707112\n",
      "-0.0012613041798273722\n",
      "-0.001261314058303833\n",
      "-0.001261325240135193\n",
      "-0.0012613171895345051\n",
      "-0.0012613274653752644\n",
      "-0.0012613128105799357\n",
      "-0.0012612090349197389\n",
      "-0.0012604731718699137\n",
      "-0.001261086654663086\n",
      "-0.001261209718386332\n",
      "-0.001261290733019511\n",
      "-0.0012612958908081054\n",
      "-0.0012613186597824097\n",
      "-0.0012613093455632527\n",
      "-0.0012613245169321697\n",
      "-0.0012613213300704956\n",
      "-0.001261321512858073\n",
      "-0.0012611127217610678\n",
      "-0.001260686190923055\n",
      "-0.0012609644810358683\n",
      "-0.0012611718654632568\n",
      "-0.0012612638394037883\n",
      "-0.0012612866004308066\n",
      "-0.00126131382783254\n",
      "-0.0012613140662511189\n",
      "-0.001261313549677531\n",
      "-0.0012613267103830973\n",
      "-0.001261325510342916\n",
      "-0.0012612712065378825\n",
      "-0.0012603759209314982\n",
      "-0.0012608282883961996\n",
      "-0.0012611480077107746\n",
      "-0.001261266311009725\n",
      "-0.001261307430267334\n",
      "-0.0012613080104192098\n",
      "-0.0012613208373387654\n",
      "-0.0012613194624582927\n",
      "-0.0012613124926884969\n",
      "-0.0012613146861394247\n",
      "-0.0012612353642781576\n",
      "-0.0012611596743265787\n",
      "-0.0012611745198567708\n",
      "-0.0012611419757207235\n",
      "-0.0012612006425857544\n",
      "-0.0012613009373346965\n",
      "-0.0012613140662511189\n",
      "-0.001261311904589335\n",
      "-0.001260773229598999\n",
      "-0.001260677194595337\n",
      "-0.0012611032644907632\n",
      "-0.0012612410306930542\n",
      "-0.0012613051176071166\n",
      "-0.0012613186120986938\n",
      "-0.0012613208691279094\n",
      "-0.0012613118648529052\n",
      "-0.001261325494448344\n",
      "-0.0012613194863001507\n",
      "-0.0012612277030944823\n",
      "-0.0012610463380813598\n",
      "-0.0012608951250712077\n",
      "-0.0012612476189931233\n",
      "-0.0012612418333689372\n",
      "-0.0012612995942433674\n",
      "-0.0012612658818562826\n",
      "-0.0012612840652465821\n",
      "-0.0012612091064453125\n",
      "-0.001260865553220113\n",
      "-0.0012611574093500772\n",
      "-0.0012612202088038127\n",
      "-0.001261282245318095\n",
      "-0.0012612980763117472\n",
      "-0.001261303440729777\n",
      "-0.0012613125483194986\n",
      "-0.0012613180001576742\n",
      "-0.0012613105297088624\n",
      "-0.001260691181818644\n",
      "-0.001260084311167399\n",
      "-0.0012609798431396483\n",
      "-0.001261217490832011\n",
      "-0.00126129633585612\n",
      "-0.001261302145322164\n",
      "-0.0012613286097844442\n",
      "-0.0012613288720448811\n",
      "-0.0012613231579462687\n",
      "-0.0012613124450047812\n",
      "-0.0012613244930903117\n",
      "-0.001261308471361796\n",
      "-0.0012613096555074057\n",
      "-0.0012613204717636108\n",
      "-0.0012613158384958904\n",
      "-0.001261042332649231\n",
      "-0.0012603919744491577\n",
      "-0.001261084508895874\n",
      "-0.0012612234592437744\n",
      "-0.0012612928787867229\n",
      "-0.0012613136450449626\n",
      "-0.001261314098040263\n",
      "-0.001261316434542338\n",
      "-0.0012613199472427368\n",
      "-0.0012613196929295858\n",
      "-0.0012613241036732992\n",
      "-0.0012613213459650675\n",
      "-0.001261264983812968\n",
      "-0.0012603724479675292\n",
      "-0.0012608068227767943\n",
      "-0.0012611430724461872\n",
      "-0.0012612595081329346\n",
      "-0.001261315647761027\n",
      "-0.0012613185087839763\n",
      "-0.0012613229751586915\n",
      "-0.0012613259156545003\n",
      "-0.0012613233963648478\n",
      "-0.0012613375981648763\n",
      "-0.0012613288084665935\n",
      "-0.0012613158941268921\n",
      "-0.001261292028427124\n",
      "-0.001260880390803019\n",
      "-0.0012604475895563762\n",
      "-0.0012610855738321941\n",
      "-0.0012612175941467286\n",
      "-0.001261280099550883\n",
      "-0.001261308757464091\n",
      "-0.0012613307237625121\n",
      "-0.0012613263289133708\n",
      "-0.0012613250732421875\n",
      "-0.0012613189697265625\n",
      "-0.0012613250732421875\n",
      "-0.0012613193194071452\n",
      "-0.0012609809637069702\n",
      "-0.0012604694684346517\n",
      "-0.0012610615650812784\n",
      "-0.001261223578453064\n",
      "-0.001261294643084208\n",
      "-0.001261307175954183\n",
      "-0.0012613288323084513\n",
      "-0.0012613277355829876\n",
      "-0.001261326018969218\n",
      "-0.0012613250811894735\n",
      "-0.0012613206386566161\n",
      "-0.001261261264483134\n",
      "-0.0012610183795293173\n",
      "-0.0012607313632965087\n",
      "-0.0012611984650293985\n",
      "-0.0012612552404403688\n",
      "-0.001261267606417338\n",
      "-0.0012613043944040934\n",
      "-0.00126131960550944\n",
      "-0.0012613177061080933\n",
      "-0.0012612633069356283\n",
      "-0.0012609492301940918\n",
      "-0.00126101819674174\n",
      "-0.0012612201134363811\n",
      "-0.0012612893978754679\n",
      "-0.0012612892866134644\n",
      "-0.0012613170703252156\n",
      "-0.0012613144874572754\n",
      "-0.0012613166968027752\n",
      "-0.00126113387743632\n",
      "-0.0012603108485539755\n",
      "-0.0012610334157943726\n",
      "-0.0012612132708231607\n",
      "-0.0012612720648447673\n",
      "-0.00126131858030955\n",
      "-0.0012613138437271118\n",
      "-0.0012613248507181804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012613163550694783\n",
      "-0.0012613179683685302\n",
      "-0.0012613264799118042\n",
      "-0.0012612237771352133\n",
      "-0.0012611975034077963\n",
      "-0.0012609471321105958\n",
      "-0.0012607680082321166\n",
      "-0.0012611506223678588\n",
      "-0.0012612475395202636\n",
      "-0.0012613101800282797\n",
      "-0.001261327846844991\n",
      "-0.0012613024950027466\n",
      "-0.0012613230228424072\n",
      "-0.0012613245328267416\n",
      "-0.0012613139788309733\n",
      "-0.0012613035202026367\n",
      "-0.00126113760471344\n",
      "-0.001260076403617859\n",
      "-0.001260976012547811\n",
      "-0.0012611772060394286\n",
      "-0.0012612787326176961\n",
      "-0.0012613166968027752\n",
      "-0.0012613120396931965\n",
      "-0.0012613256057103475\n",
      "-0.0012613325595855712\n",
      "-0.0012613256533940632\n",
      "-0.0012612737735112507\n",
      "-0.0012611587047576904\n",
      "-0.0012612820307413737\n",
      "-0.001261318604151408\n",
      "-0.0012613292296727498\n",
      "-0.001261278780301412\n",
      "-0.0012606328010559083\n",
      "-0.0012608513991038005\n",
      "-0.0012611617644627889\n",
      "-0.0012612725814183552\n",
      "-0.0012613047202428183\n",
      "-0.0012613095124562582\n",
      "-0.0012613033215204874\n",
      "-0.001261319088935852\n",
      "-0.0012613109747568766\n",
      "-0.0012613333622614542\n",
      "-0.001261318842569987\n",
      "-0.0012612606287002563\n",
      "-0.0012609840393066406\n",
      "-0.0012604994455973307\n",
      "-0.0012610611120859783\n",
      "-0.001261198353767395\n",
      "-0.001261281927426656\n",
      "-0.0012613043705622356\n",
      "-0.0012613222519556682\n",
      "-0.0012613161245981852\n",
      "-0.0012613169193267822\n",
      "-0.0012613189140955607\n",
      "-0.001261307152112325\n",
      "-0.0012611098766326903\n",
      "-0.0012606607834498088\n",
      "-0.0012611566861470541\n",
      "-0.001261271071434021\n",
      "-0.0012613053321838378\n",
      "-0.001261301565170288\n",
      "-0.0012613139231999716\n",
      "-0.00126131960550944\n",
      "-0.0012613224426905314\n",
      "-0.0012613327026367188\n",
      "-0.0012612992604573569\n",
      "-0.0012610427141189575\n",
      "-0.0012600605885187785\n",
      "-0.0012609843015670777\n",
      "-0.0012611672639846801\n",
      "-0.0012613068024317424\n",
      "-0.0012613121191660563\n",
      "-0.0012613253434499104\n",
      "-0.0012613197962443035\n",
      "-0.0012613301992416381\n",
      "-0.001261306643486023\n",
      "-0.0012613288561503092\n",
      "-0.0012612683534622192\n",
      "-0.0012611140012741089\n",
      "-0.0012612680673599243\n",
      "-0.0012612945079803466\n",
      "-0.0012612770477930704\n",
      "-0.001260984245936076\n",
      "-0.0012608930508295694\n",
      "-0.0012611815929412843\n",
      "-0.0012612704277038575\n",
      "-0.0012613088369369508\n",
      "-0.001261298402150472\n",
      "-0.0012613055070241292\n",
      "-0.0012612963438034057\n",
      "-0.001261281696955363\n",
      "-0.0012611589113871256\n",
      "-0.0012607879320780436\n",
      "-0.0012612138907114666\n",
      "-0.0012612480322519938\n",
      "-0.001261250670750936\n",
      "-0.0012613199869791667\n",
      "-0.0012613014936447143\n",
      "-0.0012612818876902262\n",
      "-0.001261251926422119\n",
      "-0.0012613118886947632\n",
      "-0.0012612187623977662\n",
      "-0.0012598524490992228\n",
      "-0.001260927693049113\n",
      "-0.0012611213127772013\n",
      "-0.001261266811688741\n",
      "-0.0012613012313842775\n",
      "-0.0012613173961639405\n",
      "-0.0012613141775131226\n",
      "-0.0012613073348999024\n",
      "-0.0012611392339070637\n",
      "-0.0012612693786621095\n",
      "-0.0012613120317459107\n",
      "-0.0012613256295522055\n",
      "-0.0012613164663314818\n",
      "-0.0012613170385360718\n",
      "-0.0012613181511561076\n",
      "-0.0012613173564275106\n",
      "-0.001261317777633667\n",
      "-0.0012611519575119019\n",
      "-0.001260430653889974\n",
      "-0.0012611087878545125\n",
      "-0.00126122518380483\n",
      "-0.0012612821420033772\n",
      "-0.001261326797803243\n",
      "-0.0012613123099009195\n",
      "-0.001261315671602885\n",
      "-0.0012613241275151572\n",
      "-0.0012613259553909302\n",
      "-0.0012612037261327108\n",
      "-0.0012612312078475953\n",
      "-0.0012610285679499308\n",
      "-0.0012607488791147867\n",
      "-0.0012611691236495972\n",
      "-0.0012612513224283853\n",
      "-0.001261288324991862\n",
      "-0.0012613046725591024\n",
      "-0.001261306651433309\n",
      "-0.00126131911277771\n",
      "-0.0012613114992777506\n",
      "-0.001261316974957784\n",
      "-0.0012612508694330852\n",
      "-0.001261207906405131\n",
      "-0.0012598519881566366\n",
      "-0.0012608952522277833\n",
      "-0.001261107858022054\n",
      "-0.001261271603902181\n",
      "-0.0012612900336583456\n",
      "-0.0012613253275553385\n",
      "-0.001261324111620585\n",
      "-0.001261312953631083\n",
      "-0.0012613263765970865\n",
      "-0.0012613178809483846\n",
      "-0.0012613146702448526\n",
      "-0.0012611078262329102\n",
      "-0.0012612515528996786\n",
      "-0.001261292854944865\n",
      "-0.0012613166411717732\n",
      "-0.0012613192319869996\n",
      "-0.0012613307555516561\n",
      "-0.0012613167842229207\n",
      "-0.0012609967867533366\n",
      "-0.0012604114055633545\n",
      "-0.0012610390583674112\n",
      "-0.0012612247625986735\n",
      "-0.001261297067006429\n",
      "-0.001261322538057963\n",
      "-0.0012613137245178223\n",
      "-0.0012613296826680502\n",
      "-0.0012613232135772704\n",
      "-0.001261322585741679\n",
      "-0.0012613245884577433\n",
      "-0.0012613227446873982\n",
      "-0.001261323340733846\n",
      "-0.0012612099011739095\n",
      "-0.001260173535346985\n",
      "-0.001260967493057251\n",
      "-0.0012611905256907146\n",
      "-0.001261276650428772\n",
      "-0.0012613053798675538\n",
      "-0.0012613141695658366\n",
      "-0.0012613243103027343\n",
      "-0.0012613311211268107\n",
      "-0.0012613202571868896\n",
      "-0.001261332138379415\n",
      "-0.0012613158782323202\n",
      "-0.0012613057613372802\n",
      "-0.0012611214955647787\n",
      "-0.0012605546871821086\n",
      "-0.001261124086380005\n",
      "-0.0012612323681513468\n",
      "-0.0012612752596537272\n",
      "-0.001261311928431193\n",
      "-0.0012613198677698771\n",
      "-0.001261326535542806\n",
      "-0.001261290939648946\n",
      "-0.0012611109813054403\n",
      "-0.001261258625984192\n",
      "-0.0012612522125244141\n",
      "-0.001260964258511861\n",
      "-0.0012608616511027018\n",
      "-0.0012611580530802408\n",
      "-0.001261254088083903\n",
      "-0.0012612995862960816\n",
      "-0.0012613197485605875\n",
      "-0.0012613211552302043\n",
      "-0.0012613274017969768\n",
      "-0.0012613187074661254\n",
      "-0.0012613179922103882\n",
      "-0.001261246124903361\n",
      "-0.0012602990945180257\n",
      "-0.0012608663320541381\n",
      "-0.0012611821015675862\n",
      "-0.0012612806638081868\n",
      "-0.0012612990379333496\n",
      "-0.001261320416132609\n",
      "-0.001261315933863322\n",
      "-0.0012612592697143555\n",
      "-0.0012612043460210165\n",
      "-0.0012612848043441773\n",
      "-0.001261318850517273\n",
      "-0.0012613273779551188\n",
      "-0.0012613130331039428\n",
      "-0.0012613214174906414\n",
      "-0.0012612310806910196\n",
      "-0.001260208519299825\n",
      "-0.0012609622160593668\n",
      "-0.0012611934820810954\n",
      "-0.00126127184232076\n",
      "-0.001261309774716695\n",
      "-0.0012613139947255452\n",
      "-0.0012613128264745076\n",
      "-0.0012612069368362427\n",
      "-0.0012612794478734334\n",
      "-0.001261292862892151\n",
      "-0.0012613228877385457\n",
      "-0.0012613207578659057\n",
      "-0.0012613201061884563\n",
      "-0.0012613171418507894\n",
      "-0.001261136786142985\n",
      "-0.0012603354454040526\n",
      "-0.0012610680341720581\n",
      "-0.0012612069606781006\n",
      "-0.0012612912019093832\n",
      "-0.0012613102277119954\n",
      "-0.0012613210519154866\n",
      "-0.0012613138596216837\n",
      "-0.0012613194942474365\n",
      "-0.001261286958058675\n",
      "-0.0012610959609349569\n",
      "-0.0012612680753072102\n",
      "-0.0012613070885340372\n",
      "-0.0012613166093826294\n",
      "-0.0012611137946446738\n",
      "-0.001260172669092814\n",
      "-0.0012610016187032064\n",
      "-0.0012611860513687134\n",
      "-0.001261289127667745\n",
      "-0.0012613103389739991\n",
      "-0.0012613154331843057\n",
      "-0.0012613245248794555\n",
      "-0.0012613183895746867\n",
      "-0.001261322538057963\n",
      "-0.0012613230148951213\n",
      "-0.0012613141298294067\n",
      "-0.0012613237619400024\n",
      "-0.0012613195498784383\n",
      "-0.0012612904230753581\n",
      "-0.0012609667539596559\n",
      "-0.0012605164448420206\n",
      "-0.0012611026207605999\n",
      "-0.0012612165609995525\n",
      "-0.0012612882614135741\n",
      "-0.0012613166491190592\n",
      "-0.0012613128900527955\n",
      "-0.0012613369941711426\n",
      "-0.0012613269646962483\n",
      "-0.001261323873202006\n",
      "-0.0012613093455632527\n",
      "-0.0012611148993174235\n",
      "-0.001260709540049235\n",
      "-0.0012611777544021606\n",
      "-0.0012612585941950481\n",
      "-0.0012612912893295288\n",
      "-0.0012613009770711264\n",
      "-0.001261309846242269\n",
      "-0.0012613204876581827\n",
      "-0.0012612377405166627\n",
      "-0.0012612287521362306\n",
      "-0.0012611921310424804\n",
      "-0.001260516635576884\n",
      "-0.0012610333840052286\n",
      "-0.0012612014055252076\n",
      "-0.001261272676785787\n",
      "-0.0012612930456797281\n",
      "-0.0012613133192062378\n",
      "-0.0012613187789916993\n",
      "-0.0012613127708435059\n",
      "-0.001261326789855957\n",
      "-0.0012613195260365805\n",
      "-0.0012613186995188395\n",
      "-0.001261311904589335\n",
      "-0.0012606818596522012\n",
      "-0.001260166629155477\n",
      "-0.0012609370470046998\n",
      "-0.0012612250089645387\n",
      "-0.00126130690574646\n",
      "-0.0012613182385762534\n",
      "-0.00126133660475413\n",
      "-0.0012613291263580322\n",
      "-0.0012613137086232504\n",
      "-0.0012613203605016073\n",
      "-0.0012613351742426554\n",
      "-0.0012613217274347942\n",
      "-0.0012613185962041219\n",
      "-0.0012613318045934041\n",
      "-0.0012613163391749064\n",
      "-0.0012612136205037434\n",
      "-0.001260425329208374\n",
      "-0.0012610550721486409\n",
      "-0.0012612215518951417\n",
      "-0.0012612871408462525\n",
      "-0.0012613149325052897\n",
      "-0.0012613160848617554\n",
      "-0.0012613207419713338\n",
      "-0.001261321187019348\n",
      "-0.0012613274415334066\n",
      "-0.001261315147082011\n",
      "-0.0012613319238026937\n",
      "-0.0012612369696299234\n",
      "-0.0012610158761342367\n",
      "-0.001260273281733195\n",
      "-0.0012610235134760538\n",
      "-0.0012612040042877196\n",
      "-0.0012612806638081868\n",
      "-0.0012613147735595703\n",
      "-0.0012613269011179605\n",
      "-0.0012613308906555176\n",
      "-0.001261330238978068\n",
      "-0.0012613319635391236\n",
      "-0.0012613143603006998\n",
      "-0.0012613292535146078\n",
      "-0.001261319915453593\n",
      "-0.001261281951268514\n",
      "-0.0012605326016743978\n",
      "-0.0012607932726542155\n",
      "-0.0012611358006795247\n",
      "-0.0012612687269846598\n",
      "-0.001261312468846639\n",
      "-0.0012613278150558473\n",
      "-0.0012613184928894042\n",
      "-0.0012613262335459392\n",
      "-0.0012613136450449626\n",
      "-0.0012612911542256673\n",
      "-0.0012611858129501342\n",
      "-0.0012612828652064005\n",
      "-0.0012612870931625365\n",
      "-0.0012610347191492717\n",
      "-0.0012607343594233196\n",
      "-0.0012611489375432333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012612590471903482\n",
      "-0.0012612995306650798\n",
      "-0.0012613062461217244\n",
      "-0.0012613162438074748\n",
      "-0.0012613253672917684\n",
      "-0.0012613165775934856\n",
      "-0.001261293363571167\n",
      "-0.0012612084070841472\n",
      "-0.0012612086137135823\n",
      "-0.0012601007143656412\n",
      "-0.0012609589417775472\n",
      "-0.0012611823558807372\n",
      "-0.001261256488164266\n",
      "-0.001261318532625834\n",
      "-0.0012613117297490437\n",
      "-0.001261312182744344\n",
      "-0.001261317213376363\n",
      "-0.0012613161166508992\n",
      "-0.0012613185405731201\n",
      "-0.0012613207896550497\n",
      "-0.001261246697107951\n",
      "-0.0012611839294433594\n",
      "-0.001261250376701355\n",
      "-0.001261194602648417\n",
      "-0.0012608324845631918\n",
      "-0.0012612077713012695\n",
      "-0.001261226757367452\n",
      "-0.0012612683216730754\n",
      "-0.001261312953631083\n",
      "-0.0012613171418507894\n",
      "-0.0012613040844599405\n",
      "-0.0012613099654515585\n",
      "-0.0012611323753992717\n",
      "-0.0012605317831039429\n",
      "-0.0012610722541809081\n",
      "-0.0012612220684687296\n",
      "-0.0012612616856892904\n",
      "-0.0012613168795903523\n",
      "-0.0012613245248794555\n",
      "-0.0012613173007965089\n",
      "-0.00126131645043691\n",
      "-0.0012611375013987223\n",
      "-0.0012612433513005574\n",
      "-0.0012612956126530966\n",
      "-0.0012612944046656292\n",
      "-0.00126072781085968\n",
      "-0.0012605957825978598\n",
      "-0.0012610843181610108\n",
      "-0.0012612571318944296\n",
      "-0.0012613118092219034\n",
      "-0.0012613233009974162\n",
      "-0.0012613232056299846\n",
      "-0.0012613216241200765\n",
      "-0.001261320185661316\n",
      "-0.0012613263765970865\n",
      "-0.0012613115628560385\n",
      "-0.0012613205273946126\n",
      "-0.0012613082726796467\n",
      "-0.0012613085587819418\n",
      "-0.0012607745409011842\n",
      "-0.001260496457417806\n",
      "-0.0012610654910405476\n",
      "-0.0012612334966659545\n",
      "-0.0012612914085388183\n",
      "-0.0012613308509190878\n",
      "-0.0012613130569458007\n",
      "-0.0012613181034723917\n",
      "-0.0012613310098648072\n",
      "-0.0012613248348236083\n",
      "-0.0012613083283106487\n",
      "-0.001261319661140442\n",
      "-0.001261251449584961\n",
      "-0.001260342526435852\n",
      "-0.0012609083414077758\n",
      "-0.0012611881812413534\n",
      "-0.001261276666323344\n",
      "-0.0012613008658091227\n",
      "-0.0012613192637761434\n",
      "-0.001261322514216105\n",
      "-0.001261321234703064\n",
      "-0.001261322585741679\n",
      "-0.0012613185485204062\n",
      "-0.0012612908522288005\n",
      "-0.0012611258506774901\n",
      "-0.0012612468083699545\n",
      "-0.0012612314224243165\n",
      "-0.0012607917547225952\n",
      "-0.0012610263268152872\n",
      "-0.001261183269818624\n",
      "-0.0012612556934356689\n",
      "-0.0012612903118133544\n",
      "-0.0012613119602203368\n",
      "-0.0012613180716832479\n",
      "-0.0012613188982009888\n",
      "-0.0012613112370173137\n",
      "-0.0012611982981363932\n",
      "-0.0012607763687769572\n",
      "-0.0012611878792444864\n",
      "-0.0012612379948298137\n",
      "-0.001261259365081787\n",
      "-0.0012612797737121583\n",
      "-0.0012612688461939494\n",
      "-0.0012613230228424072\n",
      "-0.0012613011121749879\n",
      "-0.0012610595305760702\n",
      "-0.0012608254591623942\n",
      "-0.0012611899932225546\n",
      "-0.0012612715005874634\n",
      "-0.001261285408337911\n",
      "-0.0012613153457641602\n",
      "-0.0012613285144170126\n",
      "-0.0012613268852233886\n",
      "-0.001261293609937032\n",
      "-0.0012607287804285686\n",
      "-0.0012607499361038208\n",
      "-0.0012611169576644897\n",
      "-0.001261252776781718\n",
      "-0.0012613112608591715\n",
      "-0.0012613210916519165\n",
      "-0.0012613103628158569\n",
      "-0.0012612311363220214\n",
      "-0.0012612308422724405\n",
      "-0.0012612940152486164\n",
      "-0.001261306357383728\n",
      "-0.0012613162120183308\n",
      "-0.0012613099416097005\n",
      "-0.0012609434048334758\n",
      "-0.0012606208006540934\n",
      "-0.0012611046234766643\n",
      "-0.001261242397626241\n",
      "-0.0012612990140914917\n",
      "-0.0012613245089848836\n",
      "-0.0012613224744796752\n",
      "-0.0012613227605819703\n",
      "-0.0012613116900126139\n",
      "-0.0012611117045084636\n",
      "-0.0012612640937169394\n",
      "-0.0012612885475158692\n",
      "-0.0012611476182937621\n",
      "-0.001260694177945455\n",
      "-0.0012611708958943685\n",
      "-0.0012612507899602255\n",
      "-0.0012612916707992554\n",
      "-0.0012612996339797973\n",
      "-0.0012613214413324991\n",
      "-0.001261316188176473\n",
      "-0.001261319359143575\n",
      "-0.0012613310972849527\n",
      "-0.0012612969239552815\n",
      "-0.001260703984896342\n",
      "-0.001260619044303894\n",
      "-0.0012610807418823242\n",
      "-0.0012612454652786256\n",
      "-0.0012612976789474488\n",
      "-0.0012613144159317018\n",
      "-0.0012613269170125326\n",
      "-0.0012613241910934448\n",
      "-0.0012612032016118368\n",
      "-0.0012612282355626424\n",
      "-0.0012612867434819539\n",
      "-0.001261312429110209\n",
      "-0.0012613209247589112\n",
      "-0.0012613147815068563\n",
      "-0.0012610032796859742\n",
      "-0.0012605849981307984\n",
      "-0.001261099123954773\n",
      "-0.0012612324237823486\n",
      "-0.001261284605662028\n",
      "-0.001261319859822591\n",
      "-0.0012613233725229898\n",
      "-0.0012613211154937744\n",
      "-0.00126131542523702\n",
      "-0.0012612741947174073\n",
      "-0.001261150042215983\n",
      "-0.0012612756888071696\n",
      "-0.0012613011757532755\n",
      "-0.0012612680753072102\n",
      "-0.0012602308750152588\n",
      "-0.001260794480641683\n",
      "-0.0012611775000890095\n",
      "-0.0012612469514211018\n",
      "-0.0012612873395284017\n",
      "-0.0012613194147745768\n",
      "-0.001261323078473409\n",
      "-0.0012613276561101278\n",
      "-0.0012613191445668538\n",
      "-0.00126131960550944\n",
      "-0.0012613263209660848\n",
      "-0.0012613294045130412\n",
      "-0.0012613211949666341\n",
      "-0.0012613161484400432\n",
      "-0.001261058235168457\n",
      "-0.0012609360933303834\n",
      "-0.001260775327682495\n",
      "-0.001261154596010844\n",
      "-0.0012612654209136964\n",
      "-0.0012613048235575357\n",
      "-0.0012612993955612183\n",
      "-0.0012613060315450032\n",
      "-0.0012613179922103882\n",
      "-0.001261328689257304\n",
      "-0.00126132976214091\n",
      "-0.0012611831347147623\n",
      "-0.0012604498465855916\n",
      "-0.0012610786914825439\n",
      "-0.0012612311283747356\n",
      "-0.0012612895806630453\n",
      "-0.0012613051652908326\n",
      "-0.001261321751276652\n",
      "-0.0012613194227218628\n",
      "-0.0012613166650136311\n",
      "-0.0012613241036732992\n",
      "-0.0012613143682479858\n",
      "-0.0012612618525822956\n",
      "-0.0012610556205113728\n",
      "-0.0012606847922007243\n",
      "-0.0012610279719034831\n",
      "-0.0012611833413441976\n",
      "-0.0012612641016642252\n",
      "-0.0012613083680470785\n",
      "-0.0012613188664118448\n",
      "-0.0012613166014353433\n",
      "-0.0012613269249598185\n",
      "-0.0012613269249598185\n",
      "-0.0012613125801086427\n",
      "-0.0012610872745513915\n",
      "-0.0012605822960535685\n",
      "-0.001261129053433736\n",
      "-0.0012612391312917073\n",
      "-0.0012612863699595132\n",
      "-0.001261316680908203\n",
      "-0.001261310124397278\n",
      "-0.0012613242467244465\n",
      "-0.001261322053273519\n",
      "-0.0012611806074778239\n",
      "-0.0012612275441487631\n",
      "-0.0012611923297246297\n",
      "-0.0012608256498972575\n",
      "-0.001261118737856547\n",
      "-0.0012612051248550415\n",
      "-0.0012612601359685263\n",
      "-0.0012613038698832194\n",
      "-0.0012613027413686116\n",
      "-0.0012613250255584718\n",
      "-0.0012612910588582357\n",
      "-0.0012611543575922648\n",
      "-0.001260863169034322\n",
      "-0.0012612442096074422\n",
      "-0.0012612524271011353\n",
      "-0.0012612661520640055\n",
      "-0.001261318333943685\n",
      "-0.0012611812591552735\n",
      "-0.0012611590147018432\n",
      "-0.0012612443288167318\n",
      "-0.0012611353556315104\n",
      "-0.0012607727289199828\n",
      "-0.0012612035195032755\n",
      "-0.0012612682580947876\n",
      "-0.0012612807989120483\n",
      "-0.001261296518643697\n",
      "-0.0012613151868184408\n",
      "-0.0012613173961639405\n",
      "-0.001261300261815389\n",
      "-0.0012612212419509888\n",
      "-0.0012607719739278157\n",
      "-0.0012611397822697957\n",
      "-0.0012612159729003907\n",
      "-0.0012612719456354777\n",
      "-0.0012612934668858847\n",
      "-0.0012613140185674032\n",
      "-0.0012611725648244221\n",
      "-0.001261059856414795\n",
      "-0.0012612685283025105\n",
      "-0.001261299967765808\n",
      "-0.0012611498991648355\n",
      "-0.0012603269338607788\n",
      "-0.0012610660552978515\n",
      "-0.0012612013339996337\n",
      "-0.0012612846374511718\n",
      "-0.0012613112290700277\n",
      "-0.0012613109191258749\n",
      "-0.0012613182544708253\n",
      "-0.0012613167603810628\n",
      "-0.0012613250255584718\n",
      "-0.0012613140185674032\n",
      "-0.0012613253196080525\n",
      "-0.0012613359928131103\n",
      "-0.0012613179365793864\n",
      "-0.0012611238876978556\n",
      "-0.00126034251054128\n",
      "-0.0012610695997873943\n",
      "-0.0012612103541692098\n",
      "-0.001261288849512736\n",
      "-0.001261312174797058\n",
      "-0.0012613237063090006\n",
      "-0.001261292807261149\n",
      "-0.001261162535349528\n",
      "-0.0012612801551818847\n",
      "-0.001261315099398295\n",
      "-0.0012613279581069946\n",
      "-0.0012613207181294758\n",
      "-0.0012613195260365805\n",
      "-0.0012612461566925048\n",
      "-0.001260382620493571\n",
      "-0.0012609841505686443\n",
      "-0.0012612085501352945\n",
      "-0.0012612850427627565\n",
      "-0.001261303965250651\n",
      "-0.001261314312616984\n",
      "-0.0012613258123397827\n",
      "-0.001261330811182658\n",
      "-0.0012613109827041627\n",
      "-0.001261056089401245\n",
      "-0.0012612287918726604\n",
      "-0.0012612717469533284\n",
      "-0.0012613283793131511\n",
      "-0.001261304744084676\n",
      "-0.0012607796907424928\n",
      "-0.0012605584541956585\n",
      "-0.001261068844795227\n",
      "-0.00126125545501709\n",
      "-0.0012613032658894856\n",
      "-0.0012613247315088909\n",
      "-0.0012613339026769001\n",
      "-0.00126131591796875\n",
      "-0.0012613265673319498\n",
      "-0.0012613228797912599\n",
      "-0.0012613246997197469\n",
      "-0.00126132333278656\n",
      "-0.0012613216956456502\n",
      "-0.001261327330271403\n",
      "-0.0012611937046051026\n",
      "-0.0012603550513585408\n",
      "-0.0012610440095265707\n",
      "-0.0012611769199371338\n",
      "-0.0012612802267074584\n",
      "-0.00126129732131958\n",
      "-0.001261312182744344\n",
      "-0.001261323857307434\n",
      "-0.0012613239844640096\n",
      "-0.001261321512858073\n",
      "-0.0012613075176874796\n",
      "-0.0012611191908518473\n",
      "-0.001261263664563497\n",
      "-0.0012612933079401653\n",
      "-0.0012610363006591797\n",
      "-0.0012605324188868204\n",
      "-0.0012610952774683634\n",
      "-0.0012612241983413696\n",
      "-0.0012612910906473795\n",
      "-0.0012613183736801148\n",
      "-0.0012613179047902426\n",
      "-0.0012613208452860515\n",
      "-0.0012613266070683796\n",
      "-0.0012613162120183308\n",
      "-0.0012613195260365805\n",
      "-0.001261320964495341\n",
      "-0.0012612600406010945\n",
      "-0.0012605461359024048\n",
      "-0.0012608790715535481\n",
      "-0.0012611552079518635\n",
      "-0.0012612603823343913\n",
      "-0.0012613122542699177\n",
      "-0.0012613177061080933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00126132230758667\n",
      "-0.0012613242467244465\n",
      "-0.0012612505197525025\n",
      "-0.0012612372716267904\n",
      "-0.0012612959782282512\n",
      "-0.0012613145430882772\n",
      "-0.0012612889528274536\n",
      "-0.001260586134592692\n",
      "-0.001260830775896708\n",
      "-0.0012611480156580607\n",
      "-0.001261268162727356\n",
      "-0.0012613110701243083\n",
      "-0.0012613127390543619\n",
      "-0.001261324644088745\n",
      "-0.0012613219102223714\n",
      "-0.0012612838347752888\n",
      "-0.0012611372788747153\n",
      "-0.0012612641016642252\n",
      "-0.001261299188931783\n",
      "-0.0012613286018371582\n",
      "-0.0012613211790720622\n",
      "-0.0012612303733825683\n",
      "-0.0012598822673161824\n",
      "-0.0012609342575073243\n",
      "-0.0012611198027928671\n",
      "-0.0012612773100535075\n",
      "-0.0012613057295481364\n",
      "-0.0012613277355829876\n",
      "-0.0012613182226816812\n",
      "-0.0012613306999206544\n",
      "-0.0012613273779551188\n",
      "-0.001261326003074646\n",
      "-0.00126131698290507\n",
      "-0.0012613252639770507\n",
      "-0.0012613234122594197\n",
      "-0.0012611978689829508\n",
      "-0.0012610707521438598\n",
      "-0.0012612746556599935\n",
      "-0.001261302391688029\n",
      "-0.0012613036553064982\n",
      "-0.0012609695990880331\n",
      "-0.001260546056429545\n",
      "-0.001261091430981954\n",
      "-0.0012612316528956096\n",
      "-0.0012613053083419799\n",
      "-0.0012613072792689005\n",
      "-0.0012613183577855427\n",
      "-0.00126132603486379\n",
      "-0.0012613178491592406\n",
      "-0.0012613174041112263\n",
      "-0.0012613216718037922\n",
      "-0.0012613260984420777\n",
      "-0.001261327616373698\n",
      "-0.0012612935622533163\n",
      "-0.001260449242591858\n",
      "-0.0012607096751530965\n",
      "-0.001261144717534383\n",
      "-0.001261275601387024\n",
      "-0.0012613022009531657\n",
      "-0.0012613028764724731\n",
      "-0.0012612446705500284\n",
      "-0.0012613056341807048\n",
      "-0.0012613078594207764\n",
      "-0.0012613233884175617\n",
      "-0.0012613226413726807\n",
      "-0.0012613191445668538\n",
      "-0.0012613274733225504\n",
      "-0.0012612876335779827\n",
      "-0.0012611631472905476\n",
      "-0.0012609431266784668\n",
      "-0.0012607179959615072\n",
      "-0.0012611285050710041\n",
      "-0.0012612515211105346\n",
      "-0.0012612929026285808\n",
      "-0.0012613087177276612\n",
      "-0.0012613291025161744\n",
      "-0.0012613313754399618\n",
      "-0.0012613208691279094\n",
      "-0.0012613166173299154\n",
      "-0.0012613215366999307\n",
      "-0.0012612324953079224\n",
      "-0.001260459081331889\n",
      "-0.0012610317468643188\n",
      "-0.0012612099726994831\n",
      "-0.0012612822373708088\n",
      "-0.0012613103151321412\n",
      "-0.0012613189856211344\n",
      "-0.0012613276402155557\n",
      "-0.00126131755510966\n",
      "-0.0012613187948862712\n",
      "-0.001261322331428528\n",
      "-0.0012612649520238241\n",
      "-0.0012602938652038573\n",
      "-0.0012608721097310384\n",
      "-0.0012611725966135661\n",
      "-0.001261280393600464\n",
      "-0.0012613090753555297\n",
      "-0.0012613220453262328\n",
      "-0.0012613184531529746\n",
      "-0.0012613232930501301\n",
      "-0.0012613258361816406\n",
      "-0.0012613146384557088\n",
      "-0.0012613257010777792\n",
      "-0.0012613180001576742\n",
      "-0.0012613226493199666\n",
      "-0.0012612699588139852\n",
      "-0.0012605364799499511\n",
      "-0.0012608356714248657\n",
      "-0.0012611547549565634\n",
      "-0.0012612656911214193\n",
      "-0.0012612933238347372\n",
      "-0.001261321465174357\n",
      "-0.001261327838897705\n",
      "-0.0012613261143366496\n",
      "-0.0012613219022750855\n",
      "-0.001261316967010498\n",
      "-0.0012612872918446858\n",
      "-0.0012609829584757488\n",
      "-0.0012608035882314046\n",
      "-0.0012611628691355386\n",
      "-0.0012612602551778157\n",
      "-0.0012612979968388875\n",
      "-0.0012613196849822997\n",
      "-0.0012613184611002604\n",
      "-0.001261316196123759\n",
      "-0.0012613234996795655\n",
      "-0.0012613273859024049\n",
      "-0.0012612994273503621\n",
      "-0.00126084094842275\n",
      "-0.0012606935103734334\n",
      "-0.0012611239592234293\n",
      "-0.0012612525860468547\n",
      "-0.0012612972736358642\n",
      "-0.0012613128980000813\n",
      "-0.0012613105376561482\n",
      "-0.001261319955190023\n",
      "-0.0012613180716832479\n",
      "-0.0012613116979599\n",
      "-0.0012612298091252644\n",
      "-0.0012605069160461426\n",
      "-0.0012610738118489584\n",
      "-0.001261218778292338\n",
      "-0.0012612806797027587\n",
      "-0.0012613145510355632\n",
      "-0.001261323094367981\n",
      "-0.0012613225698471068\n",
      "-0.0012613220850626627\n",
      "-0.0012613187789916993\n",
      "-0.0012613166411717732\n",
      "-0.0012613195816675823\n",
      "-0.0012611565033594768\n",
      "-0.001260509244600932\n",
      "-0.001261082394917806\n",
      "-0.0012612162113189698\n",
      "-0.0012612876097361247\n",
      "-0.0012612878243128459\n",
      "-0.0012613187313079833\n",
      "-0.001261322792371114\n",
      "-0.001261328673362732\n",
      "-0.0012613226970036825\n",
      "-0.0012613113164901732\n",
      "-0.0012608078002929687\n",
      "-0.0012605495691299438\n",
      "-0.001261052640279134\n",
      "-0.0012612566391626994\n",
      "-0.001261308725674947\n",
      "-0.0012613212744394938\n",
      "-0.0012613188187281291\n",
      "-0.0012613248030344645\n",
      "-0.0012613215843836467\n",
      "-0.0012613229990005492\n",
      "-0.0012613255023956299\n",
      "-0.0012613178253173829\n",
      "-0.00126114501953125\n",
      "-0.0012612134377161662\n",
      "-0.0012609503110249838\n",
      "-0.0012609118064244589\n",
      "-0.0012611706256866455\n",
      "-0.0012612759669621786\n",
      "-0.0012613057454427083\n",
      "-0.0012612968683242797\n",
      "-0.001261310029029846\n",
      "-0.001261318556467692\n",
      "-0.0012613167603810628\n",
      "-0.0012613247235616048\n",
      "-0.001261221448580424\n",
      "-0.001260170062383016\n",
      "-0.001261018657684326\n",
      "-0.001261192242304484\n",
      "-0.0012612647612889607\n",
      "-0.0012613110303878784\n",
      "-0.0012613133827845254\n",
      "-0.0012612745841344198\n",
      "-0.0012612942854563396\n",
      "-0.0012613214095433553\n",
      "-0.001261315647761027\n",
      "-0.0012612624009450276\n",
      "-0.0012613142331441244\n",
      "-0.0012613195975621542\n",
      "-0.0012613159577051797\n",
      "-0.0012613187630971274\n",
      "-0.0012610278685887654\n",
      "-0.0012604083061218262\n",
      "-0.0012610192696253459\n",
      "-0.0012611954689025879\n",
      "-0.0012613016287485758\n",
      "-0.0012613206148147584\n",
      "-0.0012613109827041627\n",
      "-0.0012613090117772421\n",
      "-0.0012613183895746867\n",
      "-0.0012613214333852133\n",
      "-0.0012613316535949708\n",
      "-0.0012613227685292561\n",
      "-0.0012611557324727375\n",
      "-0.0012611713488896688\n",
      "-0.001261238145828247\n",
      "-0.0012607092539469401\n",
      "-0.0012610009670257567\n",
      "-0.0012611775557200113\n",
      "-0.0012612696329752605\n",
      "-0.0012612946192423504\n",
      "-0.0012613134860992431\n",
      "-0.0012613189458847045\n",
      "-0.0012613224585851033\n",
      "-0.0012613247235616048\n",
      "-0.0012613221883773804\n",
      "-0.0012613208373387654\n",
      "-0.0012612699111302693\n",
      "-0.0012601412852605183\n",
      "-0.0012608714421590169\n",
      "-0.0012611751317977906\n",
      "-0.001261258832613627\n",
      "-0.0012613023360570272\n",
      "-0.0012613217035929362\n",
      "-0.001261321528752645\n",
      "-0.0012613107840220134\n",
      "-0.0012610898017883301\n",
      "-0.0012612648566563923\n",
      "-0.0012612948099772136\n",
      "-0.0012613199710845948\n",
      "-0.0012613197247187296\n",
      "-0.0012613200823465983\n",
      "-0.0012613165140151978\n",
      "-0.001261323634783427\n",
      "-0.0012612417380015056\n",
      "-0.0012603222211201987\n",
      "-0.0012610054095586142\n",
      "-0.0012612021366755167\n",
      "-0.0012612767060597738\n",
      "-0.0012613033215204874\n",
      "-0.0012613205989201865\n",
      "-0.0012613218863805134\n",
      "-0.0012613139629364014\n",
      "-0.0012613238970438638\n",
      "-0.0012613220691680908\n",
      "-0.0012613287130991617\n",
      "-0.0012612160285313924\n",
      "-0.0012610835790634156\n",
      "-0.0012612721999486287\n",
      "-0.0012612122933069864\n",
      "-0.001260202137629191\n",
      "-0.0012609968821207683\n",
      "-0.001261192234357198\n",
      "-0.0012612661838531493\n",
      "-0.0012613115072250367\n",
      "-0.0012613184531529746\n",
      "-0.0012613289038340251\n",
      "-0.0012613178412119548\n",
      "-0.0012613260825475056\n",
      "-0.0012613200743993123\n",
      "-0.001261327846844991\n",
      "-0.0012613284508387248\n",
      "-0.0012613231658935547\n",
      "-0.0012613250414530437\n",
      "-0.0012612985054651896\n",
      "-0.0012606231689453126\n",
      "-0.0012606419483820598\n",
      "-0.0012611123164494832\n",
      "-0.0012612760066986084\n",
      "-0.0012613042990366617\n",
      "-0.0012613108396530152\n",
      "-0.0012611345370610555\n",
      "-0.0012612301508585612\n",
      "-0.0012612977027893066\n",
      "-0.0012613193909327188\n",
      "-0.001261319335301717\n",
      "-0.0012613264242808024\n",
      "-0.0012613179922103882\n",
      "-0.0012613261540730795\n",
      "-0.001261325208346049\n",
      "-0.0012611422379811604\n",
      "-0.0012602970123291016\n",
      "-0.0012610602617263793\n",
      "-0.0012611968835194907\n",
      "-0.0012612801790237427\n",
      "-0.0012613217751185099\n",
      "-0.0012613250176111857\n",
      "-0.0012613244454065958\n",
      "-0.0012613287051518758\n",
      "-0.0012612423261006674\n",
      "-0.0012611452182133991\n",
      "-0.0012612661441167195\n",
      "-0.0012613117456436156\n",
      "-0.0012613213221232095\n",
      "-0.0012613260825475056\n",
      "-0.0012612641493479411\n",
      "-0.0012602474133173625\n",
      "-0.001260910701751709\n",
      "-0.0012611912806828817\n",
      "-0.0012612622340520223\n",
      "-0.0012613007545471192\n",
      "-0.0012613162755966186\n",
      "-0.0012613192081451416\n",
      "-0.001261324143409729\n",
      "-0.0012613229195276897\n",
      "-0.0012613278786341349\n",
      "-0.001261324183146159\n",
      "-0.00126132706006368\n",
      "-0.0012613252480824788\n",
      "-0.001261181386311849\n",
      "-0.0012611112197240195\n",
      "-0.0012611363887786865\n",
      "-0.0012604300737380982\n",
      "-0.0012610969146092733\n",
      "-0.0012612228552500407\n",
      "-0.001261281402905782\n",
      "-0.0012613063097000122\n",
      "-0.0012613129138946534\n",
      "-0.0012613211234410605\n",
      "-0.0012613234043121339\n",
      "-0.0012613207499186198\n",
      "-0.0012613268534342448\n",
      "-0.001261323587099711\n",
      "-0.0012613012234369914\n",
      "-0.0012609707355499267\n",
      "-0.0012606601635615031\n",
      "-0.0012611189603805543\n",
      "-0.001261241046587626\n",
      "-0.0012612937053044636\n",
      "-0.0012613192399342854\n",
      "-0.0012613222201665242\n",
      "-0.0012613217910130818\n",
      "-0.0012613207181294758\n",
      "-0.0012612517436345419\n",
      "-0.001261122218767802\n",
      "-0.0012612561146418254\n",
      "-0.0012613090912501018\n",
      "-0.0012610976775487264\n",
      "-0.0012603481769561768\n",
      "-0.0012610579967498778\n",
      "-0.0012612018744150798\n",
      "-0.001261296812693278\n",
      "-0.0012613332827885945\n",
      "-0.0012613207260767619\n",
      "-0.0012613219579060873\n",
      "-0.00126131858030955\n",
      "-0.001261321465174357\n",
      "-0.0012613169511159262\n",
      "-0.001261314900716146\n",
      "-0.0012613160689671835\n",
      "-0.0012613199869791667\n",
      "-0.0012612659136454264\n",
      "-0.0012604459285736084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012609307924906412\n",
      "-0.001261154596010844\n",
      "-0.0012612628936767578\n",
      "-0.0012613080422083536\n",
      "-0.0012613234599431356\n",
      "-0.0012613284349441529\n",
      "-0.0012613274892171223\n",
      "-0.0012613071839014688\n",
      "-0.0012612229665120442\n",
      "-0.0012612976710001628\n",
      "-0.0012613118171691895\n",
      "-0.0012613263448079427\n",
      "-0.0012612101475397745\n",
      "-0.0012603353182474772\n",
      "-0.0012610674937566122\n",
      "-0.0012612205823262532\n",
      "-0.001261284327507019\n",
      "-0.0012613110383351645\n",
      "-0.0012613258679707844\n",
      "-0.0012613110939661662\n",
      "-0.001261186973253886\n",
      "-0.0012612725496292114\n",
      "-0.0012613144000371296\n",
      "-0.0012613158305486043\n",
      "-0.001261317483584086\n",
      "-0.0012613219340642293\n",
      "-0.0012612482865651448\n",
      "-0.0012604036569595336\n",
      "-0.0012610119422276815\n",
      "-0.0012612060546875\n",
      "-0.001261290661493937\n",
      "-0.0012613076368967692\n",
      "-0.0012613235553105672\n",
      "-0.0012613148848215738\n",
      "-0.001261224627494812\n",
      "-0.0012611441214879354\n",
      "-0.0012612680276234945\n",
      "-0.0012613101720809936\n",
      "-0.001261316688855489\n",
      "-0.0012613155682881672\n",
      "-0.0012612932205200195\n",
      "-0.0012605411211649576\n",
      "-0.0012606868346532185\n",
      "-0.0012611336628595988\n",
      "-0.0012612643241882325\n",
      "-0.0012612983226776124\n",
      "-0.0012613170305887857\n",
      "-0.0012613134066263834\n",
      "-0.0012613140424092611\n",
      "-0.001261325740814209\n",
      "-0.0012613235632578533\n",
      "-0.0012613243420918783\n",
      "-0.001261323587099711\n",
      "-0.0012613234281539916\n",
      "-0.0012613239447275798\n",
      "-0.001261094363530477\n",
      "-0.0012609726985295614\n",
      "-0.0012604288736979166\n",
      "-0.001261095094680786\n",
      "-0.0012612236897150675\n",
      "-0.0012612848917643229\n",
      "-0.0012613181591033936\n",
      "-0.0012613192081451416\n",
      "-0.0012613231658935547\n",
      "-0.0012613286336263022\n",
      "-0.0012613179365793864\n",
      "-0.0012613207181294758\n",
      "-0.0012613222996393839\n",
      "-0.0012613017559051515\n",
      "-0.0012609503587086995\n",
      "-0.0012606547117233276\n",
      "-0.0012611125310262043\n",
      "-0.0012612427473068237\n",
      "-0.0012613038221995037\n",
      "-0.0012613190094629924\n",
      "-0.0012613154331843057\n",
      "-0.0012613197485605875\n",
      "-0.0012613186915715535\n",
      "-0.001261326313018799\n",
      "-0.0012613306681315104\n",
      "-0.0012613087177276612\n",
      "-0.0012610562562942506\n",
      "-0.0012607819477717082\n",
      "-0.0012608163913091024\n",
      "-0.0012611557801564535\n",
      "-0.0012612521648406982\n",
      "-0.0012612958113352457\n",
      "-0.0012613229831059773\n",
      "-0.0012613197565078736\n",
      "-0.0012613136132558188\n",
      "-0.0012613208055496216\n",
      "-0.0012613222281138102\n",
      "-0.0012613211711247762\n",
      "-0.0012612475633621216\n",
      "-0.0012604502121607462\n",
      "-0.0012610188166300455\n",
      "-0.0012612077156702677\n",
      "-0.0012612786451975506\n",
      "-0.0012613150676091512\n",
      "-0.0012613194942474365\n",
      "-0.001261318850517273\n",
      "-0.001261329444249471\n",
      "-0.001261245044072469\n",
      "-0.0012611813068389892\n",
      "-0.0012612805604934692\n",
      "-0.0012613163948059082\n",
      "-0.0012613140185674032\n",
      "-0.001261084032058716\n",
      "-0.00126040248076121\n",
      "-0.0012610658407211303\n",
      "-0.0012612089236577351\n",
      "-0.0012612919807434082\n",
      "-0.0012613215525945029\n",
      "-0.0012613155047098796\n",
      "-0.0012613213221232095\n",
      "-0.001261317801475525\n",
      "-0.0012613136688868205\n",
      "-0.0012613220850626627\n",
      "-0.001261313279469808\n",
      "-0.0012613184611002604\n",
      "-0.0012611021518707276\n",
      "-0.0012610256512959799\n",
      "-0.0012606111685434978\n",
      "-0.0012611437638600667\n",
      "-0.0012612448692321777\n",
      "-0.0012612865686416625\n",
      "-0.0012613219817479452\n",
      "-0.001261315353711446\n",
      "-0.0012613207896550497\n",
      "-0.0012613181511561076\n",
      "-0.0012613178968429565\n",
      "-0.001261318055788676\n",
      "-0.0012611991246541342\n",
      "-0.001260635232925415\n",
      "-0.0012611438671747844\n",
      "-0.0012612386067708333\n",
      "-0.0012612825949986776\n",
      "-0.001261306627591451\n",
      "-0.001261318278312683\n",
      "-0.0012613243420918783\n",
      "-0.0012613215843836467\n",
      "-0.0012612287918726604\n",
      "-0.0012611064751942953\n",
      "-0.0012611703236897786\n",
      "-0.001260690975189209\n",
      "-0.0012611398696899415\n",
      "-0.0012612314144770304\n",
      "-0.0012612752278645834\n",
      "-0.001261292290687561\n",
      "-0.0012613167762756347\n",
      "-0.0012613179683685302\n",
      "-0.0012613195896148681\n",
      "-0.0012613192081451416\n",
      "-0.0012612693945566814\n",
      "-0.0012608301083246868\n",
      "-0.0012609780311584472\n",
      "-0.001261181624730428\n",
      "-0.0012612664063771566\n",
      "-0.0012612987200419108\n",
      "-0.0012613206466039022\n",
      "-0.0012613197167714437\n",
      "-0.0012613166650136311\n",
      "-0.0012613261540730795\n",
      "-0.0012613089323043824\n",
      "-0.00126105268796285\n",
      "-0.0012608177185058594\n",
      "-0.0012608197291692099\n",
      "-0.001261142635345459\n",
      "-0.0012612597306569417\n",
      "-0.0012612966458002727\n",
      "-0.0012613125244776407\n",
      "-0.0012613245010375976\n",
      "-0.0012613141298294067\n",
      "-0.0012613336086273192\n",
      "-0.0012613199472427368\n",
      "-0.0012612945238749185\n",
      "-0.0012610402981440227\n",
      "-0.0012606979131698608\n",
      "-0.0012611339886983235\n",
      "-0.001261245369911194\n",
      "-0.001261298712094625\n",
      "-0.001261314598719279\n",
      "-0.0012613248348236083\n",
      "-0.0012613251606623333\n",
      "-0.001261313525835673\n",
      "-0.001261167844136556\n",
      "-0.001261277969678243\n",
      "-0.0012613056659698486\n",
      "-0.0012611489057540893\n",
      "-0.0012602968613306682\n",
      "-0.0012610541264216105\n",
      "-0.0012612058480580648\n",
      "-0.001261286481221517\n",
      "-0.0012613120079040527\n",
      "-0.0012613216241200765\n",
      "-0.0012613183418909708\n",
      "-0.0012613210519154866\n",
      "-0.001261323618888855\n",
      "-0.0012613301992416381\n",
      "-0.0012613161325454713\n",
      "-0.00126121776898702\n",
      "-0.0012611959139506022\n",
      "-0.0012612770795822144\n",
      "-0.0012611709594726563\n",
      "-0.0012603837887446086\n",
      "-0.0012610944112141926\n",
      "-0.0012612293243408203\n",
      "-0.0012612843672434489\n",
      "-0.001261303424835205\n",
      "-0.0012613229354222616\n",
      "-0.0012613232056299846\n",
      "-0.0012613227287928263\n",
      "-0.001261313001314799\n",
      "-0.0012613286972045898\n",
      "-0.0012613230069478353\n",
      "-0.001261319080988566\n",
      "-0.0012613159815470377\n",
      "-0.0012611045281092326\n",
      "-0.0012602369546890258\n",
      "-0.0012608607133229573\n",
      "-0.0012611478408177694\n",
      "-0.001261286703745524\n",
      "-0.001261320169766744\n",
      "-0.0012613176981608072\n",
      "-0.0012613199234008789\n",
      "-0.001261327862739563\n",
      "-0.0012613173087437947\n",
      "-0.0012613193114598593\n",
      "-0.0012613189776738486\n",
      "-0.0012613282203674317\n",
      "-0.0012613220453262328\n",
      "-0.0012612034797668456\n",
      "-0.0012611680348714193\n",
      "-0.0012611766815185546\n",
      "-0.0012604245901107788\n",
      "-0.0012610684951146443\n",
      "-0.0012612239519755046\n",
      "-0.0012612775087356568\n",
      "-0.001261314304669698\n",
      "-0.0012613279183705647\n",
      "-0.0012613306204477947\n",
      "-0.0012613358577092488\n",
      "-0.0012613209486007691\n",
      "-0.0012613285382588703\n",
      "-0.0012613206704457601\n",
      "-0.0012613174517949423\n",
      "-0.0012611764272054036\n",
      "-0.001260463237762451\n",
      "-0.0012610960960388184\n",
      "-0.001261225660641988\n",
      "-0.0012612770318984985\n",
      "-0.0012612695614496866\n",
      "-0.0012612903833389283\n",
      "-0.0012613186597824097\n",
      "-0.0012613098700841269\n",
      "-0.001261326257387797\n",
      "-0.001261326559384664\n",
      "-0.0012613152186075846\n",
      "-0.0012612886826197307\n",
      "-0.0012609792470932007\n",
      "-0.0012605377833048502\n",
      "-0.0012611210982004801\n",
      "-0.0012612361192703247\n",
      "-0.001261291790008545\n",
      "-0.0012613122781117757\n",
      "-0.001261322553952535\n",
      "-0.0012613202651341757\n",
      "-0.0012613237380981446\n",
      "-0.001261321481068929\n",
      "-0.0012613207976023355\n",
      "-0.0012613192319869996\n",
      "-0.0012611461639404297\n",
      "-0.0012603321234385173\n",
      "-0.001261073366800944\n",
      "-0.0012612118323644002\n",
      "-0.0012612897793451946\n",
      "-0.0012613197088241577\n",
      "-0.001261247706413269\n",
      "-0.001261211895942688\n",
      "-0.0012612914164861044\n",
      "-0.001261320940653483\n",
      "-0.0012613259394963583\n",
      "-0.0012613221565882366\n",
      "-0.001261324707667033\n",
      "-0.0012613243420918783\n",
      "-0.0012613149722417196\n",
      "-0.0012608768781026205\n",
      "-0.0012606311003367106\n",
      "-0.0012610900322596232\n",
      "-0.001261250368754069\n",
      "-0.0012613019704818726\n",
      "-0.0012613235155741374\n",
      "-0.0012613016287485758\n",
      "-0.001261097224553426\n",
      "-0.0012612611691157024\n",
      "-0.0012612950960795084\n",
      "-0.0012613292217254638\n",
      "-0.0012613242944081624\n",
      "-0.0012613203128178914\n",
      "-0.00126131378809611\n",
      "-0.0012612669626871744\n",
      "-0.0012601999521255492\n",
      "-0.0012608985344568888\n",
      "-0.0012611814498901368\n",
      "-0.0012612512111663818\n",
      "-0.0012613130569458007\n",
      "-0.0012613183180491129\n",
      "-0.0012613288402557373\n",
      "-0.0012613271713256837\n",
      "-0.0012613226811091106\n",
      "-0.001261327290534973\n",
      "-0.0012613226175308228\n",
      "-0.001261313796043396\n",
      "-0.0012611087242762249\n",
      "-0.0012612664937973023\n",
      "-0.001261294388771057\n",
      "-0.0012613090674082439\n",
      "-0.0012611058712005616\n",
      "-0.0012604994058609009\n",
      "-0.0012610960642496746\n",
      "-0.0012612273375193278\n",
      "-0.00126128888130188\n",
      "-0.001261315631866455\n",
      "-0.0012613199392954508\n",
      "-0.0012613242387771607\n",
      "-0.0012613176822662353\n",
      "-0.0012613229274749756\n",
      "-0.0012613176345825194\n",
      "-0.001261320169766744\n",
      "-0.001261316482226054\n",
      "-0.001261189874013265\n",
      "-0.0012603137254714966\n",
      "-0.001260963773727417\n",
      "-0.001261187752087911\n",
      "-0.0012612744092941285\n",
      "-0.0012613178412119548\n",
      "-0.0012613175630569458\n",
      "-0.0012613145192464192\n",
      "-0.0012613275210062664\n",
      "-0.0012613257487614949\n",
      "-0.00126133021513621\n",
      "-0.0012612564007441203\n",
      "-0.001261213239034017\n",
      "-0.0012612906217575072\n",
      "-0.0012613032023111978\n",
      "-0.0012610125462214153\n",
      "-0.0012605468273162842\n",
      "-0.0012610899845759073\n",
      "-0.0012612325191497802\n",
      "-0.0012612947225570678\n",
      "-0.0012613205989201865\n",
      "-0.0012613251765569052\n",
      "-0.0012613222440083821\n",
      "-0.001261325216293335\n",
      "-0.0012613249778747559\n",
      "-0.0012613275766372681\n",
      "-0.0012612377325693766\n",
      "-0.001261176856358846\n",
      "-0.0012610538323720296\n",
      "-0.0012607619603474936\n",
      "-0.0012611763159434\n",
      "-0.0012612608909606934\n",
      "-0.0012613046566645305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012613115549087524\n",
      "-0.0012613178888956705\n",
      "-0.0012613197882970174\n",
      "-0.0012613301436106364\n",
      "-0.0012613109668095906\n",
      "-0.0012611867348353068\n",
      "-0.0012606117645899454\n",
      "-0.0012611302216847737\n",
      "-0.0012612404664357503\n",
      "-0.0012612911144892374\n",
      "-0.001261311133702596\n",
      "-0.0012612740993499757\n",
      "-0.0012611730972925822\n",
      "-0.0012612815539042154\n",
      "-0.001261314328511556\n",
      "-0.0012613221486409505\n",
      "-0.0012613202889760334\n",
      "-0.001261206038792928\n",
      "-0.0012603111743927002\n",
      "-0.0012610543966293335\n",
      "-0.0012612123886744182\n",
      "-0.00126127982934316\n",
      "-0.0012613084236780803\n",
      "-0.0012613203525543213\n",
      "-0.0012613226334253947\n",
      "-0.0012613213698069255\n",
      "-0.0012613263765970865\n",
      "-0.0012612011273701986\n",
      "-0.0012612414280573528\n",
      "-0.0012613024870554606\n",
      "-0.001261321512858073\n",
      "-0.0012613263686498007\n",
      "-0.0012611411809921264\n",
      "-0.001260328205426534\n",
      "-0.0012610622485478718\n",
      "-0.0012612097342809042\n",
      "-0.0012612936894098917\n",
      "-0.0012613146543502807\n",
      "-0.0012613255500793458\n",
      "-0.0012613184611002604\n",
      "-0.0012613203128178914\n",
      "-0.001261324683825175\n",
      "-0.001261290454864502\n",
      "-0.0012611475547154745\n",
      "-0.001261277993520101\n",
      "-0.0012613098859786988\n",
      "-0.0012613232771555582\n",
      "-0.0012612225532531737\n",
      "-0.00126033562819163\n",
      "-0.0012610477685928345\n",
      "-0.0012612157742182414\n",
      "-0.0012612772305806478\n",
      "-0.0012613097508748373\n",
      "-0.001261320424079895\n",
      "-0.0012613250176111857\n",
      "-0.0012613142172495525\n",
      "-0.0012613191843032837\n",
      "-0.0012613221168518067\n",
      "-0.0012613197326660156\n",
      "-0.0012613228638966877\n",
      "-0.001261320988337199\n",
      "-0.0012610072294871012\n",
      "-0.0012607116142908732\n",
      "-0.0012608066082000732\n",
      "-0.0012611462195714314\n",
      "-0.001261266819636027\n",
      "-0.0012613047202428183\n",
      "-0.0012613158226013183\n",
      "-0.001261322832107544\n",
      "-0.0012613246520360312\n",
      "-0.001261327870686849\n",
      "-0.0012613244533538819\n",
      "-0.0012613253911336263\n",
      "-0.0012613219976425171\n",
      "-0.0012612924655278524\n",
      "-0.0012602818489074706\n",
      "-0.0012606499036153158\n",
      "-0.0012611653884251912\n",
      "-0.0012612408876419067\n",
      "-0.0012613101641337078\n",
      "-0.0012613231182098388\n",
      "-0.0012613187074661254\n",
      "-0.0012613210757573446\n",
      "-0.0012613144954045613\n",
      "-0.0012613258441289267\n",
      "-0.0012613130728403726\n",
      "-0.0012613259236017864\n",
      "-0.00126132386525472\n",
      "-0.0012613281806310018\n",
      "-0.0012610925912857056\n",
      "-0.0012611789226531982\n",
      "-0.0012612481594085694\n",
      "-0.001261134632428487\n",
      "-0.001260836927096049\n",
      "-0.0012612290382385254\n",
      "-0.0012612605174382528\n",
      "-0.0012612776835759481\n",
      "-0.0012613078673680623\n",
      "-0.0012613084475199383\n",
      "-0.0012613244533538819\n",
      "-0.0012613232930501301\n",
      "-0.0012612860679626465\n",
      "-0.0012605552752812704\n",
      "-0.0012608527501424154\n",
      "-0.0012611589749654134\n",
      "-0.0012612774848937988\n",
      "-0.0012613095919291179\n",
      "-0.001261322514216105\n",
      "-0.0012613189935684205\n",
      "-0.0012613251765569052\n",
      "-0.0012613176107406617\n",
      "-0.0012613217989603678\n",
      "-0.0012613210519154866\n",
      "-0.0012613147417704265\n",
      "-0.0012611090103785197\n",
      "-0.0012612390200297037\n",
      "-0.001261008644104004\n",
      "-0.001260491140683492\n",
      "-0.001261082649230957\n",
      "-0.0012612279256184896\n",
      "-0.001261297313372294\n",
      "-0.0012613168001174926\n",
      "-0.001261321004231771\n",
      "-0.0012613207817077636\n",
      "-0.0012613283554712932\n",
      "-0.0012613203525543213\n",
      "-0.0012613207976023355\n",
      "-0.0012613209009170532\n",
      "-0.0012613183577855427\n",
      "-0.001261247722307841\n",
      "-0.0012605173190434775\n",
      "-0.001261029815673828\n",
      "-0.0012612104495366414\n",
      "-0.0012612765789031981\n",
      "-0.0012613076368967692\n",
      "-0.0012613190015157063\n",
      "-0.001261248747507731\n",
      "-0.0012611456314722696\n",
      "-0.0012612709045410155\n",
      "-0.0012613140106201171\n",
      "-0.001261324119567871\n",
      "-0.0012613200505574545\n",
      "-0.0012612550258636476\n",
      "-0.0012605900128682455\n",
      "-0.00126101553440094\n",
      "-0.0012612033367156983\n",
      "-0.0012612825155258179\n",
      "-0.0012613115708033243\n",
      "-0.0012613117774327596\n",
      "-0.0012613194386164347\n",
      "-0.0012613213141759237\n",
      "-0.0012613232533137003\n",
      "-0.0012613294124603272\n",
      "-0.0012613191207249959\n",
      "-0.0012613201459248861\n",
      "-0.0012613079388936362\n",
      "-0.001260584020614624\n",
      "-0.0012601842482884726\n",
      "-0.0012610503991444906\n",
      "-0.0012612093925476074\n",
      "-0.0012613014936447143\n",
      "-0.0012613154411315918\n",
      "-0.001261323626836141\n",
      "-0.0012613204638163248\n",
      "-0.0012613196690877278\n",
      "-0.0012613240242004395\n",
      "-0.001261328673362732\n",
      "-0.0012613230307896932\n",
      "-0.001261324954032898\n",
      "-0.0012613298813501995\n",
      "-0.0012613165616989135\n",
      "-0.0012610743522644043\n",
      "-0.001261271095275879\n",
      "-0.0012612841208775839\n",
      "-0.0012612137556076049\n",
      "-0.0012606857299804688\n",
      "-0.0012611307859420775\n",
      "-0.0012612337509791056\n",
      "-0.0012612856944402059\n",
      "-0.0012613040129343669\n",
      "-0.0012613086064656575\n",
      "-0.0012613269805908202\n",
      "-0.001261316442489624\n",
      "-0.0012613210439682008\n",
      "-0.0012613253196080525\n",
      "-0.0012612792094548544\n",
      "-0.0012604904174804688\n",
      "-0.001260892669359843\n",
      "-0.0012611817836761475\n",
      "-0.001261283278465271\n",
      "-0.0012613123496373493\n",
      "-0.0012613205512364706\n",
      "-0.0012612137953440347\n",
      "-0.0012612213055292764\n",
      "-0.0012612982114156087\n",
      "-0.0012613155841827393\n",
      "-0.0012613194624582927\n",
      "-0.001261317213376363\n",
      "-0.0012613283077875773\n",
      "-0.0012613288482030234\n",
      "-0.0012611936887105305\n",
      "-0.0012603456656138101\n",
      "-0.0012610815048217773\n",
      "-0.0012612231334050497\n",
      "-0.001261280083656311\n",
      "-0.0012613083680470785\n",
      "-0.0012613196849822997\n",
      "-0.0012613305966059367\n",
      "-0.0012612812995910644\n",
      "-0.0012611531496047974\n",
      "-0.0012612762928009033\n",
      "-0.0012613068103790282\n",
      "-0.0012613163073857626\n",
      "-0.0012613242149353027\n",
      "-0.0012612760305404664\n",
      "-0.00126064403851827\n",
      "-0.0012609466632207236\n",
      "-0.0012611757357915243\n",
      "-0.0012612749973932903\n",
      "-0.0012613114595413208\n",
      "-0.0012613151629765828\n",
      "-0.001261326766014099\n",
      "-0.001261316752433777\n",
      "-0.001261321218808492\n",
      "-0.0012613178173700968\n",
      "-0.0012613043705622356\n",
      "-0.0012611025015513103\n",
      "-0.0012612601200739542\n",
      "-0.00126109885374705\n",
      "-0.0012603156566619873\n",
      "-0.0012610490163167318\n",
      "-0.0012612010637919108\n",
      "-0.0012612966140111287\n",
      "-0.0012613152424494425\n",
      "-0.0012613243818283082\n",
      "-0.001261320439974467\n",
      "-0.0012613250970840454\n",
      "-0.0012613191922505697\n",
      "-0.0012613242705663045\n",
      "-0.0012613229036331176\n",
      "-0.0012613165299097697\n",
      "-0.0012613228638966877\n",
      "-0.0012613175392150878\n",
      "-0.0012611344416936239\n",
      "-0.0012603599230448404\n",
      "-0.0012610051552454631\n",
      "-0.0012611894766489664\n",
      "-0.001261278820037842\n",
      "-0.001261322585741679\n",
      "-0.0012613168160120645\n",
      "-0.0012613142410914102\n",
      "-0.0012613162676493328\n",
      "-0.001261320153872172\n",
      "-0.0012612229665120442\n",
      "-0.0012612165609995525\n",
      "-0.0012612879594167074\n",
      "-0.0012613154172897338\n",
      "-0.0012613182703653972\n",
      "-0.0012610785722732543\n",
      "-0.0012604916334152223\n",
      "-0.0012610872824986776\n",
      "-0.0012612241586049398\n",
      "-0.0012612949053446452\n",
      "-0.001261316164334615\n",
      "-0.0012613253275553385\n",
      "-0.0012613208134969077\n",
      "-0.0012613250176111857\n",
      "-0.001261323841412862\n",
      "-0.0012613207260767619\n",
      "-0.00126125172773997\n",
      "-0.001261180289586385\n",
      "-0.00126126123269399\n",
      "-0.0012610114812850951\n",
      "-0.0012606281518936157\n",
      "-0.0012611112435658772\n",
      "-0.0012612489541371664\n",
      "-0.0012612948815027873\n",
      "-0.0012613190571467083\n",
      "-0.0012613269726435344\n",
      "-0.0012613203684488932\n",
      "-0.0012613247553507486\n",
      "-0.0012613235791524252\n",
      "-0.0012613240003585815\n",
      "-0.0012613200028737386\n",
      "-0.0012613207658131917\n",
      "-0.0012612020015716552\n",
      "-0.0012601011594136557\n",
      "-0.0012609984318415323\n",
      "-0.0012611258506774901\n",
      "-0.0012611323833465577\n",
      "-0.0012612692912419637\n",
      "-0.0012613114356994628\n",
      "-0.0012613183895746867\n",
      "-0.0012613252480824788\n",
      "-0.0012613292773564658\n",
      "-0.0012613173166910808\n",
      "-0.001261322029431661\n",
      "-0.0012613218943277995\n",
      "-0.0012613210995992025\n",
      "-0.0012613258361816406\n",
      "-0.0012613239526748658\n",
      "-0.0012613228638966877\n",
      "-0.0012612576087315877\n",
      "-0.001260405135154724\n",
      "-0.0012608046293258667\n",
      "-0.0012611548821131388\n",
      "-0.0012612714926401774\n",
      "-0.0012613193988800049\n",
      "-0.001261327568689982\n",
      "-0.0012613269329071045\n",
      "-0.0012613242149353027\n",
      "-0.001261320161819458\n",
      "-0.001261321989695231\n",
      "-0.0012613276878992716\n",
      "-0.0012613189776738486\n",
      "-0.0012612754265467327\n",
      "-0.0012605955839157105\n",
      "-0.0012609666347503663\n",
      "-0.0012611916303634644\n",
      "-0.001261283818880717\n",
      "-0.001261312222480774\n",
      "-0.0012613253037134806\n",
      "-0.0012613221009572346\n",
      "-0.0012613269011179605\n",
      "-0.0012613256216049194\n",
      "-0.0012613187948862712\n",
      "-0.0012613168319066366\n",
      "-0.0012612082878748576\n",
      "-0.0012611006339391072\n",
      "-0.001260807474454244\n",
      "-0.0012608107089996338\n",
      "-0.0012611411492029826\n",
      "-0.0012612678289413452\n",
      "-0.0012612968921661377\n",
      "-0.0012613157192866008\n",
      "-0.0012613202889760334\n",
      "-0.0012613205989201865\n",
      "-0.0012613187313079833\n",
      "-0.0012613263289133708\n",
      "-0.0012613229513168335\n",
      "-0.001261322808265686\n",
      "-0.0012613168319066366\n",
      "-0.0012609913190205892\n",
      "-0.0012601388772328695\n",
      "-0.0012609148025512695\n",
      "-0.0012612526655197145\n",
      "-0.001261275299390157\n",
      "-0.0012613158941268921\n",
      "-0.0012613192001978556\n",
      "-0.001261323912938436\n",
      "-0.0012613144238789876\n",
      "-0.001261199164390564\n",
      "-0.0012612857898076375\n",
      "-0.0012613099813461304\n",
      "-0.0012613208214441935\n",
      "-0.0012613230307896932\n",
      "-0.0012613200585047404\n",
      "-0.0012613238175710041\n",
      "-0.0012613228956858318\n",
      "-0.001261320455869039\n",
      "-0.001261293339729309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.001260442344347636\n",
      "-0.001260753830273946\n",
      "-0.0012611632347106934\n",
      "-0.001261281402905782\n",
      "-0.0012612974484761555\n",
      "-0.001261326281229655\n",
      "-0.0012613248348236083\n",
      "-0.0012613245884577433\n",
      "-0.0012612071752548218\n",
      "-0.001261242135365804\n",
      "-0.0012613020817438762\n",
      "-0.0012613224267959595\n",
      "-0.0012613162358601888\n",
      "-0.001261323356628418\n",
      "-0.0012613229433695475\n",
      "-0.0012613219817479452\n",
      "-0.0012612706661224366\n",
      "-0.0012598430554072062\n",
      "-0.0012608863512674968\n",
      "-0.001261100713411967\n",
      "-0.0012612737735112507\n",
      "-0.0012613013505935668\n",
      "-0.0012613118251164753\n",
      "-0.0012613148848215738\n",
      "-0.0012613292773564658\n",
      "-0.0012612456639607748\n",
      "-0.0012612209558486938\n",
      "-0.0012612945079803466\n",
      "-0.0012613229115804037\n",
      "-0.0012613253752390544\n",
      "-0.0012613206307093303\n",
      "-0.0012613213141759237\n",
      "-0.0012613224744796752\n",
      "-0.0012613206386566161\n",
      "-0.001261322037378947\n",
      "-0.001261323912938436\n",
      "-0.0012612871328989664\n",
      "-0.0012605288902918497\n",
      "-0.0012606920878092448\n",
      "-0.0012611257155736287\n",
      "-0.0012612743616104126\n",
      "-0.0012612953344980876\n",
      "-0.001261312222480774\n",
      "-0.0012613214731216431\n",
      "-0.0012613221883773804\n",
      "-0.0012613300005594888\n",
      "-0.001261310052871704\n",
      "-0.0012611862421035766\n",
      "-0.001261276650428772\n",
      "-0.0012613161404927571\n",
      "-0.001261322585741679\n",
      "-0.0012613185962041219\n",
      "-0.0012611345688501993\n",
      "-0.0012603933890660605\n",
      "-0.0012610805749893189\n",
      "-0.0012612162510553997\n",
      "-0.0012612829367319742\n",
      "-0.001261319931348165\n",
      "-0.0012613257090250652\n",
      "-0.0012613232851028443\n",
      "-0.0012613152901331584\n",
      "-0.0012611818631490072\n",
      "-0.0012612460851669312\n",
      "-0.0012612996260325114\n",
      "-0.0012613159418106078\n",
      "-0.0012613191445668538\n",
      "-0.0012612894455591838\n",
      "-0.0012607750097910562\n",
      "-0.0012608795881271363\n",
      "-0.0012611666520436606\n",
      "-0.0012612655798594156\n",
      "-0.0012613126834233601\n",
      "-0.0012613194465637208\n",
      "-0.001261322291692098\n",
      "-0.0012613246997197469\n",
      "-0.001261328935623169\n",
      "-0.0012613241751988729\n",
      "-0.0012612958510716755\n",
      "-0.0012611141284306843\n",
      "-0.0012610350052515665\n",
      "-0.0012606101751327515\n",
      "-0.0012611313978830973\n",
      "-0.0012612483501434327\n",
      "-0.0012612926403681437\n",
      "-0.0012613234519958496\n",
      "-0.0012613259394963583\n",
      "-0.0012613268772761028\n",
      "-0.0012613254229227701\n",
      "-0.001261326519648234\n",
      "-0.001261322808265686\n",
      "-0.0012613279740015665\n",
      "-0.0012613086700439453\n",
      "-0.0012606703281402589\n",
      "-0.0012605348507563274\n",
      "-0.0012611003081003825\n",
      "-0.0012612650712331135\n",
      "-0.0012612952868143717\n",
      "-0.0012613166014353433\n",
      "-0.001261319335301717\n",
      "-0.0012612444400787353\n",
      "-0.0012612236420313518\n",
      "-0.0012612846851348877\n",
      "-0.0012613218545913696\n",
      "-0.0012613223393758137\n",
      "-0.0012613260746002198\n",
      "-0.0012613220453262328\n",
      "-0.001261325510342916\n",
      "-0.00126131378809611\n",
      "-0.0012612666447957357\n",
      "-0.0012601910750071209\n",
      "-0.001260896857579549\n",
      "-0.0012611805518468221\n",
      "-0.001261243971188863\n",
      "-0.00126131645043691\n",
      "-0.0012613202412923177\n",
      "-0.0012612828969955445\n",
      "-0.0012612746000289917\n",
      "-0.0012613192796707153\n",
      "-0.001261313549677531\n",
      "-0.0012613224824269613\n",
      "-0.0012613220771153769\n",
      "-0.0012613200187683105\n",
      "-0.0012612565358479818\n",
      "-0.0012612913370132447\n",
      "-0.0012613075971603393\n",
      "-0.0012612666447957357\n",
      "-0.0012604151328404745\n",
      "-0.0012609641710917155\n",
      "-0.0012612016598383586\n",
      "-0.0012612819115320841\n",
      "-0.0012613046169281006\n",
      "-0.0012613202730814615\n",
      "-0.0012612760861714682\n",
      "-0.0012612305402755737\n",
      "-0.0012612863938013712\n",
      "-0.0012613149404525758\n",
      "-0.001261325494448344\n",
      "-0.0012613260904947917\n",
      "-0.0012613213618596394\n",
      "-0.0012613195896148681\n",
      "-0.0012612928708394368\n",
      "-0.001260324764251709\n",
      "-0.0012607168594996135\n",
      "-0.0012611710468928019\n",
      "-0.0012612450838088989\n",
      "-0.0012613070011138917\n",
      "-0.0012612635691960652\n",
      "-0.0012612590948740642\n",
      "-0.0012613069693247477\n",
      "-0.001261319406827291\n",
      "-0.0012613245010375976\n",
      "-0.0012613192081451416\n",
      "-0.0012613171180089315\n",
      "-0.0012613287210464477\n",
      "-0.0012613238334655762\n",
      "-0.001261273725827535\n",
      "-0.0012611945708592733\n",
      "-0.0012612120628356933\n",
      "-0.0012607607920964558\n",
      "-0.0012611452182133991\n",
      "-0.0012612404266993205\n",
      "-0.0012612721602121989\n",
      "-0.0012612976789474488\n",
      "-0.0012613145351409911\n",
      "-0.0012613128900527955\n",
      "-0.0012613278071085612\n",
      "-0.0012613276799519856\n",
      "-0.0012612526098887125\n",
      "-0.0012606632312138876\n",
      "-0.0012610536257425945\n",
      "-0.0012612044731775919\n",
      "-0.0012612788915634156\n",
      "-0.0012612951358159383\n",
      "-0.0012613187154134114\n",
      "-0.0012613173405329387\n",
      "-0.001261249876022339\n",
      "-0.0012611898104349772\n",
      "-0.0012612611929575602\n",
      "-0.0012612619717915852\n",
      "-0.0012610718568166097\n",
      "-0.0012609694321950276\n",
      "-0.0012612276156743368\n",
      "-0.001261291495958964\n",
      "-0.0012612828731536866\n",
      "-0.0012613180239995322\n",
      "-0.0012613112052281697\n",
      "-0.0012613166332244873\n",
      "-0.0012612475633621216\n",
      "-0.0012607515573501587\n",
      "-0.001261107850074768\n",
      "-0.001261211371421814\n",
      "-0.0012612782955169677\n",
      "-0.0012612991333007812\n",
      "-0.0012613123019536336\n",
      "-0.0012611993312835693\n",
      "-0.0012612718264261881\n",
      "-0.0012613041162490846\n",
      "-0.0012613148689270019\n",
      "-0.0012612237056096394\n",
      "-0.001260564136505127\n",
      "-0.001261093513170878\n",
      "-0.0012612189372380575\n",
      "-0.0012612868626912434\n",
      "-0.0012613149483998616\n",
      "-0.001261323062578837\n",
      "-0.0012613195021947226\n",
      "-0.001261317459742228\n",
      "-0.001261323634783427\n",
      "-0.0012613176743189493\n",
      "-0.0012610823472340902\n",
      "-0.0012612067858378093\n",
      "-0.0012608342011769613\n",
      "-0.0012609096686045328\n",
      "-0.0012611694653828938\n",
      "-0.0012612602551778157\n",
      "-0.0012613009452819824\n",
      "-0.0012613186995188395\n",
      "-0.0012613212505976359\n",
      "-0.0012613193909327188\n",
      "-0.001261321210861206\n",
      "-0.0012613231261571249\n",
      "-0.0012612899700800578\n",
      "-0.0012609596967697143\n",
      "-0.0012609078168869018\n",
      "-0.0012611738602320353\n",
      "-0.0012612588167190551\n",
      "-0.0012612974882125854\n",
      "-0.0012613226493199666\n",
      "-0.0012613175948460898\n",
      "-0.0012613213698069255\n",
      "-0.0012613210916519165\n",
      "-0.0012613133986790976\n",
      "-0.0012612091779708862\n",
      "-0.0012610668182373047\n",
      "-0.0012604545831680298\n",
      "-0.001261074701944987\n",
      "-0.001261229109764099\n",
      "-0.0012613034327824911\n",
      "-0.0012613272984822591\n",
      "-0.0012613267421722413\n",
      "-0.0012613237460454305\n",
      "-0.0012613213777542115\n",
      "-0.0012613190253575643\n",
      "-0.0012613271236419678\n",
      "-0.0012613198359807333\n",
      "-0.0012613282521565755\n",
      "-0.001261316180229187\n",
      "-0.0012607956329981486\n",
      "-0.0012605051040649414\n",
      "-0.0012610612074534099\n",
      "-0.0012612421035766601\n",
      "-0.0012613086382548015\n",
      "-0.0012613272269566855\n",
      "-0.0012613241990407308\n",
      "-0.001261323618888855\n",
      "-0.0012613306681315104\n",
      "-0.001261320734024048\n",
      "-0.001261328069368998\n",
      "-0.001261324691772461\n",
      "-0.0012613204956054688\n",
      "-0.0012613322099049886\n",
      "-0.001261324151357015\n",
      "-0.0012612999439239503\n",
      "-0.0012597585995992025\n",
      "-0.001260661021868388\n",
      "-0.0012610248406728108\n",
      "-0.0012612358490626018\n",
      "-0.001261306071281433\n",
      "-0.0012613099813461304\n",
      "-0.001261299459139506\n",
      "-0.0012611888408660889\n",
      "-0.0012612768491109213\n",
      "-0.0012613195021947226\n",
      "-0.0012613299051920572\n",
      "-0.0012613187154134114\n",
      "-0.0012613259553909302\n",
      "-0.0012613194624582927\n",
      "-0.001261326789855957\n",
      "-0.001261322816212972\n",
      "-0.0012613271156946817\n",
      "-0.001261328673362732\n",
      "-0.0012613030513127644\n",
      "-0.0012611523787180582\n",
      "-0.0012612625042597453\n",
      "-0.0012613068342208862\n",
      "-0.001261312429110209\n",
      "-0.0012611104647318521\n",
      "-0.0012605823596318563\n",
      "-0.0012611243724822997\n",
      "-0.0012612450838088989\n",
      "-0.0012612924098968506\n",
      "-0.0012613177935282389\n",
      "-0.001261319859822591\n",
      "-0.0012613242626190186\n",
      "-0.001261319406827291\n",
      "-0.0012613319873809815\n",
      "-0.0012613162914911905\n",
      "-0.001261321465174357\n",
      "-0.0012612359762191772\n",
      "-0.0012604523181915283\n",
      "-0.001260988958676656\n",
      "-0.0012611857573191325\n",
      "-0.001261285146077474\n",
      "-0.001261314312616984\n",
      "-0.0012613208134969077\n",
      "-0.001261320980389913\n",
      "-0.0012613280296325685\n",
      "-0.0012613271554311116\n",
      "-0.0012613219658533731\n",
      "-0.0012613230148951213\n",
      "-0.0012613247791926066\n",
      "-0.0012613178968429565\n",
      "-0.0012606844981511434\n",
      "-0.001260339339574178\n",
      "-0.0012610865831375123\n",
      "-0.0012612430413564046\n",
      "-0.001261294158299764\n",
      "-0.001261300746599833\n",
      "-0.0012612277348836264\n",
      "-0.0012613036473592123\n",
      "-0.0012613149881362915\n",
      "-0.0012613211393356324\n",
      "-0.0012613253037134806\n",
      "-0.0012613205115000407\n",
      "-0.0012613197724024455\n",
      "-0.0012613200108210247\n",
      "-0.0012613179604212444\n",
      "-0.0012612199465433756\n",
      "-0.0012612725178400676\n",
      "-0.001261242914199829\n",
      "-0.0012609443346659342\n",
      "-0.0012610602140426636\n",
      "-0.001261200475692749\n",
      "-0.0012612784465154013\n",
      "-0.0012613147258758546\n",
      "-0.001261307692527771\n",
      "-0.0012613253593444823\n",
      "-0.0012613224347432454\n",
      "-0.0012613253196080525\n",
      "-0.0012612136363983155\n",
      "-0.001260444688796997\n",
      "-0.0012609249114990235\n",
      "-0.0012611775239308675\n",
      "-0.0012612792332967122\n",
      "-0.0012613088528315227\n",
      "-0.0012613195021947226\n",
      "-0.001261325184504191\n",
      "-0.001261327886581421\n",
      "-0.0012613223155339558\n",
      "-0.0012613196929295858\n",
      "-0.0012613282283147175\n",
      "-0.0012613236904144287\n",
      "-0.0012613112529118856\n",
      "-0.0012610198259353637\n",
      "-0.0012603672822316487\n",
      "-0.0012610026915868123\n",
      "-0.0012612266381581624\n",
      "-0.0012613053878148396\n",
      "-0.0012613091786702474\n",
      "-0.001261331836382548\n",
      "-0.0012612896919250488\n",
      "-0.001261166254679362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0012612836440404256\n",
      "-0.0012613191445668538\n",
      "-0.0012613165855407714\n",
      "-0.0012613198757171632\n",
      "-0.001261322331428528\n",
      "-0.0012613214095433553\n",
      "-0.0012613236506779989\n",
      "-0.0012613121668497722\n",
      "-0.001260781208674113\n",
      "-0.001260355814297994\n",
      "-0.0012610487143198648\n",
      "-0.0012612608353296916\n",
      "-0.001261295509338379\n",
      "-0.0012613194783528646\n",
      "-0.0012613212664922078\n",
      "-0.0012612931648890177\n",
      "-0.001261217427253723\n",
      "-0.0012612971544265748\n"
     ]
    }
   ],
   "source": [
    "nb_of_epochs=4000\n",
    "batch_size=int(x_variable.size(0)/10)\n",
    "\n",
    "for epoch in range(nb_of_epochs):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "\n",
    "    for b in range(0,x_variable.size(0),batch_size):\n",
    "        recon_batch,mu,logvar = model(x_variable.narrow(0,b,batch_size))                \n",
    "        loss = loss_function(recon_batch, y_variable.narrow(0,b,batch_size),mu,logvar)     \n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "        train_loss+=loss.data[0]\n",
    "    print(train_loss/x_variable.size(0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VAE(\n",
       "   (fc1): Linear(in_features=24, out_features=14, bias=True)\n",
       "   (fc21): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc22): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc3): Linear(in_features=2, out_features=14, bias=True)\n",
       "   (fc4): Linear(in_features=14, out_features=24, bias=True)\n",
       " ), Variable containing:\n",
       " (  0  ,.,.) = \n",
       "   7.2590e-01  1.7160e-01  8.4208e-01  ...  -8.6179e-01  6.2929e-01 -2.9497e-01\n",
       " \n",
       " (  1  ,.,.) = \n",
       "   1.1056e+00  8.5052e-02  1.0595e+00  ...  -8.8101e-01  5.4964e-01 -4.5626e-01\n",
       " \n",
       " (  2  ,.,.) = \n",
       "   1.3043e+00 -1.6374e-01  7.1454e-01  ...  -1.0654e+00  3.5652e-01 -3.6640e-01\n",
       "  ...  \n",
       " \n",
       " (59997,.,.) = \n",
       "   1.0052e+00  1.3537e-01  7.4466e-01  ...  -8.4981e-01  6.0202e-01 -5.4357e-01\n",
       " \n",
       " (59998,.,.) = \n",
       "   7.9475e-01 -1.6893e-01  4.9571e-01  ...  -4.6446e-01  1.2147e+00 -5.7865e-01\n",
       " \n",
       " (59999,.,.) = \n",
       "   1.3487e+00  1.5864e-02  7.7996e-01  ...  -4.4117e-01  5.4940e-01 -2.1820e-01\n",
       " [torch.cuda.FloatTensor of size 60000x1x24 (GPU 0)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable=model.cuda(),x_variable.cuda()\n",
    "model.eval()\n",
    "model,x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1fa97f21dbca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_variable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolygon_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-14edde0d2c3a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparametrize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-14edde0d2c3a>\u001b[0m in \u001b[0;36mreparametrize\u001b[1;34m(self, mu, logvar)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of type Variable[torch.cuda.FloatTensor] but found type Variable[torch.FloatTensor] for argument #1 'other'"
     ]
    }
   ],
   "source": [
    "polygon_prediction=model(x_variable)\n",
    "polygon_prediction=polygon_prediction[14].data.cpu()\n",
    "polygon_prediction=polygon_prediction.numpy()\n",
    "polygon_prediction=polygon_prediction.reshape(12,2)\n",
    "\n",
    "plot_contour(polygon_prediction)\n",
    "plot_contour(polygons[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(VAE(\n",
       "   (fc1): Linear(in_features=24, out_features=14, bias=True)\n",
       "   (fc21): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc22): Linear(in_features=14, out_features=2, bias=True)\n",
       "   (fc3): Linear(in_features=2, out_features=14, bias=True)\n",
       "   (fc4): Linear(in_features=14, out_features=24, bias=True)\n",
       " ), Variable containing:\n",
       " (  0  ,.,.) = \n",
       "   7.2590e-01  1.7160e-01  8.4208e-01  ...  -8.6179e-01  6.2929e-01 -2.9497e-01\n",
       " \n",
       " (  1  ,.,.) = \n",
       "   1.1056e+00  8.5052e-02  1.0595e+00  ...  -8.8101e-01  5.4964e-01 -4.5626e-01\n",
       " \n",
       " (  2  ,.,.) = \n",
       "   1.3043e+00 -1.6374e-01  7.1454e-01  ...  -1.0654e+00  3.5652e-01 -3.6640e-01\n",
       "  ...  \n",
       " \n",
       " (59997,.,.) = \n",
       "   1.0052e+00  1.3537e-01  7.4466e-01  ...  -8.4981e-01  6.0202e-01 -5.4357e-01\n",
       " \n",
       " (59998,.,.) = \n",
       "   7.9475e-01 -1.6893e-01  4.9571e-01  ...  -4.6446e-01  1.2147e+00 -5.7865e-01\n",
       " \n",
       " (59999,.,.) = \n",
       "   1.3487e+00  1.5864e-02  7.7996e-01  ...  -4.4117e-01  5.4940e-01 -2.1820e-01\n",
       " [torch.FloatTensor of size 60000x1x24])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
